{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/10_algorithmes_genetiques/10_exercices.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('üì¶ Installation des packages...')\n",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn\n",
    "    print('‚úÖ Installation termin√©e !')\n",
    "else:\n",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 10 - Exercices : Algorithmes G√©n√©tiques\n",
    "\n",
    "Ce notebook contient des exercices pratiques pour consolider les concepts du Chapitre 10.\n",
    "\n",
    "**Instructions** :\n",
    "- Compl√©tez les cellules marqu√©es `# VOTRE CODE ICI`\n",
    "- Les solutions sont disponibles dans `10_exercices_solutions.ipynb`\n",
    "- N'h√©sitez pas √† consulter la documentation (NumPy, scikit-learn)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports n√©cessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine, load_breast_cancer, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Biblioth√®ques import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 1 : Algorithme G√©n√©tique de Base\n",
    "\n",
    "**Objectif** : Impl√©menter un AG pour optimiser la fonction de Rastrigin.\n",
    "\n",
    "### 1.1 Fonction de Rastrigin\n",
    "\n",
    "La fonction de Rastrigin est une fonction de benchmark classique pour tester les algorithmes d'optimisation :\n",
    "\n",
    "$$f(\\mathbf{x}) = 10n + \\sum_{i=1}^{n} \\left[x_i^2 - 10\\cos(2\\pi x_i)\\right]$$\n",
    "\n",
    "Minimum global : $f(0, 0, ..., 0) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impl√©mentez la fonction de Rastrigin\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "def rastrigin(x):\n",
    "    \"\"\"\n",
    "    Fonction de Rastrigin.\n",
    "    Args:\n",
    "        x: vecteur de dimension n (valeurs entre -5.12 et 5.12)\n",
    "    Returns:\n",
    "        Valeur de la fonction (√† MINIMISER)\n",
    "    \"\"\"\n",
    "    # TODO: Impl√©mentez la formule\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "x_test = np.array([0, 0])\n",
    "print(f\"Rastrigin([0, 0]) = {rastrigin(x_test):.6f} (attendu : 0.0)\")\n",
    "\n",
    "x_test2 = np.array([1, 1])\n",
    "print(f\"Rastrigin([1, 1]) = {rastrigin(x_test2):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualisation de la Fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisez la fonction de Rastrigin en 2D\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "x = np.linspace(-5.12, 5.12, 200)\n",
    "y = np.linspace(-5.12, 5.12, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# TODO: Calculez Z = rastrigin pour chaque point (X, Y)\n",
    "\n",
    "# TODO: Tracez un contour plot ou surface 3D\n",
    "plt.figure(figsize=(10, 8))\n",
    "# contourf, colorbar, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Impl√©mentation de l'AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impl√©mentez l'algorithme g√©n√©tique\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "class GeneticAlgorithm:\n",
    "    def __init__(self, fitness_func, dim, bounds, pop_size=50, mutation_rate=0.1, crossover_rate=0.8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fitness_func: Fonction √† MINIMISER\n",
    "            dim: Dimension du probl√®me\n",
    "            bounds: Tuple (min, max) pour les valeurs des g√®nes\n",
    "            pop_size: Taille de la population\n",
    "            mutation_rate: Probabilit√© de mutation\n",
    "            crossover_rate: Probabilit√© de crossover\n",
    "        \"\"\"\n",
    "        self.fitness_func = fitness_func\n",
    "        self.dim = dim\n",
    "        self.bounds = bounds\n",
    "        self.pop_size = pop_size\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.population = None\n",
    "        self.best_fitness_history = []\n",
    "        self.avg_fitness_history = []\n",
    "    \n",
    "    def initialize_population(self):\n",
    "        \"\"\"Initialise la population al√©atoirement.\"\"\"\n",
    "        # TODO: Cr√©ez une population de pop_size individus\n",
    "        # Chaque individu est un vecteur de dimension dim avec valeurs dans bounds\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, individual):\n",
    "        \"\"\"√âvalue un individu (fitness).\"\"\"\n",
    "        # TODO: Retournez fitness_func(individual)\n",
    "        pass\n",
    "    \n",
    "    def selection(self):\n",
    "        \"\"\"S√©lection par tournoi (taille 3).\"\"\"\n",
    "        # TODO: Choisissez 3 individus al√©atoires et retournez le meilleur\n",
    "        pass\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        \"\"\"Crossover uniforme.\"\"\"\n",
    "        if np.random.rand() > self.crossover_rate:\n",
    "            return parent1.copy(), parent2.copy()\n",
    "        \n",
    "        # TODO: Pour chaque g√®ne, choisissez al√©atoirement parent1 ou parent2\n",
    "        pass\n",
    "    \n",
    "    def mutation(self, individual):\n",
    "        \"\"\"Mutation gaussienne.\"\"\"\n",
    "        # TODO: Pour chaque g√®ne, avec probabilit√© mutation_rate,\n",
    "        # ajoutez un bruit gaussien (sigma=0.5) et clippez dans bounds\n",
    "        pass\n",
    "    \n",
    "    def evolve(self, n_generations=100):\n",
    "        \"\"\"Boucle principale de l'AG.\"\"\"\n",
    "        # TODO: Initialisez la population\n",
    "        \n",
    "        for gen in range(n_generations):\n",
    "            # TODO: √âvaluez la population\n",
    "            fitnesses = None  # [self.evaluate(ind) for ind in self.population]\n",
    "            \n",
    "            # TODO: Stockez les statistiques\n",
    "            best_fitness = None\n",
    "            avg_fitness = None\n",
    "            self.best_fitness_history.append(best_fitness)\n",
    "            self.avg_fitness_history.append(avg_fitness)\n",
    "            \n",
    "            if (gen + 1) % 20 == 0:\n",
    "                print(f\"G√©n√©ration {gen+1}/{n_generations} | Best: {best_fitness:.6f} | Avg: {avg_fitness:.6f}\")\n",
    "            \n",
    "            # TODO: Cr√©ez la nouvelle g√©n√©ration\n",
    "            # 1. √âlitisme : gardez les 2 meilleurs\n",
    "            # 2. Pour le reste : s√©lection, crossover, mutation\n",
    "            pass\n",
    "        \n",
    "        # Retournez le meilleur individu\n",
    "        fitnesses = [self.evaluate(ind) for ind in self.population]\n",
    "        best_idx = np.argmin(fitnesses)\n",
    "        return self.population[best_idx], fitnesses[best_idx]\n",
    "\n",
    "print(\"‚úì Classe GeneticAlgorithm cr√©√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Ex√©cution de l'AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex√©cutez l'AG sur la fonction de Rastrigin\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "ga = GeneticAlgorithm(\n",
    "    fitness_func=rastrigin,\n",
    "    dim=2,\n",
    "    bounds=(-5.12, 5.12),\n",
    "    pop_size=50,\n",
    "    mutation_rate=0.1,\n",
    "    crossover_rate=0.8\n",
    ")\n",
    "\n",
    "best_individual, best_fitness = ga.evolve(n_generations=100)\n",
    "\n",
    "print(f\"\\n‚úì Optimisation termin√©e\")\n",
    "print(f\"Meilleur individu : {best_individual}\")\n",
    "print(f\"Meilleure fitness : {best_fitness:.6f}\")\n",
    "print(f\"Cible : [0, 0] avec fitness = 0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Visualisation de la Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracez l'√©volution de la fitness\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# TODO: Tracez best_fitness_history\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# TODO: Tracez avg_fitness_history\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 2 : Optimisation d'Hyperparam√®tres avec AG\n",
    "\n",
    "**Dataset** : Wine\n",
    "\n",
    "**Objectif** : Utiliser un AG pour optimiser les hyperparam√®tres d'un RandomForest.\n",
    "\n",
    "### 2.1 Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargez le dataset Wine\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target  # type: ignore\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train : {X_train.shape}, Test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fonction de Fitness pour Hyperparam√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finissez la fonction de fitness\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "def rf_fitness(params):\n",
    "    \"\"\"\n",
    "    Fitness pour RandomForest.\n",
    "    Args:\n",
    "        params: [n_estimators, max_depth, min_samples_split, min_samples_leaf]\n",
    "    Returns:\n",
    "        -cv_score (pour minimisation)\n",
    "    \"\"\"\n",
    "    # TODO: D√©codez les param√®tres (assurez-vous qu'ils sont des entiers)\n",
    "    n_estimators = int(params[0])\n",
    "    max_depth = int(params[1])\n",
    "    min_samples_split = int(params[2])\n",
    "    min_samples_leaf = int(params[3])\n",
    "    \n",
    "    # TODO: Cr√©ez le mod√®le\n",
    "    model = None  # RandomForestClassifier(...)\n",
    "    \n",
    "    # TODO: Calculez le CV score (5-fold)\n",
    "    cv_scores = None  # cross_val_score(...)\n",
    "    \n",
    "    # Retournez -mean(cv_scores) car on minimise\n",
    "    return -cv_scores.mean()\n",
    "\n",
    "# Test\n",
    "test_params = [100, 10, 2, 1]\n",
    "print(f\"Fitness test : {rf_fitness(test_params):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 D√©finition des Bornes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finissez les bornes pour chaque hyperparam√®tre\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# n_estimators : [10, 200]\n",
    "# max_depth : [2, 30]\n",
    "# min_samples_split : [2, 20]\n",
    "# min_samples_leaf : [1, 10]\n",
    "\n",
    "# Note : L'AG actuel utilise des bornes uniformes. Vous devrez adapter\n",
    "# pour g√©rer des bornes diff√©rentes par dimension (optionnel : bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Adaptation de l'AG pour Hyperparam√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptez l'AG pour g√©rer des bornes diff√©rentes par param√®tre\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Option 1 : Modifiez la classe GeneticAlgorithm pour accepter bounds=[(...), (...), ...]\n",
    "# Option 2 : Cr√©ez une nouvelle classe GeneticAlgorithmHyperparams\n",
    "# Option 3 : Normalisez les param√®tres (0-1) puis d√©normalisez dans fitness_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Ex√©cution et Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex√©cutez l'AG pour trouver les meilleurs hyperparam√®tres\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Comparez avec les valeurs par d√©faut de RandomForest\n",
    "\n",
    "# Baseline : RandomForest avec valeurs par d√©faut\n",
    "rf_baseline = RandomForestClassifier(random_state=42)\n",
    "baseline_cv = cross_val_score(rf_baseline, X_train, y_train, cv=5)\n",
    "print(f\"Baseline CV Score : {baseline_cv.mean():.4f} ¬± {baseline_cv.std():.4f}\")\n",
    "\n",
    "# TODO: Ex√©cutez l'AG\n",
    "# TODO: Entra√Ænez un mod√®le avec les meilleurs hyperparam√®tres\n",
    "# TODO: Comparez les scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 3 : S√©lection de Features avec AG\n",
    "\n",
    "**Dataset** : Breast Cancer\n",
    "\n",
    "**Objectif** : Utiliser un AG pour s√©lectionner les features les plus pertinentes.\n",
    "\n",
    "### 3.1 Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargez Breast Cancer\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target  # type: ignore\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Nombre de features : {X.shape[1]}\")\n",
    "print(f\"Train : {X_train.shape}, Test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encodage Binaire pour Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'individu sera un vecteur binaire de longueur 30\n",
    "# 1 = feature s√©lectionn√©e, 0 = feature ignor√©e\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "def feature_selection_fitness(binary_mask):\n",
    "    \"\"\"\n",
    "    Fitness pour feature selection.\n",
    "    Args:\n",
    "        binary_mask: vecteur binaire (0 ou 1) de longueur n_features\n",
    "    Returns:\n",
    "        fitness (√† minimiser) = -accuracy + p√©nalit√© pour nombre de features\n",
    "    \"\"\"\n",
    "    # TODO: Convertissez en masque bool√©en\n",
    "    mask = binary_mask > 0.5  # Seuil pour interpr√©ter comme binaire\n",
    "    \n",
    "    # TODO: V√©rifiez qu'au moins une feature est s√©lectionn√©e\n",
    "    if mask.sum() == 0:\n",
    "        return 1000  # P√©nalit√© √©lev√©e\n",
    "    \n",
    "    # TODO: S√©lectionnez les features\n",
    "    X_train_selected = None  # X_train[:, mask]\n",
    "    \n",
    "    # TODO: Entra√Ænez un mod√®le et calculez le CV score\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    cv_scores = None  # cross_val_score(...)\n",
    "    \n",
    "    # TODO: Calculez la fitness avec p√©nalit√© pour complexit√©\n",
    "    # fitness = -cv_score + lambda * (nombre_de_features / total_features)\n",
    "    # lambda = 0.1 par exemple\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test_mask = np.random.randint(0, 2, size=30)\n",
    "print(f\"Features s√©lectionn√©es : {test_mask.sum()}/30\")\n",
    "print(f\"Fitness test : {feature_selection_fitness(test_mask):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Adaptation de l'AG pour Vecteurs Binaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptez l'AG pour des vecteurs binaires\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Conseil : Pour mutation binaire, flip chaque bit avec probabilit√© mutation_rate\n",
    "# Pour crossover, utilisez le crossover uniforme standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Ex√©cution et Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex√©cutez l'AG pour feature selection\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# TODO: Ex√©cutez l'AG\n",
    "# TODO: Affichez les features s√©lectionn√©es\n",
    "# TODO: Comparez avec utilisation de toutes les features\n",
    "\n",
    "# Baseline : Toutes les features\n",
    "rf_all = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "cv_all = cross_val_score(rf_all, X_train, y_train, cv=5)\n",
    "print(f\"Baseline (toutes features) : {cv_all.mean():.4f} ¬± {cv_all.std():.4f}\")\n",
    "\n",
    "# TODO: Comparez avec les features s√©lectionn√©es par l'AG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 4 : Comparaison AG vs GridSearch (Bonus)\n",
    "\n",
    "**Objectif** : Comparer l'efficacit√© de l'AG par rapport √† GridSearchCV.\n",
    "\n",
    "### 4.1 GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex√©cutez GridSearchCV sur les m√™mes hyperparam√®tres\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'max_depth': [2, 5, 10, 15, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 10]\n",
    "}\n",
    "\n",
    "# TODO: Ex√©cutez GridSearchCV et comptez le nombre d'√©valuations\n",
    "# TODO: Comparez le temps d'ex√©cution et le meilleur score avec l'AG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analyse Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ez un tableau comparatif\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Colonnes : M√©thode | Nombre d'√©valuations | Temps (s) | Meilleur Score | Meilleurs Params\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'M√©thode': ['AG', 'GridSearch', 'RandomSearch'],\n",
    "    'N¬∞ √âvaluations': [None, None, None],\n",
    "    'Temps (s)': [None, None, None],\n",
    "    'Meilleur Score': [None, None, None]\n",
    "})\n",
    "\n",
    "# TODO: Remplissez le DataFrame\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 5 : Questions de R√©flexion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** : Quels sont les avantages des AG par rapport √† GridSearch et RandomSearch ?\n",
    "\n",
    "**VOTRE R√âPONSE ICI**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 2** : Dans quels cas les AG ne sont PAS appropri√©s ?\n",
    "\n",
    "**VOTRE R√âPONSE ICI**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 3** : Comment √©viter la convergence pr√©matur√©e (stagnation) dans un AG ?\n",
    "\n",
    "**VOTRE R√âPONSE ICI**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 4** : Pourquoi utilise-t-on l'√©litisme dans les AG ?\n",
    "\n",
    "**VOTRE R√âPONSE ICI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "F√©licitations pour avoir compl√©t√© ces exercices !\n",
    "\n",
    "**Points cl√©s √† retenir** :\n",
    "- Les AG sont des m√©taheuristiques d'optimisation sans gradient\n",
    "- Ils fonctionnent bien sur des espaces de recherche complexes et non-convexes\n",
    "- Les op√©rateurs g√©n√©tiques (s√©lection, crossover, mutation) √©quilibrent exploration et exploitation\n",
    "- Les AG sont efficaces pour l'optimisation d'hyperparam√®tres et la s√©lection de features\n",
    "- Le design de la fonction de fitness est crucial (objectif + p√©nalit√©s)\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "- Consultez les solutions dans `10_exercices_solutions.ipynb`\n",
    "- Testez les AG sur d'autres probl√®mes d'optimisation\n",
    "- Explorez des variantes : Differential Evolution, Particle Swarm Optimization\n",
    "- Passez au Chapitre 11 (S√©ries Temporelles)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
