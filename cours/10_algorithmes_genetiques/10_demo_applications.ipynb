{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/10_algorithmes_genetiques/10_demo_applications.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '10_demo_applications.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D√©monstration : Applications Avanc√©es des Algorithmes G√©n√©tiques\n",
    "\n",
    "Ce notebook explore des applications pratiques des algorithmes g√©n√©tiques en Machine Learning :\n",
    "1. **Hyperparameter Tuning** : Optimisation des hyperparam√®tres d'un RandomForest\n",
    "2. **Feature Selection** : S√©lection automatique des features pertinentes\n",
    "3. **Comparaison** : AG vs GridSearch vs RandomSearch\n",
    "\n",
    "**Datasets** : Iris, Breast Cancer (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration de visualisation\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Biblioth√®ques import√©es avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classe Algorithme G√©n√©tique G√©n√©rique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticAlgorithm:\n",
    "    \"\"\"Algorithme g√©n√©tique g√©n√©rique pour optimisation.\"\"\"\n",
    "    \n",
    "    def __init__(self, fitness_func, bounds, pop_size=50, generations=30, \n",
    "                 mutation_rate=0.1, crossover_rate=0.8, elitism=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fitness_func: Fonction de fitness (√† maximiser)\n",
    "            bounds: Liste de tuples (min, max) pour chaque param√®tre\n",
    "            pop_size: Taille de la population\n",
    "            generations: Nombre de g√©n√©rations\n",
    "            mutation_rate: Taux de mutation\n",
    "            crossover_rate: Taux de crossover\n",
    "            elitism: Proportion d'√©lite √† pr√©server\n",
    "        \"\"\"\n",
    "        self.fitness_func = fitness_func\n",
    "        self.bounds = bounds\n",
    "        self.n_params = len(bounds)\n",
    "        self.pop_size = pop_size\n",
    "        self.generations = generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.elitism = elitism\n",
    "        self.n_elite = int(pop_size * elitism)\n",
    "        \n",
    "        # Historique\n",
    "        self.best_fitness_history = []\n",
    "        self.avg_fitness_history = []\n",
    "        self.best_individual = None\n",
    "        self.best_fitness = -np.inf\n",
    "    \n",
    "    def initialize_population(self):\n",
    "        \"\"\"Initialise la population al√©atoirement.\"\"\"\n",
    "        population = []\n",
    "        for _ in range(self.pop_size):\n",
    "            individual = [np.random.uniform(low, high) for low, high in self.bounds]\n",
    "            population.append(individual)\n",
    "        return np.array(population)\n",
    "    \n",
    "    def evaluate_population(self, population):\n",
    "        \"\"\"√âvalue la fitness de toute la population.\"\"\"\n",
    "        fitness_values = []\n",
    "        for individual in population:\n",
    "            fitness = self.fitness_func(individual)\n",
    "            fitness_values.append(fitness)\n",
    "        return np.array(fitness_values)\n",
    "    \n",
    "    def selection(self, population, fitness_values):\n",
    "        \"\"\"S√©lection par tournoi.\"\"\"\n",
    "        tournament_size = 3\n",
    "        selected_idx = np.random.choice(len(population), tournament_size, replace=False)\n",
    "        tournament_fitness = fitness_values[selected_idx]\n",
    "        winner_idx = selected_idx[np.argmax(tournament_fitness)]\n",
    "        return population[winner_idx]\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        \"\"\"Crossover uniforme.\"\"\"\n",
    "        if np.random.rand() > self.crossover_rate:\n",
    "            return parent1.copy(), parent2.copy()\n",
    "        \n",
    "        child1, child2 = parent1.copy(), parent2.copy()\n",
    "        for i in range(self.n_params):\n",
    "            if np.random.rand() < 0.5:\n",
    "                child1[i], child2[i] = child2[i], child1[i]\n",
    "        return child1, child2\n",
    "    \n",
    "    def mutate(self, individual):\n",
    "        \"\"\"Mutation gaussienne.\"\"\"\n",
    "        for i in range(self.n_params):\n",
    "            if np.random.rand() < self.mutation_rate:\n",
    "                low, high = self.bounds[i]\n",
    "                mutation = np.random.normal(0, (high - low) * 0.1)\n",
    "                individual[i] = np.clip(individual[i] + mutation, low, high)\n",
    "        return individual\n",
    "    \n",
    "    def evolve(self, verbose=True):\n",
    "        \"\"\"Ex√©cute l'algorithme g√©n√©tique.\"\"\"\n",
    "        population = self.initialize_population()\n",
    "        \n",
    "        for gen in range(self.generations):\n",
    "            # √âvaluation\n",
    "            fitness_values = self.evaluate_population(population)\n",
    "            \n",
    "            # Statistiques\n",
    "            best_idx = np.argmax(fitness_values)\n",
    "            best_gen_fitness = fitness_values[best_idx]\n",
    "            avg_gen_fitness = np.mean(fitness_values)\n",
    "            \n",
    "            self.best_fitness_history.append(best_gen_fitness)\n",
    "            self.avg_fitness_history.append(avg_gen_fitness)\n",
    "            \n",
    "            if best_gen_fitness > self.best_fitness:\n",
    "                self.best_fitness = best_gen_fitness\n",
    "                self.best_individual = population[best_idx].copy()\n",
    "            \n",
    "            if verbose and gen % 5 == 0:\n",
    "                print(f\"Gen {gen:3d} | Best: {best_gen_fitness:.4f} | Avg: {avg_gen_fitness:.4f}\")\n",
    "            \n",
    "            # √âlitisme\n",
    "            elite_indices = np.argsort(fitness_values)[-self.n_elite:]\n",
    "            elite = population[elite_indices]\n",
    "            \n",
    "            # Nouvelle g√©n√©ration\n",
    "            new_population = list(elite)\n",
    "            \n",
    "            while len(new_population) < self.pop_size:\n",
    "                parent1 = self.selection(population, fitness_values)\n",
    "                parent2 = self.selection(population, fitness_values)\n",
    "                child1, child2 = self.crossover(parent1, parent2)\n",
    "                child1 = self.mutate(child1)\n",
    "                child2 = self.mutate(child2)\n",
    "                new_population.extend([child1, child2])\n",
    "            \n",
    "            population = np.array(new_population[:self.pop_size])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nMeilleur individu: {self.best_individual}\")\n",
    "            print(f\"Meilleure fitness: {self.best_fitness:.4f}\")\n",
    "        \n",
    "        return self.best_individual, self.best_fitness\n",
    "    \n",
    "    def plot_convergence(self):\n",
    "        \"\"\"Visualise la convergence de l'algorithme.\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.best_fitness_history, label='Best Fitness', linewidth=2)\n",
    "        plt.plot(self.avg_fitness_history, label='Average Fitness', linewidth=2, alpha=0.7)\n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel('Fitness (Accuracy)')\n",
    "        plt.title('Convergence de l\\'Algorithme G√©n√©tique')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Classe GeneticAlgorithm cr√©√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Application 1 : Hyperparameter Tuning avec AG\n",
    "\n",
    "Optimisons les hyperparam√®tres d'un **RandomForest** sur le dataset Iris :\n",
    "- `n_estimators` : nombre d'arbres [10, 200]\n",
    "- `max_depth` : profondeur max [2, 20]\n",
    "- `min_samples_split` : √©chantillons min pour split [2, 20]\n",
    "- `min_samples_leaf` : √©chantillons min par feuille [1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset Iris\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "print(f\"Dataset Iris:\")\n",
    "print(f\"  Train: {X_train_iris.shape}, Test: {X_test_iris.shape}\")\n",
    "print(f\"  Classes: {np.unique(y_iris)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de fitness pour hyperparameter tuning\n",
    "def fitness_rf_hyperparams(params):\n",
    "    \"\"\"Fitness = accuracy du RandomForest avec validation crois√©e.\"\"\"\n",
    "    n_estimators = int(params[0])\n",
    "    max_depth = int(params[1])\n",
    "    min_samples_split = int(params[2])\n",
    "    min_samples_leaf = int(params[3])\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Validation crois√©e 3-fold\n",
    "    scores = cross_val_score(rf, X_train_iris, y_train_iris, cv=3, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "# Bounds pour les hyperparam√®tres\n",
    "bounds_rf = [\n",
    "    (10, 200),   # n_estimators\n",
    "    (2, 20),     # max_depth\n",
    "    (2, 20),     # min_samples_split\n",
    "    (1, 10)      # min_samples_leaf\n",
    "]\n",
    "\n",
    "print(\"Fonction de fitness RF d√©finie !\")\n",
    "print(f\"Hyperparam√®tres √† optimiser: {['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation avec AG\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMISATION HYPERPARAM√àTRES AVEC ALGORITHME G√âN√âTIQUE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "ga_rf = GeneticAlgorithm(\n",
    "    fitness_func=fitness_rf_hyperparams,\n",
    "    bounds=bounds_rf,\n",
    "    pop_size=20,\n",
    "    generations=20,\n",
    "    mutation_rate=0.15,\n",
    "    crossover_rate=0.8,\n",
    "    elitism=0.1\n",
    ")\n",
    "\n",
    "best_params_ga, best_fitness_ga = ga_rf.evolve(verbose=True)\n",
    "\n",
    "ga_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTemps d'ex√©cution: {ga_time:.2f}s\")\n",
    "print(f\"\\nMeilleurs hyperparam√®tres (AG):\")\n",
    "print(f\"  n_estimators: {int(best_params_ga[0])}\")\n",
    "print(f\"  max_depth: {int(best_params_ga[1])}\")\n",
    "print(f\"  min_samples_split: {int(best_params_ga[2])}\")\n",
    "print(f\"  min_samples_leaf: {int(best_params_ga[3])}\")\n",
    "print(f\"  Accuracy (CV): {best_fitness_ga:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation convergence\n",
    "ga_rf.plot_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur ensemble de test\n",
    "rf_best_ga = RandomForestClassifier(\n",
    "    n_estimators=int(best_params_ga[0]),\n",
    "    max_depth=int(best_params_ga[1]),\n",
    "    min_samples_split=int(best_params_ga[2]),\n",
    "    min_samples_leaf=int(best_params_ga[3]),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_best_ga.fit(X_train_iris, y_train_iris)\n",
    "y_pred_ga = rf_best_ga.predict(X_test_iris)\n",
    "acc_ga = accuracy_score(y_test_iris, y_pred_ga)\n",
    "\n",
    "print(f\"Accuracy sur test set: {acc_ga:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_iris, y_pred_ga, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparaison : AG vs GridSearch vs RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "print(\"=\" * 60)\n",
    "print(\"GRID SEARCH CV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'max_depth': [2, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"Temps d'ex√©cution: {grid_time:.2f}s\")\n",
    "print(f\"Nombre de combinaisons test√©es: {len(grid_search.cv_results_['params'])}\")\n",
    "print(f\"\\nMeilleurs hyperparam√®tres (GridSearch):\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Meilleur score (CV): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "y_pred_grid = grid_search.predict(X_test_iris)\n",
    "acc_grid = accuracy_score(y_test_iris, y_pred_grid)\n",
    "print(f\"Accuracy sur test set: {acc_grid:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOMIZED SEARCH CV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(10, 200),\n",
    "    'max_depth': randint(2, 20),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_distributions,\n",
    "    n_iter=100,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "random_time = time.time() - start_time\n",
    "\n",
    "print(f\"Temps d'ex√©cution: {random_time:.2f}s\")\n",
    "print(f\"Nombre de combinaisons test√©es: {len(random_search.cv_results_['params'])}\")\n",
    "print(f\"\\nMeilleurs hyperparam√®tres (RandomSearch):\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"Meilleur score (CV): {random_search.best_score_:.4f}\")\n",
    "\n",
    "y_pred_random = random_search.predict(X_test_iris)\n",
    "acc_random = accuracy_score(y_test_iris, y_pred_random)\n",
    "print(f\"Accuracy sur test set: {acc_random:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif\n",
    "comparison_df = pd.DataFrame({\n",
    "    'M√©thode': ['Algorithme G√©n√©tique', 'Grid Search', 'Random Search'],\n",
    "    'Temps (s)': [ga_time, grid_time, random_time],\n",
    "    'CV Score': [best_fitness_ga, grid_search.best_score_, random_search.best_score_],\n",
    "    'Test Accuracy': [acc_ga, acc_grid, acc_random],\n",
    "    'N¬∞ √âvaluations': [ga_rf.pop_size * ga_rf.generations, \n",
    "                       len(grid_search.cv_results_['params']),\n",
    "                       len(random_search.cv_results_['params'])]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARAISON DES M√âTHODES D'OPTIMISATION\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Temps d'ex√©cution\n",
    "axes[0].bar(comparison_df['M√©thode'], comparison_df['Temps (s)'], color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "axes[0].set_ylabel('Temps (secondes)')\n",
    "axes[0].set_title('Temps d\\'Ex√©cution')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# CV Score\n",
    "axes[1].bar(comparison_df['M√©thode'], comparison_df['CV Score'], color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Score de Validation Crois√©e')\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Nombre d'√©valuations\n",
    "axes[2].bar(comparison_df['M√©thode'], comparison_df['N¬∞ √âvaluations'], color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "axes[2].set_ylabel('Nombre d\\'√©valuations')\n",
    "axes[2].set_title('Nombre d\\'√âvaluations')\n",
    "axes[2].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application 2 : Feature Selection avec AG\n",
    "\n",
    "S√©lection automatique des features les plus pertinentes sur le dataset **Breast Cancer** (30 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset Breast Cancer\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.3, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "print(f\"Dataset Breast Cancer:\")\n",
    "print(f\"  Train: {X_train_cancer.shape}, Test: {X_test_cancer.shape}\")\n",
    "print(f\"  Nombre de features: {X_cancer.shape[1]}\")\n",
    "print(f\"  Classes: {np.unique(y_cancer)} (0=malignant, 1=benign)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de fitness pour feature selection\n",
    "def fitness_feature_selection(binary_mask):\n",
    "    \"\"\"\n",
    "    Fitness = accuracy avec p√©nalit√© pour trop de features.\n",
    "    binary_mask: vecteur binaire (1 = feature s√©lectionn√©e, 0 = rejet√©e)\n",
    "    \"\"\"\n",
    "    # Convertir en masque binaire\n",
    "    mask = (binary_mask > 0.5).astype(bool)\n",
    "    \n",
    "    # Au moins 1 feature doit √™tre s√©lectionn√©e\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # S√©lection des features\n",
    "    X_train_selected = X_train_cancer[:, mask]\n",
    "    \n",
    "    # Entra√Ænement d'un RandomForest\n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "    scores = cross_val_score(rf, X_train_selected, y_train_cancer, cv=3, scoring='accuracy')\n",
    "    accuracy = scores.mean()\n",
    "    \n",
    "    # P√©nalit√© pour trop de features (encourager la parcimonie)\n",
    "    n_features_selected = mask.sum()\n",
    "    penalty = 0.01 * (n_features_selected / len(mask))\n",
    "    \n",
    "    return accuracy - penalty\n",
    "\n",
    "# Bounds : [0, 1] pour chaque feature (sera binaris√© avec seuil 0.5)\n",
    "n_features = X_cancer.shape[1]\n",
    "bounds_features = [(0, 1)] * n_features\n",
    "\n",
    "print(f\"Fonction de fitness feature selection d√©finie !\")\n",
    "print(f\"Nombre de features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation avec AG\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE SELECTION AVEC ALGORITHME G√âN√âTIQUE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "ga_fs = GeneticAlgorithm(\n",
    "    fitness_func=fitness_feature_selection,\n",
    "    bounds=bounds_features,\n",
    "    pop_size=30,\n",
    "    generations=25,\n",
    "    mutation_rate=0.1,\n",
    "    crossover_rate=0.8,\n",
    "    elitism=0.15\n",
    ")\n",
    "\n",
    "best_mask_ga, best_fitness_fs = ga_fs.evolve(verbose=True)\n",
    "\n",
    "fs_time = time.time() - start_time\n",
    "\n",
    "# Convertir en masque binaire\n",
    "best_mask_binary = (best_mask_ga > 0.5).astype(bool)\n",
    "selected_features = np.array(cancer.feature_names)[best_mask_binary]\n",
    "\n",
    "print(f\"\\nTemps d'ex√©cution: {fs_time:.2f}s\")\n",
    "print(f\"Nombre de features s√©lectionn√©es: {best_mask_binary.sum()} / {n_features}\")\n",
    "print(f\"Fitness: {best_fitness_fs:.4f}\")\n",
    "print(f\"\\nFeatures s√©lectionn√©es:\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    print(f\"  {i}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation convergence\n",
    "ga_fs.plot_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur ensemble de test\n",
    "X_train_selected = X_train_cancer[:, best_mask_binary]\n",
    "X_test_selected = X_test_cancer[:, best_mask_binary]\n",
    "\n",
    "rf_fs = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_fs.fit(X_train_selected, y_train_cancer)\n",
    "y_pred_fs = rf_fs.predict(X_test_selected)\n",
    "acc_fs = accuracy_score(y_test_cancer, y_pred_fs)\n",
    "\n",
    "# Comparaison avec toutes les features\n",
    "rf_all = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_all.fit(X_train_cancer, y_train_cancer)\n",
    "y_pred_all = rf_all.predict(X_test_cancer)\n",
    "acc_all = accuracy_score(y_test_cancer, y_pred_all)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARAISON : FEATURES S√âLECTIONN√âES VS TOUTES LES FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features s√©lectionn√©es ({best_mask_binary.sum()}):\")\n",
    "print(f\"  Accuracy: {acc_fs:.4f}\")\n",
    "print(f\"\\nToutes les features ({n_features}):\")\n",
    "print(f\"  Accuracy: {acc_all:.4f}\")\n",
    "print(f\"\\nR√©duction de features: {(1 - best_mask_binary.sum() / n_features) * 100:.1f}%\")\n",
    "print(f\"Diff√©rence d'accuracy: {(acc_fs - acc_all) * 100:+.2f}%\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "methods = ['Features S√©lectionn√©es', 'Toutes les Features']\n",
    "accuracies = [acc_fs, acc_all]\n",
    "colors = ['#2ecc71', '#95a5a6']\n",
    "axes[0].bar(methods, accuracies, color=colors)\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "axes[0].set_title('Comparaison Accuracy')\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 0.005, f\"{v:.4f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# Feature count\n",
    "feature_counts = [best_mask_binary.sum(), n_features]\n",
    "axes[1].bar(methods, feature_counts, color=colors)\n",
    "axes[1].set_ylabel('Nombre de Features')\n",
    "axes[1].set_title('Nombre de Features Utilis√©es')\n",
    "for i, v in enumerate(feature_counts):\n",
    "    axes[1].text(i, v + 0.5, f\"{int(v)}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "cm_fs = confusion_matrix(y_test_cancer, y_pred_fs)\n",
    "cm_all = confusion_matrix(y_test_cancer, y_pred_all)\n",
    "\n",
    "sns.heatmap(cm_fs, annot=True, fmt='d', cmap='Greens', ax=axes[0], cbar=False)\n",
    "axes[0].set_title(f'Features S√©lectionn√©es ({best_mask_binary.sum()})')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "sns.heatmap(cm_all, annot=True, fmt='d', cmap='Blues', ax=axes[1], cbar=False)\n",
    "axes[1].set_title(f'Toutes les Features ({n_features})')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "### Points Cl√©s\n",
    "\n",
    "**Hyperparameter Tuning** :\n",
    "- Les AG offrent un bon compromis entre exploration et exploitation\n",
    "- Plus rapides que GridSearch pour de grands espaces de recherche\n",
    "- Performance comparable √† RandomSearch avec moins d'√©valuations\n",
    "\n",
    "**Feature Selection** :\n",
    "- R√©duction significative du nombre de features sans perte d'accuracy\n",
    "- Mod√®les plus simples et plus interpr√©tables\n",
    "- Utile pour √©viter l'overfitting et r√©duire le temps d'inf√©rence\n",
    "\n",
    "### Avantages des AG\n",
    "1. **Exploration globale** : √âvitent les minima locaux\n",
    "2. **Flexibilit√©** : Peuvent optimiser des fonctions objectives complexes\n",
    "3. **Parall√©lisables** : √âvaluation de la population en parall√®le\n",
    "4. **Peu d'hypoth√®ses** : Pas besoin de gradients ou de continuit√©\n",
    "\n",
    "### Limitations\n",
    "1. **Temps de calcul** : Peuvent √™tre lents pour des √©valuations co√ªteuses\n",
    "2. **Hyperparam√®tres** : N√©cessitent du tuning (taille population, taux mutation, etc.)\n",
    "3. **Convergence** : Pas de garantie de trouver l'optimum global\n",
    "4. **Comparaison** : GridSearch reste plus exhaustif pour petits espaces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}