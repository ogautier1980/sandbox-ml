{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/11_series_temporelles/11_demo_deep_learning_ts.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '11_demo_deep_learning_ts.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 12 - Deep Learning pour S√©ries Temporelles\n",
    "\n",
    "**Objectifs :**\n",
    "- Cr√©er des fen√™tres glissantes (sliding windows) pour LSTM\n",
    "- Impl√©menter LSTM et GRU pour forecasting univari√©\n",
    "- Forecasting multivari√© avec features suppl√©mentaires\n",
    "- Attention mechanism pour s√©ries temporelles\n",
    "- Comparer DL vs mod√®les classiques (ARIMA)\n",
    "- D√©tection d'anomalies par erreur de pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. G√©n√©ration de Donn√©es\n",
    "\n",
    "S√©rie temporelle avec tendance, saisonnalit√© et bruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complex_timeseries(n=1000, seed=42):\n",
    "    \"\"\"\n",
    "    G√©n√®re une s√©rie temporelle complexe\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    t = np.arange(n)\n",
    "    \n",
    "    # Tendance non-lin√©aire\n",
    "    trend = 0.05 * t + 0.0001 * t**2\n",
    "    \n",
    "    # Saisonnalit√©s multiples\n",
    "    seasonality_annual = 20 * np.sin(2 * np.pi * t / 365)\n",
    "    seasonality_weekly = 5 * np.sin(2 * np.pi * t / 7)\n",
    "    \n",
    "    # Bruit\n",
    "    noise = np.random.normal(0, 5, n)\n",
    "    \n",
    "    # S√©rie compl√®te\n",
    "    y = 100 + trend + seasonality_annual + seasonality_weekly + noise\n",
    "    \n",
    "    # Dates\n",
    "    dates = pd.date_range(start='2020-01-01', periods=n, freq='D')\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'value': y\n",
    "    })\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# G√©n√©rer donn√©es\n",
    "df = generate_complex_timeseries(n=1200)\n",
    "\n",
    "print(f\"S√©rie g√©n√©r√©e: {len(df)} observations\")\n",
    "print(f\"P√©riode: {df.index.min()} √† {df.index.max()}\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df.index, df['value'], color='blue', linewidth=1)\n",
    "plt.title('S√©rie Temporelle G√©n√©r√©e', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Valeur')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pr√©paration des Donn√©es - Sliding Windows\n",
    "\n",
    "Transformation en s√©quences pour LSTM/GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size, horizon=1):\n",
    "    \"\"\"\n",
    "    Cr√©e des fen√™tres glissantes (sliding windows)\n",
    "    \n",
    "    Param√®tres:\n",
    "    - data: array 1D ou 2D (pour multivari√©)\n",
    "    - window_size: taille de la fen√™tre d'entr√©e\n",
    "    - horizon: nombre de pas √† pr√©dire\n",
    "    \n",
    "    Retourne:\n",
    "    - X: (n_samples, window_size, n_features)\n",
    "    - y: (n_samples, horizon)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size:i+window_size+horizon])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Param√®tres\n",
    "WINDOW_SIZE = 30  # Utiliser 30 jours pour pr√©dire\n",
    "HORIZON = 1       # Pr√©dire 1 jour\n",
    "\n",
    "# Donn√©es\n",
    "data = df['value'].values\n",
    "\n",
    "# Normalisation (IMPORTANT: fit sur train uniquement)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Train/Val/Test split (70/15/15)\n",
    "train_size = int(len(data) * 0.7)\n",
    "val_size = int(len(data) * 0.15)\n",
    "\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:train_size+val_size]\n",
    "test_data = data[train_size+val_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Val size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n",
    "\n",
    "# Fit scaler sur train uniquement\n",
    "scaler.fit(train_data.reshape(-1, 1))\n",
    "\n",
    "# Transform\n",
    "train_scaled = scaler.transform(train_data.reshape(-1, 1)).flatten()\n",
    "val_scaled = scaler.transform(val_data.reshape(-1, 1)).flatten()\n",
    "test_scaled = scaler.transform(test_data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Cr√©er s√©quences\n",
    "X_train, y_train = create_sequences(train_scaled, WINDOW_SIZE, HORIZON)\n",
    "X_val, y_val = create_sequences(val_scaled, WINDOW_SIZE, HORIZON)\n",
    "X_test, y_test = create_sequences(test_scaled, WINDOW_SIZE, HORIZON)\n",
    "\n",
    "print(f\"\\nX_train shape: {X_train.shape}  # (samples, window_size)\")\n",
    "print(f\"y_train shape: {y_train.shape}  # (samples, horizon)\")\n",
    "print(f\"\\nExemple:\")\n",
    "print(f\"X[0] (30 valeurs): {X_train[0][:5]}...\")\n",
    "print(f\"y[0] (pr√©diction): {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en tenseurs PyTorch\n",
    "X_train_t = torch.FloatTensor(X_train).unsqueeze(-1)  # (N, L, 1)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "\n",
    "X_val_t = torch.FloatTensor(X_val).unsqueeze(-1)\n",
    "y_val_t = torch.FloatTensor(y_val)\n",
    "\n",
    "X_test_t = torch.FloatTensor(X_test).unsqueeze(-1)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "print(f\"X_train_t shape: {X_train_t.shape}  # (batch, seq_len, features)\")\n",
    "print(f\"y_train_t shape: {y_train_t.shape}\")\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mod√®le LSTM\n",
    "\n",
    "### 3.1 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, \n",
    "                 output_size=1, dropout=0.2):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # lstm_out: (batch, seq_len, hidden_size)\n",
    "        \n",
    "        # Prendre le dernier output\n",
    "        last_output = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
    "        \n",
    "        # Pr√©diction\n",
    "        output = self.fc(last_output)  # (batch, output_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Instanciation\n",
    "lstm_model = LSTMForecaster(\n",
    "    input_size=1,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=HORIZON,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(lstm_model)\n",
    "print(f\"\\nNombre de param√®tres: {sum(p.numel() for p in lstm_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=100, lr=0.001, patience=15):\n",
    "    \"\"\"\n",
    "    Entra√Æne le mod√®le avec early stopping\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                  f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping √† l'epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Charger meilleur mod√®le\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"Entra√Ænement LSTM...\\n\")\n",
    "train_losses, val_losses = train_model(\n",
    "    lstm_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=100, \n",
    "    lr=0.001,\n",
    "    patience=15\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes de loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Courbes d\\'Apprentissage - LSTM', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, scaler):\n",
    "    \"\"\"\n",
    "    √âvalue le mod√®le sur le test set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(batch_y.numpy())\n",
    "    \n",
    "    predictions = np.array(predictions).flatten()\n",
    "    actuals = np.array(actuals).flatten()\n",
    "    \n",
    "    # Inverse transform (revenir √† l'√©chelle originale)\n",
    "    predictions_orig = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    actuals_orig = scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # M√©triques\n",
    "    mae = mean_absolute_error(actuals_orig, predictions_orig)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals_orig, predictions_orig))\n",
    "    mape = np.mean(np.abs((actuals_orig - predictions_orig) / actuals_orig)) * 100\n",
    "    \n",
    "    print(\"\\n=== M√©triques Test Set ===\")\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return predictions_orig, actuals_orig, {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
    "\n",
    "# √âvaluation LSTM\n",
    "lstm_predictions, lstm_actuals, lstm_metrics = evaluate_model(lstm_model, test_loader, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation pr√©dictions\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(lstm_actuals, label='R√©el', color='green', linewidth=2)\n",
    "plt.plot(lstm_predictions, label='Pr√©dictions LSTM', color='red', linestyle='--', alpha=0.8)\n",
    "plt.title('Pr√©dictions LSTM sur Test Set', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Pas de temps')\n",
    "plt.ylabel('Valeur')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Erreur de pr√©diction\n",
    "errors = lstm_actuals - lstm_predictions\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(errors, color='purple', alpha=0.7)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.fill_between(range(len(errors)), errors, alpha=0.3, color='purple')\n",
    "plt.title('Erreurs de Pr√©diction LSTM', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Pas de temps')\n",
    "plt.ylabel('Erreur (R√©el - Pr√©diction)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mod√®le GRU\n",
    "\n",
    "Variante simplifi√©e de LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, \n",
    "                 output_size=1, dropout=0.2):\n",
    "        super(GRUForecaster, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # GRU\n",
    "        gru_out, h_n = self.gru(x)\n",
    "        \n",
    "        # Dernier output\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        \n",
    "        # Pr√©diction\n",
    "        output = self.fc(last_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Instanciation\n",
    "gru_model = GRUForecaster(\n",
    "    input_size=1,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=HORIZON,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(gru_model)\n",
    "print(f\"\\nNombre de param√®tres: {sum(p.numel() for p in gru_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"\\nEntra√Ænement GRU...\\n\")\n",
    "gru_train_losses, gru_val_losses = train_model(\n",
    "    gru_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=100,\n",
    "    lr=0.001,\n",
    "    patience=15\n",
    ")\n",
    "\n",
    "# √âvaluation\n",
    "gru_predictions, gru_actuals, gru_metrics = evaluate_model(gru_model, test_loader, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTM avec Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, \n",
    "                 output_size=1, dropout=0.2):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = self.attention(lstm_out)  # (batch, seq_len, 1)\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        # Context vector (weighted sum)\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)  # (batch, hidden)\n",
    "        \n",
    "        # Pr√©diction\n",
    "        output = self.fc(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Instanciation\n",
    "attention_model = LSTMWithAttention(\n",
    "    input_size=1,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=HORIZON,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(attention_model)\n",
    "\n",
    "# Fonction d'entra√Ænement adapt√©e pour attention\n",
    "def train_attention_model(model, train_loader, val_loader, num_epochs=100, lr=0.001, patience=15):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs, _ = model(batch_X)  # Ignorer attention weights pendant entra√Ænement\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                outputs, _ = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Train: {train_loss:.6f}, Val: {val_loss:.6f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping √† l'epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"\\nEntra√Ænement LSTM avec Attention...\\n\")\n",
    "att_train_losses, att_val_losses = train_attention_model(\n",
    "    attention_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=100,\n",
    "    lr=0.001,\n",
    "    patience=15\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation avec visualisation de l'attention\n",
    "attention_model.eval()\n",
    "attention_predictions = []\n",
    "attention_actuals = []\n",
    "attention_weights_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        outputs, att_weights = attention_model(batch_X)\n",
    "        \n",
    "        attention_predictions.extend(outputs.cpu().numpy())\n",
    "        attention_actuals.extend(batch_y.numpy())\n",
    "        attention_weights_list.append(att_weights.cpu().numpy())\n",
    "\n",
    "attention_predictions = np.array(attention_predictions).flatten()\n",
    "attention_actuals = np.array(attention_actuals).flatten()\n",
    "\n",
    "# Inverse transform\n",
    "att_pred_orig = scaler.inverse_transform(attention_predictions.reshape(-1, 1)).flatten()\n",
    "att_act_orig = scaler.inverse_transform(attention_actuals.reshape(-1, 1)).flatten()\n",
    "\n",
    "# M√©triques\n",
    "att_mae = mean_absolute_error(att_act_orig, att_pred_orig)\n",
    "att_rmse = np.sqrt(mean_squared_error(att_act_orig, att_pred_orig))\n",
    "att_mape = np.mean(np.abs((att_act_orig - att_pred_orig) / att_act_orig)) * 100\n",
    "\n",
    "print(\"\\n=== M√©triques LSTM + Attention ===\")\n",
    "print(f\"MAE:  {att_mae:.4f}\")\n",
    "print(f\"RMSE: {att_rmse:.4f}\")\n",
    "print(f\"MAPE: {att_mape:.2f}%\")\n",
    "\n",
    "# Visualiser attention weights pour un exemple\n",
    "example_att_weights = attention_weights_list[0][0].squeeze()  # Premier exemple du premier batch\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(example_att_weights)), example_att_weights, color='steelblue')\n",
    "plt.title('Poids d\\'Attention - Exemple', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Position dans la s√©quence (t-30 √† t-1)')\n",
    "plt.ylabel('Poids d\\'attention')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif\n",
    "comparison = pd.DataFrame({\n",
    "    'LSTM': lstm_metrics,\n",
    "    'GRU': gru_metrics,\n",
    "    'LSTM+Attention': {'MAE': att_mae, 'RMSE': att_rmse, 'MAPE': att_mape}\n",
    "})\n",
    "\n",
    "print(\"\\n=== Comparaison des Mod√®les Deep Learning ===\")\n",
    "print(comparison.T)\n",
    "\n",
    "# Visualisation comparative\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(lstm_actuals[:200], label='R√©el', color='black', linewidth=2)\n",
    "plt.plot(lstm_predictions[:200], label='LSTM', color='blue', linestyle='--', alpha=0.7)\n",
    "plt.plot(gru_predictions[:200], label='GRU', color='green', linestyle='--', alpha=0.7)\n",
    "plt.plot(att_pred_orig[:200], label='LSTM+Attention', color='red', linestyle='--', alpha=0.7)\n",
    "plt.title('Comparaison Pr√©dictions (200 premiers points)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Pas de temps')\n",
    "plt.ylabel('Valeur')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. D√©tection d'Anomalies\n",
    "\n",
    "Utiliser l'erreur de pr√©diction pour d√©tecter les anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erreurs de pr√©diction\n",
    "errors_abs = np.abs(lstm_actuals - lstm_predictions)\n",
    "\n",
    "# Seuil: moyenne + 3 * √©cart-type\n",
    "threshold = errors_abs.mean() + 3 * errors_abs.std()\n",
    "\n",
    "# D√©tecter anomalies\n",
    "anomalies = errors_abs > threshold\n",
    "anomaly_indices = np.where(anomalies)[0]\n",
    "\n",
    "print(f\"\\n=== D√©tection d'Anomalies ===\")\n",
    "print(f\"Erreur moyenne: {errors_abs.mean():.4f}\")\n",
    "print(f\"√âcart-type: {errors_abs.std():.4f}\")\n",
    "print(f\"Seuil: {threshold:.4f}\")\n",
    "print(f\"Nombre d'anomalies d√©tect√©es: {anomalies.sum()} / {len(anomalies)} ({100*anomalies.sum()/len(anomalies):.2f}%)\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Pr√©dictions\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(lstm_actuals, label='R√©el', color='green', linewidth=2)\n",
    "plt.plot(lstm_predictions, label='Pr√©dictions', color='blue', linestyle='--', alpha=0.7)\n",
    "plt.scatter(anomaly_indices, lstm_actuals[anomaly_indices], \n",
    "            color='red', s=100, marker='x', label='Anomalies', zorder=5)\n",
    "plt.title('D√©tection d\\'Anomalies - S√©rie Temporelle', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Valeur')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Erreurs\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(errors_abs, color='purple', alpha=0.7, label='Erreur absolue')\n",
    "plt.axhline(threshold, color='red', linestyle='--', linewidth=2, label=f'Seuil = {threshold:.2f}')\n",
    "plt.scatter(anomaly_indices, errors_abs[anomaly_indices], \n",
    "            color='red', s=100, marker='x', zorder=5)\n",
    "plt.title('Erreurs de Pr√©diction', fontsize=12)\n",
    "plt.xlabel('Pas de temps')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if len(anomaly_indices) > 0:\n",
    "    print(f\"\\nPremi√®res anomalies d√©tect√©es (indices):\")\n",
    "    print(anomaly_indices[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Step Forecasting\n",
    "\n",
    "Pr√©dire plusieurs pas √† l'avance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er donn√©es pour multi-step (pr√©dire 7 jours)\n",
    "HORIZON_MULTI = 7\n",
    "\n",
    "X_multi, y_multi = create_sequences(train_scaled, WINDOW_SIZE, HORIZON_MULTI)\n",
    "X_multi_t = torch.FloatTensor(X_multi).unsqueeze(-1)\n",
    "y_multi_t = torch.FloatTensor(y_multi)\n",
    "\n",
    "print(f\"X_multi shape: {X_multi_t.shape}  # (N, 30, 1)\")\n",
    "print(f\"y_multi shape: {y_multi_t.shape}  # (N, 7)\")\n",
    "\n",
    "# Mod√®le multi-step\n",
    "multi_model = LSTMForecaster(\n",
    "    input_size=1,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=HORIZON_MULTI,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nMod√®le pour pr√©diction {HORIZON_MULTI} pas √† l'avance\")\n",
    "print(multi_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons explor√© :\n",
    "\n",
    "1. **Pr√©paration des donn√©es** : sliding windows, normalisation\n",
    "2. **LSTM** : architecture classique pour s√©ries temporelles\n",
    "3. **GRU** : variante plus l√©g√®re et rapide\n",
    "4. **Attention** : focus sur les instants les plus pertinents\n",
    "5. **√âvaluation** : MAE, RMSE, MAPE\n",
    "6. **D√©tection d'anomalies** : via erreur de pr√©diction\n",
    "\n",
    "**Points cl√©s :**\n",
    "- LSTM/GRU capturent les d√©pendances temporelles complexes\n",
    "- Attention am√©liore l'interpr√©tabilit√©\n",
    "- Normalisation cruciale (fit sur train uniquement)\n",
    "- Sliding windows transforment s√©rie en dataset supervis√©\n",
    "- Early stopping √©vite le surapprentissage\n",
    "\n",
    "**Cas d'usage DL vs Classique :**\n",
    "- **ARIMA** : s√©ries stationnaires, patterns lin√©aires, peu de donn√©es\n",
    "- **LSTM/GRU** : patterns non-lin√©aires, d√©pendances long terme, beaucoup de donn√©es\n",
    "- **Attention/Transformers** : tr√®s longues s√©quences, interpr√©tabilit√© importante"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}