{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/11_series_temporelles/11_exercices.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '11_exercices.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 12 - S√©ries Temporelles : Exercices\n",
    "\n",
    "Ce notebook contient 3 exercices pratiques avec solutions compl√®tes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# S√©ries temporelles\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 1 : Pr√©diction de Ventes Mensuelles avec ARIMA\n",
    "\n",
    "**Contexte :** Vous travaillez pour un retailer qui souhaite pr√©dire ses ventes mensuelles.\n",
    "\n",
    "**Objectifs :**\n",
    "1. G√©n√©rer des ventes mensuelles synth√©tiques avec tendance et saisonnalit√©\n",
    "2. Analyser la s√©rie (d√©composition, stationnarit√©)\n",
    "3. Ajuster un mod√®le ARIMA\n",
    "4. Faire des pr√©dictions sur 12 mois\n",
    "5. √âvaluer avec RMSE et MAPE\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: G√©n√©rer ventes mensuelles (5 ans = 60 mois)\n",
    "# - Tendance croissante: +2% par mois\n",
    "# - Saisonnalit√© annuelle (pic en d√©cembre)\n",
    "# - Bruit gaussien\n",
    "\n",
    "def generate_monthly_sales(n_months=60, base_sales=10000, trend_rate=0.02, noise_std=500, seed=42):\n",
    "    # TODO: Impl√©menter\n",
    "    pass\n",
    "\n",
    "# G√©n√©rer\n",
    "# sales_df = generate_monthly_sales(n_months=60)\n",
    "\n",
    "# TODO: Visualiser la s√©rie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Exploratoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: D√©composition de la s√©rie (trend, seasonality, residual)\n",
    "# Utiliser seasonal_decompose avec period=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test de stationnarit√© (ADF test)\n",
    "# Si non-stationnaire, diff√©rencier la s√©rie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mod√®le ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split train/test (80/20)\n",
    "# TODO: S√©lectionner param√®tres ARIMA (p, d, q)\n",
    "# TODO: Entra√Æner ARIMA\n",
    "# TODO: Pr√©dire sur test set\n",
    "# TODO: Calculer RMSE et MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SOLUTION Exercice 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: G√©n√©ration des donn√©es\n",
    "def generate_monthly_sales(n_months=60, base_sales=10000, trend_rate=0.02, noise_std=500, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    dates = pd.date_range(start='2019-01-01', periods=n_months, freq='MS')\n",
    "    t = np.arange(n_months)\n",
    "    \n",
    "    # Tendance\n",
    "    trend = base_sales * (1 + trend_rate) ** t\n",
    "    \n",
    "    # Saisonnalit√© (pic en d√©cembre = mois 11)\n",
    "    seasonality = 2000 * np.sin(2 * np.pi * t / 12 - np.pi/2)  # Max au mois 12\n",
    "    \n",
    "    # Bruit\n",
    "    noise = np.random.normal(0, noise_std, n_months)\n",
    "    \n",
    "    # Ventes\n",
    "    sales = trend + seasonality + noise\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'sales': sales\n",
    "    })\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# G√©n√©rer\n",
    "sales_df = generate_monthly_sales(n_months=60)\n",
    "\n",
    "print(f\"Ventes g√©n√©r√©es: {len(sales_df)} mois\")\n",
    "print(sales_df.head())\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(sales_df.index, sales_df['sales'], marker='o', color='blue')\n",
    "plt.title('Ventes Mensuelles - 5 ans', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Ventes ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: D√©composition\n",
    "decomposition = seasonal_decompose(sales_df['sales'], model='additive', period=12)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "decomposition.observed.plot(ax=axes[0], title='Ventes Observ√©es', color='blue')\n",
    "decomposition.trend.plot(ax=axes[1], title='Tendance', color='green')\n",
    "decomposition.seasonal.plot(ax=axes[2], title='Saisonnalit√©', color='orange')\n",
    "decomposition.resid.plot(ax=axes[3], title='R√©sidus', color='red')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Test ADF\n",
    "result = adfuller(sales_df['sales'])\n",
    "print(f\"ADF Statistic: {result[0]:.6f}\")\n",
    "print(f\"p-value: {result[1]:.6f}\")\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"‚úÖ S√©rie stationnaire\")\n",
    "else:\n",
    "    print(\"‚ùå S√©rie non-stationnaire -> diff√©renciation n√©cessaire\")\n",
    "\n",
    "# Diff√©renciation\n",
    "sales_diff = sales_df['sales'].diff().dropna()\n",
    "result_diff = adfuller(sales_diff)\n",
    "print(f\"\\nApr√®s diff√©renciation:\")\n",
    "print(f\"ADF Statistic: {result_diff[0]:.6f}\")\n",
    "print(f\"p-value: {result_diff[1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: ACF et PACF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "plot_acf(sales_diff, lags=20, ax=axes[0])\n",
    "plot_pacf(sales_diff, lags=20, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: ARIMA\n",
    "train_size = int(len(sales_df) * 0.8)\n",
    "train = sales_df['sales'][:train_size]\n",
    "test = sales_df['sales'][train_size:]\n",
    "\n",
    "print(f\"Train: {len(train)} mois\")\n",
    "print(f\"Test: {len(test)} mois\")\n",
    "\n",
    "# ARIMA(1,1,1) bas√© sur ACF/PACF\n",
    "model = ARIMA(train, order=(1, 1, 1))\n",
    "fitted = model.fit()\n",
    "\n",
    "print(f\"\\nAIC: {fitted.aic:.2f}\")\n",
    "print(f\"BIC: {fitted.bic:.2f}\")\n",
    "\n",
    "# Pr√©dictions\n",
    "forecast = fitted.forecast(steps=len(test))\n",
    "\n",
    "# M√©triques\n",
    "rmse = np.sqrt(mean_squared_error(test, forecast))\n",
    "mape = np.mean(np.abs((test - forecast) / test)) * 100\n",
    "\n",
    "print(f\"\\n=== M√©triques ===\")\n",
    "print(f\"RMSE: {rmse:.2f}$\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(train.index, train, label='Train', color='blue')\n",
    "plt.plot(test.index, test, label='Test (R√©el)', color='green', marker='o')\n",
    "plt.plot(test.index, forecast, label='Pr√©dictions ARIMA', color='red', linestyle='--', marker='x')\n",
    "plt.axvline(train.index[-1], color='black', linestyle=':', label='Train/Test Split')\n",
    "plt.title('Pr√©diction Ventes Mensuelles - ARIMA(1,1,1)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Ventes ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 2 : Forecasting de Consommation √âlectrique avec LSTM\n",
    "\n",
    "**Contexte :** Pr√©dire la consommation √©lectrique horaire en utilisant des features multivari√©es.\n",
    "\n",
    "**Objectifs :**\n",
    "1. Cr√©er une s√©rie multivari√©e (consommation, temp√©rature, heure du jour, jour de la semaine)\n",
    "2. Feature engineering (lags, rolling stats)\n",
    "3. Cr√©er des sliding windows\n",
    "4. Entra√Æner un LSTM\n",
    "5. √âvaluer les pr√©dictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: G√©n√©rer donn√©es horaires sur 30 jours\n",
    "# Features:\n",
    "# - temperature: 15-30¬∞C avec variation journali√®re\n",
    "# - hour: 0-23\n",
    "# - weekday: 0-6\n",
    "# - consumption: corr√©l√©e avec temp√©rature et heure (pic en journ√©e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering et Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cr√©er features suppl√©mentaires\n",
    "# - Lags de consommation (lag 1, 24, 168)\n",
    "# - Rolling mean (fen√™tre 24h)\n",
    "# - Features cycliques pour hour (sin/cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Multivari√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cr√©er sliding windows avec features multivari√©es\n",
    "# TODO: Normaliser les donn√©es\n",
    "# TODO: D√©finir architecture LSTM\n",
    "# TODO: Entra√Æner\n",
    "# TODO: √âvaluer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SOLUTION Exercice 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: G√©n√©ration donn√©es\n",
    "def generate_energy_consumption(n_hours=30*24, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    dates = pd.date_range(start='2024-01-01', periods=n_hours, freq='H')\n",
    "    \n",
    "    # Features temporelles\n",
    "    hour = dates.hour\n",
    "    weekday = dates.weekday\n",
    "    \n",
    "    # Temp√©rature (variation journali√®re + bruit)\n",
    "    t = np.arange(n_hours)\n",
    "    temp = 20 + 5 * np.sin(2 * np.pi * t / 24 - np.pi/2) + np.random.normal(0, 1, n_hours)\n",
    "    \n",
    "    # Consommation (d√©pend de heure et temp√©rature)\n",
    "    # Base + pic journ√©e (8h-20h) + effet temp√©rature + weekend\n",
    "    consumption_base = 5000\n",
    "    hour_effect = 1500 * np.sin(2 * np.pi * (hour - 6) / 24)\n",
    "    hour_effect[hour_effect < 0] = 0\n",
    "    temp_effect = 50 * (temp - 20)\n",
    "    weekend_effect = -500 * ((weekday == 5) | (weekday == 6))\n",
    "    noise = np.random.normal(0, 200, n_hours)\n",
    "    \n",
    "    consumption = consumption_base + hour_effect + temp_effect + weekend_effect + noise\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'temperature': temp,\n",
    "        'hour': hour,\n",
    "        'weekday': weekday,\n",
    "        'consumption': consumption\n",
    "    })\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "energy_df = generate_energy_consumption(n_hours=30*24)\n",
    "\n",
    "print(f\"Donn√©es g√©n√©r√©es: {len(energy_df)} heures\")\n",
    "print(energy_df.head(10))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "axes[0].plot(energy_df.index, energy_df['consumption'], color='blue', linewidth=0.8)\n",
    "axes[0].set_title('Consommation √âlectrique Horaire', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Consommation (kWh)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(energy_df.index, energy_df['temperature'], color='red', linewidth=0.8)\n",
    "axes[1].set_title('Temp√©rature', fontsize=12)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Temp√©rature (¬∞C)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Feature Engineering\n",
    "df_features = energy_df.copy()\n",
    "\n",
    "# Lags\n",
    "df_features['consumption_lag1'] = df_features['consumption'].shift(1)\n",
    "df_features['consumption_lag24'] = df_features['consumption'].shift(24)\n",
    "\n",
    "# Rolling mean (24h)\n",
    "df_features['consumption_rolling_mean_24'] = df_features['consumption'].rolling(window=24).mean()\n",
    "\n",
    "# Features cycliques pour heure\n",
    "df_features['hour_sin'] = np.sin(2 * np.pi * df_features['hour'] / 24)\n",
    "df_features['hour_cos'] = np.cos(2 * np.pi * df_features['hour'] / 24)\n",
    "\n",
    "# Supprimer NaN\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "print(f\"Features cr√©√©es: {df_features.shape[1]} colonnes\")\n",
    "print(df_features.columns.tolist())\n",
    "print(f\"\\nDonn√©es apr√®s feature engineering: {len(df_features)} heures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Pr√©paration pour LSTM\n",
    "# Features: toutes sauf consumption (target)\n",
    "feature_cols = ['temperature', 'hour', 'weekday', 'consumption_lag1', \n",
    "                'consumption_lag24', 'consumption_rolling_mean_24', \n",
    "                'hour_sin', 'hour_cos']\n",
    "\n",
    "X = df_features[feature_cols].values\n",
    "y = df_features['consumption'].values\n",
    "\n",
    "# Train/Val/Test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_val = X[train_size:train_size+val_size]\n",
    "y_val = y[train_size:train_size+val_size]\n",
    "X_test = X[train_size+val_size:]\n",
    "y_test = y[train_size+val_size:]\n",
    "\n",
    "# Normalisation\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Sliding windows\n",
    "def create_sequences_multivariate(X, y, window_size):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - window_size):\n",
    "        X_seq.append(X[i:i+window_size])\n",
    "        y_seq.append(y[i+window_size])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "WINDOW_SIZE = 24  # 24h\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences_multivariate(X_train_scaled, y_train_scaled, WINDOW_SIZE)\n",
    "X_val_seq, y_val_seq = create_sequences_multivariate(X_val_scaled, y_val_scaled, WINDOW_SIZE)\n",
    "X_test_seq, y_test_seq = create_sequences_multivariate(X_test_scaled, y_test_scaled, WINDOW_SIZE)\n",
    "\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}  # (samples, window, features)\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "\n",
    "# Tensors\n",
    "X_train_t = torch.FloatTensor(X_train_seq)\n",
    "y_train_t = torch.FloatTensor(y_train_seq)\n",
    "X_val_t = torch.FloatTensor(X_val_seq)\n",
    "y_val_t = torch.FloatTensor(y_val_seq)\n",
    "X_test_t = torch.FloatTensor(X_test_seq)\n",
    "y_test_t = torch.FloatTensor(y_test_seq)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Mod√®le LSTM Multivari√©\n",
    "class LSTMMultivariate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super(LSTMMultivariate, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_output)\n",
    "        return output.squeeze()\n",
    "\n",
    "model = LSTMMultivariate(input_size=len(feature_cols), hidden_size=64, num_layers=2).to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nParam√®tres: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Entra√Ænement\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train: {train_loss:.6f}, Val: {val_loss:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: √âvaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actuals.extend(batch_y.numpy())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "\n",
    "# Inverse transform\n",
    "predictions_orig = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "actuals_orig = scaler_y.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "\n",
    "# M√©triques\n",
    "mae = mean_absolute_error(actuals_orig, predictions_orig)\n",
    "rmse = np.sqrt(mean_squared_error(actuals_orig, predictions_orig))\n",
    "mape = np.mean(np.abs((actuals_orig - predictions_orig) / actuals_orig)) * 100\n",
    "\n",
    "print(\"\\n=== M√©triques LSTM Multivari√© ===\")\n",
    "print(f\"MAE:  {mae:.2f} kWh\")\n",
    "print(f\"RMSE: {rmse:.2f} kWh\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(actuals_orig[:200], label='R√©el', color='green', linewidth=1.5)\n",
    "plt.plot(predictions_orig[:200], label='Pr√©dictions LSTM', color='red', linestyle='--', alpha=0.8)\n",
    "plt.title('Pr√©diction Consommation √âlectrique - LSTM Multivari√©', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Heures')\n",
    "plt.ylabel('Consommation (kWh)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 3 : D√©tection d'Anomalies dans une S√©rie Temporelle\n",
    "\n",
    "**Contexte :** Monitorer des transactions bancaires et d√©tecter les fraudes.\n",
    "\n",
    "**Objectifs :**\n",
    "1. G√©n√©rer s√©rie de transactions avec anomalies inject√©es\n",
    "2. Entra√Æner LSTM pour pr√©dire transactions normales\n",
    "3. D√©tecter anomalies via erreur de pr√©diction\n",
    "4. √âvaluer avec pr√©cision/rappel si labels disponibles\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SOLUTION Exercice 3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: G√©n√©ration avec anomalies\n",
    "def generate_transactions_with_anomalies(n=1000, anomaly_rate=0.05, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Transactions normales (pattern journalier + bruit)\n",
    "    t = np.arange(n)\n",
    "    base = 1000\n",
    "    daily_pattern = 300 * np.sin(2 * np.pi * t / 24)\n",
    "    noise = np.random.normal(0, 50, n)\n",
    "    transactions = base + daily_pattern + noise\n",
    "    \n",
    "    # Injecter anomalies (valeurs extr√™mes)\n",
    "    n_anomalies = int(n * anomaly_rate)\n",
    "    anomaly_indices = np.random.choice(n, size=n_anomalies, replace=False)\n",
    "    \n",
    "    labels = np.zeros(n)\n",
    "    for idx in anomaly_indices:\n",
    "        # Anomalie = transaction 3-5x sup√©rieure √† la normale\n",
    "        transactions[idx] *= np.random.uniform(3, 5)\n",
    "        labels[idx] = 1\n",
    "    \n",
    "    dates = pd.date_range(start='2024-01-01', periods=n, freq='H')\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'amount': transactions,\n",
    "        'is_anomaly': labels\n",
    "    })\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "transactions_df = generate_transactions_with_anomalies(n=1000, anomaly_rate=0.05)\n",
    "\n",
    "print(f\"Transactions g√©n√©r√©es: {len(transactions_df)}\")\n",
    "print(f\"Anomalies: {transactions_df['is_anomaly'].sum()} ({100*transactions_df['is_anomaly'].mean():.1f}%)\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 6))\n",
    "normal = transactions_df[transactions_df['is_anomaly'] == 0]\n",
    "anomalies = transactions_df[transactions_df['is_anomaly'] == 1]\n",
    "\n",
    "plt.plot(normal.index, normal['amount'], color='blue', linewidth=0.8, label='Normal')\n",
    "plt.scatter(anomalies.index, anomalies['amount'], color='red', s=100, marker='x', \n",
    "           label=f'Anomalies ({len(anomalies)})', zorder=5)\n",
    "plt.title('Transactions Bancaires avec Anomalies', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Montant ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Pr√©paration donn√©es\n",
    "data = transactions_df['amount'].values\n",
    "labels = transactions_df['is_anomaly'].values\n",
    "\n",
    "# Normalisation\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Windows\n",
    "WINDOW_SIZE = 24\n",
    "\n",
    "def create_sequences(data, labels, window_size):\n",
    "    X, y, y_labels = [], [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "        y_labels.append(labels[i+window_size])\n",
    "    return np.array(X), np.array(y), np.array(y_labels)\n",
    "\n",
    "X, y, y_labels = create_sequences(data_scaled, labels, WINDOW_SIZE)\n",
    "\n",
    "# Split (70/30)\n",
    "train_size = int(len(X) * 0.7)\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_test = y[train_size:]\n",
    "y_test_labels = y_labels[train_size:]\n",
    "\n",
    "# Tensors\n",
    "X_train_t = torch.FloatTensor(X_train).unsqueeze(-1)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_test_t = torch.FloatTensor(X_test).unsqueeze(-1)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: LSTM pour d√©tection\n",
    "class LSTMAnomaly(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=1):\n",
    "        super(LSTMAnomaly, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output.squeeze()\n",
    "\n",
    "anomaly_model = LSTMAnomaly(input_size=1, hidden_size=32).to(device)\n",
    "\n",
    "# Entra√Ænement\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(anomaly_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    anomaly_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        outputs = anomaly_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: D√©tection d'anomalies\n",
    "anomaly_model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        outputs = anomaly_model(batch_X)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actuals.extend(batch_y.numpy())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "\n",
    "# Erreur de pr√©diction\n",
    "errors = np.abs(actuals - predictions)\n",
    "\n",
    "# Seuil (moyenne + 3 std)\n",
    "threshold = errors.mean() + 3 * errors.std()\n",
    "detected_anomalies = errors > threshold\n",
    "\n",
    "print(f\"\\n=== D√©tection d'Anomalies ===\")\n",
    "print(f\"Seuil: {threshold:.6f}\")\n",
    "print(f\"Anomalies d√©tect√©es: {detected_anomalies.sum()} / {len(detected_anomalies)}\")\n",
    "print(f\"Vraies anomalies (labels): {y_test_labels.sum()}\")\n",
    "\n",
    "# √âvaluation si labels disponibles\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "precision = precision_score(y_test_labels, detected_anomalies)\n",
    "recall = recall_score(y_test_labels, detected_anomalies)\n",
    "f1 = f1_score(y_test_labels, detected_anomalies)\n",
    "\n",
    "print(f\"\\nPr√©cision: {precision:.3f}\")\n",
    "print(f\"Rappel:    {recall:.3f}\")\n",
    "print(f\"F1-Score:  {f1:.3f}\")\n",
    "\n",
    "print(f\"\\nMatrice de Confusion:\")\n",
    "print(confusion_matrix(y_test_labels, detected_anomalies))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(actuals, label='R√©el', color='blue', linewidth=1)\n",
    "plt.plot(predictions, label='Pr√©dictions', color='green', linestyle='--', alpha=0.7)\n",
    "true_anomalies = np.where(y_test_labels == 1)[0]\n",
    "detected = np.where(detected_anomalies)[0]\n",
    "plt.scatter(true_anomalies, actuals[true_anomalies], color='red', s=100, marker='o', \n",
    "           label='Vraies anomalies', zorder=5)\n",
    "plt.scatter(detected, actuals[detected], color='orange', s=50, marker='x', \n",
    "           label='D√©tect√©es', zorder=4)\n",
    "plt.title('D√©tection d\\'Anomalies - Transactions', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Montant (normalis√©)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(errors, color='purple', alpha=0.7)\n",
    "plt.axhline(threshold, color='red', linestyle='--', linewidth=2, label=f'Seuil = {threshold:.4f}')\n",
    "plt.scatter(detected, errors[detected], color='red', s=50, marker='x', zorder=5)\n",
    "plt.title('Erreurs de Pr√©diction', fontsize=12)\n",
    "plt.xlabel('Pas de temps')\n",
    "plt.ylabel('Erreur absolue')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion des Exercices\n",
    "\n",
    "### Exercice 1 - ARIMA\n",
    "- D√©composition et test de stationnarit√© essentiels\n",
    "- Diff√©renciation pour stationnariser\n",
    "- S√©lection de (p,d,q) via ACF/PACF\n",
    "- ARIMA efficace pour s√©ries univari√©es avec patterns lin√©aires\n",
    "\n",
    "### Exercice 2 - LSTM Multivari√©\n",
    "- Feature engineering crucial (lags, rolling stats, features cycliques)\n",
    "- LSTM capte d√©pendances complexes entre features\n",
    "- Normalisation et windowing appropri√©s\n",
    "- Excellent pour forecasting avec variables exog√®nes\n",
    "\n",
    "### Exercice 3 - D√©tection d'Anomalies\n",
    "- Mod√®le entra√Æn√© sur patterns normaux\n",
    "- Anomalies = erreurs de pr√©diction √©lev√©es\n",
    "- Seuil bas√© sur distribution des erreurs\n",
    "- Trade-off pr√©cision/rappel selon seuil\n",
    "\n",
    "**Points cl√©s g√©n√©raux :**\n",
    "- Toujours visualiser les donn√©es d'abord\n",
    "- Time series split (jamais CV classique)\n",
    "- Normalisation sur train uniquement\n",
    "- M√©triques appropri√©es (MAE, RMSE, MAPE)\n",
    "- Choisir mod√®le selon nature des donn√©es"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}