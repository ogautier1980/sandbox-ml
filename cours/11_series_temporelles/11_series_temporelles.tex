% Chapitre 11 - S√©ries Temporelles et Forecasting
% Cours Machine Learning - Sandbox-ML

\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}

% Math√©matiques
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

% Mise en page
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}

% Graphiques et couleurs
\usepackage{graphicx}
\usepackage{xcolor}

% ===== UNICODE CHARACTERS SUPPORT =====
\usepackage{newunicodechar}

% Emojis et symboles
\newunicodechar{‚úÖ}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚ùå}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚úì}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚úó}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚ö†}{\textcolor{orange!80!black}{\textbf{/!\textbackslash}}}
\newunicodechar{üí°}{\textcolor{blue!70!black}{\textbf{(i)}}}
\newunicodechar{üéØ}{\textcolor{purple!70!black}{\textbf{$\star$}}}
\newunicodechar{üìä}{\textcolor{blue!70!black}{\textbf{[=]}}}

% √âtoiles (pour tableaux)
\newunicodechar{‚òÖ}{\textcolor{orange!80!black}{$\star$}}
\newunicodechar{‚òÜ}{\textcolor{gray!50}{$\star$}}

% Fl√®ches
\newunicodechar{‚Üí}{$\rightarrow$}
\newunicodechar{‚Üê}{$\leftarrow$}
\newunicodechar{‚Üë}{$\uparrow$}
\newunicodechar{‚Üì}{$\downarrow$}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric}

% Tableaux
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}

% Code et algorithmes
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Hyperliens
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 11 - S√©ries Temporelles et Forecasting},
    pdfauthor={Cours ML},
}

% Boxes color√©es
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}


% ===== TCOLORBOX AVEC EMOJIS =====
\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö† Attention,
    breakable
}

\newtcolorbox{definition}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition,
    breakable
}

\newtcolorbox{astuce}{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

\newtcolorbox{remarque}{
    colback=yellow!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=üí° Remarque,
    breakable
}

\newtcolorbox{important}{
    colback=purple!5!white,
    colframe=purple!75!black,
    fonttitle=\bfseries,
    title=‚ö† Important,
    breakable
}

\newtcolorbox{exemple}{
    colback=gray!5!white,
    colframe=gray!75!black,
    fonttitle=\bfseries,
    title=Exemple,
    breakable
}

% En-t√™tes et pieds de page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 12 - S√©ries Temporelles et Forecasting}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIGURATION LISTINGS (code Python) =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}
\lstset{style=pythonstyle}

% ===== CONFIGURATION TCOLORBOX =====


\newtcolorbox{theoreme}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Th√©or√®me: #1,
    breakable
}







% ===== COMMANDES PERSONNALIS√âES =====
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}

% ===== D√âBUT DU DOCUMENT =====
\begin{document}

% ===== PAGE DE TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]

    \vspace{1cm}

    {\LARGE Chapitre 12}\\[0.3cm]
    {\LARGE\bfseries S√©ries Temporelles et Forecasting}\\[2cm]

    \vfill

    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Comprendre les composantes et propri√©t√©s des s√©ries temporelles
        \item Ma√Ætriser les mod√®les classiques (ARIMA, SARIMA, Prophet)
        \item Appliquer le Deep Learning au forecasting (LSTM, GRU, Transformers)
        \item √âvaluer et valider les pr√©dictions temporelles
        \item Impl√©menter des pipelines de pr√©diction robustes
    \end{itemize}
    }

    \vfill

    {\large
    \textbf{Pr√©requis :} Chapitres 06 (R√©seaux de Neurones), 08 (RNN/LSTM)\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 6-8 heures\\[0.3cm]
    \textbf{Notebooks :} \texttt{11\_demo\_*.ipynb}, \texttt{11\_exercices.ipynb}
    }

    \vfill

    {\large Cours ML - Sandbox-ML\\
    Version 1.0 - 2026}
\end{titlepage}

% ===== TABLE DES MATI√àRES =====
\tableofcontents
\newpage

% ===== SECTION 1: MOTIVATION =====
\section{Motivation}

Les s√©ries temporelles sont omnipr√©sentes dans notre monde : cours boursiers, m√©t√©orologie, trafic r√©seau, consommation d'√©nergie, ventes de produits, signaux biologiques, etc. La capacit√© √† analyser et pr√©dire ces donn√©es s√©quentielles est cruciale pour de nombreuses applications industrielles et scientifiques.

\begin{exemple}{Probl√®mes de forecasting r√©els}
\begin{itemize}
    \item \textbf{Finance :} Pr√©dire le prix d'une action dans les prochains jours
    \item \textbf{√ânergie :} Anticiper la demande √©lectrique pour optimiser la production
    \item \textbf{E-commerce :} Pr√©voir les ventes pour g√©rer les stocks
    \item \textbf{Sant√© :} Monitorer les signes vitaux et d√©tecter les anomalies
    \item \textbf{M√©t√©o :} Pr√©dire la temp√©rature et les pr√©cipitations
\end{itemize}
\end{exemple}

Contrairement aux probl√®mes de ML classiques o√π les observations sont suppos√©es i.i.d. (ind√©pendantes et identiquement distribu√©es), les s√©ries temporelles pr√©sentent une \textbf{d√©pendance temporelle} : la valeur √† l'instant $t$ d√©pend des valeurs pass√©es. Cette propri√©t√© fondamentale n√©cessite des approches sp√©cifiques.

\subsection{Diff√©rences avec le ML classique}

\begin{table}[h]
\centering
\caption{ML Classique vs S√©ries Temporelles}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{ML Classique} & \textbf{S√©ries Temporelles} \\
\midrule
Structure & Observations i.i.d. & D√©pendance temporelle \\
Validation & Cross-validation & Time series split \\
Features & Donn√©es tabulaires & S√©quences temporelles \\
Objectif & Classification/R√©gression & Forecasting/D√©tection \\
Train/Test Split & Al√©atoire & Chronologique \\
\bottomrule
\end{tabular}
\end{table}

% ===== SECTION 2: FONDEMENTS TH√âORIQUES =====
\section{Fondements Th√©oriques}

\subsection{D√©finition et Notation}

\begin{definition}{S√©rie Temporelle}
Une s√©rie temporelle est une s√©quence d'observations $\{y_t\}$ index√©es par le temps $t$, o√π $t \in \{1, 2, \ldots, T\}$ pour des donn√©es discr√®tes.

\[
\{y_1, y_2, y_3, \ldots, y_T\}
\]

\textbf{Notation :}
\begin{itemize}
    \item $y_t$ : valeur observ√©e au temps $t$
    \item $\hat{y}_{t+h}$ : pr√©diction √† l'horizon $h$ (forecast)
    \item $T$ : longueur de la s√©rie (nombre d'observations)
    \item $h$ : horizon de pr√©diction (forecast horizon)
\end{itemize}
\end{definition}

\subsection{Composantes d'une S√©rie Temporelle}

Une s√©rie temporelle peut g√©n√©ralement √™tre d√©compos√©e en plusieurs composantes :

\begin{definition}{D√©composition Additive}
\[
y_t = T_t + S_t + R_t
\]

o√π :
\begin{itemize}
    \item $T_t$ : \textbf{Tendance (Trend)} - mouvement √† long terme (croissance, d√©croissance)
    \item $S_t$ : \textbf{Saisonnalit√© (Seasonality)} - motifs p√©riodiques r√©p√©titifs
    \item $R_t$ : \textbf{R√©sidu (Residual)} - fluctuations al√©atoires (bruit)
\end{itemize}
\end{definition}

\begin{definition}{D√©composition Multiplicative}
\[
y_t = T_t \times S_t \times R_t
\]

Utilis√©e quand la variance augmente avec le niveau de la s√©rie (croissance exponentielle).
\end{definition}

\begin{exemple}{Ventes mensuelles d'un produit}
\begin{itemize}
    \item \textbf{Tendance :} Croissance annuelle de 10\% (expansion du march√©)
    \item \textbf{Saisonnalit√© :} Pic en d√©cembre (f√™tes de fin d'ann√©e)
    \item \textbf{R√©sidu :} Variations al√©atoires dues √† la m√©t√©o, promotions
\end{itemize}
\end{exemple}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.9]
    % Original Time Series
    \begin{scope}
        \draw[->] (0,0) -- (10,0) node[right, font=\footnotesize] {Temps};
        \draw[->] (0,0) -- (0,2.5) node[above, font=\footnotesize] {$y_t$};

        % Combined signal (Trend + Seasonality + Noise)
        \draw[blue, very thick, smooth] plot[domain=0:9.5, samples=100]
            (\x, {0.8 + 0.12*\x + 0.3*sin(\x*60*2) + 0.08*sin(\x*180)});

        \node[left, font=\small\bfseries, blue] at (0, 1.2) {S√©rie Originale};
        \node[right, font=\tiny] at (10, 1.5) {$y_t = T_t + S_t + R_t$};

        % Grid
        \draw[gray!20, very thin] (0,0) grid[step=0.5] (10,2.5);
    \end{scope}

    % Decomposition arrow
    \draw[->, very thick, red!70!black] (5, -0.8) -- (5, -1.5);
    \node[font=\small, red!70!black] at (5, -1.15) {D√©composition};

    % Trend Component
    \begin{scope}[yshift=-3.5cm]
        \draw[->] (0,0) -- (10,0) node[right, font=\footnotesize] {Temps};
        \draw[->] (0,0) -- (0,2.5) node[above, font=\footnotesize] {$T_t$};

        % Linear trend
        \draw[green!60!black, very thick, smooth] plot[domain=0:9.5, samples=50]
            (\x, {0.8 + 0.12*\x});

        \node[left, font=\small\bfseries, green!60!black] at (0, 1.2) {Tendance (Trend)};
        \node[right, font=\tiny, align=left] at (10, 1.5) {Mouvement\\long terme};

        \draw[gray!20, very thin] (0,0) grid[step=0.5] (10,2.5);
    \end{scope}

    % Seasonality Component
    \begin{scope}[yshift=-7cm]
        \draw[->] (0,0) -- (10,0) node[right, font=\footnotesize] {Temps};
        \draw[->] (0,-1.2) -- (0,1.2) node[above, font=\footnotesize] {$S_t$};

        % Seasonal pattern (sine wave)
        \draw[orange!80!black, very thick, smooth] plot[domain=0:9.5, samples=100]
            (\x, {0.3*sin(\x*60*2)});

        % Zero line
        \draw[dashed, gray] (0,0) -- (10,0);

        \node[left, font=\small\bfseries, orange!80!black] at (0, 0.6) {Saisonnalit√©};
        \node[right, font=\tiny, align=left] at (10, 0.6) {Motifs\\p√©riodiques};

        \draw[gray!20, very thin] (0,-1.2) grid[step=0.5] (10,1.2);

        % Period annotation
        \draw[<->, thick, purple] (0.5, -1.5) -- (4.7, -1.5);
        \node[below, font=\tiny, purple] at (2.6, -1.5) {P√©riode $s$};
    \end{scope}

    % Residual Component
    \begin{scope}[yshift=-10.5cm]
        \draw[->] (0,0) -- (10,0) node[right, font=\footnotesize] {Temps};
        \draw[->] (0,-0.8) -- (0,0.8) node[above, font=\footnotesize] {$R_t$};

        % Random noise
        \draw[red!70!black, thick, smooth] plot[domain=0:9.5, samples=100]
            (\x, {0.08*sin(\x*180) + 0.05*sin(\x*250)});

        % Zero line
        \draw[dashed, gray] (0,0) -- (10,0);

        \node[left, font=\small\bfseries, red!70!black] at (0, 0.4) {R√©sidu (Bruit)};
        \node[right, font=\tiny, align=left] at (10, 0.4) {Fluctuations\\al√©atoires};

        \draw[gray!20, very thin] (0,-0.8) grid[step=0.5] (10,0.8);
    \end{scope}

    % Equation annotation
    \node[draw, rectangle, fill=yellow!10, font=\small, align=center] at (12, -5.25) {
        \textbf{D√©composition}\\
        \textbf{Additive:}\\[0.2cm]
        $y_t = T_t + S_t + R_t$
    };

\end{tikzpicture}
\caption{D√©composition additive d'une s√©rie temporelle. \textbf{S√©rie originale} (haut): combinaison de trois composantes. \textbf{Tendance $T_t$}: mouvement √† long terme (ici croissance lin√©aire). \textbf{Saisonnalit√© $S_t$}: motifs p√©riodiques r√©p√©titifs (oscillations autour de 0). \textbf{R√©sidu $R_t$}: fluctuations al√©atoires (bruit blanc). La d√©composition permet d'isoler chaque composante pour mieux comprendre les dynamiques sous-jacentes de la s√©rie et faciliter la pr√©vision.}
\label{fig:time_series_decomposition}
\end{figure}

\clearpage

\subsection{Stationnarit√©}

La stationnarit√© est une propri√©t√© fondamentale qui simplifie grandement l'analyse et la mod√©lisation.

\begin{definition}{Stationnarit√© Forte}
Une s√©rie $\{y_t\}$ est strictement stationnaire si sa distribution jointe est invariante par translation temporelle :

\[
P(y_{t_1}, y_{t_2}, \ldots, y_{t_k}) = P(y_{t_1+h}, y_{t_2+h}, \ldots, y_{t_k+h})
\]

pour tout $h$ et tous indices $t_1, \ldots, t_k$.
\end{definition}

\begin{definition}{Stationnarit√© Faible (du Second Ordre)}
Une s√©rie $\{y_t\}$ est faiblement stationnaire si :
\begin{enumerate}
    \item $\E[y_t] = \mu$ est constante (moyenne constante)
    \item $\Var(y_t) = \sigma^2$ est constante (variance constante)
    \item $\Cov(y_t, y_{t+h}) = \gamma(h)$ ne d√©pend que de $h$ (autocovariance stationnaire)
\end{enumerate}
\end{definition}

\begin{attention}
La plupart des mod√®les classiques (ARIMA) supposent la stationnarit√©. Il faut donc transformer les s√©ries non-stationnaires avant de les mod√©liser.
\end{attention}

\subsubsection{Techniques de Stationnarisation}

\begin{enumerate}
    \item \textbf{Diff√©renciation :} $\nabla y_t = y_t - y_{t-1}$ (√©limine la tendance)
    \item \textbf{Transformation log :} $\log(y_t)$ (stabilise la variance)
    \item \textbf{Diff√©rence saisonni√®re :} $\nabla_s y_t = y_t - y_{t-s}$ avec $s$ = p√©riode saisonni√®re
    \item \textbf{D√©trending :} Soustraire une tendance estim√©e (lin√©aire, polynomiale)
\end{enumerate}

\subsection{Autocorr√©lation et Autocorr√©lation Partielle}

\begin{definition}{Fonction d'Autocorr√©lation (ACF)}
Mesure la corr√©lation lin√©aire entre $y_t$ et $y_{t-k}$ (lag $k$) :

\[
\rho(k) = \frac{\Cov(y_t, y_{t-k})}{\sqrt{\Var(y_t) \Var(y_{t-k})}} = \frac{\gamma(k)}{\gamma(0)}
\]

o√π $\gamma(k) = \E[(y_t - \mu)(y_{t-k} - \mu)]$ est l'autocovariance au lag $k$.
\end{definition}

\begin{definition}{Fonction d'Autocorr√©lation Partielle (PACF)}
Mesure la corr√©lation entre $y_t$ et $y_{t-k}$ apr√®s avoir √©limin√© l'influence des lags interm√©diaires $1, 2, \ldots, k-1$.

Utilis√©e pour identifier l'ordre $p$ d'un mod√®le AR$(p)$.
\end{definition}

\begin{astuce}
\textbf{Interpr√©tation ACF/PACF :}
\begin{itemize}
    \item ACF d√©cro√Æt lentement : s√©rie non-stationnaire (tendance)
    \item Pic significatif √† lag $s$ : saisonnalit√© de p√©riode $s$
    \item PACF coupe apr√®s lag $p$ : mod√®le AR$(p)$ appropri√©
    \item ACF coupe apr√®s lag $q$ : mod√®le MA$(q)$ appropri√©
\end{itemize}
\end{astuce}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.85]
    % ACF Plot for AR(2) process
    \begin{scope}
        % Title
        \node[font=\small\bfseries] at (5, 3.5) {ACF - Processus AR(2)};

        % Axes
        \draw[->] (0,0) -- (10.5,0) node[right, font=\footnotesize] {Lag $k$};
        \draw[->] (0,-1.5) -- (0,1.5) node[above, font=\footnotesize] {$\rho(k)$};

        % Confidence bands
        \fill[blue!10, opacity=0.5] (0,0.3) rectangle (10,-0.3);
        \draw[dashed, blue, thin] (0,0.3) -- (10,0.3) node[right, font=\tiny] {$+1.96/\sqrt{n}$};
        \draw[dashed, blue, thin] (0,-0.3) -- (10,-0.3) node[right, font=\tiny] {$-1.96/\sqrt{n}$};
        \draw[gray] (0,0) -- (10,0);

        % ACF bars (exponentially decaying)
        \foreach \x/\h in {1/1.2, 2/0.9, 3/0.6, 4/0.4, 5/0.25, 6/0.15, 7/0.08, 8/0.04, 9/0.02, 10/0.01} {
            \draw[fill=red!60, thick] (\x, 0) -- (\x, \h);
        }

        % Grid
        \draw[gray!20, very thin] (0,-1.5) grid[xstep=1, ystep=0.5] (10,1.5);

        % Lag labels
        \foreach \x in {0,2,4,6,8,10}
            \node[below, font=\tiny] at (\x, 0) {\x};

        % Annotation
        \node[font=\tiny, align=center, fill=yellow!20, draw, dashed] at (7, -1.2) {D√©croissance\\exponentielle};
        \draw[->, dashed] (7, -1) -- (5, -0.5);

    \end{scope}

    % PACF Plot for AR(2) process
    \begin{scope}[yshift=-5.5cm]
        % Title
        \node[font=\small\bfseries] at (5, 3.5) {PACF - Processus AR(2)};

        % Axes
        \draw[->] (0,0) -- (10.5,0) node[right, font=\footnotesize] {Lag $k$};
        \draw[->] (0,-1.5) -- (0,1.5) node[above, font=\footnotesize] {$\phi_{kk}$};

        % Confidence bands
        \fill[blue!10, opacity=0.5] (0,0.3) rectangle (10,-0.3);
        \draw[dashed, blue, thin] (0,0.3) -- (10,0.3);
        \draw[dashed, blue, thin] (0,-0.3) -- (10,-0.3);
        \draw[gray] (0,0) -- (10,0);

        % PACF bars (cut-off after lag 2 for AR(2))
        \draw[fill=green!60, thick] (1, 0) -- (1, 1.0);
        \draw[fill=green!60, thick] (2, 0) -- (2, 0.6);
        % Insignificant lags
        \foreach \x in {3,4,5,6,7,8,9,10} {
            \draw[fill=gray!30, thick] (\x, 0) -- (\x, 0.05);
        }

        % Grid
        \draw[gray!20, very thin] (0,-1.5) grid[xstep=1, ystep=0.5] (10,1.5);

        % Lag labels
        \foreach \x in {0,2,4,6,8,10}
            \node[below, font=\tiny] at (\x, 0) {\x};

        % Annotations
        \node[font=\tiny, align=center, fill=yellow!20, draw, dashed] at (2.5, -1.2) {Coupure\\apr√®s lag 2};
        \draw[->, dashed] (2.5, -1) -- (2, -0.5);

        \draw[<->, very thick, purple] (0.5, -1.7) -- (2.5, -1.7);
        \node[below, font=\tiny, purple] at (1.5, -1.7) {$p = 2$};

    \end{scope}

    % Legend and interpretation
    \node[draw, rectangle, fill=gray!10, minimum width=3.5cm, minimum height=3cm, font=\tiny, align=left] at (13, -1.25) {
        \textbf{Identification:}\\[0.2cm]
        \textbf{AR(p):}\\
        ‚Ä¢ ACF: d√©croissance\\
        ‚Ä¢ PACF: coupe √† $p$\\[0.2cm]
        \textbf{MA(q):}\\
        ‚Ä¢ ACF: coupe √† $q$\\
        ‚Ä¢ PACF: d√©croissance\\[0.2cm]
        \textbf{ARMA(p,q):}\\
        ‚Ä¢ ACF: d√©croissance\\
        ‚Ä¢ PACF: d√©croissance
    };

\end{tikzpicture}
\caption{Correlograms ACF et PACF pour un processus AR(2). \textbf{ACF (haut):} d√©croissance exponentielle vers 0, caract√©ristique des processus autor√©gressifs. \textbf{PACF (bas):} coupure nette apr√®s lag $p=2$, indiquant un mod√®le AR(2). Les bandes en bleu repr√©sentent l'intervalle de confiance √† 95\% ($\pm 1.96/\sqrt{n}$). Les valeurs √† l'int√©rieur de ces bandes sont consid√©r√©es comme non significativement diff√©rentes de 0. Ce pattern ACF/PACF permet d'identifier l'ordre optimal du mod√®le ARIMA.}
\label{fig:acf_pacf_correlograms}
\end{figure}

\clearpage

\subsection{Tests de Stationnarit√©}

\begin{definition}{Test de Dickey-Fuller Augment√© (ADF)}
Test statistique pour d√©tecter la pr√©sence d'une racine unitaire (non-stationnarit√©).

\textbf{Hypoth√®ses :}
\begin{itemize}
    \item $H_0$ : La s√©rie poss√®de une racine unitaire (non-stationnaire)
    \item $H_1$ : La s√©rie est stationnaire
\end{itemize}

\textbf{D√©cision :}
\begin{itemize}
    \item Si $p$-value $< 0.05$ : rejeter $H_0$ $\Rightarrow$ s√©rie stationnaire
    \item Si $p$-value $\geq 0.05$ : ne pas rejeter $H_0$ $\Rightarrow$ s√©rie non-stationnaire
\end{itemize}
\end{definition}

\begin{lstlisting}[language=Python, caption=Test ADF avec statsmodels]
from statsmodels.tsa.stattools import adfuller

# Test de stationnarite
result = adfuller(timeseries)
adf_statistic = result[0]
p_value = result[1]

print(f"ADF Statistic: {adf_statistic:.4f}")
print(f"p-value: {p_value:.4f}")

if p_value < 0.05:
    print("Serie stationnaire (rejeter H0)")
else:
    print("Serie non-stationnaire (ne pas rejeter H0)")
\end{lstlisting}

% ===== SECTION 3: MOD√àLES CLASSIQUES =====
\section{Mod√®les Classiques de S√©ries Temporelles}

\subsection{Mod√®le AutoR√©gressif AR(p)}

\begin{definition}{Mod√®le AR(p)}
Un processus autor√©gressif d'ordre $p$ exprime $y_t$ comme une combinaison lin√©aire des $p$ valeurs pass√©es plus un bruit blanc :

\[
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \epsilon_t
\]

o√π :
\begin{itemize}
    \item $c$ : constante (intercept)
    \item $\phi_1, \ldots, \phi_p$ : coefficients autor√©gressifs
    \item $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ : bruit blanc (innovations)
\end{itemize}
\end{definition}

\begin{exemple}{AR(1) - Mod√®le autor√©gressif d'ordre 1}
\[
y_t = c + \phi_1 y_{t-1} + \epsilon_t
\]

Si $|\phi_1| < 1$ : processus stationnaire convergeant vers la moyenne $\mu = \frac{c}{1 - \phi_1}$
\end{exemple}

\textbf{Identification de l'ordre $p$ :}
\begin{itemize}
    \item PACF coupe apr√®s lag $p$
    \item ACF d√©cro√Æt exponentiellement ou avec oscillations amorties
\end{itemize}

\subsection{Mod√®le Moyenne Mobile MA(q)}

\begin{definition}{Mod√®le MA(q)}
Un processus moyenne mobile d'ordre $q$ exprime $y_t$ comme une combinaison lin√©aire des $q$ innovations pass√©es :

\[
y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}
\]

o√π :
\begin{itemize}
    \item $\mu$ : moyenne du processus
    \item $\theta_1, \ldots, \theta_q$ : coefficients MA
    \item $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ : bruit blanc
\end{itemize}
\end{definition}

\textbf{Identification de l'ordre $q$ :}
\begin{itemize}
    \item ACF coupe apr√®s lag $q$
    \item PACF d√©cro√Æt exponentiellement
\end{itemize}

\subsection{Mod√®le ARMA(p,q)}

\begin{definition}{Mod√®le ARMA(p,q)}
Combine les composantes AR et MA :

\[
y_t = c + \phi_1 y_{t-1} + \cdots + \phi_p y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \cdots + \theta_q \epsilon_{t-q}
\]

Plus flexible que AR ou MA seuls, capture √† la fois autocorr√©lations et moyennes mobiles.
\end{definition}

\subsection{Mod√®le ARIMA(p,d,q)}

\begin{definition}{Mod√®le ARIMA(p,d,q)}
ARIMA = AutoRegressive Integrated Moving Average

\textbf{Param√®tres :}
\begin{itemize}
    \item $p$ : ordre autor√©gressif (nombre de lags $y_{t-i}$)
    \item $d$ : ordre de diff√©renciation (nombre de diff√©rences pour stationnariser)
    \item $q$ : ordre moyenne mobile (nombre de lags $\epsilon_{t-i}$)
\end{itemize}

\textbf{Formulation :}
\[
\nabla^d y_t = c + \phi_1 \nabla^d y_{t-1} + \cdots + \phi_p \nabla^d y_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + \cdots + \theta_q \epsilon_{t-q}
\]

o√π $\nabla^d$ repr√©sente $d$ diff√©renciations successives.
\end{definition}

\begin{exemple}{ARIMA(1,1,1) - Mod√®le simple}
\begin{itemize}
    \item $d=1$ : une diff√©renciation $\nabla y_t = y_t - y_{t-1}$
    \item $p=1$ : un terme autor√©gressif $\phi_1 \nabla y_{t-1}$
    \item $q=1$ : un terme moyenne mobile $\theta_1 \epsilon_{t-1}$
\end{itemize}

√âquation : $\nabla y_t = c + \phi_1 \nabla y_{t-1} + \epsilon_t + \theta_1 \epsilon_{t-1}$
\end{exemple}

\subsubsection{S√©lection des param√®tres (p,d,q)}

\begin{enumerate}
    \item \textbf{Ordre $d$ :} Diff√©rencier jusqu'√† obtenir la stationnarit√© (test ADF)
    \item \textbf{Ordre $p$ :} Analyser PACF de la s√©rie diff√©renci√©e
    \item \textbf{Ordre $q$ :} Analyser ACF de la s√©rie diff√©renci√©e
    \item \textbf{Crit√®res d'information :} Comparer AIC/BIC pour diff√©rents $(p,q)$
\end{enumerate}

\begin{definition}{Crit√®res AIC et BIC}
\textbf{AIC (Akaike Information Criterion) :}
\[
\text{AIC} = 2k - 2\ln(\hat{L})
\]

\textbf{BIC (Bayesian Information Criterion) :}
\[
\text{BIC} = k \ln(n) - 2\ln(\hat{L})
\]

o√π $k$ = nombre de param√®tres, $n$ = taille √©chantillon, $\hat{L}$ = vraisemblance maximale.

\textbf{Objectif :} Minimiser AIC ou BIC (p√©nalise la complexit√© du mod√®le).
\end{definition}

\subsection{Mod√®le SARIMA(p,d,q)(P,D,Q)s}

\begin{definition}{Mod√®le SARIMA - Seasonal ARIMA}
Extension d'ARIMA pour capturer la saisonnalit√© p√©riodique de p√©riode $s$.

\textbf{Param√®tres non-saisonniers :} $(p, d, q)$

\textbf{Param√®tres saisonniers :} $(P, D, Q)_s$
\begin{itemize}
    \item $P$ : ordre AR saisonnier (lags $y_{t-s}, y_{t-2s}, \ldots$)
    \item $D$ : ordre de diff√©renciation saisonni√®re
    \item $Q$ : ordre MA saisonnier
    \item $s$ : p√©riode saisonni√®re (12 pour mensuel, 7 pour quotidien hebdomadaire)
\end{itemize}
\end{definition}

\begin{exemple}{SARIMA(1,1,1)(1,1,1)12 pour donn√©es mensuelles}
\begin{itemize}
    \item Tendance : ARIMA(1,1,1)
    \item Saisonnalit√© annuelle : p√©riode $s=12$ (12 mois)
    \item Composante saisonni√®re : AR(1) MA(1) avec diff√©renciation saisonni√®re
\end{itemize}

Capture √† la fois les corr√©lations √† court terme et les patterns annuels.
\end{exemple}

\begin{lstlisting}[language=Python, caption=ARIMA avec statsmodels]
from statsmodels.tsa.arima.model import ARIMA

# Ajuster ARIMA(1,1,1)
model = ARIMA(timeseries, order=(1, 1, 1))
fitted_model = model.fit()

# Resume du modele
print(fitted_model.summary())

# Predictions
forecast = fitted_model.forecast(steps=10)
print(f"Predictions 10 prochains pas: {forecast}")
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=SARIMA avec statsmodels]
from statsmodels.tsa.statespace.sarimax import SARIMAX

# SARIMA(1,1,1)(1,1,1,12) pour donnees mensuelles
model = SARIMAX(timeseries,
                order=(1, 1, 1),
                seasonal_order=(1, 1, 1, 12))
fitted_model = model.fit()

# Predictions
forecast = fitted_model.forecast(steps=24)  # 24 mois
\end{lstlisting}

\subsection{Prophet - Mod√®le de Facebook}

\begin{definition}{Prophet}
Mod√®le de forecasting d√©velopp√© par Facebook (Meta) pour des s√©ries temporelles avec :
\begin{itemize}
    \item Tendances fortes (croissance lin√©aire ou logistique)
    \item Saisonnalit√©s multiples (annuelle, hebdomadaire, journali√®re)
    \item Jours f√©ri√©s et √©v√©nements sp√©ciaux
    \item Observations manquantes et outliers
\end{itemize}

\textbf{D√©composition additive :}
\[
y(t) = g(t) + s(t) + h(t) + \epsilon_t
\]

o√π :
\begin{itemize}
    \item $g(t)$ : fonction de tendance (lin√©aire ou logistique avec changepoints)
    \item $s(t)$ : saisonnalit√© p√©riodique (s√©ries de Fourier)
    \item $h(t)$ : effets des jours f√©ri√©s
    \item $\epsilon_t$ : erreur r√©siduelle
\end{itemize}
\end{definition}

\textbf{Avantages de Prophet :}
\begin{itemize}
    \item Interface simple (peu de tuning)
    \item Robuste aux donn√©es manquantes et outliers
    \item D√©tection automatique des changepoints (ruptures de tendance)
    \item Saisonnalit√©s multiples (annuelle, hebdomadaire, custom)
    \item Gestion native des jours f√©ri√©s (calendriers pr√©d√©finis)
    \item Intervalles de confiance automatiques
\end{itemize}

\begin{lstlisting}[language=Python, caption=Prophet de Facebook]
from prophet import Prophet
import pandas as pd

# Donnees au format Prophet (colonnes 'ds' et 'y')
df = pd.DataFrame({'ds': dates, 'y': values})

# Creer et ajuster le modele
model = Prophet(yearly_seasonality=True,
                weekly_seasonality=True,
                daily_seasonality=False)
model.fit(df)

# Predictions sur 365 jours
future = model.make_future_dataframe(periods=365)
forecast = model.predict(future)

# Visualisation
model.plot(forecast)
model.plot_components(forecast)  # tendance + saisonnalites
\end{lstlisting}

\begin{attention}
Prophet est excellent pour des s√©ries avec saisonnalit√©s claires et tendances changeantes, mais peut √™tre moins performant que ARIMA sur des s√©ries purement stationnaires ou avec des patterns complexes non-p√©riodiques.
\end{attention}

% ===== SECTION 4: DEEP LEARNING POUR S√âRIES TEMPORELLES =====
\section{Deep Learning pour S√©ries Temporelles}

Les r√©seaux de neurones r√©currents (RNN) et leurs variantes (LSTM, GRU) sont particuli√®rement adapt√©s aux donn√©es s√©quentielles gr√¢ce √† leur capacit√© √† mod√©liser les d√©pendances temporelles √† long terme.

\subsection{LSTM pour Forecasting}

\begin{definition}{LSTM pour S√©ries Temporelles}
Un r√©seau LSTM (Long Short-Term Memory) peut apprendre des d√©pendances temporelles complexes gr√¢ce √† sa cellule m√©moire et ses portes de r√©gulation.

\textbf{Architecture typique :}
\begin{enumerate}
    \item \textbf{Input :} S√©quence de longueur $L$ : $(y_{t-L+1}, \ldots, y_{t-1}, y_t)$
    \item \textbf{LSTM layers :} 1-3 couches avec hidden size $h$
    \item \textbf{Output :} Pr√©diction $\hat{y}_{t+1}$ ou $(\hat{y}_{t+1}, \ldots, \hat{y}_{t+h})$
\end{enumerate}
\end{definition}

\subsubsection{Pr√©paration des Donn√©es - Windowing}

Pour utiliser un LSTM, il faut transformer la s√©rie temporelle en s√©quences (windows) :

\begin{exemple}{Cr√©ation de fen√™tres glissantes (sliding windows)}
S√©rie : $[y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8]$

Avec window size $L=3$, horizon $h=1$ :
\begin{align*}
X_1 &= [y_1, y_2, y_3], \quad Y_1 = y_4 \\
X_2 &= [y_2, y_3, y_4], \quad Y_2 = y_5 \\
X_3 &= [y_3, y_4, y_5], \quad Y_3 = y_6 \\
&\vdots
\end{align*}

Dataset final : $\{(X_i, Y_i)\}_{i=1}^{N}$
\end{exemple}

\begin{lstlisting}[language=Python, caption=Cr√©ation de fen√™tres glissantes]
import numpy as np

def create_windows(data, window_size, horizon=1):
    X, y = [], []
    for i in range(len(data) - window_size - horizon + 1):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size:i+window_size+horizon])
    return np.array(X), np.array(y)

# Exemple
timeseries = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
X, y = create_windows(timeseries, window_size=3, horizon=1)

print(f"X shape: {X.shape}")  # (7, 3)
print(f"y shape: {y.shape}")  # (7, 1)
print(f"Premier exemple: X={X[0]} -> y={y[0]}")
\end{lstlisting}

\subsubsection{Architecture LSTM pour Forecasting}

\begin{lstlisting}[language=Python, caption=LSTM avec PyTorch]
import torch
import torch.nn as nn

class LSTMForecaster(nn.Module):
    def __init__(self, input_size=1, hidden_size=64,
                 num_layers=2, output_size=1, dropout=0.2):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM layers
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # Fully connected output
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x: (batch, seq_len, input_size)
        lstm_out, (h_n, c_n) = self.lstm(x)

        # Prendre le dernier output
        last_output = lstm_out[:, -1, :]  # (batch, hidden_size)

        # Prediction
        output = self.fc(last_output)  # (batch, output_size)
        return output

# Instanciation
model = LSTMForecaster(
    input_size=1,      # features univariees
    hidden_size=64,    # taille hidden state
    num_layers=2,      # 2 couches LSTM
    output_size=1,     # prediction 1 pas
    dropout=0.2
)

print(model)
\end{lstlisting}

\subsubsection{Entra√Ænement}

\begin{lstlisting}[language=Python, caption=Entra√Ænement LSTM]
import torch.optim as optim

# Hyperparametres
learning_rate = 0.001
num_epochs = 100
batch_size = 32

# Loss et optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# DataLoader
from torch.utils.data import TensorDataset, DataLoader

X_train = torch.FloatTensor(X_train).unsqueeze(-1)  # (N, L, 1)
y_train = torch.FloatTensor(y_train)

dataset = TensorDataset(X_train, y_train)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Boucle d'entrainement
model.train()
for epoch in range(num_epochs):
    epoch_loss = 0.0
    for batch_X, batch_y in dataloader:
        # Forward
        predictions = model(batch_X)
        loss = criterion(predictions.squeeze(), batch_y)

        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(dataloader):.4f}")
\end{lstlisting}

\subsection{GRU pour Forecasting}

\begin{definition}{GRU - Gated Recurrent Unit}
Variante simplifi√©e de LSTM avec seulement 2 portes (reset et update) au lieu de 3.

\textbf{Avantages :}
\begin{itemize}
    \item Moins de param√®tres que LSTM (entra√Ænement plus rapide)
    \item Performances souvent comparables √† LSTM
    \item Moins susceptible au surapprentissage
\end{itemize}
\end{definition}

Remplacer simplement \texttt{nn.LSTM} par \texttt{nn.GRU} dans le code pr√©c√©dent.

\subsection{S√©ries Temporelles Multivari√©es}

\begin{definition}{Forecasting Multivari√©}
Pr√©dire $y_t$ en utilisant plusieurs features $\vect{x}_t = [x_t^{(1)}, x_t^{(2)}, \ldots, x_t^{(d)}]^T$.

\textbf{Exemple :} Pr√©dire la consommation √©lectrique avec :
\begin{itemize}
    \item $x_t^{(1)}$ : temp√©rature
    \item $x_t^{(2)}$ : jour de la semaine (encod√©)
    \item $x_t^{(3)}$ : consommation pass√©e
\end{itemize}
\end{definition}

\begin{lstlisting}[language=Python, caption=LSTM multivari√©]
# input_size = nombre de features
model = LSTMForecaster(
    input_size=5,      # 5 features en entree
    hidden_size=128,
    num_layers=2,
    output_size=1      # 1 valeur predite (univarie output)
)

# X shape: (batch, seq_len, 5)
# y shape: (batch, 1)
\end{lstlisting}

\subsection{Attention pour S√©ries Temporelles}

Le m√©canisme d'attention permet au mod√®le de se concentrer sur les instants les plus pertinents pour la pr√©diction.

\begin{definition}{Temporal Attention}
Au lieu de n'utiliser que le dernier hidden state, utiliser une combinaison pond√©r√©e de tous les hidden states :

\[
\vect{c} = \sum_{t=1}^{L} \alpha_t \vect{h}_t
\]

o√π $\alpha_t$ sont des poids d'attention calcul√©s dynamiquement :

\[
\alpha_t = \frac{\exp(e_t)}{\sum_{i=1}^{L} \exp(e_i)}, \quad e_t = \text{score}(\vect{h}_t, \vect{h}_L)
\]
\end{definition}

\begin{lstlisting}[language=Python, caption=LSTM avec Attention]
class LSTMWithAttention(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.attention = nn.Linear(hidden_size, 1)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # LSTM
        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden)

        # Attention scores
        scores = self.attention(lstm_out)  # (batch, seq_len, 1)
        attention_weights = torch.softmax(scores, dim=1)

        # Context vector (weighted sum)
        context = torch.sum(attention_weights * lstm_out, dim=1)  # (batch, hidden)

        # Prediction
        output = self.fc(context)
        return output
\end{lstlisting}

\subsection{Transformers pour S√©ries Temporelles}

Les Transformers, initialement d√©velopp√©s pour le NLP, sont de plus en plus utilis√©s pour les s√©ries temporelles gr√¢ce au m√©canisme d'attention multi-t√™tes.

\begin{definition}{Transformer pour Forecasting}
Architecture bas√©e sur :
\begin{itemize}
    \item \textbf{Positional Encoding :} Ajouter l'information temporelle
    \item \textbf{Multi-Head Self-Attention :} Capturer les relations entre tous les instants
    \item \textbf{Feed-Forward Networks :} Transformations non-lin√©aires
\end{itemize}

\textbf{Avantages :}
\begin{itemize}
    \item Capture des d√©pendances √† tr√®s long terme
    \item Parall√©lisation (pas de r√©currence s√©quentielle)
    \item Interpr√©tabilit√© via les poids d'attention
\end{itemize}
\end{definition}

\begin{exemple}{Mod√®les Transformer pour Time Series}
\begin{itemize}
    \item \textbf{Temporal Fusion Transformer (TFT)} : Google Research
    \item \textbf{Autoformer} : D√©composition auto-corr√©lation
    \item \textbf{Informer} : Attention sparse pour longues s√©quences
    \item \textbf{PatchTST} : D√©coupage en patches comme en vision
\end{itemize}
\end{exemple}

% ===== SECTION 5: M√âTRIQUES D'√âVALUATION =====
\section{M√©triques d'√âvaluation}

\subsection{M√©triques de R√©gression Classiques}

\begin{definition}{Mean Absolute Error (MAE)}
\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]

Erreur absolue moyenne, interpr√©table dans l'unit√© des donn√©es.
\end{definition}

\begin{definition}{Mean Squared Error (MSE)}
\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

P√©nalise fortement les grandes erreurs (carr√©).
\end{definition}

\begin{definition}{Root Mean Squared Error (RMSE)}
\[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\]

Erreur quadratique moyenne, m√™me unit√© que les donn√©es.
\end{definition}

\subsection{M√©triques Sp√©cifiques aux S√©ries Temporelles}

\begin{definition}{Mean Absolute Percentage Error (MAPE)}
\[
\text{MAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \left|\frac{y_i - \hat{y}_i}{y_i}\right|
\]

Erreur en pourcentage, utile pour comparer des s√©ries d'√©chelles diff√©rentes.

\textbf{Attention :} Ind√©fini si $y_i = 0$.
\end{definition}

\begin{definition}{Symmetric MAPE (sMAPE)}
\[
\text{sMAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{(|y_i| + |\hat{y}_i|) / 2}
\]

Variante sym√©trique de MAPE, born√©e entre 0\% et 200\%.
\end{definition}

\begin{definition}{Mean Absolute Scaled Error (MASE)}
\[
\text{MASE} = \frac{\text{MAE}}{\text{MAE}_{\text{naive}}}
\]

o√π $\text{MAE}_{\text{naive}}$ est l'erreur d'un mod√®le na√Øf (ex: pr√©dire $\hat{y}_t = y_{t-1}$).

\textbf{Interpr√©tation :}
\begin{itemize}
    \item MASE $< 1$ : meilleur que le mod√®le na√Øf
    \item MASE $= 1$ : √©quivalent au mod√®le na√Øf
    \item MASE $> 1$ : pire que le mod√®le na√Øf
\end{itemize}
\end{definition}

\begin{lstlisting}[language=Python, caption=Calcul des m√©triques]
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

def mape(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def smape(y_true, y_pred):
    return np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))) * 100

# Evaluation
mae = mean_absolute_error(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mape_score = mape(y_test, predictions)

print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAPE: {mape_score:.2f}%")
\end{lstlisting}

% ===== SECTION 6: VALIDATION =====
\section{Validation des Mod√®les de S√©ries Temporelles}

\subsection{Time Series Split}

\begin{attention}
Ne JAMAIS utiliser la validation crois√©e classique (K-Fold al√©atoire) pour les s√©ries temporelles ! Cela cr√©erait du \textbf{data leakage} (utiliser le futur pour pr√©dire le pass√©).
\end{attention}

\begin{definition}{Time Series Split (Forward Chaining)}
Validation s√©quentielle respectant l'ordre temporel :

\begin{itemize}
    \item \textbf{Fold 1 :} Train $[1, \ldots, t_1]$, Test $[t_1+1, \ldots, t_2]$
    \item \textbf{Fold 2 :} Train $[1, \ldots, t_2]$, Test $[t_2+1, \ldots, t_3]$
    \item \textbf{Fold 3 :} Train $[1, \ldots, t_3]$, Test $[t_3+1, \ldots, t_4]$
    \item $\vdots$
\end{itemize}

Le training set grandit progressivement (expanding window).
\end{definition}

\begin{lstlisting}[language=Python, caption=Time Series Split avec scikit-learn]
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
    print(f"Fold {fold+1}:")
    print(f"  Train: {train_idx[0]} to {train_idx[-1]}")
    print(f"  Test:  {test_idx[0]} to {test_idx[-1]}")

    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # Entrainer et evaluer le modele
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    print(f"  Score: {score:.4f}\n")
\end{lstlisting}

\subsection{Backtesting}

\begin{definition}{Backtesting}
Simuler des pr√©dictions historiques pour √©valuer la performance du mod√®le :

\begin{enumerate}
    \item Entra√Æner sur $[1, \ldots, t]$
    \item Pr√©dire $\hat{y}_{t+1}$
    \item Observer la vraie valeur $y_{t+1}$
    \item Calculer l'erreur $|y_{t+1} - \hat{y}_{t+1}|$
    \item Avancer d'un pas : $t \leftarrow t+1$
    \item R√©p√©ter (potentiellement r√©-entra√Æner le mod√®le)
\end{enumerate}

\textbf{Variantes :}
\begin{itemize}
    \item \textbf{Expanding window :} Training set grandit continuellement
    \item \textbf{Rolling window :} Training set de taille fixe (glisse)
\end{itemize}
\end{definition}

\begin{lstlisting}[language=Python, caption=Backtesting simple]
def backtest_model(model, data, window_size, test_size):
    predictions = []
    actuals = []

    for i in range(len(data) - window_size - test_size):
        # Train/test split
        train_end = window_size + i
        X_train = data[:train_end]
        y_test = data[train_end]

        # Entrainer
        model.fit(X_train)

        # Predire
        pred = model.predict(steps=1)[0]
        predictions.append(pred)
        actuals.append(y_test)

    # Evaluation
    mae = mean_absolute_error(actuals, predictions)
    return predictions, actuals, mae
\end{lstlisting}

% ===== SECTION 7: D√âTECTION D'ANOMALIES =====
\section{D√©tection d'Anomalies dans les S√©ries Temporelles}

\begin{definition}{Anomalie Temporelle}
Observation anormale qui d√©vie significativement du pattern attendu.

\textbf{Types d'anomalies :}
\begin{itemize}
    \item \textbf{Point anomaly :} Valeur isol√©e inhabituelle (spike)
    \item \textbf{Contextual anomaly :} Valeur normale ailleurs mais anormale √† cet instant
    \item \textbf{Collective anomaly :} S√©quence anormale (changement de r√©gime)
\end{itemize}
\end{definition}

\subsection{M√©thodes Statistiques}

\begin{definition}{D√©tection par Seuil (Z-score)}
Une observation $y_t$ est anormale si :

\[
|z_t| = \left|\frac{y_t - \mu}{\sigma}\right| > \text{seuil}
\]

Seuil typique : 3 (99.7\% des donn√©es dans $[\mu - 3\sigma, \mu + 3\sigma]$)
\end{definition}

\subsection{M√©thodes par Pr√©diction}

\begin{definition}{Anomalie par Erreur de Pr√©diction}
\begin{enumerate}
    \item Entra√Æner un mod√®le de forecasting (ARIMA, LSTM)
    \item Calculer l'erreur de pr√©diction $e_t = |y_t - \hat{y}_t|$
    \item Anomalie si $e_t > \text{seuil}$ (ex: $\mu_e + 3\sigma_e$)
\end{enumerate}

\textbf{Intuition :} Une valeur difficile √† pr√©dire est potentiellement anormale.
\end{definition}

\begin{lstlisting}[language=Python, caption=D√©tection d'anomalies par pr√©diction]
# Predictions LSTM
model.eval()
predictions = model(X_test).squeeze().detach().numpy()
actuals = y_test.numpy()

# Erreurs de prediction
errors = np.abs(actuals - predictions)

# Seuil (moyenne + 3 * ecart-type)
threshold = errors.mean() + 3 * errors.std()

# Anomalies
anomalies = errors > threshold
print(f"Nombre d'anomalies detectees: {anomalies.sum()}")
print(f"Indices: {np.where(anomalies)[0]}")
\end{lstlisting}

% ===== SECTION 8: BEST PRACTICES =====
\section{Best Practices pour le Forecasting}

\subsection{Preprocessing}

\begin{enumerate}
    \item \textbf{Gestion des valeurs manquantes :}
    \begin{itemize}
        \item Interpolation lin√©aire, spline, forward/backward fill
        \item Mod√®les robustes (Prophet g√®re nativement)
    \end{itemize}

    \item \textbf{D√©tection et traitement des outliers :}
    \begin{itemize}
        \item Z-score, IQR, isolation forest
        \item Winsorization (clipping), remplacement
    \end{itemize}

    \item \textbf{Normalisation/Standardisation :}
    \begin{itemize}
        \item Min-Max scaling : $x' = \frac{x - \min}{\max - \min}$
        \item Standardisation : $x' = \frac{x - \mu}{\sigma}$
        \item \textbf{Important :} Calculer stats sur train uniquement !
    \end{itemize}
\end{enumerate}

\begin{lstlisting}[language=Python, caption=Normalisation correcte]
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Fit sur train uniquement
scaler.fit(X_train)

# Transform train et test
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Predictions
predictions_scaled = model.predict(X_test_scaled)

# Inverse transform pour revenir a l'echelle originale
predictions = scaler.inverse_transform(predictions_scaled)
\end{lstlisting}

\subsection{Feature Engineering}

\begin{enumerate}
    \item \textbf{Features temporelles :}
    \begin{itemize}
        \item Jour de la semaine, mois, trimestre, ann√©e
        \item Week-end vs jour de semaine
        \item Jours f√©ri√©s
    \end{itemize}

    \item \textbf{Lags :} $y_{t-1}, y_{t-2}, \ldots, y_{t-k}$

    \item \textbf{Rolling statistics :}
    \begin{itemize}
        \item Moyenne mobile : $\text{MA}_k(t) = \frac{1}{k}\sum_{i=0}^{k-1} y_{t-i}$
        \item √âcart-type mobile
        \item Min/Max mobile
    \end{itemize}

    \item \textbf{Diff√©renciation :} $\Delta y_t = y_t - y_{t-1}$

    \item \textbf{Features cycliques :} Pour encoder p√©riodicit√©
    \begin{align*}
    \text{hour\_sin} &= \sin\left(\frac{2\pi \cdot \text{hour}}{24}\right) \\
    \text{hour\_cos} &= \cos\left(\frac{2\pi \cdot \text{hour}}{24}\right)
    \end{align*}
\end{enumerate}

\subsection{S√©lection de Mod√®le}

\begin{table}[h]
\centering
\caption{Quand utiliser quel mod√®le ?}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Mod√®le} & \textbf{Cas d'usage} \\
\midrule
ARIMA & S√©ries univari√©es stationnaires, patterns lin√©aires, taille mod√©r√©e \\
SARIMA & Saisonnalit√© claire et p√©riodique \\
Prophet & Saisonnalit√©s multiples, jours f√©ri√©s, tendances changeantes, donn√©es manquantes \\
LSTM/GRU & D√©pendances long terme, patterns non-lin√©aires, multivari√© \\
Transformer & Tr√®s longues s√©quences, attention importante, ressources suffisantes \\
XGBoost & Caract√©ristiques con√ßues riches, r√©gression avec lags \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ensemble Methods}

Combiner plusieurs mod√®les pour am√©liorer la robustesse :

\begin{lstlisting}[language=Python, caption=Ensemble de pr√©dictions]
# Predictions de plusieurs modeles
pred_arima = model_arima.forecast(steps=10)
pred_lstm = model_lstm.predict(X_test)
pred_prophet = model_prophet.predict(future)['yhat'].values

# Moyenne simple
pred_ensemble = (pred_arima + pred_lstm + pred_prophet) / 3

# Moyenne ponderee (poids bases sur performance validation)
weights = [0.3, 0.5, 0.2]  # ARIMA, LSTM, Prophet
pred_ensemble = (weights[0] * pred_arima +
                weights[1] * pred_lstm +
                weights[2] * pred_prophet)
\end{lstlisting}

% ===== SECTION 9: APPLICATIONS =====
\section{Applications Pratiques}

\begin{enumerate}
    \item \textbf{Finance et Trading :}
    \begin{itemize}
        \item Pr√©diction de prix d'actions, indices boursiers
        \item Volatilit√©, d√©tection d'anomalies (flash crashes)
        \item Trading algorithmique
    \end{itemize}

    \item \textbf{√ânergie :}
    \begin{itemize}
        \item Forecasting de la demande √©lectrique (load forecasting)
        \item Pr√©diction de production √©olienne/solaire
        \item Optimisation de la distribution
    \end{itemize}

    \item \textbf{E-commerce et Retail :}
    \begin{itemize}
        \item Pr√©vision des ventes pour gestion des stocks
        \item Optimisation des prix dynamiques
        \item D√©tection de fraudes (transactions anormales)
    \end{itemize}

    \item \textbf{IoT et Industrie :}
    \begin{itemize}
        \item Maintenance pr√©dictive (d√©tection de pannes)
        \item Monitoring de capteurs
        \item Optimisation de production
    \end{itemize}

    \item \textbf{Sant√© :}
    \begin{itemize}
        \item Monitoring de signes vitaux (ECG, EEG)
        \item Pr√©diction d'√©pid√©mies
        \item D√©tection pr√©coce de pathologies
    \end{itemize}

    \item \textbf{M√©t√©orologie :}
    \begin{itemize}
        \item Pr√©visions m√©t√©o courte/moyenne √©ch√©ance
        \item Mod√®les climatiques
    \end{itemize}
\end{enumerate}

% ===== SECTION 10: R√âSUM√â =====
\section{R√©sum√© du Chapitre}

\subsection{Points Cl√©s}

\begin{itemize}
    \item \textbf{Stationnarit√© :} Propri√©t√© essentielle pour les mod√®les classiques (ARIMA)
    \item \textbf{Composantes :} Tendance, saisonnalit√©, r√©sidu
    \item \textbf{ARIMA :} Mod√®le statistique puissant pour s√©ries univari√©es
    \item \textbf{Prophet :} Framework simple pour saisonnalit√©s multiples et jours f√©ri√©s
    \item \textbf{LSTM/GRU :} Deep Learning pour d√©pendances complexes et long terme
    \item \textbf{Attention/Transformers :} √âtat de l'art pour s√©quences tr√®s longues
    \item \textbf{Validation :} Time Series Split, backtesting (jamais de CV classique)
    \item \textbf{M√©triques :} MAE, RMSE, MAPE, MASE
    \item \textbf{Feature Engineering :} Lags, rolling stats, features temporelles
\end{itemize}

\subsection{Formules Essentielles}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Formules √† retenir]
\textbf{ARIMA(p,d,q) :}
\[
\nabla^d y_t = c + \sum_{i=1}^p \phi_i \nabla^d y_{t-i} + \epsilon_t + \sum_{j=1}^q \theta_j \epsilon_{t-j}
\]

\textbf{Prophet :}
\[
y(t) = g(t) + s(t) + h(t) + \epsilon_t
\]

\textbf{M√©triques :}
\begin{align*}
\text{MAE} &= \frac{1}{n}\sum |y_i - \hat{y}_i| \\
\text{RMSE} &= \sqrt{\frac{1}{n}\sum (y_i - \hat{y}_i)^2} \\
\text{MAPE} &= \frac{100\%}{n}\sum \left|\frac{y_i - \hat{y}_i}{y_i}\right|
\end{align*}
\end{tcolorbox}

% ===== SECTION 11: EXERCICES =====
\section{Exercices}

\subsection{Questions de compr√©hension}

\begin{enumerate}
    \item Pourquoi la validation crois√©e classique (K-Fold) est-elle inappropri√©e pour les s√©ries temporelles ?
    \item Quelle est la diff√©rence entre un mod√®le AR et un mod√®le MA ?
    \item Expliquez le r√¥le de chaque param√®tre dans ARIMA(p,d,q).
    \item Quand utiliser Prophet plut√¥t qu'ARIMA ?
    \item Comment g√©rer une s√©rie temporelle avec tendance croissante ?
\end{enumerate}

\subsection{Exercices pratiques}

\begin{enumerate}
    \item \textbf{Pr√©diction de ventes mensuelles :}
    \begin{itemize}
        \item Charger un dataset de ventes (ex: retail sales)
        \item Analyser la d√©composition (tendance, saisonnalit√©)
        \item Tester la stationnarit√© (ADF test)
        \item Comparer ARIMA vs Prophet
    \end{itemize}

    \item \textbf{Forecasting de consommation √©lectrique :}
    \begin{itemize}
        \item Dataset multivari√© (temp√©rature, jour, heure)
        \item Feature engineering (lags, rolling stats)
        \item Entra√Æner un LSTM
        \item √âvaluer avec RMSE et MAPE
    \end{itemize}

    \item \textbf{D√©tection d'anomalies :}
    \begin{itemize}
        \item Charger un dataset avec anomalies
        \item Entra√Æner un mod√®le LSTM
        \item D√©tecter les anomalies via erreur de pr√©diction
        \item Visualiser les r√©sultats
    \end{itemize}
\end{enumerate}

\textit{Solutions disponibles dans} \texttt{11\_exercices.ipynb}

% ===== SECTION 12: POUR ALLER PLUS LOIN =====
\section{Pour Aller Plus Loin}

\subsection{Lectures Recommand√©es}

\begin{itemize}
    \item Hyndman, R.J., \& Athanasopoulos, G. (2021). \textit{Forecasting: Principles and Practice} (3rd ed.) - Livre de r√©f√©rence gratuit en ligne
    \item Box, G.E.P., Jenkins, G.M., Reinsel, G.C., \& Ljung, G.M. (2015). \textit{Time Series Analysis: Forecasting and Control}
    \item Taylor, S.J., \& Letham, B. (2018). "Forecasting at Scale". \textit{The American Statistician}
    \item Lim, B., et al. (2021). "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting". \textit{International Journal of Forecasting}
\end{itemize}

\subsection{Ressources en Ligne}

\begin{itemize}
    \item Documentation statsmodels : \url{https://www.statsmodels.org/stable/tsa.html}
    \item Prophet (Meta) : \url{https://facebook.github.io/prophet/}
    \item PyTorch Forecasting : \url{https://pytorch-forecasting.readthedocs.io/}
    \item Darts (time series library) : \url{https://unit8co.github.io/darts/}
    \item Course "Time Series" de Penn State : \url{https://online.stat.psu.edu/stat510/}
\end{itemize}

\subsection{Biblioth√®ques Python}

\begin{itemize}
    \item \textbf{statsmodels} : ARIMA, SARIMA, d√©composition, tests statistiques
    \item \textbf{prophet} : Framework de Meta pour forecasting
    \item \textbf{darts} : Biblioth√®que compl√®te (classique + DL)
    \item \textbf{sktime} : Extension scikit-learn pour time series
    \item \textbf{pytorch-forecasting} : DL pour forecasting (Temporal Fusion Transformer)
    \item \textbf{tslearn} : ML pour s√©ries temporelles (clustering, DTW)
\end{itemize}

\subsection{Prochaines √âtapes}

\begin{itemize}
    \item Approfondir les Transformers pour s√©ries temporelles
    \item √âtudier les mod√®les probabilistes (Bay√©siens, GARCH)
    \item Explorer la causalit√© dans les s√©ries temporelles (Granger causality)
    \item Appliquer √† des probl√®mes r√©els de votre domaine
\end{itemize}

% ===== BIBLIOGRAPHIE =====
\section*{R√©f√©rences}

\begin{enumerate}
    \item Box, G. E. P., Jenkins, G. M., Reinsel, G. C., \& Ljung, G. M. (2015). \textit{Time Series Analysis: Forecasting and Control} (5th ed.). John Wiley \& Sons.

    \item Hyndman, R. J., \& Athanasopoulos, G. (2021). \textit{Forecasting: Principles and Practice} (3rd ed.). OTexts. \url{https://otexts.com/fpp3/}

    \item Taylor, S. J., \& Letham, B. (2018). Forecasting at scale. \textit{The American Statistician}, 72(1), 37-45.

    \item Hochreiter, S., \& Schmidhuber, J. (1997). Long short-term memory. \textit{Neural Computation}, 9(8), 1735-1780.

    \item Vaswani, A., et al. (2017). Attention is all you need. \textit{Advances in Neural Information Processing Systems}, 30.

    \item Lim, B., Arƒ±k, S. √ñ., Loeff, N., \& Pfister, T. (2021). Temporal Fusion Transformers for interpretable multi-horizon time series forecasting. \textit{International Journal of Forecasting}, 37(4), 1748-1764.

    \item Seabold, S., \& Perktold, J. (2010). Statsmodels: Econometric and statistical modeling with Python. \textit{Proceedings of the 9th Python in Science Conference}, 57, 61.
\end{enumerate}

\end{document}
