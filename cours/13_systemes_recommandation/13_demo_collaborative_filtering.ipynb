{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/13_systemes_recommandation/13_demo_collaborative_filtering.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '13_demo_collaborative_filtering.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 14 - D√©monstration : Collaborative Filtering\n",
    "\n",
    "Ce notebook illustre les diff√©rentes approches de **Collaborative Filtering** pour les syst√®mes de recommandation :\n",
    "\n",
    "1. **Exploration du dataset MovieLens**\n",
    "2. **User-Based Collaborative Filtering**\n",
    "3. **Item-Based Collaborative Filtering**\n",
    "4. **Matrix Factorization avec SVD**\n",
    "5. **√âvaluation et comparaison**\n",
    "6. **Visualisation des embeddings**\n",
    "\n",
    "Dataset : **MovieLens 100K** (100,000 ratings de 943 utilisateurs sur 1,682 films)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Exploration du Dataset MovieLens\n",
    "\n",
    "Nous utilisons le dataset MovieLens 100K disponible via la biblioth√®que `surprise` ou t√©l√©chargeable directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©l√©charger et charger MovieLens 100K\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import train_test_split as surprise_split\n",
    "\n",
    "# Charger le dataset\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "ratings_df = pd.DataFrame(data.raw_ratings, columns=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "print(f\"Dataset shape: {ratings_df.shape}\")\n",
    "print(f\"\\nNombre d'utilisateurs: {ratings_df['user_id'].nunique()}\")\n",
    "print(f\"Nombre de films: {ratings_df['item_id'].nunique()}\")\n",
    "print(f\"Nombre de ratings: {len(ratings_df)}\")\n",
    "print(f\"\\nRatings min/max: {ratings_df['rating'].min()}/{ratings_df['rating'].max()}\")\n",
    "\n",
    "ratings_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "print(\"\\n=== Statistiques des Ratings ===\")\n",
    "print(ratings_df['rating'].describe())\n",
    "\n",
    "# Distribution des ratings\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Distribution des ratings\n",
    "ratings_df['rating'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Distribution des Ratings')\n",
    "axes[0].set_xlabel('Rating')\n",
    "axes[0].set_ylabel('Nombre')\n",
    "\n",
    "# Nombre de ratings par utilisateur\n",
    "user_counts = ratings_df.groupby('user_id').size()\n",
    "user_counts.hist(bins=50, ax=axes[1], color='coral')\n",
    "axes[1].set_title('Ratings par Utilisateur')\n",
    "axes[1].set_xlabel('Nombre de ratings')\n",
    "axes[1].set_ylabel('Nombre d\\'utilisateurs')\n",
    "axes[1].axvline(user_counts.mean(), color='red', linestyle='--', label=f'Moyenne: {user_counts.mean():.1f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Nombre de ratings par film\n",
    "item_counts = ratings_df.groupby('item_id').size()\n",
    "item_counts.hist(bins=50, ax=axes[2], color='mediumseagreen')\n",
    "axes[2].set_title('Ratings par Film')\n",
    "axes[2].set_xlabel('Nombre de ratings')\n",
    "axes[2].set_ylabel('Nombre de films')\n",
    "axes[2].axvline(item_counts.mean(), color='red', linestyle='--', label=f'Moyenne: {item_counts.mean():.1f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRatings par utilisateur - Min: {user_counts.min()}, Max: {user_counts.max()}, Moyenne: {user_counts.mean():.1f}\")\n",
    "print(f\"Ratings par film - Min: {item_counts.min()}, Max: {item_counts.max()}, Moyenne: {item_counts.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser la sparsit√© de la matrice\n",
    "n_users = ratings_df['user_id'].nunique()\n",
    "n_items = ratings_df['item_id'].nunique()\n",
    "n_ratings = len(ratings_df)\n",
    "\n",
    "sparsity = 1 - (n_ratings / (n_users * n_items))\n",
    "density = n_ratings / (n_users * n_items)\n",
    "\n",
    "print(f\"\\n=== Sparsit√© de la Matrice User-Item ===\")\n",
    "print(f\"Taille de la matrice: {n_users} users √ó {n_items} items = {n_users * n_items:,} cellules\")\n",
    "print(f\"Ratings observ√©s: {n_ratings:,}\")\n",
    "print(f\"Densit√©: {density:.2%}\")\n",
    "print(f\"Sparsit√©: {sparsity:.2%}\")\n",
    "print(f\"\\nEn moyenne, chaque utilisateur a rat√© seulement {density * n_items:.1f} films sur {n_items}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pr√©paration des Donn√©es\n",
    "\n",
    "Nous cr√©ons la matrice user-item et effectuons un split train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des IDs num√©riques cons√©cutifs pour indexing\n",
    "user_ids = ratings_df['user_id'].unique()\n",
    "item_ids = ratings_df['item_id'].unique()\n",
    "\n",
    "user_id_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "item_id_map = {iid: idx for idx, iid in enumerate(item_ids)}\n",
    "\n",
    "ratings_df['user_idx'] = ratings_df['user_id'].map(user_id_map)\n",
    "ratings_df['item_idx'] = ratings_df['item_id'].map(item_id_map)\n",
    "\n",
    "# Train/Test split (80/20)\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train set: {len(train_df)} ratings\")\n",
    "print(f\"Test set: {len(test_df)} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er la matrice user-item (train)\n",
    "def create_user_item_matrix(df, n_users, n_items):\n",
    "    \"\"\"Cr√©er une matrice dense user-item.\"\"\"\n",
    "    matrix = np.zeros((n_users, n_items))\n",
    "    for _, row in df.iterrows():\n",
    "        matrix[int(row['user_idx']), int(row['item_idx'])] = row['rating']\n",
    "    return matrix\n",
    "\n",
    "n_users = len(user_ids)\n",
    "n_items = len(item_ids)\n",
    "\n",
    "train_matrix = create_user_item_matrix(train_df, n_users, n_items)\n",
    "print(f\"Train matrix shape: {train_matrix.shape}\")\n",
    "print(f\"Non-zero entries: {np.count_nonzero(train_matrix)}\")\n",
    "\n",
    "# Visualiser une petite partie de la matrice\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(train_matrix[:50, :50], cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Rating')\n",
    "plt.title('Matrice User-Item (50 premiers users √ó 50 premiers items)\\n(Noir = pas de rating)')\n",
    "plt.xlabel('Item Index')\n",
    "plt.ylabel('User Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. User-Based Collaborative Filtering\n",
    "\n",
    "Recommander bas√© sur les utilisateurs similaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la similarit√© cosine entre tous les utilisateurs\n",
    "# Note: pour √©viter les divisions par z√©ro, on ajoute un epsilon\n",
    "user_similarity = cosine_similarity(train_matrix + 1e-9)\n",
    "\n",
    "print(f\"User similarity matrix shape: {user_similarity.shape}\")\n",
    "print(f\"Similarit√© moyenne: {user_similarity.mean():.4f}\")\n",
    "print(f\"Similarit√© min/max: {user_similarity.min():.4f} / {user_similarity.max():.4f}\")\n",
    "\n",
    "# Visualiser les similarit√©s\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(user_similarity[:100, :100], cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "plt.colorbar(label='Similarit√© Cosine')\n",
    "plt.title('Matrice de Similarit√© User-User (100 premiers utilisateurs)')\n",
    "plt.xlabel('User Index')\n",
    "plt.ylabel('User Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user_based(train_matrix, user_similarity, user_idx, item_idx, k=20):\n",
    "    \"\"\"\n",
    "    Pr√©dire le rating pour (user_idx, item_idx) avec user-based CF.\n",
    "    \n",
    "    Args:\n",
    "        train_matrix: matrice user-item (n_users, n_items)\n",
    "        user_similarity: matrice de similarit√© (n_users, n_users)\n",
    "        user_idx: index de l'utilisateur\n",
    "        item_idx: index de l'item\n",
    "        k: nombre de voisins √† consid√©rer\n",
    "    \"\"\"\n",
    "    # Trouver les utilisateurs qui ont rat√© cet item\n",
    "    users_who_rated = np.where(train_matrix[:, item_idx] > 0)[0]\n",
    "    \n",
    "    if len(users_who_rated) == 0:\n",
    "        # Aucun utilisateur n'a rat√© cet item -> retourner la moyenne globale\n",
    "        return train_matrix[train_matrix > 0].mean()\n",
    "    \n",
    "    # Similarit√©s avec ces utilisateurs\n",
    "    sims = user_similarity[user_idx, users_who_rated]\n",
    "    \n",
    "    # Prendre les k plus similaires\n",
    "    top_k_indices = np.argsort(sims)[-k:][::-1]\n",
    "    top_k_users = users_who_rated[top_k_indices]\n",
    "    top_k_sims = sims[top_k_indices]\n",
    "    \n",
    "    # Ratings moyens\n",
    "    user_mean = train_matrix[user_idx][train_matrix[user_idx] > 0].mean()\n",
    "    neighbor_means = np.array([train_matrix[u][train_matrix[u] > 0].mean() for u in top_k_users])\n",
    "    \n",
    "    # Ratings des voisins pour cet item\n",
    "    neighbor_ratings = train_matrix[top_k_users, item_idx]\n",
    "    \n",
    "    # Pr√©diction pond√©r√©e (mean-centered)\n",
    "    if top_k_sims.sum() == 0:\n",
    "        return user_mean\n",
    "    \n",
    "    prediction = user_mean + np.sum(top_k_sims * (neighbor_ratings - neighbor_means)) / np.sum(np.abs(top_k_sims))\n",
    "    \n",
    "    # Clip entre 1 et 5\n",
    "    return np.clip(prediction, 1, 5)\n",
    "\n",
    "# Test sur quelques pr√©dictions\n",
    "print(\"=== Test User-Based CF ===\")\n",
    "for i in range(5):\n",
    "    row = test_df.iloc[i]\n",
    "    user_idx = int(row['user_idx'])\n",
    "    item_idx = int(row['item_idx'])\n",
    "    true_rating = row['rating']\n",
    "    pred_rating = predict_user_based(train_matrix, user_similarity, user_idx, item_idx, k=20)\n",
    "    print(f\"User {row['user_id']}, Item {row['item_id']}: True={true_rating:.1f}, Pred={pred_rating:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur le test set (sur un √©chantillon pour acc√©l√©rer)\n",
    "print(\"\\n√âvaluation User-Based CF (cela peut prendre quelques minutes)...\")\n",
    "\n",
    "# Prendre un √©chantillon du test set\n",
    "test_sample = test_df.sample(n=min(1000, len(test_df)), random_state=42)\n",
    "\n",
    "y_true = []\n",
    "y_pred_user = []\n",
    "\n",
    "for _, row in test_sample.iterrows():\n",
    "    user_idx = int(row['user_idx'])\n",
    "    item_idx = int(row['item_idx'])\n",
    "    true_rating = row['rating']\n",
    "    pred_rating = predict_user_based(train_matrix, user_similarity, user_idx, item_idx, k=20)\n",
    "    \n",
    "    y_true.append(true_rating)\n",
    "    y_pred_user.append(pred_rating)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred_user = np.array(y_pred_user)\n",
    "\n",
    "rmse_user = np.sqrt(mean_squared_error(y_true, y_pred_user))\n",
    "mae_user = mean_absolute_error(y_true, y_pred_user)\n",
    "\n",
    "print(f\"\\nUser-Based CF Results:\")\n",
    "print(f\"RMSE: {rmse_user:.4f}\")\n",
    "print(f\"MAE: {mae_user:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Item-Based Collaborative Filtering\n",
    "\n",
    "Recommander bas√© sur les items similaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la similarit√© cosine entre tous les items\n",
    "item_similarity = cosine_similarity(train_matrix.T + 1e-9)\n",
    "\n",
    "print(f\"Item similarity matrix shape: {item_similarity.shape}\")\n",
    "print(f\"Similarit√© moyenne: {item_similarity.mean():.4f}\")\n",
    "\n",
    "# Visualiser les similarit√©s\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(item_similarity[:100, :100], cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "plt.colorbar(label='Similarit√© Cosine')\n",
    "plt.title('Matrice de Similarit√© Item-Item (100 premiers items)')\n",
    "plt.xlabel('Item Index')\n",
    "plt.ylabel('Item Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_item_based(train_matrix, item_similarity, user_idx, item_idx, k=20):\n",
    "    \"\"\"\n",
    "    Pr√©dire le rating pour (user_idx, item_idx) avec item-based CF.\n",
    "    \n",
    "    Args:\n",
    "        train_matrix: matrice user-item (n_users, n_items)\n",
    "        item_similarity: matrice de similarit√© (n_items, n_items)\n",
    "        user_idx: index de l'utilisateur\n",
    "        item_idx: index de l'item\n",
    "        k: nombre de voisins √† consid√©rer\n",
    "    \"\"\"\n",
    "    # Trouver les items rat√©s par cet utilisateur\n",
    "    items_rated = np.where(train_matrix[user_idx, :] > 0)[0]\n",
    "    \n",
    "    if len(items_rated) == 0:\n",
    "        # L'utilisateur n'a rat√© aucun item -> retourner la moyenne globale\n",
    "        return train_matrix[train_matrix > 0].mean()\n",
    "    \n",
    "    # Similarit√©s avec ces items\n",
    "    sims = item_similarity[item_idx, items_rated]\n",
    "    \n",
    "    # Prendre les k plus similaires\n",
    "    top_k_indices = np.argsort(sims)[-k:][::-1]\n",
    "    top_k_items = items_rated[top_k_indices]\n",
    "    top_k_sims = sims[top_k_indices]\n",
    "    \n",
    "    # Ratings de l'utilisateur pour ces items\n",
    "    user_ratings = train_matrix[user_idx, top_k_items]\n",
    "    \n",
    "    # Pr√©diction pond√©r√©e\n",
    "    if top_k_sims.sum() == 0:\n",
    "        return train_matrix[user_idx][train_matrix[user_idx] > 0].mean()\n",
    "    \n",
    "    prediction = np.sum(top_k_sims * user_ratings) / np.sum(np.abs(top_k_sims))\n",
    "    \n",
    "    # Clip entre 1 et 5\n",
    "    return np.clip(prediction, 1, 5)\n",
    "\n",
    "# Test sur quelques pr√©dictions\n",
    "print(\"=== Test Item-Based CF ===\")\n",
    "for i in range(5):\n",
    "    row = test_df.iloc[i]\n",
    "    user_idx = int(row['user_idx'])\n",
    "    item_idx = int(row['item_idx'])\n",
    "    true_rating = row['rating']\n",
    "    pred_rating = predict_item_based(train_matrix, item_similarity, user_idx, item_idx, k=20)\n",
    "    print(f\"User {row['user_id']}, Item {row['item_id']}: True={true_rating:.1f}, Pred={pred_rating:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur le test set\n",
    "print(\"\\n√âvaluation Item-Based CF...\")\n",
    "\n",
    "y_pred_item = []\n",
    "\n",
    "for _, row in test_sample.iterrows():\n",
    "    user_idx = int(row['user_idx'])\n",
    "    item_idx = int(row['item_idx'])\n",
    "    pred_rating = predict_item_based(train_matrix, item_similarity, user_idx, item_idx, k=20)\n",
    "    y_pred_item.append(pred_rating)\n",
    "\n",
    "y_pred_item = np.array(y_pred_item)\n",
    "\n",
    "rmse_item = np.sqrt(mean_squared_error(y_true, y_pred_item))\n",
    "mae_item = mean_absolute_error(y_true, y_pred_item)\n",
    "\n",
    "print(f\"\\nItem-Based CF Results:\")\n",
    "print(f\"RMSE: {rmse_item:.4f}\")\n",
    "print(f\"MAE: {mae_item:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix Factorization avec SVD\n",
    "\n",
    "Utilisons la biblioth√®que **Surprise** pour une impl√©mentation efficace de SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD, NMF\n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "\n",
    "# Pr√©parer les donn√©es pour Surprise\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Entra√Æner SVD\n",
    "print(\"=== Entra√Ænement SVD ===\")\n",
    "svd = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\n",
    "svd.fit(trainset)\n",
    "\n",
    "print(\"\\nMod√®le SVD entra√Æn√©!\")\n",
    "print(f\"Nombre de facteurs latents: {svd.n_factors}\")\n",
    "print(f\"User embeddings shape: {svd.pu.shape}\")\n",
    "print(f\"Item embeddings shape: {svd.qi.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "print(\"\\n=== Cross-Validation (5-fold) ===\")\n",
    "cv_results = cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "print(f\"\\nRMSE moyen: {cv_results['test_rmse'].mean():.4f} ¬± {cv_results['test_rmse'].std():.4f}\")\n",
    "print(f\"MAE moyen: {cv_results['test_mae'].mean():.4f} ¬± {cv_results['test_mae'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur test set\n",
    "print(\"\\n√âvaluation SVD sur test set...\")\n",
    "\n",
    "y_pred_svd = []\n",
    "\n",
    "for _, row in test_sample.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    item_id = row['item_id']\n",
    "    pred = svd.predict(user_id, item_id)\n",
    "    y_pred_svd.append(pred.est)\n",
    "\n",
    "y_pred_svd = np.array(y_pred_svd)\n",
    "\n",
    "rmse_svd = np.sqrt(mean_squared_error(y_true, y_pred_svd))\n",
    "mae_svd = mean_absolute_error(y_true, y_pred_svd)\n",
    "\n",
    "print(f\"\\nSVD Results:\")\n",
    "print(f\"RMSE: {rmse_svd:.4f}\")\n",
    "print(f\"MAE: {mae_svd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison des Approches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif\n",
    "results_df = pd.DataFrame({\n",
    "    'M√©thode': ['User-Based CF', 'Item-Based CF', 'SVD'],\n",
    "    'RMSE': [rmse_user, rmse_item, rmse_svd],\n",
    "    'MAE': [mae_user, mae_item, mae_svd]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Comparaison des M√©thodes ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "methods = results_df['M√©thode']\n",
    "x_pos = np.arange(len(methods))\n",
    "\n",
    "axes[0].bar(x_pos, results_df['RMSE'], color=['coral', 'steelblue', 'mediumseagreen'])\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(methods, rotation=15)\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE par M√©thode (plus bas = meilleur)')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].bar(x_pos, results_df['MAE'], color=['coral', 'steelblue', 'mediumseagreen'])\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(methods, rotation=15)\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE par M√©thode (plus bas = meilleur)')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots : pr√©dictions vs v√©rit√©\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# User-Based\n",
    "axes[0].scatter(y_true, y_pred_user, alpha=0.3, s=20)\n",
    "axes[0].plot([1, 5], [1, 5], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Ratings R√©els')\n",
    "axes[0].set_ylabel('Ratings Pr√©dits')\n",
    "axes[0].set_title(f'User-Based CF\\nRMSE={rmse_user:.3f}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Item-Based\n",
    "axes[1].scatter(y_true, y_pred_item, alpha=0.3, s=20, color='steelblue')\n",
    "axes[1].plot([1, 5], [1, 5], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Ratings R√©els')\n",
    "axes[1].set_ylabel('Ratings Pr√©dits')\n",
    "axes[1].set_title(f'Item-Based CF\\nRMSE={rmse_item:.3f}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# SVD\n",
    "axes[2].scatter(y_true, y_pred_svd, alpha=0.3, s=20, color='mediumseagreen')\n",
    "axes[2].plot([1, 5], [1, 5], 'r--', lw=2)\n",
    "axes[2].set_xlabel('Ratings R√©els')\n",
    "axes[2].set_ylabel('Ratings Pr√©dits')\n",
    "axes[2].set_title(f'SVD\\nRMSE={rmse_svd:.3f}')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des Embeddings (SVD)\n",
    "\n",
    "Visualisons les embeddings des films avec t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# R√©cup√©rer les embeddings des items\n",
    "item_embeddings = svd.qi  # (n_items, n_factors)\n",
    "\n",
    "print(f\"Item embeddings shape: {item_embeddings.shape}\")\n",
    "\n",
    "# R√©duire √† 2D avec t-SNE (sur un √©chantillon pour acc√©l√©rer)\n",
    "n_items_viz = min(500, item_embeddings.shape[0])\n",
    "sample_indices = np.random.choice(item_embeddings.shape[0], n_items_viz, replace=False)\n",
    "sample_embeddings = item_embeddings[sample_indices]\n",
    "\n",
    "print(f\"\\nR√©duction dimensionnalit√© avec t-SNE (cela peut prendre ~1 minute)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(sample_embeddings)\n",
    "\n",
    "print(\"t-SNE termin√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les embeddings\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6, s=30, c='steelblue')\n",
    "plt.title('Visualisation t-SNE des Embeddings de Films (SVD)\\n500 films √©chantillonn√©s')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLes films proches dans cet espace ont des embeddings similaires,\")\n",
    "print(\"ce qui signifie qu'ils sont rat√©s de fa√ßon similaire par les utilisateurs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Top-K Recommendations\n",
    "\n",
    "G√©n√©rons des recommandations top-10 pour un utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_top_k_svd(svd_model, user_id, train_df, k=10):\n",
    "    \"\"\"\n",
    "    G√©n√©rer les top-K recommandations pour un utilisateur.\n",
    "    \n",
    "    Args:\n",
    "        svd_model: mod√®le SVD entra√Æn√©\n",
    "        user_id: ID de l'utilisateur\n",
    "        train_df: DataFrame des ratings d'entra√Ænement\n",
    "        k: nombre de recommandations\n",
    "    \"\"\"\n",
    "    # Items d√©j√† rat√©s par l'utilisateur\n",
    "    rated_items = set(train_df[train_df['user_id'] == user_id]['item_id'])\n",
    "    \n",
    "    # Tous les items\n",
    "    all_items = set(train_df['item_id'].unique())\n",
    "    \n",
    "    # Candidats = items non rat√©s\n",
    "    candidates = all_items - rated_items\n",
    "    \n",
    "    # Pr√©dire pour tous les candidats\n",
    "    predictions = []\n",
    "    for item_id in candidates:\n",
    "        pred = svd_model.predict(user_id, item_id)\n",
    "        predictions.append((item_id, pred.est))\n",
    "    \n",
    "    # Trier par score d√©croissant\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return predictions[:k]\n",
    "\n",
    "# Exemple pour un utilisateur\n",
    "example_user_id = ratings_df['user_id'].iloc[0]\n",
    "\n",
    "print(f\"=== Recommandations Top-10 pour l'utilisateur {example_user_id} ===\")\n",
    "top_10 = recommend_top_k_svd(svd, example_user_id, train_df, k=10)\n",
    "\n",
    "for rank, (item_id, score) in enumerate(top_10, 1):\n",
    "    print(f\"{rank}. Item {item_id}: score pr√©dit = {score:.2f}\")\n",
    "\n",
    "# Montrer les films d√©j√† rat√©s par cet utilisateur\n",
    "user_ratings = train_df[train_df['user_id'] == example_user_id][['item_id', 'rating']].sort_values('rating', ascending=False)\n",
    "print(f\"\\n=== Films d√©j√† rat√©s par l'utilisateur {example_user_id} (top 5) ===\")\n",
    "print(user_ratings.head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons explor√© diff√©rentes approches de **Collaborative Filtering** :\n",
    "\n",
    "1. **User-Based CF** : bas√© sur les utilisateurs similaires\n",
    "2. **Item-Based CF** : bas√© sur les items similaires\n",
    "3. **Matrix Factorization (SVD)** : d√©composition de la matrice en facteurs latents\n",
    "\n",
    "**R√©sultats typiques** :\n",
    "- **SVD** obtient g√©n√©ralement les meilleurs r√©sultats (RMSE le plus bas)\n",
    "- **Item-Based CF** est souvent meilleur que User-Based pour MovieLens\n",
    "- Les approches neighborhood (user/item-based) sont plus explicables mais moins performantes\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "- Explorer les approches de **Deep Learning** (NCF, autoencoders)\n",
    "- √âvaluer avec des m√©triques de ranking (Precision@K, NDCG)\n",
    "- Combiner avec du content-based filtering (syst√®mes hybrides)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}