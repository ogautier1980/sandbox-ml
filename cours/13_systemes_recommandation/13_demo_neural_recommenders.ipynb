{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/13_systemes_recommandation/13_demo_neural_recommenders.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '13_demo_neural_recommenders.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 14 - D√©monstration : Neural Recommenders\n",
    "\n",
    "Ce notebook explore les approches de **Deep Learning** pour les syst√®mes de recommandation :\n",
    "\n",
    "1. **Neural Collaborative Filtering (NCF)**\n",
    "2. **Autoencoders pour Recommandation**\n",
    "3. **Two-Tower Model**\n",
    "4. **√âvaluation avec m√©triques de ranking** (Precision@K, Recall@K, NDCG@K)\n",
    "5. **Comparaison avec approches classiques**\n",
    "\n",
    "Dataset : **MovieLens 100K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger MovieLens 100K\n",
    "from surprise import Dataset\n",
    "\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "ratings_df = pd.DataFrame(data.raw_ratings, columns=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "# Cr√©er des IDs num√©riques cons√©cutifs\n",
    "user_ids = ratings_df['user_id'].unique()\n",
    "item_ids = ratings_df['item_id'].unique()\n",
    "\n",
    "user_id_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "item_id_map = {iid: idx for idx, iid in enumerate(item_ids)}\n",
    "\n",
    "ratings_df['user_idx'] = ratings_df['user_id'].map(user_id_map)\n",
    "ratings_df['item_idx'] = ratings_df['item_id'].map(item_id_map)\n",
    "\n",
    "n_users = len(user_ids)\n",
    "n_items = len(item_ids)\n",
    "\n",
    "print(f\"Nombre d'utilisateurs: {n_users}\")\n",
    "print(f\"Nombre de films: {n_items}\")\n",
    "print(f\"Nombre de ratings: {len(ratings_df)}\")\n",
    "\n",
    "# Train/Validation/Test split\n",
    "train_df, temp_df = train_test_split(ratings_df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\nTrain: {len(train_df)} | Validation: {len(val_df)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.LongTensor(df['user_idx'].values)\n",
    "        self.items = torch.LongTensor(df['item_idx'].values)\n",
    "        self.ratings = torch.FloatTensor(df['rating'].values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = RatingDataset(train_df)\n",
    "val_dataset = RatingDataset(val_df)\n",
    "test_dataset = RatingDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Collaborative Filtering (NCF)\n",
    "\n",
    "Architecture combinant embeddings + MLP pour mod√©liser les interactions user-item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim=64, hidden_layers=[128, 64, 32], dropout=0.2):\n",
    "        super(NCF, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        # MLP layers\n",
    "        layers = []\n",
    "        input_dim = embedding_dim * 2\n",
    "        \n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(hidden_layers[-1], 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "    \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # Embeddings\n",
    "        user_emb = self.user_embedding(user_ids)  # (batch, embedding_dim)\n",
    "        item_emb = self.item_embedding(item_ids)  # (batch, embedding_dim)\n",
    "        \n",
    "        # Concatenate\n",
    "        x = torch.cat([user_emb, item_emb], dim=-1)  # (batch, 2*embedding_dim)\n",
    "        \n",
    "        # MLP\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        # Output\n",
    "        rating = self.output(x).squeeze()  # (batch,)\n",
    "        return rating\n",
    "\n",
    "# Instantiate model\n",
    "ncf_model = NCF(n_users, n_items, embedding_dim=64, hidden_layers=[128, 64, 32], dropout=0.2)\n",
    "ncf_model = ncf_model.to(device)\n",
    "\n",
    "print(ncf_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in ncf_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for user_ids, item_ids, ratings in loader:\n",
    "        user_ids = user_ids.to(device)\n",
    "        item_ids = item_ids.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        predictions = model(user_ids, item_ids)\n",
    "        loss = criterion(predictions, ratings)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_ids, item_ids, ratings in loader:\n",
    "            user_ids = user_ids.to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            \n",
    "            predictions = model(user_ids, item_ids)\n",
    "            \n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(ratings.numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    \n",
    "    return rmse, mae, all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NCF\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ncf_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "n_epochs = 15\n",
    "train_losses = []\n",
    "val_rmses = []\n",
    "\n",
    "print(\"=== Training NCF ===\")\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(ncf_model, train_loader, criterion, optimizer, device)\n",
    "    val_rmse, val_mae, _, _ = evaluate(ncf_model, val_loader, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_rmses.append(val_rmse)\n",
    "    \n",
    "    scheduler.step(val_rmse)\n",
    "    \n",
    "    if (epoch + 1) % 3 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val RMSE: {val_rmse:.4f} | Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train Loss', marker='o')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(val_rmses, label='Validation RMSE', marker='o', color='coral')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Validation RMSE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_rmse_ncf, test_mae_ncf, test_preds_ncf, test_targets = evaluate(ncf_model, test_loader, device)\n",
    "\n",
    "print(\"=== NCF Test Results ===\")\n",
    "print(f\"RMSE: {test_rmse_ncf:.4f}\")\n",
    "print(f\"MAE: {test_mae_ncf:.4f}\")\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(test_targets, test_preds_ncf, alpha=0.3, s=20)\n",
    "plt.plot([1, 5], [1, 5], 'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Ratings R√©els')\n",
    "plt.ylabel('Ratings Pr√©dits (NCF)')\n",
    "plt.title(f'NCF: Pr√©dictions vs R√©alit√©\\nRMSE = {test_rmse_ncf:.3f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autoencoder pour Recommandation (AutoRec)\n",
    "\n",
    "Autoencoder qui reconstruit le vecteur de ratings d'un utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRec(nn.Module):\n",
    "    def __init__(self, n_items, hidden_dim=256, dropout=0.3):\n",
    "        super(AutoRec, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_items, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, n_items)\n",
    "        )\n",
    "    \n",
    "    def forward(self, ratings):\n",
    "        # ratings: (batch, n_items) avec 0 pour items non rat√©s\n",
    "        h = self.encoder(ratings)  # (batch, hidden_dim)\n",
    "        reconstructed = self.decoder(h)  # (batch, n_items)\n",
    "        return reconstructed\n",
    "\n",
    "autorec_model = AutoRec(n_items, hidden_dim=256, dropout=0.3)\n",
    "autorec_model = autorec_model.to(device)\n",
    "\n",
    "print(autorec_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in autorec_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user-item matrices for AutoRec\n",
    "def create_user_vectors(df, n_users, n_items):\n",
    "    \"\"\"Create user vectors (n_users, n_items) with ratings.\"\"\"\n",
    "    user_vectors = np.zeros((n_users, n_items))\n",
    "    for _, row in df.iterrows():\n",
    "        user_vectors[int(row['user_idx']), int(row['item_idx'])] = row['rating']\n",
    "    return user_vectors\n",
    "\n",
    "train_user_vectors = create_user_vectors(train_df, n_users, n_items)\n",
    "val_user_vectors = create_user_vectors(val_df, n_users, n_items)\n",
    "test_user_vectors = create_user_vectors(test_df, n_users, n_items)\n",
    "\n",
    "# Create masks (1 where rated, 0 otherwise)\n",
    "train_mask = (train_user_vectors > 0).astype(np.float32)\n",
    "val_mask = (val_user_vectors > 0).astype(np.float32)\n",
    "test_mask = (test_user_vectors > 0).astype(np.float32)\n",
    "\n",
    "# Convert to tensors\n",
    "train_user_vectors = torch.FloatTensor(train_user_vectors).to(device)\n",
    "train_mask = torch.FloatTensor(train_mask).to(device)\n",
    "val_user_vectors = torch.FloatTensor(val_user_vectors).to(device)\n",
    "val_mask = torch.FloatTensor(val_mask).to(device)\n",
    "test_user_vectors = torch.FloatTensor(test_user_vectors).to(device)\n",
    "test_mask = torch.FloatTensor(test_mask).to(device)\n",
    "\n",
    "print(f\"Train user vectors shape: {train_user_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training AutoRec\n",
    "def train_autorec(model, user_vectors, mask, criterion, optimizer):\n",
    "    model.train()\n",
    "    reconstructed = model(user_vectors)\n",
    "    \n",
    "    # Loss only on observed ratings\n",
    "    loss = criterion(reconstructed * mask, user_vectors * mask)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_autorec(model, user_vectors, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(user_vectors)\n",
    "        \n",
    "        # Extract predictions for observed ratings\n",
    "        mask_np = mask.cpu().numpy()\n",
    "        preds = reconstructed.cpu().numpy()[mask_np > 0]\n",
    "        targets = user_vectors.cpu().numpy()[mask_np > 0]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(targets, preds))\n",
    "        mae = mean_absolute_error(targets, preds)\n",
    "    \n",
    "    return rmse, mae\n",
    "\n",
    "# Train\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(autorec_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "n_epochs_ae = 50\n",
    "train_losses_ae = []\n",
    "val_rmses_ae = []\n",
    "\n",
    "print(\"=== Training AutoRec ===\")\n",
    "for epoch in range(n_epochs_ae):\n",
    "    train_loss = train_autorec(autorec_model, train_user_vectors, train_mask, criterion_ae, optimizer_ae)\n",
    "    val_rmse, val_mae = evaluate_autorec(autorec_model, val_user_vectors, val_mask)\n",
    "    \n",
    "    train_losses_ae.append(train_loss)\n",
    "    val_rmses_ae.append(val_rmse)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs_ae} | Train Loss: {train_loss:.4f} | Val RMSE: {val_rmse:.4f} | Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_rmse_ae, test_mae_ae = evaluate_autorec(autorec_model, test_user_vectors, test_mask)\n",
    "\n",
    "print(\"=== AutoRec Test Results ===\")\n",
    "print(f\"RMSE: {test_rmse_ae:.4f}\")\n",
    "print(f\"MAE: {test_mae_ae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Two-Tower Model\n",
    "\n",
    "Architecture avec deux tours s√©par√©es pour utilisateurs et items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim=64, hidden_dim=128):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        \n",
    "        # User Tower\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Item Tower\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "    \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # User embedding\n",
    "        user_emb = self.user_embedding(user_ids)  # (batch, embedding_dim)\n",
    "        user_vec = self.user_tower(user_emb)  # (batch, embedding_dim)\n",
    "        \n",
    "        # Item embedding\n",
    "        item_emb = self.item_embedding(item_ids)  # (batch, embedding_dim)\n",
    "        item_vec = self.item_tower(item_emb)  # (batch, embedding_dim)\n",
    "        \n",
    "        # Dot product\n",
    "        scores = torch.sum(user_vec * item_vec, dim=-1)  # (batch,)\n",
    "        return scores\n",
    "    \n",
    "    def get_user_embedding(self, user_ids):\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        return self.user_tower(user_emb)\n",
    "    \n",
    "    def get_item_embedding(self, item_ids):\n",
    "        item_emb = self.item_embedding(item_ids)\n",
    "        return self.item_tower(item_emb)\n",
    "\n",
    "two_tower_model = TwoTowerModel(n_users, n_items, embedding_dim=64, hidden_dim=128)\n",
    "two_tower_model = two_tower_model.to(device)\n",
    "\n",
    "print(two_tower_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in two_tower_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Two-Tower\n",
    "criterion_tt = nn.MSELoss()\n",
    "optimizer_tt = optim.Adam(two_tower_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "n_epochs_tt = 15\n",
    "train_losses_tt = []\n",
    "val_rmses_tt = []\n",
    "\n",
    "print(\"=== Training Two-Tower Model ===\")\n",
    "for epoch in range(n_epochs_tt):\n",
    "    train_loss = train_epoch(two_tower_model, train_loader, criterion_tt, optimizer_tt, device)\n",
    "    val_rmse, val_mae, _, _ = evaluate(two_tower_model, val_loader, device)\n",
    "    \n",
    "    train_losses_tt.append(train_loss)\n",
    "    val_rmses_tt.append(val_rmse)\n",
    "    \n",
    "    if (epoch + 1) % 3 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs_tt} | Train Loss: {train_loss:.4f} | Val RMSE: {val_rmse:.4f} | Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_rmse_tt, test_mae_tt, test_preds_tt, _ = evaluate(two_tower_model, test_loader, device)\n",
    "\n",
    "print(\"=== Two-Tower Test Results ===\")\n",
    "print(f\"RMSE: {test_rmse_tt:.4f}\")\n",
    "print(f\"MAE: {test_mae_tt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. M√©triques de Ranking (Precision@K, Recall@K, NDCG@K)\n",
    "\n",
    "√âvaluons les mod√®les avec des m√©triques de ranking pour les recommandations top-K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommendations, relevant_items, k):\n",
    "    \"\"\"Precision@K: proportion d'items pertinents dans les K recommand√©s.\"\"\"\n",
    "    top_k = recommendations[:k]\n",
    "    relevant_in_top_k = len(set(top_k) & relevant_items)\n",
    "    return relevant_in_top_k / k if k > 0 else 0\n",
    "\n",
    "def recall_at_k(recommendations, relevant_items, k):\n",
    "    \"\"\"Recall@K: proportion d'items pertinents retrouv√©s dans les K recommand√©s.\"\"\"\n",
    "    top_k = recommendations[:k]\n",
    "    relevant_in_top_k = len(set(top_k) & relevant_items)\n",
    "    return relevant_in_top_k / len(relevant_items) if len(relevant_items) > 0 else 0\n",
    "\n",
    "def ndcg_at_k(recommendations, true_relevances, k):\n",
    "    \"\"\"NDCG@K: Normalized Discounted Cumulative Gain.\"\"\"\n",
    "    # DCG\n",
    "    dcg = 0\n",
    "    for i, item_id in enumerate(recommendations[:k]):\n",
    "        relevance = true_relevances.get(item_id, 0)\n",
    "        dcg += (2**relevance - 1) / np.log2(i + 2)  # i+2 car log2(1) = 0\n",
    "    \n",
    "    # IDCG (ideal DCG avec tri parfait)\n",
    "    ideal_relevances = sorted(true_relevances.values(), reverse=True)[:k]\n",
    "    idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_relevances))\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def evaluate_ranking(model, test_df, train_df, k_list=[5, 10, 20], threshold=4.0, n_users_sample=100):\n",
    "    \"\"\"\n",
    "    √âvaluer les m√©triques de ranking pour un √©chantillon d'utilisateurs.\n",
    "    \n",
    "    Args:\n",
    "        model: mod√®le de recommandation\n",
    "        test_df: DataFrame de test\n",
    "        train_df: DataFrame de train\n",
    "        k_list: liste des valeurs de K\n",
    "        threshold: rating >= threshold est consid√©r√© comme pertinent\n",
    "        n_users_sample: nombre d'utilisateurs √† √©chantillonner\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # √âchantillonner des utilisateurs ayant assez de ratings dans test\n",
    "    user_test_counts = test_df.groupby('user_idx').size()\n",
    "    users_with_data = user_test_counts[user_test_counts >= 5].index.tolist()\n",
    "    sample_users = np.random.choice(users_with_data, min(n_users_sample, len(users_with_data)), replace=False)\n",
    "    \n",
    "    results = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in k_list}\n",
    "    \n",
    "    for user_idx in sample_users:\n",
    "        # Items rat√©s dans train (√† exclure des recommandations)\n",
    "        train_items = set(train_df[train_df['user_idx'] == user_idx]['item_idx'])\n",
    "        \n",
    "        # Items pertinents dans test (rating >= threshold)\n",
    "        test_user_df = test_df[test_df['user_idx'] == user_idx]\n",
    "        relevant_items = set(test_user_df[test_user_df['rating'] >= threshold]['item_idx'])\n",
    "        \n",
    "        if len(relevant_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Cr√©er dict des relevances (ratings)\n",
    "        true_relevances = test_user_df.set_index('item_idx')['rating'].to_dict()\n",
    "        \n",
    "        # Pr√©dire pour tous les items candidats (non dans train)\n",
    "        candidate_items = [i for i in range(n_items) if i not in train_items]\n",
    "        \n",
    "        user_tensor = torch.LongTensor([user_idx] * len(candidate_items)).to(device)\n",
    "        item_tensor = torch.LongTensor(candidate_items).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(user_tensor, item_tensor).cpu().numpy()\n",
    "        \n",
    "        # Trier par score d√©croissant\n",
    "        sorted_indices = np.argsort(predictions)[::-1]\n",
    "        recommendations = [candidate_items[i] for i in sorted_indices]\n",
    "        \n",
    "        # Calculer les m√©triques pour chaque K\n",
    "        for k in k_list:\n",
    "            prec = precision_at_k(recommendations, relevant_items, k)\n",
    "            rec = recall_at_k(recommendations, relevant_items, k)\n",
    "            ndcg = ndcg_at_k(recommendations, true_relevances, k)\n",
    "            \n",
    "            results[k]['precision'].append(prec)\n",
    "            results[k]['recall'].append(rec)\n",
    "            results[k]['ndcg'].append(ndcg)\n",
    "    \n",
    "    # Moyennes\n",
    "    avg_results = {}\n",
    "    for k in k_list:\n",
    "        avg_results[k] = {\n",
    "            'precision': np.mean(results[k]['precision']),\n",
    "            'recall': np.mean(results[k]['recall']),\n",
    "            'ndcg': np.mean(results[k]['ndcg'])\n",
    "        }\n",
    "    \n",
    "    return avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer NCF avec m√©triques de ranking\n",
    "print(\"\\n=== √âvaluation Ranking NCF (cela peut prendre quelques minutes) ===\")\n",
    "ranking_results_ncf = evaluate_ranking(ncf_model, test_df, train_df, k_list=[5, 10, 20], n_users_sample=50)\n",
    "\n",
    "print(\"\\nNCF Ranking Results:\")\n",
    "for k, metrics in ranking_results_ncf.items():\n",
    "    print(f\"\\nK={k}:\")\n",
    "    print(f\"  Precision@{k}: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall@{k}: {metrics['recall']:.4f}\")\n",
    "    print(f\"  NDCG@{k}: {metrics['ndcg']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer Two-Tower avec m√©triques de ranking\n",
    "print(\"\\n=== √âvaluation Ranking Two-Tower ===\")\n",
    "ranking_results_tt = evaluate_ranking(two_tower_model, test_df, train_df, k_list=[5, 10, 20], n_users_sample=50)\n",
    "\n",
    "print(\"\\nTwo-Tower Ranking Results:\")\n",
    "for k, metrics in ranking_results_tt.items():\n",
    "    print(f\"\\nK={k}:\")\n",
    "    print(f\"  Precision@{k}: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall@{k}: {metrics['recall']:.4f}\")\n",
    "    print(f\"  NDCG@{k}: {metrics['ndcg']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des mod√®les\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Mod√®le': ['NCF', 'AutoRec', 'Two-Tower'],\n",
    "    'RMSE': [test_rmse_ncf, test_rmse_ae, test_rmse_tt],\n",
    "    'MAE': [test_mae_ncf, test_mae_ae, test_mae_tt]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Comparaison des Mod√®les (Rating Prediction) ===\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "models = comparison_df['Mod√®le']\n",
    "x_pos = np.arange(len(models))\n",
    "\n",
    "axes[0].bar(x_pos, comparison_df['RMSE'], color=['steelblue', 'coral', 'mediumseagreen'])\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE par Mod√®le')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].bar(x_pos, comparison_df['MAE'], color=['steelblue', 'coral', 'mediumseagreen'])\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(models)\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE par Mod√®le')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les m√©triques de ranking\n",
    "k_values = [5, 10, 20]\n",
    "ncf_precisions = [ranking_results_ncf[k]['precision'] for k in k_values]\n",
    "ncf_recalls = [ranking_results_ncf[k]['recall'] for k in k_values]\n",
    "ncf_ndcgs = [ranking_results_ncf[k]['ndcg'] for k in k_values]\n",
    "\n",
    "tt_precisions = [ranking_results_tt[k]['precision'] for k in k_values]\n",
    "tt_recalls = [ranking_results_tt[k]['recall'] for k in k_values]\n",
    "tt_ndcgs = [ranking_results_tt[k]['ndcg'] for k in k_values]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Precision@K\n",
    "axes[0].plot(k_values, ncf_precisions, marker='o', label='NCF', linewidth=2)\n",
    "axes[0].plot(k_values, tt_precisions, marker='s', label='Two-Tower', linewidth=2)\n",
    "axes[0].set_xlabel('K')\n",
    "axes[0].set_ylabel('Precision@K')\n",
    "axes[0].set_title('Precision@K')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Recall@K\n",
    "axes[1].plot(k_values, ncf_recalls, marker='o', label='NCF', linewidth=2)\n",
    "axes[1].plot(k_values, tt_recalls, marker='s', label='Two-Tower', linewidth=2)\n",
    "axes[1].set_xlabel('K')\n",
    "axes[1].set_ylabel('Recall@K')\n",
    "axes[1].set_title('Recall@K')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# NDCG@K\n",
    "axes[2].plot(k_values, ncf_ndcgs, marker='o', label='NCF', linewidth=2)\n",
    "axes[2].plot(k_values, tt_ndcgs, marker='s', label='Two-Tower', linewidth=2)\n",
    "axes[2].set_xlabel('K')\n",
    "axes[2].set_ylabel('NDCG@K')\n",
    "axes[2].set_title('NDCG@K')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons explor√© les approches de **Deep Learning** pour les syst√®mes de recommandation :\n",
    "\n",
    "1. **Neural Collaborative Filtering (NCF)** : combine embeddings + MLP pour capturer des interactions non-lin√©aires\n",
    "2. **AutoRec** : autoencoder qui reconstruit les vecteurs de ratings utilisateurs\n",
    "3. **Two-Tower Model** : architecture scalable avec deux tours s√©par√©es\n",
    "\n",
    "**Observations** :\n",
    "- Les mod√®les deep learning obtiennent g√©n√©ralement des performances comparables ou meilleures que SVD\n",
    "- NCF et Two-Tower capturent mieux les interactions complexes\n",
    "- AutoRec peut souffrir de la sparsit√© extr√™me de la matrice\n",
    "- Les m√©triques de ranking (Precision@K, NDCG) sont plus pertinentes pour √©valuer les recommandations top-K\n",
    "\n",
    "**Avantages du Deep Learning** :\n",
    "- Flexibilit√© pour incorporer des features suppl√©mentaires (content, contexte)\n",
    "- Meilleure capture des patterns non-lin√©aires\n",
    "- Scalabilit√© avec les donn√©es massives\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "- Incorporer des features utilisateurs/items (hybrid models)\n",
    "- Explorer les mod√®les s√©quentiels (GRU4Rec, SASRec) pour les sessions\n",
    "- D√©ployer en production avec recherche ANN (FAISS) pour la scalabilit√©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}