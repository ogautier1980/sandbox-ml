{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/13_systemes_recommandation/13_exercices.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '13_exercices.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 14 - Exercices : Syst√®mes de Recommandation\n",
    "\n",
    "Ce notebook contient des exercices pratiques sur les syst√®mes de recommandation avec leurs solutions compl√®tes.\n",
    "\n",
    "## Exercices\n",
    "\n",
    "1. **Recommandation de films avec Collaborative Filtering**\n",
    "2. **Syst√®me Content-Based avec TF-IDF**\n",
    "3. **Syst√®me Hybride (CF + Content-Based)**\n",
    "\n",
    "Chaque exercice inclut :\n",
    "- Description du probl√®me\n",
    "- Donn√©es et contexte\n",
    "- Consignes d√©taill√©es\n",
    "- Solution compl√®te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports communs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 1 : Recommandation de Films avec Collaborative Filtering\n",
    "\n",
    "### Objectif\n",
    "Impl√©menter un syst√®me de recommandation de films en utilisant **user-based** et **item-based collaborative filtering**, puis les comparer.\n",
    "\n",
    "### Contexte\n",
    "Vous travaillez pour une plateforme de streaming vid√©o et devez recommander des films aux utilisateurs bas√©s sur leurs historiques de visionnage.\n",
    "\n",
    "### Consignes\n",
    "\n",
    "1. Charger le dataset MovieLens 100K\n",
    "2. Cr√©er la matrice user-item\n",
    "3. Impl√©menter user-based CF avec similarit√© Pearson\n",
    "4. Impl√©menter item-based CF avec similarit√© cosine\n",
    "5. Comparer les performances (RMSE, MAE)\n",
    "6. G√©n√©rer des recommandations top-10 pour 3 utilisateurs\n",
    "7. Analyser la diversit√© des recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Exercice 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Charger les donn√©es\n",
    "from surprise import Dataset\n",
    "\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "ratings_df = pd.DataFrame(data.raw_ratings, columns=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "# Mapping IDs\n",
    "user_ids = ratings_df['user_id'].unique()\n",
    "item_ids = ratings_df['item_id'].unique()\n",
    "user_id_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "item_id_map = {iid: idx for idx, iid in enumerate(item_ids)}\n",
    "\n",
    "ratings_df['user_idx'] = ratings_df['user_id'].map(user_id_map)\n",
    "ratings_df['item_idx'] = ratings_df['item_id'].map(item_id_map)\n",
    "\n",
    "n_users = len(user_ids)\n",
    "n_items = len(item_ids)\n",
    "\n",
    "print(f\"Dataset charg√©: {len(ratings_df)} ratings, {n_users} users, {n_items} items\")\n",
    "\n",
    "# Train/Test split\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cr√©er la matrice user-item\n",
    "def create_user_item_matrix(df, n_users, n_items):\n",
    "    matrix = np.zeros((n_users, n_items))\n",
    "    for _, row in df.iterrows():\n",
    "        matrix[int(row['user_idx']), int(row['item_idx'])] = row['rating']\n",
    "    return matrix\n",
    "\n",
    "train_matrix = create_user_item_matrix(train_df, n_users, n_items)\n",
    "print(f\"Train matrix shape: {train_matrix.shape}\")\n",
    "print(f\"Sparsit√©: {(1 - np.count_nonzero(train_matrix) / train_matrix.size):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. User-Based CF avec corr√©lation de Pearson\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_pearson_similarity(matrix):\n",
    "    \"\"\"Calculer la similarit√© Pearson entre utilisateurs.\"\"\"\n",
    "    n_users = matrix.shape[0]\n",
    "    similarity = np.zeros((n_users, n_users))\n",
    "    \n",
    "    for i in range(n_users):\n",
    "        for j in range(i, n_users):\n",
    "            # Items rat√©s par les deux utilisateurs\n",
    "            mask = (matrix[i] > 0) & (matrix[j] > 0)\n",
    "            if mask.sum() > 1:  # Au moins 2 items en commun\n",
    "                corr, _ = pearsonr(matrix[i][mask], matrix[j][mask])\n",
    "                similarity[i, j] = corr if not np.isnan(corr) else 0\n",
    "                similarity[j, i] = similarity[i, j]\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "print(\"Calcul de la similarit√© Pearson (cela peut prendre 1-2 minutes)...\")\n",
    "user_similarity_pearson = compute_pearson_similarity(train_matrix)\n",
    "print(f\"Similarit√© moyenne: {user_similarity_pearson.mean():.4f}\")\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(user_similarity_pearson[:100, :100], cmap='RdYlGn', vmin=-1, vmax=1, aspect='auto')\n",
    "plt.colorbar(label='Corr√©lation de Pearson')\n",
    "plt.title('Similarit√© User-User (Pearson) - 100 premiers users')\n",
    "plt.xlabel('User Index')\n",
    "plt.ylabel('User Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de pr√©diction user-based\n",
    "def predict_user_based_pearson(train_matrix, user_similarity, user_idx, item_idx, k=30):\n",
    "    users_who_rated = np.where(train_matrix[:, item_idx] > 0)[0]\n",
    "    \n",
    "    if len(users_who_rated) == 0:\n",
    "        return train_matrix[train_matrix > 0].mean()\n",
    "    \n",
    "    sims = user_similarity[user_idx, users_who_rated]\n",
    "    top_k_indices = np.argsort(sims)[-k:][::-1]\n",
    "    top_k_users = users_who_rated[top_k_indices]\n",
    "    top_k_sims = sims[top_k_indices]\n",
    "    \n",
    "    # Mean-centered prediction\n",
    "    user_mean = train_matrix[user_idx][train_matrix[user_idx] > 0].mean()\n",
    "    neighbor_means = np.array([train_matrix[u][train_matrix[u] > 0].mean() for u in top_k_users])\n",
    "    neighbor_ratings = train_matrix[top_k_users, item_idx]\n",
    "    \n",
    "    if np.abs(top_k_sims).sum() == 0:\n",
    "        return user_mean\n",
    "    \n",
    "    prediction = user_mean + np.sum(top_k_sims * (neighbor_ratings - neighbor_means)) / np.sum(np.abs(top_k_sims))\n",
    "    return np.clip(prediction, 1, 5)\n",
    "\n",
    "# √âvaluation\n",
    "print(\"\\n√âvaluation User-Based CF (Pearson)...\")\n",
    "test_sample = test_df.sample(n=min(1000, len(test_df)), random_state=42)\n",
    "\n",
    "y_true_ub = []\n",
    "y_pred_ub = []\n",
    "\n",
    "for _, row in test_sample.iterrows():\n",
    "    user_idx = int(row['user_idx'])\n",
    "    item_idx = int(row['item_idx'])\n",
    "    true_rating = row['rating']\n",
    "    pred_rating = predict_user_based_pearson(train_matrix, user_similarity_pearson, user_idx, item_idx, k=30)\n",
    "    \n",
    "    y_true_ub.append(true_rating)\n",
    "    y_pred_ub.append(pred_rating)\n",
    "\n",
    "rmse_ub = np.sqrt(mean_squared_error(y_true_ub, y_pred_ub))\n",
    "mae_ub = mean_absolute_error(y_true_ub, y_pred_ub)\n",
    "\n",
    "print(f\"User-Based CF (Pearson):\")\n",
    "print(f\"  RMSE: {rmse_ub:.4f}\")\n",
    "print(f\"  MAE: {mae_ub:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Item-Based CF avec similarit√© cosine\n",
    "item_similarity_cosine = cosine_similarity(train_matrix.T + 1e-9)\n",
    "\n",
    "print(f\"\\nItem similarity shape: {item_similarity_cosine.shape}\")\n",
    "print(f\"Similarit√© moyenne: {item_similarity_cosine.mean():.4f}\")\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(item_similarity_cosine[:100, :100], cmap='YlOrRd', vmin=0, vmax=1, aspect='auto')\n",
    "plt.colorbar(label='Similarit√© Cosine')\n",
    "plt.title('Similarit√© Item-Item (Cosine) - 100 premiers items')\n",
    "plt.xlabel('Item Index')\n",
    "plt.ylabel('Item Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de pr√©diction item-based\n",
    "def predict_item_based_cosine(train_matrix, item_similarity, user_idx, item_idx, k=30):\n",
    "    items_rated = np.where(train_matrix[user_idx, :] > 0)[0]\n",
    "    \n",
    "    if len(items_rated) == 0:\n",
    "        return train_matrix[train_matrix > 0].mean()\n",
    "    \n",
    "    sims = item_similarity[item_idx, items_rated]\n",
    "    top_k_indices = np.argsort(sims)[-k:][::-1]\n",
    "    top_k_items = items_rated[top_k_indices]\n",
    "    top_k_sims = sims[top_k_indices]\n",
    "    \n",
    "    user_ratings = train_matrix[user_idx, top_k_items]\n",
    "    \n",
    "    if top_k_sims.sum() == 0:\n",
    "        return train_matrix[user_idx][train_matrix[user_idx] > 0].mean()\n",
    "    \n",
    "    prediction = np.sum(top_k_sims * user_ratings) / top_k_sims.sum()\n",
    "    return np.clip(prediction, 1, 5)\n",
    "\n",
    "# √âvaluation\n",
    "print(\"\\n√âvaluation Item-Based CF (Cosine)...\")\n",
    "\n",
    "y_pred_ib = []\n",
    "\n",
    "for _, row in test_sample.iterrows():\n",
    "    user_idx = int(row['user_idx'])\n",
    "    item_idx = int(row['item_idx'])\n",
    "    pred_rating = predict_item_based_cosine(train_matrix, item_similarity_cosine, user_idx, item_idx, k=30)\n",
    "    y_pred_ib.append(pred_rating)\n",
    "\n",
    "rmse_ib = np.sqrt(mean_squared_error(y_true_ub, y_pred_ib))\n",
    "mae_ib = mean_absolute_error(y_true_ub, y_pred_ib)\n",
    "\n",
    "print(f\"Item-Based CF (Cosine):\")\n",
    "print(f\"  RMSE: {rmse_ib:.4f}\")\n",
    "print(f\"  MAE: {mae_ib:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Comparaison des performances\n",
    "results_df = pd.DataFrame({\n",
    "    'M√©thode': ['User-Based (Pearson)', 'Item-Based (Cosine)'],\n",
    "    'RMSE': [rmse_ub, rmse_ib],\n",
    "    'MAE': [mae_ub, mae_ib]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Comparaison des M√©thodes ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "methods = results_df['M√©thode']\n",
    "x_pos = np.arange(len(methods))\n",
    "\n",
    "axes[0].bar(x_pos, results_df['RMSE'], color=['steelblue', 'coral'])\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(methods, rotation=15, ha='right')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE par M√©thode')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].bar(x_pos, results_df['MAE'], color=['steelblue', 'coral'])\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(methods, rotation=15, ha='right')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE par M√©thode')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. G√©n√©rer des recommandations top-10\n",
    "def recommend_top_k_item_based(train_matrix, item_similarity, user_idx, k=10):\n",
    "    \"\"\"G√©n√©rer top-K recommandations avec item-based CF.\"\"\"\n",
    "    rated_items = set(np.where(train_matrix[user_idx, :] > 0)[0])\n",
    "    candidate_items = [i for i in range(train_matrix.shape[1]) if i not in rated_items]\n",
    "    \n",
    "    predictions = []\n",
    "    for item_idx in candidate_items:\n",
    "        pred = predict_item_based_cosine(train_matrix, item_similarity, user_idx, item_idx, k=30)\n",
    "        predictions.append((item_idx, pred))\n",
    "    \n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return predictions[:k]\n",
    "\n",
    "# Recommandations pour 3 utilisateurs\n",
    "example_users = [0, 10, 50]\n",
    "\n",
    "print(\"\\n=== Recommandations Top-10 (Item-Based CF) ===\")\n",
    "for user_idx in example_users:\n",
    "    print(f\"\\nUtilisateur {user_idx}:\")\n",
    "    top_10 = recommend_top_k_item_based(train_matrix, item_similarity_cosine, user_idx, k=10)\n",
    "    for rank, (item_idx, score) in enumerate(top_10, 1):\n",
    "        print(f\"  {rank}. Item {item_idx}: score = {score:.2f}\")\n",
    "    \n",
    "    # Montrer les items d√©j√† rat√©s\n",
    "    user_ratings = train_df[train_df['user_idx'] == user_idx][['item_idx', 'rating']].sort_values('rating', ascending=False).head(5)\n",
    "    print(f\"  Items d√©j√† rat√©s (top 5): {user_ratings['item_idx'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Analyser la diversit√© des recommandations\n",
    "def compute_diversity(recommendations, item_similarity):\n",
    "    \"\"\"Calculer la diversit√© intra-liste (1 - similarit√© moyenne).\"\"\"\n",
    "    if len(recommendations) < 2:\n",
    "        return 0\n",
    "    \n",
    "    item_indices = [item_idx for item_idx, _ in recommendations]\n",
    "    total_sim = 0\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(item_indices)):\n",
    "        for j in range(i+1, len(item_indices)):\n",
    "            total_sim += item_similarity[item_indices[i], item_indices[j]]\n",
    "            count += 1\n",
    "    \n",
    "    avg_sim = total_sim / count if count > 0 else 0\n",
    "    diversity = 1 - avg_sim\n",
    "    return diversity\n",
    "\n",
    "print(\"\\n=== Analyse de Diversit√© ===\")\n",
    "diversities = []\n",
    "\n",
    "for user_idx in range(min(50, n_users)):\n",
    "    top_10 = recommend_top_k_item_based(train_matrix, item_similarity_cosine, user_idx, k=10)\n",
    "    div = compute_diversity(top_10, item_similarity_cosine)\n",
    "    diversities.append(div)\n",
    "\n",
    "print(f\"Diversit√© moyenne: {np.mean(diversities):.4f}\")\n",
    "print(f\"Diversit√© min/max: {np.min(diversities):.4f} / {np.max(diversities):.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(diversities, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Diversit√©')\n",
    "plt.ylabel('Nombre d\\'utilisateurs')\n",
    "plt.title('Distribution de la Diversit√© des Recommandations')\n",
    "plt.axvline(np.mean(diversities), color='red', linestyle='--', label=f'Moyenne: {np.mean(diversities):.3f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 2 : Syst√®me Content-Based avec TF-IDF\n",
    "\n",
    "### Objectif\n",
    "Cr√©er un syst√®me de recommandation content-based bas√© sur les descriptions/genres de films avec TF-IDF.\n",
    "\n",
    "### Contexte\n",
    "Vous devez recommander des films similaires bas√©s uniquement sur leurs caract√©ristiques (genres, descriptions), sans utiliser les ratings des autres utilisateurs.\n",
    "\n",
    "### Consignes\n",
    "\n",
    "1. Cr√©er un dataset synth√©tique de films avec genres et descriptions\n",
    "2. Vectoriser les features textuelles avec TF-IDF\n",
    "3. Calculer la similarit√© cosine entre films\n",
    "4. Impl√©menter une fonction de recommandation\n",
    "5. Recommander les 5 films les plus similaires pour 3 films donn√©s\n",
    "6. Analyser la couverture du syst√®me (% d'items recommand√©s au moins une fois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Exercice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cr√©er un dataset synth√©tique de films\n",
    "movies_data = {\n",
    "    'movie_id': list(range(20)),\n",
    "    'title': [\n",
    "        'Inception', 'The Matrix', 'Interstellar', 'Blade Runner 2049', 'Tenet',\n",
    "        'The Notebook', 'Titanic', 'La La Land', 'Pride and Prejudice', 'Before Sunrise',\n",
    "        'Avengers: Endgame', 'Iron Man', 'Thor', 'Black Panther', 'Spider-Man',\n",
    "        'The Godfather', 'Goodfellas', 'Scarface', 'Casino', 'The Departed'\n",
    "    ],\n",
    "    'genres': [\n",
    "        'Sci-Fi Thriller Action', 'Sci-Fi Action', 'Sci-Fi Drama Adventure', 'Sci-Fi Thriller', 'Sci-Fi Thriller Action',\n",
    "        'Romance Drama', 'Romance Drama Disaster', 'Romance Musical Drama', 'Romance Drama Period', 'Romance Drama',\n",
    "        'Action Superhero Adventure', 'Action Superhero', 'Action Superhero Fantasy', 'Action Superhero Drama', 'Action Superhero',\n",
    "        'Crime Drama', 'Crime Drama Thriller', 'Crime Drama Thriller', 'Crime Drama', 'Crime Drama Thriller'\n",
    "    ],\n",
    "    'description': [\n",
    "        'A thief who steals corporate secrets through dream-sharing technology',\n",
    "        'A hacker discovers reality is a simulation',\n",
    "        'Astronauts travel through a wormhole to save humanity',\n",
    "        'A blade runner must find and eliminate rogue replicants',\n",
    "        'A protagonist tries to prevent World War III through time manipulation',\n",
    "        'A poor young man falls in love with a rich young woman',\n",
    "        'A seventeen-year-old aristocrat falls in love with a poor artist aboard a ship',\n",
    "        'A jazz pianist falls for an aspiring actress in Los Angeles',\n",
    "        'A woman navigates love and society in 19th century England',\n",
    "        'A romantic trilogy about two people meeting over nine years',\n",
    "        'Superheroes assemble to defeat a powerful villain and save the universe',\n",
    "        'A billionaire builds an armor suit to fight evil and terrorism',\n",
    "        'The Norse god of thunder protects Earth and the Nine Realms',\n",
    "        'The king of Wakanda fights to protect his nation',\n",
    "        'A teenager gains spider-like abilities and fights crime',\n",
    "        'The aging patriarch of an organized crime dynasty transfers control to his son',\n",
    "        'The story of Henry Hill and his life in the mob',\n",
    "        'A Cuban refugee builds a drug empire in Miami',\n",
    "        'A tale of greed, deception, money, power in Las Vegas',\n",
    "        'An undercover cop and a mole try to identify each other'\n",
    "    ]\n",
    "}\n",
    "\n",
    "movies_df = pd.DataFrame(movies_data)\n",
    "print(f\"Dataset cr√©√©: {len(movies_df)} films\")\n",
    "movies_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Vectoriser avec TF-IDF\n",
    "# Combiner genres et description\n",
    "movies_df['content'] = movies_df['genres'] + ' ' + movies_df['description']\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "tfidf_matrix = tfidf.fit_transform(movies_df['content'])\n",
    "\n",
    "print(f\"\\nTF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulaire (premiers 20 termes): {list(tfidf.vocabulary_.keys())[:20]}\")\n",
    "\n",
    "# Visualiser la matrice TF-IDF\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(tfidf_matrix.toarray(), cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='TF-IDF Score')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Movie Index')\n",
    "plt.title('Matrice TF-IDF des Films')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculer la similarit√© cosine\n",
    "movie_similarity = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "print(f\"\\nMovie similarity matrix shape: {movie_similarity.shape}\")\n",
    "print(f\"Similarit√© moyenne: {movie_similarity.mean():.4f}\")\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(movie_similarity, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
    "plt.colorbar(label='Similarit√© Cosine')\n",
    "plt.title('Matrice de Similarit√© Films (Content-Based)')\n",
    "plt.xlabel('Movie Index')\n",
    "plt.ylabel('Movie Index')\n",
    "\n",
    "# Annoter avec les titres\n",
    "plt.xticks(range(len(movies_df)), movies_df['title'], rotation=90, fontsize=8)\n",
    "plt.yticks(range(len(movies_df)), movies_df['title'], fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fonction de recommandation\n",
    "def recommend_similar_movies(movie_idx, similarity_matrix, movies_df, k=5):\n",
    "    \"\"\"Recommander les K films les plus similaires.\"\"\"\n",
    "    # Similarit√©s avec tous les autres films\n",
    "    sims = similarity_matrix[movie_idx]\n",
    "    \n",
    "    # Trier par similarit√© d√©croissante (exclure le film lui-m√™me)\n",
    "    similar_indices = np.argsort(sims)[::-1][1:k+1]\n",
    "    \n",
    "    recommendations = []\n",
    "    for idx in similar_indices:\n",
    "        recommendations.append({\n",
    "            'movie_id': movies_df.iloc[idx]['movie_id'],\n",
    "            'title': movies_df.iloc[idx]['title'],\n",
    "            'genres': movies_df.iloc[idx]['genres'],\n",
    "            'similarity': sims[idx]\n",
    "        })\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Recommandations pour 3 films\n",
    "example_movies = [0, 5, 10]  # Inception, The Notebook, Avengers\n",
    "\n",
    "print(\"\\n=== Recommandations Content-Based (Top 5) ===\")\n",
    "for movie_idx in example_movies:\n",
    "    movie_title = movies_df.iloc[movie_idx]['title']\n",
    "    movie_genres = movies_df.iloc[movie_idx]['genres']\n",
    "    \n",
    "    print(f\"\\n--- Film: {movie_title} ({movie_genres}) ---\")\n",
    "    \n",
    "    recommendations = recommend_similar_movies(movie_idx, movie_similarity, movies_df, k=5)\n",
    "    \n",
    "    for rank, rec in enumerate(recommendations, 1):\n",
    "        print(f\"  {rank}. {rec['title']} ({rec['genres']}) - Similarit√©: {rec['similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Analyser la couverture\n",
    "def compute_coverage(movies_df, similarity_matrix, k=5):\n",
    "    \"\"\"Calculer le % d'items recommand√©s au moins une fois.\"\"\"\n",
    "    recommended_items = set()\n",
    "    \n",
    "    for movie_idx in range(len(movies_df)):\n",
    "        recs = recommend_similar_movies(movie_idx, similarity_matrix, movies_df, k=k)\n",
    "        for rec in recs:\n",
    "            recommended_items.add(rec['movie_id'])\n",
    "    \n",
    "    coverage = len(recommended_items) / len(movies_df)\n",
    "    return coverage, recommended_items\n",
    "\n",
    "coverage, recommended_items = compute_coverage(movies_df, movie_similarity, k=5)\n",
    "\n",
    "print(f\"\\n=== Analyse de Couverture (K=5) ===\")\n",
    "print(f\"Items recommand√©s au moins une fois: {len(recommended_items)} / {len(movies_df)}\")\n",
    "print(f\"Couverture: {coverage:.2%}\")\n",
    "\n",
    "# Items jamais recommand√©s\n",
    "all_items = set(movies_df['movie_id'])\n",
    "never_recommended = all_items - recommended_items\n",
    "\n",
    "if never_recommended:\n",
    "    print(f\"\\nFilms jamais recommand√©s: {never_recommended}\")\n",
    "    for movie_id in never_recommended:\n",
    "        title = movies_df[movies_df['movie_id'] == movie_id]['title'].values[0]\n",
    "        print(f\"  - {title}\")\n",
    "else:\n",
    "    print(\"\\nTous les films sont recommand√©s au moins une fois!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 3 : Syst√®me Hybride (CF + Content-Based)\n",
    "\n",
    "### Objectif\n",
    "Cr√©er un syst√®me hybride combinant collaborative filtering et content-based filtering pour am√©liorer les recommandations.\n",
    "\n",
    "### Contexte\n",
    "Vous voulez b√©n√©ficier des avantages des deux approches : CF pour capturer les pr√©f√©rences collectives, et content-based pour g√©rer le cold start et assurer la diversit√©.\n",
    "\n",
    "### Consignes\n",
    "\n",
    "1. Utiliser les r√©sultats des exercices 1 et 2\n",
    "2. Impl√©menter une strat√©gie de combinaison weighted (pond√©r√©e)\n",
    "3. Tester plusieurs valeurs de poids alpha (0.0 √† 1.0)\n",
    "4. Comparer les performances avec CF seul et content-based seul\n",
    "5. Analyser les cas o√π le syst√®me hybride est meilleur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Exercice 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pr√©parer les scores des deux syst√®mes\n",
    "# Pour simplifier, nous allons cr√©er des scores synth√©tiques pour MovieLens\n",
    "\n",
    "# Simuler des scores content-based pour MovieLens (normalement on aurait des features r√©elles)\n",
    "# Ici on utilise les genres du dataset MovieLens si disponibles, sinon on simule\n",
    "\n",
    "# Pour cet exercice, nous allons combiner:\n",
    "# - Scores CF (item-based) de l'exercice 1\n",
    "# - Scores content-based simul√©s (bas√©s sur la similarit√© al√©atoire ou genres)\n",
    "\n",
    "print(\"=== Syst√®me Hybride : CF + Content-Based ===\")\n",
    "print(\"\\nNous allons combiner les scores de deux syst√®mes avec une pond√©ration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Impl√©menter la strat√©gie weighted\n",
    "def hybrid_recommendation(user_idx, train_matrix, item_similarity_cf, item_similarity_content, alpha=0.5, k=10):\n",
    "    \"\"\"\n",
    "    Recommandation hybride : score = alpha * CF + (1-alpha) * Content\n",
    "    \n",
    "    Args:\n",
    "        user_idx: index de l'utilisateur\n",
    "        train_matrix: matrice user-item pour CF\n",
    "        item_similarity_cf: similarit√© item-item pour CF\n",
    "        item_similarity_content: similarit√© item-item pour content-based\n",
    "        alpha: poids pour CF (0=content only, 1=CF only)\n",
    "        k: nombre de recommandations\n",
    "    \"\"\"\n",
    "    rated_items = set(np.where(train_matrix[user_idx, :] > 0)[0])\n",
    "    candidate_items = [i for i in range(train_matrix.shape[1]) if i not in rated_items]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for item_idx in candidate_items:\n",
    "        # Score CF\n",
    "        score_cf = predict_item_based_cosine(train_matrix, item_similarity_cf, user_idx, item_idx, k=30)\n",
    "        \n",
    "        # Score content-based (moyenne pond√©r√©e par similarit√© avec items rat√©s)\n",
    "        if len(rated_items) > 0:\n",
    "            rated_items_list = list(rated_items)\n",
    "            content_sims = item_similarity_content[item_idx, rated_items_list]\n",
    "            user_ratings_rated = train_matrix[user_idx, rated_items_list]\n",
    "            \n",
    "            if content_sims.sum() > 0:\n",
    "                score_content = np.sum(content_sims * user_ratings_rated) / content_sims.sum()\n",
    "            else:\n",
    "                score_content = train_matrix[user_idx][train_matrix[user_idx] > 0].mean()\n",
    "        else:\n",
    "            score_content = train_matrix[train_matrix > 0].mean()\n",
    "        \n",
    "        # Combiner\n",
    "        hybrid_score = alpha * score_cf + (1 - alpha) * score_content\n",
    "        predictions.append((item_idx, hybrid_score))\n",
    "    \n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return predictions[:k]\n",
    "\n",
    "print(\"\\nFonction de recommandation hybride impl√©ment√©e.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour cet exercice, nous allons simuler une similarit√© content-based pour MovieLens\n",
    "# En pratique, cela viendrait des genres, descriptions, etc.\n",
    "\n",
    "# Simuler une matrice de similarit√© content (bas√©e sur des clusters al√©atoires pour l'exemple)\n",
    "np.random.seed(42)\n",
    "n_clusters = 10\n",
    "item_clusters = np.random.randint(0, n_clusters, n_items)\n",
    "\n",
    "# Similarit√© = 1 si m√™me cluster, 0.3 sinon\n",
    "item_similarity_content_sim = np.zeros((n_items, n_items))\n",
    "for i in range(n_items):\n",
    "    for j in range(n_items):\n",
    "        if item_clusters[i] == item_clusters[j]:\n",
    "            item_similarity_content_sim[i, j] = 1.0\n",
    "        else:\n",
    "            item_similarity_content_sim[i, j] = 0.3\n",
    "\n",
    "print(f\"Matrice de similarit√© content simul√©e: {item_similarity_content_sim.shape}\")\n",
    "print(f\"Similarit√© moyenne: {item_similarity_content_sim.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tester plusieurs valeurs de alpha\n",
    "alpha_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "print(\"\\n=== Test de diff√©rentes valeurs de alpha ===\")\n",
    "print(\"alpha=0.0: Content-Based uniquement\")\n",
    "print(\"alpha=1.0: CF uniquement\")\n",
    "print(\"alpha=0.5: √âquilibre 50/50\\n\")\n",
    "\n",
    "# √âvaluer sur un √©chantillon de test\n",
    "test_sample_hybrid = test_df.sample(n=min(500, len(test_df)), random_state=42)\n",
    "\n",
    "results_hybrid = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    print(f\"√âvaluation alpha={alpha:.2f}...\")\n",
    "    \n",
    "    y_true_h = []\n",
    "    y_pred_h = []\n",
    "    \n",
    "    for _, row in test_sample_hybrid.iterrows():\n",
    "        user_idx = int(row['user_idx'])\n",
    "        item_idx = int(row['item_idx'])\n",
    "        true_rating = row['rating']\n",
    "        \n",
    "        # Score CF\n",
    "        score_cf = predict_item_based_cosine(train_matrix, item_similarity_cosine, user_idx, item_idx, k=30)\n",
    "        \n",
    "        # Score content\n",
    "        rated_items = np.where(train_matrix[user_idx, :] > 0)[0]\n",
    "        if len(rated_items) > 0:\n",
    "            content_sims = item_similarity_content_sim[item_idx, rated_items]\n",
    "            user_ratings = train_matrix[user_idx, rated_items]\n",
    "            score_content = np.sum(content_sims * user_ratings) / content_sims.sum() if content_sims.sum() > 0 else 3.0\n",
    "        else:\n",
    "            score_content = 3.0\n",
    "        \n",
    "        # Hybride\n",
    "        hybrid_score = alpha * score_cf + (1 - alpha) * score_content\n",
    "        \n",
    "        y_true_h.append(true_rating)\n",
    "        y_pred_h.append(hybrid_score)\n",
    "    \n",
    "    rmse_h = np.sqrt(mean_squared_error(y_true_h, y_pred_h))\n",
    "    mae_h = mean_absolute_error(y_true_h, y_pred_h)\n",
    "    \n",
    "    results_hybrid.append({\n",
    "        'alpha': alpha,\n",
    "        'RMSE': rmse_h,\n",
    "        'MAE': mae_h\n",
    "    })\n",
    "    \n",
    "    print(f\"  RMSE: {rmse_h:.4f}, MAE: {mae_h:.4f}\")\n",
    "\n",
    "results_hybrid_df = pd.DataFrame(results_hybrid)\n",
    "print(\"\\n=== R√©sultats Hybrides ===\")\n",
    "print(results_hybrid_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualiser les r√©sultats\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(results_hybrid_df['alpha'], results_hybrid_df['RMSE'], marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0].set_xlabel('Alpha (poids CF)')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE en fonction de Alpha\\n(alpha=0: Content, alpha=1: CF)')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].axvline(results_hybrid_df.loc[results_hybrid_df['RMSE'].idxmin(), 'alpha'], color='red', linestyle='--', label='Meilleur alpha')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(results_hybrid_df['alpha'], results_hybrid_df['MAE'], marker='s', linewidth=2, markersize=8, color='coral')\n",
    "axes[1].set_xlabel('Alpha (poids CF)')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE en fonction de Alpha')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].axvline(results_hybrid_df.loc[results_hybrid_df['MAE'].idxmin(), 'alpha'], color='red', linestyle='--', label='Meilleur alpha')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_alpha = results_hybrid_df.loc[results_hybrid_df['RMSE'].idxmin(), 'alpha']\n",
    "print(f\"\\nMeilleur alpha (RMSE minimum): {best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Analyser les cas o√π le syst√®me hybride est meilleur\n",
    "print(\"\\n=== Analyse : Quand le syst√®me hybride est-il meilleur ? ===\")\n",
    "print(\"\\nLe syst√®me hybride performe g√©n√©ralement mieux que les syst√®mes individuels dans les cas suivants:\")\n",
    "print(\"\\n1. Cold Start Utilisateurs:\")\n",
    "print(\"   - Les nouveaux utilisateurs avec peu de ratings b√©n√©ficient du content-based\")\n",
    "print(\"   - Le CF seul ne peut pas faire de bonnes recommandations sans historique\")\n",
    "\n",
    "print(\"\\n2. Cold Start Items:\")\n",
    "print(\"   - Les nouveaux films peuvent √™tre recommand√©s gr√¢ce √† leurs features (genres, description)\")\n",
    "print(\"   - Le CF seul ne peut pas recommander des items jamais rat√©s\")\n",
    "\n",
    "print(\"\\n3. Diversit√©:\")\n",
    "print(\"   - Le content-based introduit de la diversit√© bas√©e sur les caract√©ristiques\")\n",
    "print(\"   - √âvite la filter bubble du CF (recommander toujours les m√™mes items populaires)\")\n",
    "\n",
    "print(\"\\n4. Sparsit√©:\")\n",
    "print(\"   - Quand la matrice user-item est tr√®s sparse, le content-based compl√®te le CF\")\n",
    "print(\"   - Am√©liore la robustesse des pr√©dictions\")\n",
    "\n",
    "print(\"\\n5. Explicabilit√©:\")\n",
    "print(\"   - Le content-based rend les recommandations plus explicables\")\n",
    "print(\"   - 'Nous recommandons ce film car il a des genres similaires aux films que vous avez aim√©s'\")\n",
    "\n",
    "print(\"\\n=== Conclusion ===\")\n",
    "print(f\"Dans nos tests, le meilleur alpha est {best_alpha}, ce qui signifie que la combinaison optimale\")\n",
    "print(f\"donne un poids de {best_alpha*100:.0f}% au CF et {(1-best_alpha)*100:.0f}% au content-based.\")\n",
    "print(\"\\nLe syst√®me hybride offre le meilleur compromis entre performance et robustesse!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion des Exercices\n",
    "\n",
    "Nous avons explor√© trois approches de syst√®mes de recommandation :\n",
    "\n",
    "1. **Collaborative Filtering** (Exercice 1)\n",
    "   - User-based et Item-based\n",
    "   - Performant mais souffre du cold start\n",
    "   - Item-based souvent meilleur que User-based sur MovieLens\n",
    "\n",
    "2. **Content-Based Filtering** (Exercice 2)\n",
    "   - Bas√© sur TF-IDF des features textuelles\n",
    "   - R√©sout le cold start pour nouveaux items\n",
    "   - Peut manquer de diversit√© (filter bubble)\n",
    "\n",
    "3. **Syst√®me Hybride** (Exercice 3)\n",
    "   - Combine les avantages des deux approches\n",
    "   - Meilleure robustesse et performance\n",
    "   - Tuning du param√®tre alpha important\n",
    "\n",
    "**Points cl√©s** :\n",
    "- Aucune approche n'est universellement meilleure\n",
    "- Le choix d√©pend du contexte (donn√©es disponibles, cold start, scalabilit√©)\n",
    "- Les syst√®mes hybrides offrent souvent le meilleur compromis\n",
    "- Les m√©triques de ranking (Precision@K, NDCG) sont plus pertinentes que RMSE pour les recommandations top-K\n",
    "\n",
    "**En production** :\n",
    "- Utiliser un pipeline multi-√©tapes : candidate generation + ranking\n",
    "- Incorporer des features contextuelles (temps, localisation, device)\n",
    "- A/B testing pour valider les am√©liorations\n",
    "- Monitoring continu de la diversit√©, coverage, et fairness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}