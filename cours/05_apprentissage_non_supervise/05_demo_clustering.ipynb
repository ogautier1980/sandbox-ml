{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/05_apprentissage_non_supervise/05_demo_clustering.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '05_demo_clustering.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 05 - D√©monstration Clustering\n",
    "\n",
    "Ce notebook explore les algorithmes de clustering: K-Means, DBSCAN, et Hierarchical Clustering.\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre K-Means et choisir le nombre optimal de clusters\n",
    "- Appliquer DBSCAN pour d√©tecter des formes complexes\n",
    "- Utiliser le clustering hi√©rarchique et les dendrogrammes\n",
    "- √âvaluer la qualit√© du clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, davies_bouldin_score, calinski_harabasz_score,\n",
    "    adjusted_rand_score, normalized_mutual_info_score\n",
    ")\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : K-Means Clustering\n",
    "\n",
    "### Principe\n",
    "- Partitionnement en k clusters\n",
    "- Minimise la variance intra-cluster\n",
    "- It√©ratif: assignation puis mise √† jour des centro√Ødes\n",
    "- Sensible √† l'initialisation (k-means++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Dataset synth√©tique avec 4 clusters\n",
    "np.random.seed(42)\n",
    "X_blobs, y_true = make_blobs(n_samples=400, centers=4, cluster_std=0.8, random_state=42)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_true, cmap='viridis', \n",
    "            s=50, alpha=0.7, edgecolors='k')\n",
    "plt.title('Dataset avec 4 Clusters (labels vrais)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Application de K-Means avec k=4\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_blobs)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Labels vrais\n",
    "axes[0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_true, cmap='viridis',\n",
    "                s=50, alpha=0.7, edgecolors='k')\n",
    "axes[0].set_title('Labels Vrais')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Pr√©dictions K-Means\n",
    "axes[1].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_pred, cmap='viridis',\n",
    "                s=50, alpha=0.7, edgecolors='k')\n",
    "axes[1].scatter(centers[:, 0], centers[:, 1], c='red', s=300, alpha=0.8,\n",
    "                marker='X', edgecolors='black', linewidths=2, label='Centro√Ødes')\n",
    "axes[1].set_title('Pr√©dictions K-Means (k=4)')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Inertie (somme des distances carr√©es): {kmeans.inertia_:.2f}\")\n",
    "print(f\"Nombre d'it√©rations: {kmeans.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 M√©thode du coude (Elbow Method) pour choisir k\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans_k = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
    "    y_pred_k = kmeans_k.fit_predict(X_blobs)\n",
    "    \n",
    "    inertias.append(kmeans_k.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_blobs, y_pred_k))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Elbow Method\n",
    "axes[0].plot(K_range, inertias, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].axvline(x=4, color='r', linestyle='--', label='k optimal = 4')\n",
    "axes[0].set_xlabel('Nombre de Clusters (k)')\n",
    "axes[0].set_ylabel('Inertie (Within-Cluster Sum of Squares)')\n",
    "axes[0].set_title('M√©thode du Coude (Elbow Method)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette Score\n",
    "axes[1].plot(K_range, silhouette_scores, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "axes[1].axvline(x=4, color='r', linestyle='--', label='k optimal = 4')\n",
    "axes[1].set_xlabel('Nombre de Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score vs k')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpr√©tation:\")\n",
    "print(\"- Elbow: Chercher le 'coude' o√π l'inertie diminue moins rapidement\")\n",
    "print(\"- Silhouette: Plus proche de 1 = meilleurs clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Impact du nombre de clusters k\n",
    "k_values = [2, 3, 4, 6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, k in enumerate(k_values):\n",
    "    kmeans_k = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
    "    y_pred_k = kmeans_k.fit_predict(X_blobs)\n",
    "    centers_k = kmeans_k.cluster_centers_\n",
    "    \n",
    "    silhouette = silhouette_score(X_blobs, y_pred_k)\n",
    "    \n",
    "    axes[idx].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_pred_k, cmap='viridis',\n",
    "                      s=50, alpha=0.7, edgecolors='k')\n",
    "    axes[idx].scatter(centers_k[:, 0], centers_k[:, 1], c='red', s=300, alpha=0.8,\n",
    "                      marker='X', edgecolors='black', linewidths=2)\n",
    "    axes[idx].set_title(f'k={k}\\nSilhouette: {silhouette:.3f}, Inertie: {kmeans_k.inertia_:.1f}')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Limitations de K-Means: formes non convexes\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "X_circles, _ = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "datasets = [('Moons', X_moons), ('Circles', X_circles)]\n",
    "\n",
    "for row, (name, X) in enumerate(datasets):\n",
    "    # Donn√©es brutes\n",
    "    axes[row, 0].scatter(X[:, 0], X[:, 1], s=50, alpha=0.7, edgecolors='k')\n",
    "    axes[row, 0].set_title(f'{name} Dataset')\n",
    "    axes[row, 0].set_xlabel('Feature 1')\n",
    "    axes[row, 0].set_ylabel('Feature 2')\n",
    "    \n",
    "    # K-Means (k=2)\n",
    "    kmeans_shape = KMeans(n_clusters=2, random_state=42)\n",
    "    y_pred_shape = kmeans_shape.fit_predict(X)\n",
    "    centers_shape = kmeans_shape.cluster_centers_\n",
    "    \n",
    "    axes[row, 1].scatter(X[:, 0], X[:, 1], c=y_pred_shape, cmap='viridis',\n",
    "                         s=50, alpha=0.7, edgecolors='k')\n",
    "    axes[row, 1].scatter(centers_shape[:, 0], centers_shape[:, 1], \n",
    "                         c='red', s=300, alpha=0.8, marker='X', \n",
    "                         edgecolors='black', linewidths=2)\n",
    "    axes[row, 1].set_title(f'{name} - K-Means (k=2)\\n√âchec sur formes non convexes')\n",
    "    axes[row, 1].set_xlabel('Feature 1')\n",
    "    axes[row, 1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Limitation de K-Means:\")\n",
    "print(\"- Suppose des clusters sph√©riques/convexes\")\n",
    "print(\"- √âchoue sur formes complexes (moons, circles, etc.)\")\n",
    "print(\"- Solution: DBSCAN ou autres algorithmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : DBSCAN (Density-Based Spatial Clustering)\n",
    "\n",
    "### Principe\n",
    "- Clustering bas√© sur la densit√©\n",
    "- D√©tecte des formes arbitraires\n",
    "- Identifie les outliers (points de bruit)\n",
    "- Hyperparam√®tres: eps (rayon), min_samples (densit√©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 DBSCAN sur Moons et Circles\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "datasets = [('Moons', X_moons), ('Circles', X_circles)]\n",
    "\n",
    "for row, (name, X) in enumerate(datasets):\n",
    "    # Donn√©es brutes\n",
    "    axes[row, 0].scatter(X[:, 0], X[:, 1], s=50, alpha=0.7, edgecolors='k')\n",
    "    axes[row, 0].set_title(f'{name} Dataset')\n",
    "    axes[row, 0].set_xlabel('Feature 1')\n",
    "    axes[row, 0].set_ylabel('Feature 2')\n",
    "    \n",
    "    # K-Means\n",
    "    kmeans_db = KMeans(n_clusters=2, random_state=42)\n",
    "    y_km = kmeans_db.fit_predict(X)\n",
    "    \n",
    "    axes[row, 1].scatter(X[:, 0], X[:, 1], c=y_km, cmap='viridis',\n",
    "                         s=50, alpha=0.7, edgecolors='k')\n",
    "    axes[row, 1].set_title(f'{name} - K-Means (√©chec)')\n",
    "    axes[row, 1].set_xlabel('Feature 1')\n",
    "    axes[row, 1].set_ylabel('Feature 2')\n",
    "    \n",
    "    # DBSCAN\n",
    "    dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "    y_db = dbscan.fit_predict(X)\n",
    "    \n",
    "    # Outliers (label -1) en noir\n",
    "    unique_labels = set(y_db)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels) - (1 if -1 in unique_labels else 0)))\n",
    "    \n",
    "    for k, col in zip([l for l in unique_labels if l != -1], colors):\n",
    "        class_member_mask = (y_db == k)\n",
    "        axes[row, 2].scatter(X[class_member_mask, 0], X[class_member_mask, 1],\n",
    "                             c=[col], s=50, alpha=0.7, edgecolors='k')\n",
    "    \n",
    "    # Outliers\n",
    "    if -1 in unique_labels:\n",
    "        outliers_mask = (y_db == -1)\n",
    "        axes[row, 2].scatter(X[outliers_mask, 0], X[outliers_mask, 1],\n",
    "                             c='black', s=50, alpha=0.5, edgecolors='red', \n",
    "                             linewidths=1, label='Outliers')\n",
    "    \n",
    "    n_clusters = len(set(y_db)) - (1 if -1 in y_db else 0)\n",
    "    n_outliers = list(y_db).count(-1)\n",
    "    \n",
    "    axes[row, 2].set_title(f'{name} - DBSCAN (succ√®s)\\nClusters: {n_clusters}, Outliers: {n_outliers}')\n",
    "    axes[row, 2].set_xlabel('Feature 1')\n",
    "    axes[row, 2].set_ylabel('Feature 2')\n",
    "    if n_outliers > 0:\n",
    "        axes[row, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Impact des hyperparam√®tres eps et min_samples\n",
    "eps_values = [0.1, 0.2, 0.3, 0.5]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, eps in enumerate(eps_values):\n",
    "    dbscan_eps = DBSCAN(eps=eps, min_samples=5)\n",
    "    y_pred_eps = dbscan_eps.fit_predict(X_moons)\n",
    "    \n",
    "    n_clusters = len(set(y_pred_eps)) - (1 if -1 in y_pred_eps else 0)\n",
    "    n_outliers = list(y_pred_eps).count(-1)\n",
    "    \n",
    "    unique_labels = set(y_pred_eps)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, max(1, len(unique_labels) - (1 if -1 in unique_labels else 0))))\n",
    "    \n",
    "    for k, col in zip([l for l in unique_labels if l != -1], colors):\n",
    "        class_member_mask = (y_pred_eps == k)\n",
    "        axes[idx].scatter(X_moons[class_member_mask, 0], X_moons[class_member_mask, 1],\n",
    "                         c=[col], s=50, alpha=0.7, edgecolors='k')\n",
    "    \n",
    "    if -1 in unique_labels:\n",
    "        outliers_mask = (y_pred_eps == -1)\n",
    "        axes[idx].scatter(X_moons[outliers_mask, 0], X_moons[outliers_mask, 1],\n",
    "                         c='black', s=50, alpha=0.5, edgecolors='red', linewidths=1)\n",
    "    \n",
    "    axes[idx].set_title(f'eps={eps}, min_samples=5\\nClusters: {n_clusters}, Outliers: {n_outliers}')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Impact de eps:\")\n",
    "print(\"- eps petit: Plus de clusters, plus d'outliers\")\n",
    "print(\"- eps grand: Moins de clusters, fusion possible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : Clustering Hi√©rarchique\n",
    "\n",
    "### Principe\n",
    "- Construit une hi√©rarchie de clusters\n",
    "- Agglom√©ratif (bottom-up) ou divisif (top-down)\n",
    "- Linkage: single, complete, average, ward\n",
    "- Visualisation avec dendrogramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Clustering hi√©rarchique sur dataset Blobs\n",
    "# Utiliser un sous-√©chantillon pour la visualisation\n",
    "n_samples = 100\n",
    "X_small, y_small = make_blobs(n_samples=n_samples, centers=3, \n",
    "                              cluster_std=0.8, random_state=42)\n",
    "\n",
    "# Calcul de la matrice de linkage\n",
    "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, method in enumerate(linkage_methods):\n",
    "    Z = linkage(X_small, method=method)\n",
    "    \n",
    "    dendrogram(Z, ax=axes[idx], truncate_mode='lastp', p=20)\n",
    "    axes[idx].set_title(f'Dendrogramme - Linkage: {method.capitalize()}')\n",
    "    axes[idx].set_xlabel('Index ou Cluster')\n",
    "    axes[idx].set_ylabel('Distance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"M√©thodes de linkage:\")\n",
    "print(\"- Single: Distance minimale entre points (sensible aux outliers)\")\n",
    "print(\"- Complete: Distance maximale entre points (forme des clusters compacts)\")\n",
    "print(\"- Average: Distance moyenne entre points (compromis)\")\n",
    "print(\"- Ward: Minimise la variance intra-cluster (recommand√©)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Clustering hi√©rarchique avec AgglomerativeClustering\n",
    "n_clusters = 3\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, method in enumerate(linkage_methods):\n",
    "    agg = AgglomerativeClustering(n_clusters=n_clusters, linkage=method)\n",
    "    y_agg = agg.fit_predict(X_small)\n",
    "    \n",
    "    axes[idx].scatter(X_small[:, 0], X_small[:, 1], c=y_agg, cmap='viridis',\n",
    "                      s=100, alpha=0.7, edgecolors='k')\n",
    "    axes[idx].set_title(f'Agglomerative Clustering\\nLinkage: {method.capitalize()}, k={n_clusters}')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Dendrogramme avec seuil de coupe\n",
    "Z_ward = linkage(X_small, method='ward')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Dendrogramme complet\n",
    "dendrogram(Z_ward, ax=axes[0])\n",
    "axes[0].set_title('Dendrogramme Complet (Ward)')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Distance')\n",
    "\n",
    "# Dendrogramme avec ligne de coupe\n",
    "threshold = 15\n",
    "dendrogram(Z_ward, ax=axes[1], color_threshold=threshold)\n",
    "axes[1].axhline(y=threshold, c='red', linestyle='--', linewidth=2, label=f'Threshold={threshold}')\n",
    "axes[1].set_title('Dendrogramme avec Seuil de Coupe')\n",
    "axes[1].set_xlabel('Index')\n",
    "axes[1].set_ylabel('Distance')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpr√©tation:\")\n",
    "print(\"- Hauteur de fusion = dissimilarit√© entre clusters\")\n",
    "print(\"- Couper √† un certain seuil d√©termine le nombre de clusters\")\n",
    "print(\"- Grand saut vertical = bon nombre de clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 4 : √âvaluation de la Qualit√© du Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 M√©triques internes (sans labels vrais)\n",
    "k_range = range(2, 10)\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_eval = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans_eval.fit_predict(X_blobs)\n",
    "    \n",
    "    silhouette_scores.append(silhouette_score(X_blobs, labels))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(X_blobs, labels))\n",
    "    calinski_harabasz_scores.append(calinski_harabasz_score(X_blobs, labels))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Silhouette Score (plus haut = mieux)\n",
    "axes[0].plot(k_range, silhouette_scores, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].axvline(x=4, color='r', linestyle='--', label='k=4 (vrai)')\n",
    "axes[0].set_xlabel('Nombre de Clusters (k)')\n",
    "axes[0].set_ylabel('Silhouette Score')\n",
    "axes[0].set_title('Silhouette Score\\n(plus haut = mieux)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Davies-Bouldin Index (plus bas = mieux)\n",
    "axes[1].plot(k_range, davies_bouldin_scores, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "axes[1].axvline(x=4, color='r', linestyle='--', label='k=4 (vrai)')\n",
    "axes[1].set_xlabel('Nombre de Clusters (k)')\n",
    "axes[1].set_ylabel('Davies-Bouldin Index')\n",
    "axes[1].set_title('Davies-Bouldin Index\\n(plus bas = mieux)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Calinski-Harabasz Index (plus haut = mieux)\n",
    "axes[2].plot(k_range, calinski_harabasz_scores, 'o-', linewidth=2, markersize=8, color='green')\n",
    "axes[2].axvline(x=4, color='r', linestyle='--', label='k=4 (vrai)')\n",
    "axes[2].set_xlabel('Nombre de Clusters (k)')\n",
    "axes[2].set_ylabel('Calinski-Harabasz Index')\n",
    "axes[2].set_title('Calinski-Harabasz Index\\n(plus haut = mieux)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 M√©triques externes (avec labels vrais)\n",
    "kmeans_final = KMeans(n_clusters=4, random_state=42)\n",
    "y_pred_final = kmeans_final.fit_predict(X_blobs)\n",
    "\n",
    "# Adjusted Rand Index et Normalized Mutual Information\n",
    "ari = adjusted_rand_score(y_true, y_pred_final)\n",
    "nmi = normalized_mutual_info_score(y_true, y_pred_final)\n",
    "\n",
    "print(\"M√©triques externes (comparaison avec labels vrais):\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "print(\"\\nInterpr√©tation:\")\n",
    "print(\"- ARI et NMI varient entre 0 (al√©atoire) et 1 (parfait)\")\n",
    "print(\"- ARI et NMI proches de 1 = clustering tr√®s similaire aux labels vrais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©capitulatif\n",
    "\n",
    "### K-Means\n",
    "\n",
    "**Avantages:**\n",
    "- Simple et rapide\n",
    "- Efficace pour clusters sph√©riques/convexes\n",
    "- Scalable (grands datasets)\n",
    "\n",
    "**Inconv√©nients:**\n",
    "- N√©cessite de sp√©cifier k √† l'avance\n",
    "- Sensible √† l'initialisation\n",
    "- Suppose des clusters de forme sph√©rique\n",
    "- Sensible aux outliers\n",
    "\n",
    "**Choix de k:**\n",
    "- M√©thode du coude (Elbow)\n",
    "- Silhouette Score\n",
    "- Davies-Bouldin Index\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "**Avantages:**\n",
    "- D√©tecte des formes arbitraires\n",
    "- Identifie les outliers\n",
    "- Pas besoin de sp√©cifier k\n",
    "- Robuste au bruit\n",
    "\n",
    "**Inconv√©nients:**\n",
    "- Choix de eps et min_samples d√©licat\n",
    "- Difficult√© avec densit√©s variables\n",
    "- Moins efficace en haute dimension\n",
    "\n",
    "### Clustering Hi√©rarchique\n",
    "\n",
    "**Avantages:**\n",
    "- Produit une hi√©rarchie compl√®te\n",
    "- Dendrogramme facilite l'interpr√©tation\n",
    "- Pas besoin de k √† l'avance\n",
    "- D√©terministe\n",
    "\n",
    "**Inconv√©nients:**\n",
    "- Co√ªt calculatoire √©lev√© (O(n¬≤) ou O(n¬≥))\n",
    "- Pas scalable pour grands datasets\n",
    "- D√©cisions irr√©versibles\n",
    "\n",
    "### M√©triques d'√âvaluation\n",
    "\n",
    "**Internes (sans labels):**\n",
    "- Silhouette Score: [-1, 1], plus haut = mieux\n",
    "- Davies-Bouldin Index: [0, ‚àû), plus bas = mieux\n",
    "- Calinski-Harabasz Index: [0, ‚àû), plus haut = mieux\n",
    "\n",
    "**Externes (avec labels):**\n",
    "- Adjusted Rand Index (ARI): [-1, 1], 1 = parfait\n",
    "- Normalized Mutual Information (NMI): [0, 1], 1 = parfait\n",
    "\n",
    "### Quand utiliser quoi?\n",
    "\n",
    "**K-Means:**\n",
    "- Clusters sph√©riques\n",
    "- K connu ou estimable\n",
    "- Grands datasets\n",
    "\n",
    "**DBSCAN:**\n",
    "- Formes complexes\n",
    "- Pr√©sence d'outliers\n",
    "- K inconnu\n",
    "\n",
    "**Hi√©rarchique:**\n",
    "- Petits/moyens datasets\n",
    "- Besoin de hi√©rarchie\n",
    "- Exploration des donn√©es"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}