{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/05_apprentissage_non_supervise/05_exercices.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '05_exercices.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 05 - Exercices d'Apprentissage Non-Supervis√©\n",
    "\n",
    "Exercices pratiques sur le clustering, la r√©duction de dimensionnalit√© et la d√©tection d'anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, load_wine\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 : Segmentation Client (K-Means + PCA)\n",
    "\n",
    "**Objectif**: Segmenter des clients sur Wine dataset.\n",
    "\n",
    "**Consignes**:\n",
    "1. Charger Wine dataset\n",
    "2. Appliquer K-Means avec diff√©rents k\n",
    "3. Visualiser avec PCA 2D\n",
    "4. Interpr√©ter les segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chargement\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "\n",
    "# Standardisation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_wine)\n",
    "\n",
    "print(f\"Shape: {X_wine.shape}\")\n",
    "print(f\"Features: {wine.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Elbow method\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(K_range, inertias, 'o-', linewidth=2)\n",
    "axes[0].set_xlabel('k')\n",
    "axes[0].set_ylabel('Inertie')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(K_range, silhouettes, 'o-', linewidth=2, color='orange')\n",
    "axes[1].set_xlabel('k')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "optimal_k = K_range[np.argmax(silhouettes)]\n",
    "print(f\"k optimal: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Clustering et visualisation PCA\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# PCA 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Labels vrais\n",
    "axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_wine, cmap='viridis',\n",
    "                s=80, alpha=0.7, edgecolors='k')\n",
    "axes[0].set_title('Labels Vrais')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "\n",
    "# Clusters K-Means\n",
    "axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis',\n",
    "                s=80, alpha=0.7, edgecolors='k')\n",
    "centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "axes[1].scatter(centers_pca[:, 0], centers_pca[:, 1], c='red', s=300,\n",
    "                alpha=0.8, marker='X', edgecolors='black', linewidths=2)\n",
    "axes[1].set_title(f'Clusters K-Means (k={optimal_k})')\n",
    "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Interpr√©tation des segments\n",
    "df = pd.DataFrame(X_wine, columns=wine.feature_names)\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "print(\"Profil moyen des clusters:\")\n",
    "print(df.groupby('Cluster').mean().round(2))\n",
    "\n",
    "print(f\"\\nTaille des clusters: {np.bincount(clusters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : D√©tection d'Anomalies\n",
    "\n",
    "**Objectif**: D√©tecter des outliers avec Isolation Forest et One-Class SVM.\n",
    "\n",
    "**Consignes**:\n",
    "1. G√©n√©rer dataset avec outliers\n",
    "2. Appliquer Isolation Forest\n",
    "3. Appliquer One-Class SVM\n",
    "4. Comparer les r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. G√©n√©ration dataset avec outliers\n",
    "np.random.seed(42)\n",
    "X_normal, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.5)\n",
    "X_outliers = np.random.uniform(low=-6, high=6, size=(20, 2))\n",
    "X_anomaly = np.vstack([X_normal, X_outliers])\n",
    "y_true = np.array([1]*300 + [-1]*20)  # 1=normal, -1=outlier\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_normal[:, 0], X_normal[:, 1], label='Normal', alpha=0.6)\n",
    "plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red', label='Outliers', \n",
    "            s=100, marker='x', linewidths=2)\n",
    "plt.title('Dataset avec Outliers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "y_pred_iso = iso_forest.fit_predict(X_anomaly)\n",
    "\n",
    "# 3. One-Class SVM\n",
    "ocsvm = OneClassSVM(nu=0.1, kernel='rbf', gamma='auto')\n",
    "y_pred_svm = ocsvm.fit_predict(X_anomaly)\n",
    "\n",
    "# 4. Visualisation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "methods = [\n",
    "    ('V√©rit√© Terrain', y_true),\n",
    "    ('Isolation Forest', y_pred_iso),\n",
    "    ('One-Class SVM', y_pred_svm)\n",
    "]\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(methods):\n",
    "    normal_mask = (y_pred == 1)\n",
    "    outlier_mask = (y_pred == -1)\n",
    "    \n",
    "    axes[idx].scatter(X_anomaly[normal_mask, 0], X_anomaly[normal_mask, 1],\n",
    "                      label='Normal', alpha=0.6, s=50)\n",
    "    axes[idx].scatter(X_anomaly[outlier_mask, 0], X_anomaly[outlier_mask, 1],\n",
    "                      c='red', label='Outliers', s=100, marker='x', linewidths=2)\n",
    "    axes[idx].set_title(f'{name}\\n{outlier_mask.sum()} outliers d√©tect√©s')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# M√©triques\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "for name, y_pred in [('Isolation Forest', y_pred_iso), ('One-Class SVM', y_pred_svm)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"  Precision: {precision_score(y_true, y_pred, pos_label=-1):.3f}\")\n",
    "    print(f\"  Recall: {recall_score(y_true, y_pred, pos_label=-1):.3f}\")\n",
    "    print(f\"  F1: {f1_score(y_true, y_pred, pos_label=-1):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 : Pipeline Complet Non-Supervis√©\n",
    "\n",
    "**Objectif**: Combiner PCA + Clustering + Visualisation.\n",
    "\n",
    "**Consignes**:\n",
    "1. G√©n√©rer dataset haute dimension\n",
    "2. R√©duire avec PCA (95% variance)\n",
    "3. Clustering avec K-Means et DBSCAN\n",
    "4. Visualiser avec t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset haute dimension\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_high, _ = make_classification(n_samples=500, n_features=50, n_informative=20,\n",
    "                                n_redundant=10, n_clusters_per_class=2, random_state=42)\n",
    "\n",
    "scaler_high = StandardScaler()\n",
    "X_high_scaled = scaler_high.fit_transform(X_high)\n",
    "\n",
    "print(f\"Shape originale: {X_high.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PCA pour 95% variance\n",
    "pca_95 = PCA(n_components=0.95)\n",
    "X_pca_95 = pca_95.fit_transform(X_high_scaled)\n",
    "\n",
    "print(f\"Composantes pour 95% variance: {pca_95.n_components_}\")\n",
    "print(f\"Variance expliqu√©e: {pca_95.explained_variance_ratio_.sum():.1%}\")\n",
    "print(f\"R√©duction: {X_high.shape[1]} ‚Üí {X_pca_95.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Clustering\n",
    "# K-Means\n",
    "kmeans_high = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "clusters_kmeans = kmeans_high.fit_predict(X_pca_95)\n",
    "\n",
    "# DBSCAN\n",
    "dbscan_high = DBSCAN(eps=3, min_samples=10)\n",
    "clusters_dbscan = dbscan_high.fit_predict(X_pca_95)\n",
    "\n",
    "print(f\"K-Means: {len(np.unique(clusters_kmeans))} clusters\")\n",
    "print(f\"DBSCAN: {len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)} clusters\")\n",
    "print(f\"DBSCAN outliers: {list(clusters_dbscan).count(-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualisation t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, n_jobs=-1)\n",
    "X_tsne = tsne.fit_transform(X_pca_95)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# K-Means\n",
    "axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters_kmeans, cmap='viridis',\n",
    "                s=50, alpha=0.7, edgecolors='k', linewidths=0.5)\n",
    "axes[0].set_title('K-Means Clustering (t-SNE visualization)')\n",
    "axes[0].set_xlabel('t-SNE 1')\n",
    "axes[0].set_ylabel('t-SNE 2')\n",
    "\n",
    "# DBSCAN\n",
    "unique_labels = set(clusters_dbscan)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels) - (1 if -1 in unique_labels else 0)))\n",
    "\n",
    "for k, col in zip([l for l in unique_labels if l != -1], colors):\n",
    "    class_member_mask = (clusters_dbscan == k)\n",
    "    axes[1].scatter(X_tsne[class_member_mask, 0], X_tsne[class_member_mask, 1],\n",
    "                    c=[col], s=50, alpha=0.7, edgecolors='k', linewidths=0.5)\n",
    "\n",
    "if -1 in unique_labels:\n",
    "    outliers_mask = (clusters_dbscan == -1)\n",
    "    axes[1].scatter(X_tsne[outliers_mask, 0], X_tsne[outliers_mask, 1],\n",
    "                    c='black', s=50, alpha=0.5, marker='x', linewidths=2, label='Outliers')\n",
    "    axes[1].legend()\n",
    "\n",
    "axes[1].set_title('DBSCAN Clustering (t-SNE visualization)')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©capitulatif\n",
    "\n",
    "### Points cl√©s\n",
    "\n",
    "1. **Segmentation Client (K-Means + PCA)**\n",
    "   - Standardisation essentielle\n",
    "   - Elbow + Silhouette pour choisir k\n",
    "   - PCA pour visualisation 2D\n",
    "   - Interpr√©ter les segments par leurs moyennes\n",
    "\n",
    "2. **D√©tection d'Anomalies**\n",
    "   - Isolation Forest: Rapide, peu de param√®tres\n",
    "   - One-Class SVM: Plus robuste mais plus lent\n",
    "   - Contamination/nu: Proportion attendue d'outliers\n",
    "   - M√©triques: Pr√©cision, Rappel, F1\n",
    "\n",
    "3. **Pipeline Non-Supervis√©**\n",
    "   - PCA d'abord pour r√©duire dimension\n",
    "   - Puis clustering sur espace r√©duit\n",
    "   - t-SNE uniquement pour visualisation finale\n",
    "   - Ne pas faire confiance aux distances t-SNE\n",
    "\n",
    "### Workflow recommand√©\n",
    "\n",
    "1. **Pr√©paration**\n",
    "   - Nettoyage et standardisation\n",
    "   - PCA exploratoire\n",
    "\n",
    "2. **Clustering**\n",
    "   - Tester plusieurs algorithmes\n",
    "   - Optimiser hyperparam√®tres\n",
    "   - Valider avec m√©triques internes\n",
    "\n",
    "3. **Interpr√©tation**\n",
    "   - Visualisation 2D/3D\n",
    "   - Profils des clusters\n",
    "   - Validation m√©tier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}