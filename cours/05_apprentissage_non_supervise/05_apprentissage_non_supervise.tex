% Chapitre 05 - Apprentissage Non-Supervis√©

\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}
\usepackage{graphicx}
\usepackage{xcolor}

% ===== UNICODE CHARACTERS SUPPORT =====
\usepackage{newunicodechar}

% Emojis et symboles
\newunicodechar{‚úÖ}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚ùå}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚úì}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚úó}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚ö†}{\textcolor{orange!80!black}{\textbf{/!\textbackslash}}}
\newunicodechar{üí°}{\textcolor{blue!70!black}{\textbf{(i)}}}
\newunicodechar{üéØ}{\textcolor{purple!70!black}{\textbf{$\star$}}}
\newunicodechar{üìä}{\textcolor{blue!70!black}{\textbf{[=]}}}

% √âtoiles (pour tableaux)
\newunicodechar{‚òÖ}{\textcolor{orange!80!black}{$\star$}}
\newunicodechar{‚òÜ}{\textcolor{gray!50}{$\star$}}

% Fl√®ches
\newunicodechar{‚Üí}{$\rightarrow$}
\newunicodechar{‚Üê}{$\leftarrow$}
\newunicodechar{‚Üë}{$\uparrow$}
\newunicodechar{‚Üì}{$\downarrow$}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 05 - Apprentissage Non-Supervis√©},
    pdfauthor={Cours ML},
}
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}

% ===== TCOLORBOX AVEC EMOJIS =====
\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö† Attention,
    breakable
}

\newtcolorbox{definition}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition,
    breakable
}

\newtcolorbox{astuce}{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

\newtcolorbox{remarque}{
    colback=yellow!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=üí° Remarque,
    breakable
}

\newtcolorbox{important}{
    colback=purple!5!white,
    colframe=purple!75!black,
    fonttitle=\bfseries,
    title=‚ö† Important,
    breakable
}

\newtcolorbox{exemple}{
    colback=gray!5!white,
    colframe=gray!75!black,
    fonttitle=\bfseries,
    title=Exemple,
    breakable
}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 05 - Apprentissage Non-Supervis√©}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIG =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}
\lstset{style=pythonstyle}


\newtcolorbox{theoreme}[1]{colback=green!5!white, colframe=green!75!black, fonttitle=\bfseries, title=Th√©or√®me: #1, breakable}




\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}

\begin{document}

% ===== TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]
    \vspace{1cm}
    {\LARGE Chapitre 05}\\[0.3cm]
    {\LARGE\bfseries Apprentissage Non-Supervis√©}\\[2cm]
    \vfill
    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Ma√Ætriser les algorithmes de clustering (K-Means, DBSCAN, Hierarchical)
        \item Comprendre la r√©duction de dimensionnalit√© (PCA, t-SNE, UMAP)
        \item D√©tecter des anomalies dans les donn√©es
        \item Appliquer ces techniques √† des probl√®mes r√©els
    \end{itemize}}
    \vfill
    {\large
    \textbf{Pr√©requis :} Chapitres 00, 01, 02\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 5-7 heures\\[0.3cm]
    \textbf{Notebooks :} \texttt{05\_*.ipynb}}
    \vfill
    {\large Cours ML - Sandbox-ML\\ Version 1.0 - 2026}
\end{titlepage}

\tableofcontents
\newpage

% ===== INTRO =====
\section{Introduction}

\begin{definition}{Apprentissage Non-Supervis√©}
L'apprentissage non-supervis√© consiste √† d√©couvrir des structures cach√©es dans des donn√©es \textbf{non √©tiquet√©es} $\{\vect{x}_i\}_{i=1}^n$ sans labels $y$.
\end{definition}

\textbf{Diff√©rence avec apprentissage supervis√© :}
\begin{itemize}
    \item \textbf{Supervis√© :} Apprendre $f: X \to Y$ √† partir de $\{(\vect{x}_i, y_i)\}$
    \item \textbf{Non-supervis√© :} D√©couvrir la structure de $\{\vect{x}_i\}$ (groupes, dimensions, anomalies)
\end{itemize}

\textbf{T√¢ches principales :}
\begin{enumerate}
    \item \textbf{Clustering :} Regrouper instances similaires
    \item \textbf{R√©duction de dimensionnalit√© :} Projeter donn√©es dans espace r√©duit
    \item \textbf{D√©tection d'anomalies :} Identifier points atypiques
    \item \textbf{Apprentissage de repr√©sentation :} Autoencoders, embeddings
\end{enumerate}

\begin{exemple}{Applications}
\begin{itemize}
    \item \textbf{Marketing :} Segmentation de clients (clustering)
    \item \textbf{Biologie :} D√©couverte de nouveaux types cellulaires
    \item \textbf{Compression :} PCA pour r√©duire taille des donn√©es
    \item \textbf{Visualisation :} t-SNE pour visualiser donn√©es haute dimension
    \item \textbf{S√©curit√© :} D√©tection de fraudes, intrusions r√©seau
\end{itemize}
\end{exemple}

% ===== PARTIE 1: CLUSTERING =====
\part{Clustering}

\section{K-Means}

\subsection{Principe}

\begin{definition}{K-Means}
K-Means partitionne $n$ instances en $K$ clusters en minimisant la variance intra-cluster.
\end{definition}

\textbf{Objectif :} Minimiser l'inertie (somme des distances carr√©es au centro√Øde) :
\begin{equation}
    J = \sum_{k=1}^K \sum_{\vect{x}_i \in C_k} \|\vect{x}_i - \vect{\mu}_k\|^2
\end{equation}
o√π $\vect{\mu}_k$ est le centro√Øde du cluster $C_k$.

\subsection{Algorithme}

\begin{algorithm}[H]
\caption{K-Means (Lloyd's Algorithm)}
\begin{algorithmic}[1]
\REQUIRE Donn√©es $\{\vect{x}_i\}_{i=1}^n$, nombre de clusters $K$
\ENSURE Clusters $\{C_1, \ldots, C_K\}$, centro√Ødes $\{\vect{\mu}_1, \ldots, \vect{\mu}_K\}$
\STATE Initialiser $K$ centro√Ødes al√©atoirement
\REPEAT
    \STATE \textbf{(E-step)} Assigner chaque point au centro√Øde le plus proche :
    \STATE \quad $C_k = \{\vect{x}_i : k = \argmin_j \|\vect{x}_i - \vect{\mu}_j\|^2\}$
    \STATE \textbf{(M-step)} Recalculer centro√Ødes :
    \STATE \quad $\vect{\mu}_k = \frac{1}{|C_k|} \sum_{\vect{x}_i \in C_k} \vect{x}_i$
\UNTIL{Convergence (centro√Ødes ne bougent plus)}
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.7]
    % It√©ration 0: Initialisation
    \begin{scope}[xshift=0cm]
        \node at (3, 5) {\textbf{Iter 0: Init}};
        \draw[->] (0,0) -- (6,0);
        \draw[->] (0,0) -- (0,4.5);

        % Points (fixes pour toutes les it√©rations)
        \foreach \x/\y in {1/1, 1.5/1.5, 1.2/2, 4/1, 4.5/1.5, 4.2/0.8, 2/3.5, 2.5/4, 1.8/3.2}
            \filldraw[gray] (\x,\y) circle (2pt);

        % Centro√Ødes initiaux al√©atoires
        \filldraw[red] (1.5,0.5) circle (4pt);
        \filldraw[blue] (3,2) circle (4pt);
        \filldraw[green!60!black] (5,3.5) circle (4pt);

        \node[font=\tiny] at (3, -0.5) {Centro√Ødes al√©atoires};
    \end{scope}

    % It√©ration 1: E-step
    \begin{scope}[xshift=7.5cm]
        \node at (3, 5) {\textbf{Iter 1: E-step}};
        \draw[->] (0,0) -- (6,0);
        \draw[->] (0,0) -- (0,4.5);

        % Points color√©s selon cluster le plus proche
        \foreach \x/\y in {1/1, 1.5/1.5, 1.2/2}
            \filldraw[red] (\x,\y) circle (2pt);
        \foreach \x/\y in {4/1, 4.5/1.5, 4.2/0.8}
            \filldraw[blue] (\x,\y) circle (2pt);
        \foreach \x/\y in {2/3.5, 2.5/4, 1.8/3.2}
            \filldraw[green!60!black] (\x,\y) circle (2pt);

        % Anciens centro√Ødes
        \filldraw[red!50] (1.5,0.5) circle (4pt);
        \filldraw[blue!50] (3,2) circle (4pt);
        \filldraw[green!50!black] (5,3.5) circle (4pt);

        \node[font=\tiny] at (3, -0.5) {Assigner aux clusters};
    \end{scope}

    % It√©ration 1: M-step
    \begin{scope}[xshift=15cm]
        \node at (3, 5) {\textbf{Iter 1: M-step}};
        \draw[->] (0,0) -- (6,0);
        \draw[->] (0,0) -- (0,4.5);

        % Points color√©s
        \foreach \x/\y in {1/1, 1.5/1.5, 1.2/2}
            \filldraw[red] (\x,\y) circle (2pt);
        \foreach \x/\y in {4/1, 4.5/1.5, 4.2/0.8}
            \filldraw[blue] (\x,\y) circle (2pt);
        \foreach \x/\y in {2/3.5, 2.5/4, 1.8/3.2}
            \filldraw[green!60!black] (\x,\y) circle (2pt);

        % Nouveaux centro√Ødes (moyennes)
        \filldraw[red] (1.23,1.5) circle (5pt);
        \filldraw[blue] (4.23,1.1) circle (5pt);
        \filldraw[green!60!black] (2.1,3.57) circle (5pt);

        % Fl√®ches montrant d√©placement
        \draw[->, thick, red!50] (1.5,0.5) -- (1.23,1.5);
        \draw[->, thick, blue!50] (3,2) -- (4.23,1.1);
        \draw[->, thick, green!50!black] (5,3.5) -- (2.1,3.57);

        \node[font=\tiny] at (3, -0.5) {Recalculer centro√Ødes};
    \end{scope}

    % Convergence
    \begin{scope}[xshift=22.5cm]
        \node at (3, 5) {\textbf{Converg√©}};
        \draw[->] (0,0) -- (6,0);
        \draw[->] (0,0) -- (0,4.5);

        % Points color√©s
        \foreach \x/\y in {1/1, 1.5/1.5, 1.2/2}
            \filldraw[red] (\x,\y) circle (2pt);
        \foreach \x/\y in {4/1, 4.5/1.5, 4.2/0.8}
            \filldraw[blue] (\x,\y) circle (2pt);
        \foreach \x/\y in {2/3.5, 2.5/4, 1.8/3.2}
            \filldraw[green!60!black] (\x,\y) circle (2pt);

        % Centro√Ødes finaux
        \filldraw[red] (1.23,1.5) circle (5pt);
        \filldraw[blue] (4.23,1.1) circle (5pt);
        \filldraw[green!60!black] (2.1,3.57) circle (5pt);

        \node[red, above, font=\tiny] at (1.23,1.5) {$\mu_1$};
        \node[blue, above, font=\tiny] at (4.23,1.1) {$\mu_2$};
        \node[green!60!black, above, font=\tiny] at (2.1,3.57) {$\mu_3$};

        \node[font=\tiny] at (3, -0.5) {Centro√Ødes stables};
    \end{scope}

\end{tikzpicture}
\caption{It√©rations de K-Means: (1) initialisation al√©atoire des centro√Ødes, (2) E-step assigne chaque point au centro√Øde le plus proche, (3) M-step recalcule les centro√Ødes comme moyenne de chaque cluster, (4) convergence quand les centro√Ødes ne bougent plus.}
\label{fig:kmeans_iterations}
\end{figure}

\textbf{Complexit√© :} $O(nKdT)$ o√π $T$ = nombre d'it√©rations (g√©n√©ralement petit).

\subsection{Initialisation}

\textbf{Probl√®me :} K-Means sensible √† l'initialisation (peut converger vers minimum local).

\textbf{Solutions :}
\begin{itemize}
    \item \textbf{Random :} Choisir $K$ points al√©atoires
    \item \textbf{K-Means++ :} Initialisation intelligente (√©loigner les centro√Ødes initiaux)
    \item \textbf{Multiple runs :} Ex√©cuter plusieurs fois, garder meilleur r√©sultat
\end{itemize}

\subsection{Choix de $K$ (M√©thode du Coude)}

\textbf{Elbow Method :}
\begin{enumerate}
    \item Ex√©cuter K-Means pour diff√©rentes valeurs de $K$
    \item Tracer inertie en fonction de $K$
    \item Choisir $K$ au "coude" de la courbe (compromis)
\end{enumerate}

\textbf{Autres m√©thodes :}
\begin{itemize}
    \item \textbf{Silhouette Score :} Mesure la coh√©sion et s√©paration des clusters
    \item \textbf{Gap Statistic :} Compare inertie √† une baseline al√©atoire
\end{itemize}

\subsection{Avantages et Limites}

\textbf{Avantages :}
\begin{itemize}
    \item Simple et rapide
    \item Scalable (grands datasets)
    \item Facile √† interpr√©ter
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item N√©cessite sp√©cifier $K$ a priori
    \item Suppose clusters sph√©riques et de taille similaire
    \item Sensible aux outliers
    \item Sensible √† l'initialisation
    \item N√©cessite normalisation des features
\end{itemize}

\begin{lstlisting}[caption=K-Means avec scikit-learn]
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Normalisation (important!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-Means
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)
labels = kmeans.fit_predict(X_scaled)
centroids = kmeans.cluster_centers_

# Inertie
print(f"Inertie: {kmeans.inertia_:.2f}")
\end{lstlisting}

% ===== DBSCAN =====
\section{DBSCAN}

\subsection{Principe}

\begin{definition}{DBSCAN (Density-Based Spatial Clustering)}
DBSCAN regroupe les points dens√©ment connect√©s et identifie les outliers. Bas√© sur deux param√®tres : $\epsilon$ (rayon) et \texttt{min\_samples} (densit√© minimale).
\end{definition}

\textbf{Types de points :}
\begin{itemize}
    \item \textbf{Core point :} Point avec au moins \texttt{min\_samples} voisins dans rayon $\epsilon$
    \item \textbf{Border point :} Voisin d'un core point mais pas core lui-m√™me
    \item \textbf{Noise point :} Ni core ni border (outlier)
\end{itemize}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.2]
    % Axes
    \draw[->] (0,0) -- (8,0) node[right] {$x_1$};
    \draw[->] (0,0) -- (0,6) node[above] {$x_2$};

    % Cluster 1 (rouge) - en bas √† gauche
    \foreach \x/\y in {1.5/1.5, 2/1.2, 1.8/2, 2.3/1.8, 1.2/1, 2.5/1.5}
        \filldraw[red] (\x,\y) circle (2.5pt);

    % Cluster 2 (bleu) - en haut √† droite
    \foreach \x/\y in {5.5/4, 6/4.5, 5.8/3.5, 6.5/4.2, 5.2/4.3, 6.2/3.8}
        \filldraw[blue] (\x,\y) circle (2.5pt);

    % Core point avec cercle epsilon
    \filldraw[red] (2,1.5) circle (3pt);
    \draw[red, dashed, thick] (2,1.5) circle (0.7);
    \node[red, below] at (2,0.7) {Core};
    \node[red, font=\tiny, right] at (2.7,1.5) {$\epsilon$};

    % Border point
    \filldraw[orange] (2.8,2.2) circle (3pt);
    \draw[orange, dashed] (2.8,2.2) circle (0.7);
    \node[orange, above] at (2.8,2.9) {Border};

    % Noise points (outliers)
    \filldraw[gray] (4,2) circle (3pt);
    \draw[gray, dashed] (4,2) circle (0.7);
    \node[gray, below] at (4,1.3) {Noise};

    \filldraw[gray] (3.5,5) circle (3pt);
    \draw[gray, dashed] (3.5,5) circle (0.7);
    \node[gray, above] at (3.5,5.7) {Noise};

    % Annotations des cercles epsilon
    \draw[->, thick, green!60!black] (1,3.5) -- (2,2.2);
    \node[green!60!black, align=center, draw, fill=yellow!10, rounded corners, font=\small] at (1,4.2) {
        Rayon $\epsilon$\\
        min\_samples=3
    };

    % L√©gende
    \node[draw, fill=white, align=left, rounded corners] at (6, 1.5) {
        \textcolor{red}{$\bullet$} Core (>= 3 voisins)\\
        \textcolor{orange}{$\bullet$} Border (<3, mais pr√®s core)\\
        \textcolor{gray}{$\bullet$} Noise (isol√©)
    };

    % Montrer qu'un core point a >= min_samples voisins
    \draw[red, very thick] (1.5,1.5) -- (2,1.5);
    \draw[red, very thick] (2,1.2) -- (2,1.5);
    \draw[red, very thick] (1.8,2) -- (2,1.5);
    \draw[red, very thick] (2.3,1.8) -- (2,1.5);

    % Montrer qu'un border point est pr√®s d'un core
    \draw[orange, thick, dotted] (2.3,1.8) -- (2.8,2.2);

\end{tikzpicture}
\caption{DBSCAN: types de points selon densit√©. Les core points (rouge) ont $\geq$ min\_samples voisins dans rayon $\epsilon$, les border points (orange) sont proches d'un core mais pas core eux-m√™mes, les noise points (gris) sont isol√©s et consid√©r√©s comme outliers.}
\label{fig:dbscan_point_types}
\end{figure}

\subsection{Algorithme}

\begin{algorithm}[H]
\caption{DBSCAN}
\begin{algorithmic}[1]
\REQUIRE Donn√©es $\{\vect{x}_i\}$, $\epsilon$, \texttt{min\_samples}
\STATE Marquer tous points comme non-visit√©s
\FOR{chaque point $\vect{x}_i$ non-visit√©}
    \STATE Marquer $\vect{x}_i$ comme visit√©
    \STATE Trouver voisins dans rayon $\epsilon$
    \IF{moins de \texttt{min\_samples} voisins}
        \STATE Marquer comme noise
    \ELSE
        \STATE Cr√©er nouveau cluster, ajouter $\vect{x}_i$
        \STATE √âtendre cluster r√©cursivement avec voisins
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Avantages et Limites}

\textbf{Avantages :}
\begin{itemize}
    \item Pas besoin de sp√©cifier $K$
    \item D√©tecte automatiquement les outliers
    \item G√®re clusters de formes arbitraires
    \item Robuste au bruit
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item Choix d√©licat de $\epsilon$ et \texttt{min\_samples}
    \item Difficile avec densit√©s variables
    \item Co√ªt : $O(n^2)$ (ou $O(n \log n)$ avec index spatial)
\end{itemize}

\begin{lstlisting}[caption=DBSCAN]
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X_scaled)

# -1 = noise/outliers
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)
print(f"Clusters: {n_clusters}, Outliers: {n_noise}")
\end{lstlisting}

% ===== HIERARCHICAL =====
\section{Clustering Hi√©rarchique}

\subsection{Principe}

\begin{definition}{Clustering Hi√©rarchique}
Construit une hi√©rarchie de clusters (dendrogramme) en fusionnant ou divisant successivement les clusters.
\end{definition}

\textbf{Deux approches :}
\begin{itemize}
    \item \textbf{Agglom√©ratif (bottom-up) :} Commence avec $n$ clusters (1 point chacun), fusionne it√©rativement
    \item \textbf{Divisif (top-down) :} Commence avec 1 cluster (tous points), divise it√©rativement
\end{itemize}

L'approche agglom√©rative est la plus courante.

\subsection{Linkage Criteria}

Crit√®res pour mesurer distance entre clusters :

\begin{itemize}
    \item \textbf{Single linkage :} Distance minimale entre points de 2 clusters
    \begin{equation}
        d(C_i, C_j) = \min_{\vect{x} \in C_i, \vect{y} \in C_j} d(\vect{x}, \vect{y})
    \end{equation}

    \item \textbf{Complete linkage :} Distance maximale
    \begin{equation}
        d(C_i, C_j) = \max_{\vect{x} \in C_i, \vect{y} \in C_j} d(\vect{x}, \vect{y})
    \end{equation}

    \item \textbf{Average linkage :} Distance moyenne
    \begin{equation}
        d(C_i, C_j) = \frac{1}{|C_i||C_j|} \sum_{\vect{x} \in C_i} \sum_{\vect{y} \in C_j} d(\vect{x}, \vect{y})
    \end{equation}

    \item \textbf{Ward :} Minimise variance intra-cluster (comme K-Means)
\end{itemize}

\subsection{Dendrogramme}

Le dendrogramme visualise la hi√©rarchie. On peut "couper" √† une hauteur pour obtenir $K$ clusters.

\begin{lstlisting}[caption=Clustering Hi√©rarchique]
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Clustering
hc = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = hc.fit_predict(X_scaled)

# Dendrogramme
linkage_matrix = linkage(X_scaled, method='ward')
plt.figure(figsize=(10, 5))
dendrogram(linkage_matrix)
plt.title('Dendrogramme')
plt.show()
\end{lstlisting}

% ===== PARTIE 2: R√âDUCTION DIMENSIONNALIT√â =====
\part{R√©duction de Dimensionnalit√©}

\section{Principal Component Analysis (PCA)}

\subsection{Motivation}

\textbf{Probl√®mes en haute dimension :}
\begin{itemize}
    \item Visualisation difficile ($d > 3$)
    \item Curse of dimensionality
    \item Redondance (features corr√©l√©es)
    \item Co√ªt de calcul √©lev√©
\end{itemize}

\textbf{Objectif PCA :} Trouver projection lin√©aire dans espace de dimension $k < d$ qui pr√©serve au maximum la variance.

\subsection{Principe Math√©matique}

\begin{definition}{PCA}
PCA cherche les directions (composantes principales) de variance maximale via d√©composition en valeurs propres de la matrice de covariance.
\end{definition}

\textbf{Algorithme :}
\begin{enumerate}
    \item Centrer les donn√©es : $\mat{X}_c = \mat{X} - \bar{\vect{x}}$
    \item Calculer matrice de covariance : $\mat{C} = \frac{1}{n-1}\mat{X}_c^T\mat{X}_c$
    \item Diagonaliser : trouver valeurs propres $\lambda_i$ et vecteurs propres $\vect{v}_i$ de $\mat{C}$
    \item Trier par $\lambda_i$ d√©croissant
    \item Garder les $k$ premiers vecteurs propres (composantes principales)
    \item Projeter : $\mat{Z} = \mat{X}_c \mat{V}_k$
\end{enumerate}

\textbf{Via SVD (plus efficace) :}
\begin{equation}
    \mat{X}_c = \mat{U}\mat{\Sigma}\mat{V}^T
\end{equation}
Les colonnes de $\mat{V}$ sont les composantes principales.

\subsection{Variance Expliqu√©e}

\textbf{Variance expliqu√©e par $k$ composantes :}
\begin{equation}
    \frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^d \lambda_i}
\end{equation}

\textbf{Choix de $k$ :}
\begin{itemize}
    \item Conserver 90\% ou 95\% de la variance
    \item M√©thode du coude (scree plot)
\end{itemize}

\subsection{Avantages et Limites}

\textbf{Avantages :}
\begin{itemize}
    \item R√©duction efficace de dimension
    \item √âlimine redondance (d√©corr√®le features)
    \item Acc√©l√®re algorithmes downstream
    \item Visualisation (projection 2D/3D)
    \item Interpr√©table (composantes = combinaisons lin√©aires)
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item Transformation \textbf{lin√©aire} uniquement
    \item Suppose variance = information (pas toujours vrai)
    \item Sensible √† l'√©chelle (n√©cessite standardisation)
    \item Perte d'interpr√©tabilit√© des features originales
\end{itemize}

\begin{lstlisting}[caption=PCA]
from sklearn.decomposition import PCA

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Variance expliqu√©e
print("Variance expliqu√©e par composante:")
print(pca.explained_variance_ratio_)
print(f"Total: {pca.explained_variance_ratio_.sum():.2%}")

# Visualisation
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()
\end{lstlisting}

% ===== t-SNE =====
\section{t-SNE}

\subsection{Principe}

\begin{definition}{t-SNE (t-Distributed Stochastic Neighbor Embedding)}
t-SNE est une technique de r√©duction de dimensionnalit√© \textbf{non-lin√©aire} optimis√©e pour la visualisation. Pr√©serve les structures locales (voisinages).
\end{definition}

\textbf{Id√©e :}
\begin{enumerate}
    \item Mod√©liser similarit√©s entre points en haute dimension (gaussienne)
    \item Mod√©liser similarit√©s en basse dimension (t-Student)
    \item Minimiser divergence KL entre les deux distributions
\end{enumerate}

\subsection{Hyperparam√®tre : Perplexity}

\texttt{perplexity} contr√¥le le nombre effectif de voisins consid√©r√©s :
\begin{itemize}
    \item Petite perplexity (5-10) : Focus sur structure locale
    \item Grande perplexity (30-50) : Focus sur structure globale
    \item Typiquement : 30-50 pour datasets moyens
\end{itemize}

\subsection{Avantages et Limites}

\textbf{Avantages :}
\begin{itemize}
    \item Excellente visualisation de structures complexes
    \item Capture relations non-lin√©aires
    \item R√©v√®le clusters naturels
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item \textbf{Co√ªt :} $O(n^2)$ (lent pour gros datasets)
    \item \textbf{Non-d√©terministe :} R√©sultats varient selon initialisation
    \item \textbf{Pas d'embedding new data :} Doit r√©entra√Æner
    \item \textbf{Distances non interpr√©tables :} Uniquement pour visualisation
    \item \textbf{Sensible √† hyperparam√®tres}
\end{itemize}

\begin{attention}
t-SNE est con√ßu pour \textbf{visualisation}, pas pour r√©duction de dimension comme preprocessing. Utiliser PCA pour preprocessing.
\end{attention}

\begin{lstlisting}[caption=t-SNE]
from sklearn.manifold import TSNE

# t-SNE (lent, r√©duire dimension avec PCA d'abord si d > 50)
if X_scaled.shape[1] > 50:
    pca = PCA(n_components=50)
    X_reduced = pca.fit_transform(X_scaled)
else:
    X_reduced = X_scaled

tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X_reduced)

plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')
plt.title('t-SNE Visualization')
plt.show()
\end{lstlisting}

% ===== UMAP =====
\section{UMAP}

\begin{definition}{UMAP (Uniform Manifold Approximation and Projection)}
UMAP est une alternative r√©cente √† t-SNE, bas√©e sur la th√©orie des vari√©t√©s. Plus rapide et pr√©serve mieux la structure globale.
\end{definition}

\textbf{Avantages vs t-SNE :}
\begin{itemize}
    \item Plus rapide ($O(n)$ avec approximations)
    \item Meilleure pr√©servation structure globale
    \item Peut projeter new data
    \item Moins sensible aux hyperparam√®tres
\end{itemize}

\begin{lstlisting}[caption=UMAP]
import umap

reducer = umap.UMAP(n_components=2, random_state=42)
X_umap = reducer.fit_transform(X_scaled)

plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='viridis')
plt.title('UMAP Visualization')
plt.show()
\end{lstlisting}

% ===== D√âTECTION ANOMALIES =====
\part{D√©tection d'Anomalies}

\section{Introduction}

\begin{definition}{Anomalie (Outlier)}
Point significativement diff√©rent de la majorit√© des donn√©es. Peut √™tre une erreur de mesure, fraude, √©v√©nement rare.
\end{definition}

\textbf{Applications :}
\begin{itemize}
    \item D√©tection de fraudes (cartes bancaires)
    \item Intrusions r√©seau, cyberattaques
    \item D√©fauts de fabrication
    \item Diagnostic m√©dical (cas atypiques)
\end{itemize}

\section{M√©thodes}

\subsection{Isolation Forest}

\textbf{Id√©e :} Anomalies sont rares et diff√©rentes $\Rightarrow$ faciles √† isoler.

\begin{definition}{Isolation Forest}
Construit arbres al√©atoires qui isolent les points. Anomalies n√©cessitent moins de splits pour √™tre isol√©es.
\end{definition}

\textbf{Anomaly Score :} Profondeur moyenne d'isolation (normalis√©e). Score proche de 1 = anomalie.

\begin{lstlisting}[caption=Isolation Forest]
from sklearn.ensemble import IsolationForest

iso_forest = IsolationForest(contamination=0.1, random_state=42)
predictions = iso_forest.fit_predict(X_scaled)
# -1 = anomalie, 1 = normal

anomaly_scores = iso_forest.score_samples(X_scaled)
# Scores n√©gatifs, plus n√©gatif = plus anormal
\end{lstlisting}

\subsection{One-Class SVM}

SVM entra√Æn√© √† apprendre la fronti√®re de la distribution "normale". Points hors fronti√®re = anomalies.

\subsection{Local Outlier Factor (LOF)}

Mesure la densit√© locale d'un point vs ses voisins. Point moins dense que ses voisins = outlier.

% ===== R√âSUM√â =====
\section{R√©sum√©}

\subsection{Points Cl√©s}

\textbf{Clustering :}
\begin{itemize}
    \item \textbf{K-Means :} Rapide, clusters sph√©riques, n√©cessite $K$
    \item \textbf{DBSCAN :} D√©tecte outliers, formes arbitraires, sensible $\epsilon$
    \item \textbf{Hi√©rarchique :} Dendrogramme, flexible
\end{itemize}

\textbf{R√©duction dimensionnalit√© :}
\begin{itemize}
    \item \textbf{PCA :} Lin√©aire, variance maximale, preprocessing
    \item \textbf{t-SNE :} Non-lin√©aire, visualisation, lent
    \item \textbf{UMAP :} Comme t-SNE mais plus rapide
\end{itemize}

\textbf{D√©tection anomalies :}
\begin{itemize}
    \item \textbf{Isolation Forest :} Rapide, efficace
    \item \textbf{One-Class SVM, LOF :} Alternatives
\end{itemize}

\section{Pour Aller Plus Loin}

Chapitre suivant : \textbf{Chapitre 06 - R√©seaux de Neurones Fondamentaux}

\section*{R√©f√©rences}

\begin{enumerate}
    \item Hastie, T., et al. (2009). \textit{The Elements of Statistical Learning}. Springer.
    \item van der Maaten, L., \& Hinton, G. (2008). "Visualizing Data using t-SNE". \textit{JMLR}.
    \item McInnes, L., et al. (2018). "UMAP: Uniform Manifold Approximation and Projection". \textit{arXiv}.
\end{enumerate}

\end{document}
