{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz d'Auto-√âvaluation - Chapitre 04 : Classification Supervis√©e\n",
    "\n",
    "**Instructions** :\n",
    "- Ce quiz contient 15 questions pour tester votre compr√©hension du chapitre\n",
    "- R√©pondez aux questions par vous-m√™me avant de regarder les r√©ponses\n",
    "- Les r√©ponses sont dans une cellule masqu√©e √† la fin\n",
    "- Comptez 1 point par bonne r√©ponse\n",
    "\n",
    "**Bar√®me** :\n",
    "- 13-15 : Excellent ! Vous ma√Ætrisez le chapitre üí™\n",
    "- 10-12 : Bien, relisez les sections o√π vous avez des lacunes\n",
    "- 7-9 : Moyen, relisez le chapitre attentivement\n",
    "- < 7 : Insuffisant, reprenez le chapitre depuis le d√©but\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Question 1 : D√©finition de la Classification\n",
    "Quelle est la diff√©rence principale entre classification et r√©gression ?\n",
    "\n",
    "A) La classification n√©cessite plus de donn√©es  \n",
    "B) La classification pr√©dit une variable cat√©gorielle, la r√©gression une variable continue  \n",
    "C) La classification est toujours plus rapide  \n",
    "D) La classification ne fonctionne qu'avec des images  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : K-Nearest Neighbors (KNN)\n",
    "Quelle affirmation sur KNN est FAUSSE ?\n",
    "\n",
    "A) KNN est un algorithme \"lazy learning\" (pas de phase d'entra√Ænement)  \n",
    "B) KNN n√©cessite de normaliser les features  \n",
    "C) Un $k$ petit risque l'underfitting  \n",
    "D) Le co√ªt de pr√©diction est $O(nd)$ par pr√©diction  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 : Arbres de D√©cision - Mesures d'Impuret√©\n",
    "Parmi ces mesures, laquelle N'est PAS utilis√©e pour √©valuer l'impuret√© dans les arbres de d√©cision ?\n",
    "\n",
    "A) Entropie de Shannon  \n",
    "B) Indice de Gini  \n",
    "C) Mean Squared Error  \n",
    "D) Misclassification Error  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 : Arbres de D√©cision - Avantages\n",
    "Quel est un AVANTAGE majeur des arbres de d√©cision par rapport aux autres algorithmes ?\n",
    "\n",
    "A) Toujours plus pr√©cis que les autres mod√®les  \n",
    "B) Tr√®s interpr√©tables et faciles √† visualiser  \n",
    "C) Jamais d'overfitting  \n",
    "D) G√®rent automatiquement les donn√©es manquantes sans pr√©traitement  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 : Random Forest - Principe\n",
    "Random Forest combine deux techniques. Lesquelles ?\n",
    "\n",
    "A) Boosting + Pruning  \n",
    "B) Bagging + S√©lection al√©atoire de features  \n",
    "C) Stacking + Cross-validation  \n",
    "D) Gradient descent + Regularization  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : Bagging vs Boosting\n",
    "Quelle est la diff√©rence principale entre bagging et boosting ?\n",
    "\n",
    "A) Bagging r√©duit le biais, boosting r√©duit la variance  \n",
    "B) Bagging entra√Æne les mod√®les en parall√®le, boosting de mani√®re s√©quentielle  \n",
    "C) Bagging utilise des arbres, boosting des r√©seaux de neurones  \n",
    "D) Bagging n√©cessite plus de donn√©es que boosting  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 : Random Forest - Hyperparam√®tre\n",
    "Dans Random Forest, combien de features sont typiquement consid√©r√©es √† chaque split pour un probl√®me de classification ?\n",
    "\n",
    "A) Toutes les features $d$  \n",
    "B) $\\sqrt{d}$ (racine carr√©e du nombre de features)  \n",
    "C) $d/2$ (la moiti√©)  \n",
    "D) Une seule feature al√©atoire  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 : Gradient Boosting\n",
    "Dans Gradient Boosting, chaque nouvel arbre :\n",
    "\n",
    "A) Pr√©dit les classes directement  \n",
    "B) Pr√©dit les r√©sidus (erreurs) du mod√®le actuel  \n",
    "C) Remplace l'arbre pr√©c√©dent  \n",
    "D) Est identique au premier arbre  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 : XGBoost, LightGBM, CatBoost\n",
    "Parmi ces biblioth√®ques de boosting, laquelle g√®re NATIVEMENT les features cat√©gorielles ?\n",
    "\n",
    "A) XGBoost  \n",
    "B) LightGBM  \n",
    "C) CatBoost  \n",
    "D) Toutes les trois de mani√®re √©quivalente  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 : SVM - Principe\n",
    "Que cherche √† maximiser un SVM (Support Vector Machine) ?\n",
    "\n",
    "A) Le nombre de support vectors  \n",
    "B) La pr√©cision sur le train set  \n",
    "C) La marge entre les classes  \n",
    "D) La complexit√© du mod√®le  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 : SVM - Hyperparam√®tre C\n",
    "Dans un SVM, un param√®tre $C$ GRAND signifie :\n",
    "\n",
    "A) Marge large, mod√®le simple, risque d'underfitting  \n",
    "B) Marge √©troite, peu d'erreurs tol√©r√©es, risque d'overfitting  \n",
    "C) Le kernel devient lin√©aire  \n",
    "D) Le SVM converge plus rapidement  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12 : SVM - Kernel RBF\n",
    "√Ä quoi sert le kernel RBF (Radial Basis Function) dans un SVM ?\n",
    "\n",
    "A) Acc√©l√©rer l'entra√Ænement  \n",
    "B) Projeter les donn√©es dans un espace de dimension sup√©rieure pour g√©rer la non-lin√©arit√©  \n",
    "C) R√©duire le nombre de support vectors  \n",
    "D) Normaliser automatiquement les features  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13 : Comparaison des Algorithmes\n",
    "Pour un probl√®me n√©cessitant une HAUTE INTERPR√âTABILIT√â, quel algorithme est le plus adapt√© ?\n",
    "\n",
    "A) Random Forest  \n",
    "B) Gradient Boosting  \n",
    "C) Arbre de d√©cision peu profond  \n",
    "D) SVM avec kernel RBF  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14 : Scalabilit√©\n",
    "Quel algorithme est le MOINS scalable pour de tr√®s gros datasets (millions d'exemples) ?\n",
    "\n",
    "A) KNN  \n",
    "B) Random Forest  \n",
    "C) LightGBM  \n",
    "D) SVM  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15 : Normalisation\n",
    "Parmi ces algorithmes, lesquels N√âCESSITENT une normalisation des features ?\n",
    "\n",
    "1. KNN  \n",
    "2. Arbres de d√©cision  \n",
    "3. SVM  \n",
    "4. Random Forest  \n",
    "\n",
    "A) 1 et 3  \n",
    "B) 2 et 4  \n",
    "C) 1, 2 et 3  \n",
    "D) Tous  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Auto-Correction\n",
    "\n",
    "Avant de regarder les r√©ponses, comptez combien de r√©ponses vous avez donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrez vos r√©ponses ici (ex: ['D', 'B', 'A', ...])\n",
    "mes_reponses = []  # TODO: remplir avec vos r√©ponses\n",
    "\n",
    "# R√©ponses correctes (masqu√©es)\n",
    "reponses_correctes = ['B', 'C', 'C', 'B', 'B', 'B', 'B', 'B', 'C', 'C', 'B', 'B', 'C', 'D', 'A']\n",
    "\n",
    "if len(mes_reponses) == 15:\n",
    "    score = sum([1 for i, r in enumerate(mes_reponses) if r.upper() == reponses_correctes[i]])\n",
    "    print(f\"Votre score : {score}/15\")\n",
    "    \n",
    "    if score >= 13:\n",
    "        print(\"\\nüéâ Excellent ! Vous ma√Ætrisez le chapitre !\")\n",
    "    elif score >= 10:\n",
    "        print(\"\\n‚úÖ Bien ! Relisez les sections o√π vous avez des lacunes.\")\n",
    "    elif score >= 7:\n",
    "        print(\"\\n‚ö†Ô∏è  Moyen. Relisez le chapitre attentivement.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Insuffisant. Reprenez le chapitre depuis le d√©but.\")\n",
    "    \n",
    "    # Afficher les erreurs\n",
    "    print(\"\\nD√©tail :\")\n",
    "    for i, (ma_rep, bonne_rep) in enumerate(zip(mes_reponses, reponses_correctes), 1):\n",
    "        if ma_rep.upper() == bonne_rep:\n",
    "            print(f\"Q{i}: ‚úì Correct\")\n",
    "        else:\n",
    "            print(f\"Q{i}: ‚úó Votre r√©ponse: {ma_rep}, Correcte: {bonne_rep}\")\n",
    "else:\n",
    "    print(\"Veuillez remplir toutes les r√©ponses (15 lettres A, B, C ou D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Explications des R√©ponses\n",
    "\n",
    "### Q1 : B\n",
    "**Classification** = pr√©dire une variable cat√©gorielle (classe). **R√©gression** = pr√©dire une variable continue (valeur num√©rique).\n",
    "\n",
    "### Q2 : C\n",
    "**FAUX** : Un $k$ petit risque l'**overfitting** (mod√®le complexe, sensible au bruit), pas l'underfitting. Un $k$ grand risque l'underfitting.\n",
    "\n",
    "### Q3 : C\n",
    "**MSE (Mean Squared Error)** est utilis√© pour la r√©gression, pas la classification. Les mesures d'impuret√© pour classification sont : Entropie, Gini, Misclassification Error.\n",
    "\n",
    "### Q4 : B\n",
    "L'avantage majeur des arbres est leur **interpr√©tabilit√©** : on peut facilement visualiser les r√®gles de d√©cision.\n",
    "\n",
    "### Q5 : B\n",
    "Random Forest = **Bagging** (bootstrap des instances) + **S√©lection al√©atoire de features** √† chaque split.\n",
    "\n",
    "### Q6 : B\n",
    "**Bagging** : mod√®les ind√©pendants en **parall√®le** (r√©duit variance). **Boosting** : mod√®les **s√©quentiels**, chacun corrige les erreurs du pr√©c√©dent (r√©duit biais).\n",
    "\n",
    "### Q7 : B\n",
    "Typiquement **$\\sqrt{d}$** features pour classification, **$d/3$** pour r√©gression (r√®gle empirique).\n",
    "\n",
    "### Q8 : B\n",
    "Chaque nouvel arbre **pr√©dit les r√©sidus** (erreurs) du mod√®le actuel, qu'on ajoute avec un petit poids (learning rate).\n",
    "\n",
    "### Q9 : C\n",
    "**CatBoost** g√®re nativement les features cat√©gorielles (d'o√π son nom). XGBoost et LightGBM n√©cessitent un encodage pr√©alable.\n",
    "\n",
    "### Q10 : C\n",
    "SVM cherche √† maximiser la **marge** = distance entre l'hyperplan de s√©paration et les points les plus proches (support vectors).\n",
    "\n",
    "### Q11 : B\n",
    "$C$ grand : peu d'erreurs tol√©r√©es, marge √©troite, risque **overfitting**. $C$ petit : plus d'erreurs tol√©r√©es, marge large, risque **underfitting**.\n",
    "\n",
    "### Q12 : B\n",
    "Le **kernel RBF** projette implicitement les donn√©es dans un espace de dimension infinie pour g√©rer les fronti√®res **non-lin√©aires** complexes.\n",
    "\n",
    "### Q13 : C\n",
    "Un **arbre de d√©cision peu profond** est le plus interpr√©table : r√®gles simples, visualisation claire. Random Forest et Gradient Boosting sont des bo√Ætes noires.\n",
    "\n",
    "### Q14 : D\n",
    "**SVM** a une complexit√© $O(n^2)$ √† $O(n^3)$, tr√®s mauvais pour gros datasets. **KNN** aussi est lent en pr√©diction mais stocke simplement les donn√©es.\n",
    "\n",
    "### Q15 : A\n",
    "**KNN** et **SVM** n√©cessitent normalisation (sensibles √† l'√©chelle des features). Les arbres et Random Forest sont invariants √† l'√©chelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaines √âtapes\n",
    "\n",
    "- **Score < 10** : Relisez le chapitre 04 attentivement\n",
    "- **Score >= 10** : Passez au Chapitre 05 (Apprentissage Non-Supervis√©)\n",
    "- **R√©vision recommand√©e** : Refaites le quiz dans 2-3 jours pour ancrer les connaissances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
