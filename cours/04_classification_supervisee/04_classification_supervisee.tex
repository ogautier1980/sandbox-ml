% Chapitre 04 - Classification Supervis√©e

\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}
\usepackage{graphicx}
\usepackage{xcolor}

% ===== UNICODE CHARACTERS SUPPORT =====
\usepackage{newunicodechar}

% Emojis et symboles
\newunicodechar{‚úÖ}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚ùå}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚úì}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚úó}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚ö†}{\textcolor{orange!80!black}{\textbf{/!\textbackslash}}}
\newunicodechar{üí°}{\textcolor{blue!70!black}{\textbf{(i)}}}
\newunicodechar{üéØ}{\textcolor{purple!70!black}{\textbf{$\star$}}}
\newunicodechar{üìä}{\textcolor{blue!70!black}{\textbf{[=]}}}

% √âtoiles (pour tableaux)
\newunicodechar{‚òÖ}{\textcolor{orange!80!black}{$\star$}}
\newunicodechar{‚òÜ}{\textcolor{gray!50}{$\star$}}

% Fl√®ches
\newunicodechar{‚Üí}{$\rightarrow$}
\newunicodechar{‚Üê}{$\leftarrow$}
\newunicodechar{‚Üë}{$\uparrow$}
\newunicodechar{‚Üì}{$\downarrow$}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 04 - Classification Supervis√©e},
    pdfauthor={Cours ML},
}
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}

% ===== TCOLORBOX AVEC EMOJIS =====
\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö† Attention,
    breakable
}

\newtcolorbox{definition}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition,
    breakable
}

\newtcolorbox{astuce}{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

\newtcolorbox{remarque}{
    colback=yellow!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=üí° Remarque,
    breakable
}

\newtcolorbox{important}{
    colback=purple!5!white,
    colframe=purple!75!black,
    fonttitle=\bfseries,
    title=‚ö† Important,
    breakable
}

\newtcolorbox{exemple}{
    colback=gray!5!white,
    colframe=gray!75!black,
    fonttitle=\bfseries,
    title=Exemple,
    breakable
}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 04 - Classification Supervis√©e}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIGURATION LISTINGS =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}
\lstset{style=pythonstyle}

% ===== TCOLORBOX =====

\newtcolorbox{theoreme}[1]{colback=green!5!white, colframe=green!75!black, fonttitle=\bfseries, title=Th√©or√®me: #1, breakable}




% ===== COMMANDES =====
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}

\begin{document}

% ===== TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]
    \vspace{1cm}
    {\LARGE Chapitre 04}\\[0.3cm]
    {\LARGE\bfseries Classification Supervis√©e}\\[2cm]
    \vfill
    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Ma√Ætriser les algorithmes de classification (KNN, arbres, SVM)
        \item Comprendre les m√©thodes d'ensemble (bagging, boosting)
        \item Impl√©menter et optimiser des classifieurs
        \item √âvaluer et comparer diff√©rents mod√®les
    \end{itemize}}
    \vfill
    {\large
    \textbf{Pr√©requis :} Chapitres 00, 01, 02, 03\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 6-8 heures\\[0.3cm]
    \textbf{Notebooks :} \texttt{04\_*.ipynb}}
    \vfill
    {\large Cours ML - Sandbox-ML\\ Version 1.0 - 2026}
\end{titlepage}

\tableofcontents
\newpage

% ===== INTRODUCTION =====
\section{Introduction √† la Classification}

\begin{definition}{Classification}
La classification est une t√¢che d'apprentissage supervis√© o√π l'objectif est de pr√©dire une variable cible \textbf{cat√©gorielle} $y \in \{1, 2, \ldots, K\}$ √† partir de features $\vect{x} \in \R^d$.
\end{definition}

\textbf{Types de classification :}
\begin{itemize}
    \item \textbf{Binaire :} $K = 2$ (ex: spam/non-spam, malade/sain)
    \item \textbf{Multi-classe :} $K > 2$ (ex: classification de chiffres 0-9, types d'animaux)
    \item \textbf{Multi-label :} Une instance peut appartenir √† plusieurs classes simultan√©ment
\end{itemize}

\begin{exemple}{Applications}
\begin{itemize}
    \item \textbf{M√©dical :} Diagnostic de maladies (sain, malade A, malade B)
    \item \textbf{Finance :} D√©tection de fraude (frauduleux/l√©gitime), scoring cr√©dit (bon/mauvais)
    \item \textbf{Vision :} Classification d'images (chat, chien, oiseau, etc.)
    \item \textbf{NLP :} Analyse de sentiment (positif/n√©gatif/neutre), classification de documents
\end{itemize}
\end{exemple}

\subsection{Fronti√®re de D√©cision}

Un classifieur apprend une \textbf{fronti√®re de d√©cision} qui s√©pare les diff√©rentes classes dans l'espace des features.

\textbf{Exemples :}
\begin{itemize}
    \item \textbf{Lin√©aire :} Droite (2D), hyperplan ($d$-dimensions) - r√©gression logistique, SVM lin√©aire
    \item \textbf{Non-lin√©aire :} Courbes, surfaces complexes - arbres, SVM kernel, r√©seaux de neurones
\end{itemize}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.9]
    % Fronti√®re lin√©aire
    \begin{scope}[xshift=0cm]
        \node at (3, 4.5) {\textbf{Fronti√®re Lin√©aire}};

        % Axes
        \draw[->] (0,0) -- (6,0) node[right] {\small $x_1$};
        \draw[->] (0,0) -- (0,4) node[above] {\small $x_2$};

        % Points classe 0 (rouge)
        \foreach \x/\y in {0.5/0.5, 0.8/1.2, 1.2/0.8, 1.5/1.5, 0.7/2.0, 1.0/2.5, 1.8/1.2, 1.3/3.0}
            \filldraw[red] (\x,\y) circle (3pt);

        % Points classe 1 (blue)
        \foreach \x/\y in {3.0/0.5, 3.5/1.2, 4.0/0.8, 4.5/1.8, 5.0/2.5, 3.8/2.8, 4.8/3.2, 5.2/1.5}
            \filldraw[blue] (\x,\y) circle (3pt);

        % Fronti√®re lin√©aire (droite)
        \draw[green!60!black, very thick] (2.2,0) -- (2.8,4);
        \node[green!60!black, above right, rotate=70] at (2.5,2) {Fronti√®re};

        % R√©gions
        \node[red, font=\small] at (0.8, 3.5) {Classe 0};
        \node[blue, font=\small] at (4.5, 3.5) {Classe 1};

        \node[font=\small, align=center] at (3, -0.7) {R√©gression logistique\\SVM lin√©aire};
    \end{scope}

    % Fronti√®re non-lin√©aire
    \begin{scope}[xshift=8cm]
        \node at (3, 4.5) {\textbf{Fronti√®re Non-Lin√©aire}};

        % Axes
        \draw[->] (0,0) -- (6,0) node[right] {\small $x_1$};
        \draw[->] (0,0) -- (0,4) node[above] {\small $x_2$};

        % Points classe 0 (rouge) - dispers√©s autour
        \foreach \x/\y in {0.5/0.5, 5.2/0.8, 0.8/3.5, 5.0/3.2, 1.2/1.0, 4.8/2.0, 0.5/2.2, 5.3/1.5}
            \filldraw[red] (\x,\y) circle (3pt);

        % Points classe 1 (blue) - regroup√©s au centre
        \foreach \x/\y in {2.5/1.5, 3.0/2.0, 2.8/2.5, 3.5/1.8, 3.2/2.8, 2.2/2.2, 3.8/2.2, 3.0/1.2}
            \filldraw[blue] (\x,\y) circle (3pt);

        % Fronti√®re non-lin√©aire (ellipse approximative)
        \draw[green!60!black, very thick, smooth] plot[domain=0:360, samples=60] ({3.0 + 1.5*cos(\x)}, {2.0 + 1.2*sin(\x)});

        % R√©gions
        \node[red, font=\small] at (0.8, 3.5) {Classe 0};
        \node[blue, font=\small] at (3.0, 2.0) {Classe 1};

        \node[font=\small, align=center] at (3, -0.7) {KNN, Arbres\\SVM kernel, R\'{e}seaux};
    \end{scope}

\end{tikzpicture}
\caption{Fronti√®res de d√©cision: lin√©aire (gauche) s√©parant par une droite/hyperplan, et non-lin√©aire (droite) capturant des patterns complexes. Le choix d√©pend de la distribution des donn√©es.}
\label{fig:decision_boundaries}
\end{figure}

% ===== KNN =====
\section{K-Nearest Neighbors (KNN)}

\subsection{Principe}

\begin{definition}{K-Nearest Neighbors}
KNN est un algorithme \textbf{bas√© sur l'instance} : pour pr√©dire la classe d'un nouveau point $\vect{x}$, on cherche les $k$ voisins les plus proches dans le train set et on vote √† la majorit√©.
\end{definition}

\begin{algorithm}[H]
\caption{KNN Classification}
\begin{algorithmic}[1]
\REQUIRE Dataset $\{(\vect{x}_i, y_i)\}_{i=1}^n$, point de test $\vect{x}_{\text{test}}$, $k$
\ENSURE Classe pr√©dite $\hat{y}$
\STATE Calculer distances : $d_i = \|\vect{x}_{\text{test}} - \vect{x}_i\|$ pour tout $i$
\STATE Trouver les $k$ plus petites distances (k voisins)
\STATE $\hat{y} = $ classe majoritaire parmi ces $k$ voisins
\RETURN $\hat{y}$
\end{algorithmic}
\end{algorithm}

\textbf{Distance utilis√©e :} Typiquement euclidienne $L^2$, mais peut √™tre Manhattan $L^1$, Minkowski, etc.

\subsection{Hyperparam√®tres}

\begin{itemize}
    \item \textbf{$k$ (nombre de voisins)} :
    \begin{itemize}
        \item $k$ petit : Mod√®le complexe, sensible au bruit, overfitting
        \item $k$ grand : Mod√®le simple, lisse, underfitting
        \item R√®gle empirique : $k = \sqrt{n}$ ou validation crois√©e
    \end{itemize}
    \item \textbf{M√©trique de distance} : Euclidienne, Manhattan, etc.
    \item \textbf{Pond√©ration} : Uniforme ou par distance (voisins proches p√®sent plus)
\end{itemize}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.75]
    % k=1
    \begin{scope}[xshift=0cm]
        \node at (3, 5.2) {\textbf{$k=1$ (Overfitting)}};

        % Axes
        \draw[->] (0,0) -- (6,0) node[right] {\small $x_1$};
        \draw[->] (0,0) -- (0,5) node[above] {\small $x_2$};

        % Points rouges
        \foreach \x/\y in {1/1, 1.5/2, 2/3.5, 0.8/4, 1.2/2.8, 0.5/1.5}
            \filldraw[red] (\x,\y) circle (2.5pt);

        % Points bleus
        \foreach \x/\y in {4/1.5, 4.5/2.8, 5/1, 4.2/3.5, 5.2/4, 4.8/2.2}
            \filldraw[blue] (\x,\y) circle (2.5pt);

        % Point de test
        \filldraw[green!60!black] (2.8,2.2) circle (4pt);
        \node[green!60!black, above] at (2.8,2.2) {\small Test};

        % Plus proche voisin (k=1)
        \draw[green!60!black, very thick] (2.8,2.2) circle (1.05);
        \draw[green!60!black, dashed] (2.8,2.2) -- (2,3.5);

        % Fronti√®re tr√®s irr√©guli√®re
        \draw[orange, very thick, smooth] plot coordinates {
            (0,0.5) (0.5,0.8) (1,1.5) (1.5,2.2) (2.2,3.0) (2.5,3.8) (2.8,4.5) (3.2,3.5) (3.8,2.8) (4.2,2.0) (5,1.5) (6,1.0)
        };

        \node[font=\small, align=center] at (3, -0.8) {Fronti√®re complexe\\Sensible au bruit};
    \end{scope}

    % k=5
    \begin{scope}[xshift=8cm]
        \node at (3, 5.2) {\textbf{$k=5$ (Bon √âquilibre)}};

        % Axes
        \draw[->] (0,0) -- (6,0) node[right] {\small $x_1$};
        \draw[->] (0,0) -- (0,5) node[above] {\small $x_2$};

        % Points rouges
        \foreach \x/\y in {1/1, 1.5/2, 2/3.5, 0.8/4, 1.2/2.8, 0.5/1.5}
            \filldraw[red] (\x,\y) circle (2.5pt);

        % Points bleus
        \foreach \x/\y in {4/1.5, 4.5/2.8, 5/1, 4.2/3.5, 5.2/4, 4.8/2.2}
            \filldraw[blue] (\x,\y) circle (2.5pt);

        % Point de test
        \filldraw[green!60!black] (2.8,2.2) circle (4pt);
        \node[green!60!black, above] at (2.8,2.2) {\small Test};

        % 5 plus proches voisins
        \draw[green!60!black, very thick] (2.8,2.2) circle (2.0);
        \node[green!60!black, font=\tiny, right] at (4.5,2.8) {5 voisins};

        % Fronti√®re liss√©e
        \draw[orange, very thick, smooth] plot coordinates {
            (0,0.5) (1,1.2) (2,2.0) (2.5,2.5) (3,2.8) (3.5,2.5) (4.5,2.0) (5.5,1.2) (6,0.8)
        };

        \node[font=\small, align=center] at (3, -0.8) {Fronti√®re liss\'{e}e\\Bon compromis};
    \end{scope}

    % k=50
    \begin{scope}[xshift=16cm]
        \node at (3, 5.2) {\textbf{$k=50$ (Underfitting)}};

        % Axes
        \draw[->] (0,0) -- (6,0) node[right] {\small $x_1$};
        \draw[->] (0,0) -- (0,5) node[above] {\small $x_2$};

        % Points rouges
        \foreach \x/\y in {1/1, 1.5/2, 2/3.5, 0.8/4, 1.2/2.8, 0.5/1.5}
            \filldraw[red] (\x,\y) circle (2.5pt);

        % Points bleus
        \foreach \x/\y in {4/1.5, 4.5/2.8, 5/1, 4.2/3.5, 5.2/4, 4.8/2.2}
            \filldraw[blue] (\x,\y) circle (2.5pt);

        % Point de test
        \filldraw[green!60!black] (2.8,2.2) circle (4pt);
        \node[green!60!black, above] at (2.8,2.2) {\small Test};

        % Grand cercle (k=50, englobe presque tout)
        \draw[green!60!black, very thick] (2.8,2.2) circle (3.5);
        \node[green!60!black, font=\tiny, below right] at (5.8,3.5) {50 voisins};

        % Fronti√®re tr√®s simple (presque droite)
        \draw[orange, very thick] (2.5,0) -- (3.2,5);

        \node[font=\small, align=center] at (3, -0.8) {Fronti√®re trop simple\\Perd les d\'{e}tails};
    \end{scope}

\end{tikzpicture}
\caption{Impact du param√®tre $k$ dans KNN: $k=1$ cr√©e une fronti√®re complexe qui s'adapte au bruit (overfitting), $k=5$ donne un bon √©quilibre, $k=50$ sur-lisse et perd les d√©tails (underfitting).}
\label{fig:knn_k_parameter}
\end{figure}

\subsection{Avantages et Limites}

\textbf{Avantages :}
\begin{itemize}
    \item Simple √† comprendre et impl√©menter
    \item Pas de phase d'entra√Ænement (lazy learning)
    \item Fonctionne bien pour fronti√®res complexes
    \item Naturellement multi-classe
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item Co√ªt de pr√©diction √©lev√© : $O(nd)$ par pr√©diction
    \item Sensible √† l'√©chelle des features (n√©cessite normalisation)
    \item Curse of dimensionality : performance d√©grad√©e en haute dimension
    \item N√©cessite beaucoup de m√©moire (stocke tout le train set)
\end{itemize}

\begin{lstlisting}[caption=KNN avec scikit-learn]
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

# Normalisation (important pour KNN!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)
y_pred = knn.predict(X_test_scaled)
\end{lstlisting}

% ===== ARBRES DE D√âCISION =====
\section{Arbres de D√©cision}

\subsection{Principe}

\begin{definition}{Arbre de d√©cision}
Un arbre de d√©cision partitionne r√©cursivement l'espace des features en r√©gions homog√®nes via des tests binaires sur les features.
\end{definition}

\textbf{Structure :}
\begin{itemize}
    \item \textbf{N≈ìud interne :} Test sur une feature ($x_j \leq \text{seuil}$)
    \item \textbf{Branche :} R√©sultat du test (gauche: vrai, droite: faux)
    \item \textbf{Feuille :} Pr√©diction finale (classe majoritaire)
\end{itemize}

\subsection{Construction de l'Arbre (CART)}

L'algorithme CART (Classification and Regression Trees) construit l'arbre de mani√®re gloutonne (greedy) :

\begin{algorithm}[H]
\caption{Construction Arbre (CART)}
\begin{algorithmic}[1]
\REQUIRE Dataset $D = \{(\vect{x}_i, y_i)\}$
\STATE Si crit√®re d'arr√™t atteint : cr√©er feuille avec classe majoritaire
\STATE Sinon :
\STATE \quad Trouver meilleure feature $j$ et seuil $\theta$ qui minimise impuret√©
\STATE \quad S√©parer $D$ en $D_{\text{left}}$ et $D_{\text{right}}$
\STATE \quad Construire r√©cursivement sous-arbres gauche et droit
\end{algorithmic}
\end{algorithm}

\subsection{Mesures d'Impuret√©}

Pour choisir le meilleur split, on minimise l'impuret√©.

\textbf{1. Entropie (Shannon) :}
\begin{equation}
    H(D) = -\sum_{k=1}^K p_k \log_2(p_k)
\end{equation}
o√π $p_k$ est la proportion de la classe $k$ dans $D$.

\textbf{2. Indice de Gini :}
\begin{equation}
    \text{Gini}(D) = 1 - \sum_{k=1}^K p_k^2
\end{equation}

\textbf{3. Misclassification Error :}
\begin{equation}
    \text{Error}(D) = 1 - \max_k p_k
\end{equation}

\textbf{Gain d'information :}
\begin{equation}
    \text{IG} = H(D) - \frac{|D_{\text{left}}|}{|D|} H(D_{\text{left}}) - \frac{|D_{\text{right}}|}{|D|} H(D_{\text{right}})
\end{equation}

On choisit le split qui maximise le gain d'information (ou minimise l'impuret√©).

\subsection{Hyperparam√®tres et R√©gularisation}

\textbf{Crit√®res d'arr√™t :}
\begin{itemize}
    \item \texttt{max\_depth} : Profondeur maximale de l'arbre
    \item \texttt{min\_samples\_split} : Nombre min d'√©chantillons pour split
    \item \texttt{min\_samples\_leaf} : Nombre min d'√©chantillons par feuille
    \item \texttt{max\_features} : Nombre max de features consid√©r√©es pour split
\end{itemize}

\textbf{Pruning (√©lagage) :}
\begin{itemize}
    \item \textbf{Pre-pruning :} Arr√™ter t√¥t la construction (via crit√®res ci-dessus)
    \item \textbf{Post-pruning :} Construire arbre complet puis √©laguer (cost-complexity pruning)
\end{itemize}

\subsection{Avantages et Limites}

\textbf{Avantages :}
\begin{itemize}
    \item Tr√®s interpr√©tables (visualisation facile)
    \item G√®rent features num√©riques et cat√©gorielles
    \item Pas besoin de normalisation
    \item Capturent des interactions non-lin√©aires
    \item Rapides en pr√©diction
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item Instables : petits changements dans les donn√©es $\Rightarrow$ arbre tr√®s diff√©rent
    \item Tendance √† l'overfitting
    \item Fronti√®res de d√©cision orthogonales (align√©es sur axes)
    \item Biais vers features avec beaucoup de valeurs
\end{itemize}

\begin{lstlisting}[caption=Arbre de d√©cision]
from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(
    max_depth=5,
    min_samples_split=20,
    criterion='gini'
)
tree.fit(X_train, y_train)
y_pred = tree.predict(X_test)

# Visualisation
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
plt.figure(figsize=(20, 10))
plot_tree(tree, filled=True, feature_names=feature_names)
plt.show()
\end{lstlisting}

% ===== RANDOM FOREST =====
\section{Random Forest}

\subsection{Principe : Ensemble Learning}

\begin{definition}{Ensemble Learning}
Combiner plusieurs mod√®les faibles pour cr√©er un mod√®le fort. L'id√©e : agr√©ger les pr√©dictions de multiples arbres pour r√©duire variance et overfitting.
\end{definition}

\subsection{Bagging (Bootstrap Aggregating)}

\textbf{Algorithme :}
\begin{enumerate}
    \item Pour $b = 1, \ldots, B$ :
    \begin{itemize}
        \item Cr√©er un √©chantillon bootstrap (tirage avec remise) de taille $n$
        \item Entra√Æner un arbre de d√©cision $T_b$ sur cet √©chantillon
    \end{itemize}
    \item Pr√©diction finale : vote majoritaire des $B$ arbres
\end{enumerate}

\textbf{Pourquoi √ßa marche ?}
\begin{itemize}
    \item Arbres individuels : haute variance (overfitting)
    \item Moyenner $B$ arbres : r√©duit la variance
    \item Les arbres sont \textbf{d√©corr√©l√©s} gr√¢ce au bootstrap
\end{itemize}

\subsection{Random Forest = Bagging + Randomisation}

\begin{definition}{Random Forest}
Extension du bagging avec randomisation suppl√©mentaire :
\begin{itemize}
    \item \textbf{Bootstrap des instances} (comme bagging)
    \item \textbf{S√©lection al√©atoire de features} : √Ä chaque split, on consid√®re seulement $m$ features al√©atoires (typiquement $m = \sqrt{d}$)
\end{itemize}
\end{definition}

\textbf{B√©n√©fice :} D√©corr√®le encore plus les arbres $\Rightarrow$ variance r√©duite.

\subsection{Hyperparam√®tres}

\begin{itemize}
    \item \texttt{n\_estimators} : Nombre d'arbres $B$ (plus = mieux, mais diminishing returns)
    \item \texttt{max\_features} : Nombre de features consid√©r√©es par split
    \item \texttt{max\_depth}, \texttt{min\_samples\_split}, etc. : Param√®tres des arbres
    \item \texttt{bootstrap} : Utiliser bootstrap ou non
\end{itemize}

\subsection{Out-of-Bag (OOB) Error}

Chaque arbre est entra√Æn√© sur ~63\% des donn√©es (bootstrap). Les ~37\% restants (out-of-bag) servent √† estimer l'erreur de g√©n√©ralisation sans validation set s√©par√©.

\subsection{Feature Importance}

Random Forest peut calculer l'importance de chaque feature :
\begin{itemize}
    \item \textbf{Mean Decrease Impurity (MDI)} : Moyenne de la r√©duction d'impuret√© apport√©e par chaque feature
    \item \textbf{Mean Decrease Accuracy (MDA)} : D√©gradation de l'accuracy quand on permute la feature
\end{itemize}

\begin{lstlisting}[caption=Random Forest]
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    max_features='sqrt',
    random_state=42
)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

# Feature importance
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]
print("Top features:")
for i in range(5):
    print(f"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}")
\end{lstlisting}

\subsection{Avantages et Limites}

\textbf{Avantages :}
\begin{itemize}
    \item Performances excellentes (souvent meilleures que arbre seul)
    \item R√©duction overfitting
    \item Robuste au bruit et outliers
    \item Feature importance automatique
    \item Parall√©lisable (arbres ind√©pendants)
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item Moins interpr√©table qu'un arbre seul
    \item Plus lent en pr√©diction (doit consulter $B$ arbres)
    \item N√©cessite plus de m√©moire
\end{itemize}

% ===== GRADIENT BOOSTING =====
\section{Gradient Boosting}

\subsection{Principe : Boosting}

\begin{definition}{Boosting}
Entra√Æner des mod√®les s√©quentiellement, o√π chaque nouveau mod√®le se concentre sur les erreurs des mod√®les pr√©c√©dents.
\end{definition}

\textbf{Diff√©rence avec Bagging :}
\begin{itemize}
    \item \textbf{Bagging :} Mod√®les ind√©pendants en parall√®le
    \item \textbf{Boosting :} Mod√®les s√©quentiels, chaque mod√®le corrige les erreurs du pr√©c√©dent
\end{itemize}

\subsection{Gradient Boosting (GBDT)}

\textbf{Id√©e :} Construire un ensemble additif de mod√®les (arbres faibles) en minimisant une fonction de perte via descente de gradient.

\begin{algorithm}[H]
\caption{Gradient Boosting}
\begin{algorithmic}[1]
\REQUIRE Dataset $\{(\vect{x}_i, y_i)\}$, fonction de perte $L$, learning rate $\eta$, $M$ iterations
\STATE Initialiser $F_0(\vect{x}) = \argmin_c \sum_{i=1}^n L(y_i, c)$
\FOR{$m = 1$ to $M$}
    \STATE Calculer pseudo-r√©sidus : $r_{im} = -\frac{\partial L(y_i, F_{m-1}(\vect{x}_i))}{\partial F_{m-1}(\vect{x}_i)}$
    \STATE Entra√Æner arbre $h_m$ sur $\{(\vect{x}_i, r_{im})\}$
    \STATE Mettre √† jour : $F_m(\vect{x}) = F_{m-1}(\vect{x}) + \eta h_m(\vect{x})$
\ENDFOR
\RETURN $F_M(\vect{x})$
\end{algorithmic}
\end{algorithm}

\textbf{Intuition :} Chaque nouvel arbre $h_m$ pr√©dit les r√©sidus (erreurs) du mod√®le actuel $F_{m-1}$, on l'ajoute avec un petit poids $\eta$.

\subsection{Hyperparam√®tres}

\begin{itemize}
    \item \texttt{n\_estimators} : Nombre d'arbres $M$
    \item \texttt{learning\_rate} $\eta$ : Poids de chaque arbre (trade-off avec $M$)
    \item \texttt{max\_depth} : Profondeur des arbres (typiquement faible : 3-6)
    \item \texttt{subsample} : Fraction de donn√©es par arbre (stochastic gradient boosting)
    \item \texttt{min\_samples\_split}, \texttt{min\_samples\_leaf} : R√©gularisation
\end{itemize}

\subsection{XGBoost, LightGBM, CatBoost}

Impl√©mentations optimis√©es et √©tendues de Gradient Boosting :

\textbf{XGBoost (Extreme Gradient Boosting) :}
\begin{itemize}
    \item R√©gularisation $L^1$ et $L^2$
    \item Gestion des valeurs manquantes
    \item Parall√©lisation et optimisations
    \item Tr√®s utilis√© en comp√©titions Kaggle
\end{itemize}

\textbf{LightGBM :}
\begin{itemize}
    \item Gradient-based One-Side Sampling (GOSS)
    \item Exclusive Feature Bundling (EFB)
    \item Tr√®s rapide et efficace en m√©moire
    \item Excellentes performances sur gros datasets
\end{itemize}

\textbf{CatBoost :}
\begin{itemize}
    \item Gestion native des features cat√©gorielles
    \item Ordered boosting (√©vite target leakage)
    \item Robuste aux hyperparam√®tres
\end{itemize}

\begin{lstlisting}[caption=Gradient Boosting avec XGBoost]
from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    random_state=42
)
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)
\end{lstlisting}

% ===== SVM =====
\section{Support Vector Machines (SVM)}

\subsection{Principe : Maximum Margin}

\begin{definition}{SVM}
SVM cherche l'hyperplan qui s√©pare les classes avec la \textbf{marge maximale}. La marge est la distance entre l'hyperplan et les points les plus proches (support vectors).
\end{definition}

\textbf{Cas lin√©airement s√©parable :}

L'hyperplan est d√©fini par :
\begin{equation}
    \vect{w}^T \vect{x} + b = 0
\end{equation}

Pr√©diction : $\hat{y} = \text{sign}(\vect{w}^T \vect{x} + b)$

\textbf{Objectif :} Maximiser la marge $\frac{2}{\|\vect{w}\|}$ $\Leftrightarrow$ Minimiser $\|\vect{w}\|^2$ sous contraintes.

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.1]
    % Axes
    \draw[->] (0,0) -- (8,0) node[right] {$x_1$};
    \draw[->] (0,0) -- (0,6) node[above] {$x_2$};

    % Points classe -1 (rouge, cercles)
    \foreach \x/\y in {1/1.5, 1.5/2.5, 2/1, 1.2/3.5, 2.5/2, 1.8/4, 2.2/3}
        \draw[red, thick] (\x,\y) circle (3pt);

    % Points classe +1 (bleu, croix)
    \foreach \x/\y in {5/2, 5.5/3.5, 6/1.5, 6.5/4, 5.8/2.8, 7/2.5, 6.2/4.5}
        \draw[blue, thick] (\x-0.15,\y-0.15) -- (\x+0.15,\y+0.15) (\x-0.15,\y+0.15) -- (\x+0.15,\y-0.15);

    % Hyperplan de d√©cision (w^T x + b = 0)
    \draw[black, very thick] (3.5,0.5) -- (4.5,5.5) node[right, black] {$\vect{w}^T\vect{x} + b = 0$};

    % Marges (w^T x + b = ¬±1)
    \draw[green!60!black, thick, dashed] (2.5,0.5) -- (3.5,5.5) node[above left, green!60!black, font=\small] {$\vect{w}^T\vect{x} + b = -1$};
    \draw[green!60!black, thick, dashed] (4.5,0.5) -- (5.5,5.5) node[above right, green!60!black, font=\small] {$\vect{w}^T\vect{x} + b = +1$};

    % Support vectors (encercl√©s)
    \draw[orange, very thick] (2,1) circle (5pt);
    \draw[orange, very thick] (2.5,2) circle (5pt);
    \draw[orange, very thick] (5,2) circle (5pt);
    \draw[orange, very thick] (6,1.5) circle (5pt);

    % Annotation support vectors
    \node[orange, below] at (4, 0.2) {Support Vectors};
    \draw[->, orange, thick] (4, 0.4) -- (2, 0.9);
    \draw[->, orange, thick] (4, 0.4) -- (6, 1.4);

    % Marge (double fl√®che)
    \draw[<->, very thick, purple] (3.5,3) -- (4.5,3);
    \node[purple, above, align=center] at (4,3) {Marge\\$\frac{2}{\|\vect{w}\|}$};

    % Vecteur normal w
    \draw[->, very thick, cyan] (4,2.5) -- (4.7,3.9);
    \node[cyan, right] at (4.7,3.9) {$\vect{w}$};

    % L√©gende
    \node[draw, fill=white, align=left, rounded corners] at (2, 5.3) {
        \textcolor{red}{$\circ$} Classe $-1$\\
        \textcolor{blue}{$\times$} Classe $+1$\\
        \textcolor{orange}{$\bigcirc$} Support vectors
    };

\end{tikzpicture}
\caption{SVM avec marge maximale: l'hyperplan noir s√©pare les classes, les marges (vertes) d√©finissent la zone, et les support vectors (orange) sont les points les plus proches qui d√©terminent l'hyperplan. La marge est maximis√©e.}
\label{fig:svm_margin}
\end{figure}

\subsection{Formulation Math√©matique}

\textbf{Hard-margin SVM (s√©parable) :}
\begin{equation}
    \min_{\vect{w}, b} \frac{1}{2} \|\vect{w}\|^2 \quad \text{s.t.} \quad y_i(\vect{w}^T\vect{x}_i + b) \geq 1, \; \forall i
\end{equation}

\textbf{Soft-margin SVM (non-s√©parable) :}
Introduire des variables de slack $\xi_i \geq 0$ pour permettre des erreurs :
\begin{equation}
    \min_{\vect{w}, b, \xi} \frac{1}{2} \|\vect{w}\|^2 + C \sum_{i=1}^n \xi_i \quad \text{s.t.} \quad y_i(\vect{w}^T\vect{x}_i + b) \geq 1 - \xi_i, \; \xi_i \geq 0
\end{equation}

\textbf{Hyperparam√®tre $C$ :}
\begin{itemize}
    \item $C$ grand : Peu d'erreurs tol√©r√©es (risque overfitting)
    \item $C$ petit : Plus d'erreurs tol√©r√©es (underfitting)
\end{itemize}

\subsection{Kernel Trick}

Pour g√©rer des donn√©es non-lin√©airement s√©parables, on projette les donn√©es dans un espace de dimension sup√©rieure via une fonction $\phi(\vect{x})$.

\textbf{Probl√®me :} Calcul co√ªteux de $\phi(\vect{x})$ en haute dimension.

\textbf{Solution : Kernel Trick}
On n'a besoin que des produits scalaires $\phi(\vect{x})^T \phi(\vect{x}')$, qu'on calcule efficacement via un \textbf{kernel} $K(\vect{x}, \vect{x}')$.

\textbf{Kernels courants :}
\begin{itemize}
    \item \textbf{Lin√©aire :} $K(\vect{x}, \vect{x}') = \vect{x}^T\vect{x}'$
    \item \textbf{Polynomial :} $K(\vect{x}, \vect{x}') = (\gamma \vect{x}^T\vect{x}' + r)^d$
    \item \textbf{RBF (Gaussian) :} $K(\vect{x}, \vect{x}') = \exp(-\gamma \|\vect{x} - \vect{x}'\|^2)$
    \item \textbf{Sigmoid :} $K(\vect{x}, \vect{x}') = \tanh(\gamma \vect{x}^T\vect{x}' + r)$
\end{itemize}

Le kernel RBF est le plus populaire (fronti√®res tr√®s flexibles).

\subsection{Hyperparam√®tres}

\begin{itemize}
    \item \texttt{C} : R√©gularisation (trade-off marge vs erreurs)
    \item \texttt{kernel} : Type de kernel
    \item \texttt{gamma} (pour RBF) : Inverse du rayon d'influence
    \begin{itemize}
        \item $\gamma$ petit : Influence large, mod√®le simple
        \item $\gamma$ grand : Influence locale, mod√®le complexe (overfitting)
    \end{itemize}
\end{itemize}

\subsection{Avantages et Limites}

\textbf{Avantages :}
\begin{itemize}
    \item Efficace en haute dimension
    \item Robuste √† l'overfitting (r√©gularisation $C$)
    \item Versatile (diff√©rents kernels)
    \item Bonne performance th√©orique
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item Co√ªt d'entra√Ænement : $O(n^2)$ √† $O(n^3)$ (mauvais pour gros datasets)
    \item Sensible au choix de kernel et hyperparam√®tres
    \item N√©cessite normalisation des features
    \item Difficile √† interpr√©ter
\end{itemize}

\begin{lstlisting}[caption=SVM]
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Normalisation importante pour SVM
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# SVM avec kernel RBF
svm = SVC(kernel='rbf', C=1.0, gamma='scale')
svm.fit(X_train_scaled, y_train)
y_pred = svm.predict(X_test_scaled)
\end{lstlisting}

% ===== COMPARAISON =====
\section{Comparaison des Algorithmes}

\begin{table}[h]
\centering
\caption{Comparaison des algorithmes de classification}
\small
\begin{tabular}{lllll}
\toprule
\textbf{Algorithme} & \textbf{Interpr√©tabilit√©} & \textbf{Vitesse} & \textbf{Performance} & \textbf{Scalabilit√©} \\
\midrule
KNN & Moyenne & Lente (test) & Bonne & Mauvaise \\
Arbre de d√©cision & Excellente & Rapide & Moyenne & Bonne \\
Random Forest & Faible & Moyenne & Excellente & Bonne \\
Gradient Boosting & Faible & Lente (train) & Excellente & Bonne \\
SVM & Faible & Lente (train) & Bonne & Mauvaise \\
\bottomrule
\end{tabular}
\end{table}

\begin{astuce}
\textbf{Recommandations pratiques :}
\begin{itemize}
    \item \textbf{Baseline rapide :} KNN, arbre de d√©cision
    \item \textbf{Haute performance :} Random Forest, Gradient Boosting (XGBoost/LightGBM)
    \item \textbf{Interpr√©tabilit√© :} Arbre de d√©cision peu profond
    \item \textbf{Haute dimension :} SVM (kernel RBF)
    \item \textbf{Gros datasets :} LightGBM, r√©gression logistique
\end{itemize}
\end{astuce}

% ===== R√âSUM√â =====
\section{R√©sum√©}

\subsection{Points Cl√©s}

\begin{itemize}
    \item \textbf{KNN :} Simple, bas√© instance, n√©cessite normalisation
    \item \textbf{Arbres :} Interpr√©tables, overfitting facile, fronti√®res orthogonales
    \item \textbf{Random Forest :} Bagging + randomisation, excellente performance, r√©duit overfitting
    \item \textbf{Gradient Boosting :} Boosting s√©quentiel, √©tat de l'art (XGBoost, LightGBM)
    \item \textbf{SVM :} Maximum margin, kernel trick, efficace haute dimension
    \item \textbf{Ensemble learning :} Combiner mod√®les am√©liore performance
\end{itemize}

% ===== EXERCICES =====
\section{Exercices}

\textit{Voir notebooks} \texttt{04\_exercices.ipynb}

\section{Pour Aller Plus Loin}

Chapitre suivant : \textbf{Chapitre 05 - Apprentissage Non-Supervis√©}

\section*{R√©f√©rences}

\begin{enumerate}
    \item Hastie, T., Tibshirani, R., \& Friedman, J. (2009). \textit{The Elements of Statistical Learning}. Springer.
    \item James, G. et al. (2021). \textit{An Introduction to Statistical Learning} (2e √©d.). Springer.
    \item G√©ron, A. (2022). \textit{Hands-On Machine Learning} (3e √©d.). O'Reilly.
\end{enumerate}

\end{document}
