{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/04_classification_supervisee/04_demo_boosting_svm.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '04_demo_boosting_svm.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 04 - D√©monstration Gradient Boosting et SVM\n",
    "\n",
    "Ce notebook explore les m√©thodes de boosting (XGBoost, LightGBM, CatBoost) et les Support Vector Machines.\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre le principe du boosting\n",
    "- Comparer XGBoost, LightGBM et CatBoost\n",
    "- Ma√Ætriser les SVM et le kernel trick\n",
    "- Optimiser les hyperparam√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.data  # type: ignoresets import load_breast_cancer, make_circles, make_classification\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, ConfusionMatrixDisplay,\n    roc_curve, auc, RocCurveDisplay\n)\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom matplotlib.colors import ListedColormap\nfrom time import time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Gradient Boosting (XGBoost, LightGBM, CatBoost)\n",
    "\n",
    "### Principe du Boosting\n",
    "- Ensemble learning s√©quentiel\n",
    "- Chaque mod√®le corrige les erreurs du pr√©c√©dent\n",
    "- Combinaison pond√©r√©e des pr√©dicteurs faibles\n",
    "- Gradient descent sur l'erreur de pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1.1 Chargement du dataset Breast Cancer\ncancer = load_breast_cancer()\nX_cancer = pd.DataFrame(cancer.data  # type: ignore, columns=cancer.feature_names  # type: ignore)\ny_cancer = cancer.target  # type: ignore\n\nprint(f\"Shape: {X_cancer.shape}\")\nprint(f\"Classes: {cancer.target  # type: ignore_names}\")\nprint(f\"Distribution: {np.bincount(y_cancer)}\")\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Comparaison des algorithmes de boosting\n",
    "models_boosting = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),\n",
    "    'CatBoost': CatBoostClassifier(iterations=100, random_state=42, verbose=0)\n",
    "}\n",
    "\n",
    "results_boosting = []\n",
    "\n",
    "for name, model in models_boosting.items():\n",
    "    # Entra√Ænement\n",
    "    start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time() - start\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    start = time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_time = time() - start\n",
    "    \n",
    "    # Pr√©dictions de probabilit√©s pour AUC-ROC\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    results_boosting.append({\n",
    "        'Model': name,\n",
    "        'Train Acc': model.score(X_train, y_train),\n",
    "        'Test Acc': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred),\n",
    "        'AUC-ROC': roc_auc,\n",
    "        'Train Time': train_time,\n",
    "        'Pred Time': pred_time\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_boosting)\n",
    "print(\"Comparaison des Algorithmes de Boosting:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Visualisation des performances\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Test Accuracy\n",
    "axes[0, 0].barh(results_df['Model'], results_df['Test Acc'], alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Accuracy')\n",
    "axes[0, 0].set_title('Test Accuracy')\n",
    "axes[0, 0].set_xlim(0.9, 1.0)\n",
    "\n",
    "# F1 Score\n",
    "axes[0, 1].barh(results_df['Model'], results_df['F1'], alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('F1 Score')\n",
    "axes[0, 1].set_title('F1 Score')\n",
    "axes[0, 1].set_xlim(0.9, 1.0)\n",
    "\n",
    "# AUC-ROC\n",
    "axes[1, 0].barh(results_df['Model'], results_df['AUC-ROC'], alpha=0.7, color='green')\n",
    "axes[1, 0].set_xlabel('AUC-ROC')\n",
    "axes[1, 0].set_title('AUC-ROC Score')\n",
    "axes[1, 0].set_xlim(0.95, 1.0)\n",
    "\n",
    "# Train Time\n",
    "axes[1, 1].barh(results_df['Model'], results_df['Train Time'], alpha=0.7, color='red')\n",
    "axes[1, 1].set_xlabel('Temps (s)')\n",
    "axes[1, 1].set_title('Temps d\\'Entra√Ænement')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Courbes ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in models_boosting.items():\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Courbes ROC - Comparaison des Mod√®les de Boosting')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Feature Importance (XGBoost)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Extraction de l'importance\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': X_cancer.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False).head(15)\n",
    "\n",
    "print(\"Top 15 Features les plus importantes (XGBoost):\")\n",
    "print(importance)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(importance['Feature'], importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance - XGBoost')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Hyperparameter tuning avec GridSearchCV (XGBoost)\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "xgb_grid = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "grid_search = GridSearchCV(xgb_grid, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Recherche des meilleurs hyperparam√®tres...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nMeilleurs param√®tres: {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score F1 (CV): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# √âvaluation avec le meilleur mod√®le\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred_best = best_xgb.predict(X_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Test F1: {f1_score(y_test, y_pred_best):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Support Vector Machines (SVM)\n",
    "\n",
    "### Principe\n",
    "- Cherche l'hyperplan optimal qui s√©pare les classes\n",
    "- Maximise la marge entre les classes\n",
    "- Support vectors: points les plus proches de la fronti√®re\n",
    "- Kernel trick: projection dans un espace de dimension sup√©rieure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2.1 SVM lin√©aire sur dataset simple\nfrom sklearn.data  # type: ignoresets import make_blobs\n\nX_blob, y_blob = make_blobs(n_samples=200, centers=2, cluster_std=1.5, random_state=42)\nX_train_blob, X_test_blob, y_train_blob, y_test_blob = train_test_split(\n    X_blob, y_blob, test_size=0.3, random_state=42\n)\n\n# SVM lin√©aire\nsvm_linear = SVC(kernel='linear', C=1.0)\nsvm_linear.fit(X_train_blob, y_train_blob)\n\n# Visualisation\ndef plot_svm_boundary(X, y, model, title):\n    \"\"\"Visualise la fronti√®re de d√©cision d'un SVM avec les support vectors.\"\"\"\n    h = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n                edgecolors='k', s=50, alpha=0.8)\n    \n    # Highlight support vectors\n    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n                s=200, linewidth=2, facecolors='none', edgecolors='red', \n                label=f'Support Vectors ({len(model.support_vectors_)})')\n    \n    plt.title(title)\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.legend()\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(X_train_blob[:, 0], X_train_blob[:, 1], c=y_train_blob, \n            cmap='viridis', edgecolors='k', s=50, alpha=0.8)\nplt.title('Dataset')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.subplot(1, 2, 2)\nplot_svm_boundary(X_train_blob, y_train_blob, svm_linear, \n                 f'SVM Lin√©aire\\nAccuracy: {svm_linear.score(X_test_blob, y_test_blob):.3f}')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Impact du param√®tre C (r√©gularisation)\n",
    "C_values = [0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, C in enumerate(C_values):\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_train_blob, y_train_blob)\n",
    "    \n",
    "    test_acc = svm.score(X_test_blob, y_test_blob)\n",
    "    n_support = len(svm.support_vectors_)\n",
    "    \n",
    "    plt.sca(axes[idx])\n",
    "    plot_svm_boundary(X_train_blob, y_train_blob, svm,\n",
    "                     f'C={C}\\nAccuracy: {test_acc:.3f}\\nSupport Vectors: {n_support}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Impact de C:\")\n",
    "print(\"- C petit: Marge large, plus de support vectors, underfitting\")\n",
    "print(\"- C grand: Marge √©troite, moins de support vectors, overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Kernel Trick - Dataset non lin√©aire\n",
    "X_circle, y_circle = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=42)\n",
    "X_train_circ, X_test_circ, y_train_circ, y_test_circ = train_test_split(\n",
    "    X_circle, y_circle, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Diff√©rents kernels\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, kernel in enumerate(kernels):\n",
    "    svm = SVC(kernel=kernel, C=1.0, gamma='scale')\n",
    "    svm.fit(X_train_circ, y_train_circ)\n",
    "    \n",
    "    test_acc = svm.score(X_test_circ, y_test_circ)\n",
    "    \n",
    "    plt.sca(axes[idx])\n",
    "    plot_svm_boundary(X_train_circ, y_train_circ, svm,\n",
    "                     f'Kernel: {kernel}\\nAccuracy: {test_acc:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 SVM RBF - Impact du param√®tre gamma\n",
    "gamma_values = [0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, gamma in enumerate(gamma_values):\n",
    "    svm = SVC(kernel='rbf', C=1.0, gamma=gamma)\n",
    "    svm.fit(X_train_circ, y_train_circ)\n",
    "    \n",
    "    train_acc = svm.score(X_train_circ, y_train_circ)\n",
    "    test_acc = svm.score(X_test_circ, y_test_circ)\n",
    "    \n",
    "    plt.sca(axes[idx])\n",
    "    plot_svm_boundary(X_train_circ, y_train_circ, svm,\n",
    "                     f'gamma={gamma}\\nTrain: {train_acc:.3f}, Test: {test_acc:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Impact de gamma (RBF):\")\n",
    "print(\"- gamma petit: Influence √©tendue, fronti√®re lisse, underfitting\")\n",
    "print(\"- gamma grand: Influence locale, fronti√®re complexe, overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 SVM sur Breast Cancer dataset\n",
    "# Standardisation (importante pour SVM!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entra√Ænement avec diff√©rents kernels\n",
    "svm_models = {\n",
    "    'SVM Linear': SVC(kernel='linear', C=1.0),\n",
    "    'SVM RBF': SVC(kernel='rbf', C=1.0, gamma='scale'),\n",
    "    'SVM Poly (d=3)': SVC(kernel='poly', degree=3, C=1.0)\n",
    "}\n",
    "\n",
    "results_svm = []\n",
    "\n",
    "for name, model in svm_models.items():\n",
    "    # Entra√Ænement\n",
    "    start = time()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_time = time() - start\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    results_svm.append({\n",
    "        'Model': name,\n",
    "        'Train Acc': model.score(X_train_scaled, y_train),\n",
    "        'Test Acc': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred),\n",
    "        'Train Time': train_time,\n",
    "        'Support Vectors': len(model.support_vectors_)\n",
    "    })\n",
    "\n",
    "results_svm_df = pd.DataFrame(results_svm)\n",
    "print(\"Performances des SVM sur Breast Cancer:\")\n",
    "print(results_svm_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Grid Search pour optimiser SVM RBF\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "svm_grid = SVC(kernel='rbf')\n",
    "grid_search_svm = GridSearchCV(svm_grid, param_grid_svm, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Recherche des meilleurs hyperparam√®tres pour SVM RBF...\")\n",
    "grid_search_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nMeilleurs param√®tres: {grid_search_svm.best_params_}\")\n",
    "print(f\"Meilleur score F1 (CV): {grid_search_svm.best_score_:.4f}\")\n",
    "\n",
    "# √âvaluation\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "y_pred_best_svm = best_svm.predict(X_test_scaled)\n",
    "print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_pred_best_svm):.4f}\")\n",
    "print(f\"Test F1: {f1_score(y_test, y_pred_best_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : Comparaison Globale Boosting vs SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 3.1 Comparaison finale sur Breast Cancer\nfinal_models = {\n    'XGBoost': best_xgb,\n    'SVM RBF': best_svm\n}\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nfor idx, (name, model) in enumerate(final_models.items()):\n    if name == 'XGBoost':\n        X_eval = X_test\n    else:\n        X_eval = X_test_scaled\n    \n    y_pred = model.predict(X_eval)\n    cm = confusion_matrix(y_test, y_pred)\n    \n    plt.sca(axes[idx])\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n                                   display_labels=cancer.target  # type: ignore_names)\n    disp.plot(ax=axes[idx], cmap='Blues')\n    axes[idx].set_title(f'{name}\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}')\n\nplt.tight_layout()\nplt.show()\n\n# Classification reports\nfor name, model in final_models.items():\n    if name == 'XGBoost':\n        X_eval = X_test\n    else:\n        X_eval = X_test_scaled\n    \n    y_pred = model.predict(X_eval)\n    print(f\"\\n{name} - Classification Report:\")\n    print(classification_report(y_test, y_pred, target_names=cancer.target  # type: ignore_names))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©capitulatif\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "**XGBoost:**\n",
    "- Tr√®s performant, optimis√©\n",
    "- R√©gularisation L1/L2\n",
    "- Gestion des valeurs manquantes\n",
    "- Parall√©lisation efficace\n",
    "\n",
    "**LightGBM:**\n",
    "- Tr√®s rapide (leaf-wise growth)\n",
    "- Faible consommation m√©moire\n",
    "- Excellent pour les grands datasets\n",
    "- Risque d'overfitting si pas r√©gularis√©\n",
    "\n",
    "**CatBoost:**\n",
    "- Gestion native des features cat√©gorielles\n",
    "- Robuste √† l'overfitting\n",
    "- Bon par d√©faut (peu de tuning)\n",
    "- Plus lent que LightGBM\n",
    "\n",
    "**Hyperparam√®tres cl√©s:**\n",
    "- `n_estimators`: Nombre d'arbres\n",
    "- `learning_rate`: Taux d'apprentissage\n",
    "- `max_depth`: Profondeur des arbres\n",
    "- `min_child_weight` / `min_samples_leaf`: R√©gularisation\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "**Avantages:**\n",
    "- Efficace en haute dimension\n",
    "- Robuste avec kernel trick\n",
    "- Th√©orie math√©matique solide\n",
    "- Bon avec peu de donn√©es\n",
    "\n",
    "**Inconv√©nients:**\n",
    "- Lent pour grands datasets (O(n¬≤) √† O(n¬≥))\n",
    "- Sensible √† l'√©chelle (standardisation requise)\n",
    "- Choix du kernel et des hyperparam√®tres crucial\n",
    "- Moins interpr√©table\n",
    "\n",
    "**Kernels:**\n",
    "- `linear`: Donn√©es lin√©airement s√©parables\n",
    "- `rbf` (Radial Basis Function): Cas g√©n√©ral, non lin√©aire\n",
    "- `poly`: Polynomiale, interactions\n",
    "- `sigmoid`: Similaire aux r√©seaux de neurones\n",
    "\n",
    "**Hyperparam√®tres cl√©s:**\n",
    "- `C`: Compromis marge/erreur (r√©gularisation)\n",
    "- `gamma`: Influence des support vectors (RBF, poly)\n",
    "- `degree`: Degr√© du polyn√¥me (poly)\n",
    "\n",
    "### Quand utiliser quoi?\n",
    "\n",
    "**Gradient Boosting (XGBoost/LightGBM/CatBoost):**\n",
    "- Comp√©titions Kaggle\n",
    "- Datasets tabulaires structur√©s\n",
    "- Features h√©t√©rog√®nes\n",
    "- Besoin de feature importance\n",
    "- Grands datasets\n",
    "\n",
    "**SVM:**\n",
    "- Datasets de petite √† moyenne taille\n",
    "- Haute dimension\n",
    "- Fronti√®res non lin√©aires complexes\n",
    "- Besoin de robustesse th√©orique\n",
    "- Classification binaire"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}