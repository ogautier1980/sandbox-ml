\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}
\usepackage{amsmath, amssymb}
\usepackage[margin=2cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.1}
\usepackage{xcolor}

% ===== UNICODE CHARACTERS SUPPORT =====
\usepackage{newunicodechar}

% Emojis et symboles
\newunicodechar{‚úÖ}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚ùå}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚úì}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚úó}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚ö†}{\textcolor{orange!80!black}{\textbf{/!\textbackslash}}}
\newunicodechar{üí°}{\textcolor{blue!70!black}{\textbf{(i)}}}
\newunicodechar{üéØ}{\textcolor{purple!70!black}{\textbf{$\star$}}}
\newunicodechar{üìä}{\textcolor{blue!70!black}{\textbf{[=]}}}

% √âtoiles (pour tableaux)
\newunicodechar{‚òÖ}{\textcolor{orange!80!black}{$\star$}}
\newunicodechar{‚òÜ}{\textcolor{gray!50}{$\star$}}

% Fl√®ches
\newunicodechar{‚Üí}{$\rightarrow$}
\newunicodechar{‚Üê}{$\leftarrow$}
\newunicodechar{‚Üë}{$\uparrow$}
\newunicodechar{‚Üì}{$\downarrow$}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{listings}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, pdftitle={Annexes - Cours ML}}
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Annexes - Cours ML}
\fancyhead[R]{\small Sandbox-ML}
\fancyfoot[C]{\thepage}

\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
}
\lstset{style=pythonstyle}

\newtcolorbox{formbox}[1]{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=#1,
    breakable
}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]
    \vspace{1cm}
    {\LARGE Annexes}\\[0.3cm]
    {\LARGE Aide-m√©moire, Glossaire, Ressources}\\[2cm]
    \vfill
    {\large
    \textbf{Contenu :}\\[0.5cm]
    \begin{itemize}
        \item Aide-m√©moire : Formules essentielles
        \item Glossaire : Terminologie ML
        \item Ressources : Biblioth√®ques, datasets, liens
        \item FAQ : Pi√®ges courants et solutions
    \end{itemize}
    }
    \vfill
    {\large Cours ML - Sandbox-ML\\ Version 1.0 - 2026}
\end{titlepage}

\tableofcontents
\newpage

% ===== AIDE-M√âMOIRE =====
\section{Aide-M√©moire : Formules Essentielles}

\subsection{Alg√®bre Lin√©aire}

\begin{formbox}{Produit Matriciel}
\begin{align*}
\mat{C} &= \mat{A}\mat{B} \quad \text{o√π } C_{ij} = \sum_{k} A_{ik}B_{kj} \\
\text{Dimensions} &: (m \times n) \cdot (n \times p) = (m \times p)
\end{align*}
\end{formbox}

\begin{formbox}{Valeurs Propres}
\begin{equation*}
\mat{A}\vect{v} = \lambda \vect{v} \quad \Leftrightarrow \quad \det(\mat{A} - \lambda \mat{I}) = 0
\end{equation*}
\end{formbox}

\begin{formbox}{SVD (Singular Value Decomposition)}
\begin{equation*}
\mat{A} = \mat{U}\mat{\Sigma}\mat{V}^T
\end{equation*}
o√π $\mat{U}, \mat{V}$ orthogonales, $\mat{\Sigma}$ diagonale (valeurs singuli√®res)
\end{formbox}

\subsection{Probabilit√©s}

\begin{formbox}{Th√©or√®me de Bayes}
\begin{equation*}
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\end{equation*}
\end{formbox}

\begin{formbox}{Loi Normale}
\begin{equation*}
\mathcal{N}(\mu, \sigma^2) : \quad f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation*}
\end{formbox}

\subsection{R√©gression}

\begin{formbox}{R√©gression Lin√©aire}
\begin{align*}
\text{Mod√®le} &: \hat{y} = \vect{w}^T\vect{x} + b \\
\text{Solution} &: \vect{w}^* = (\mat{X}^T\mat{X})^{-1}\mat{X}^T\vect{y} \\
\text{MSE} &: \frac{1}{m}\sum_{i=1}^m (y_i - \hat{y}_i)^2
\end{align*}
\end{formbox}

\begin{formbox}{Ridge (L2)}
\begin{equation*}
\vect{w}^* = (\mat{X}^T\mat{X} + \lambda\mat{I})^{-1}\mat{X}^T\vect{y}
\end{equation*}
\end{formbox}

\begin{formbox}{Lasso (L1)}
\begin{equation*}
\min_{\vect{w}} \|\mat{X}\vect{w} - \vect{y}\|_2^2 + \lambda\|\vect{w}\|_1
\end{equation*}
\end{formbox}

\subsection{Classification}

\begin{formbox}{Logistic Regression}
\begin{align*}
\sigma(z) &= \frac{1}{1 + e^{-z}} \\
P(y=1|\vect{x}) &= \sigma(\vect{w}^T\vect{x} + b) \\
\text{Loss} &: -\frac{1}{m}\sum_{i=1}^m [y_i\log\hat{y}_i + (1-y_i)\log(1-\hat{y}_i)]
\end{align*}
\end{formbox}

\begin{formbox}{Softmax}
\begin{equation*}
P(y=k|\vect{x}) = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}
\end{equation*}
\end{formbox}

\subsection{M√©triques}

\begin{formbox}{Classification}
\begin{align*}
\text{Accuracy} &= \frac{\text{TP + TN}}{\text{Total}} \\
\text{Precision} &= \frac{\text{TP}}{\text{TP + FP}} \\
\text{Recall} &= \frac{\text{TP}}{\text{TP + FN}} \\
\text{F1-score} &= 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision + Recall}}
\end{align*}
\end{formbox}

\subsection{Clustering}

\begin{formbox}{K-Means}
\begin{equation*}
\min_{\mu_1,\ldots,\mu_K} \sum_{i=1}^n \min_{k=1,\ldots,K} \|\vect{x}_i - \mu_k\|^2
\end{equation*}
\end{formbox}

\subsection{R√©seaux de Neurones}

\begin{formbox}{Forward Pass}
\begin{align*}
\vect{z}^{[l]} &= \mat{W}^{[l]}\vect{a}^{[l-1]} + \vect{b}^{[l]} \\
\vect{a}^{[l]} &= \sigma^{[l]}(\vect{z}^{[l]})
\end{align*}
\end{formbox}

\begin{formbox}{Backpropagation}
\begin{align*}
\delta^{[l]} &= \frac{\partial L}{\partial \vect{z}^{[l]}} \\
\frac{\partial L}{\partial \mat{W}^{[l]}} &= \delta^{[l]}(\vect{a}^{[l-1]})^T \\
\frac{\partial L}{\partial \vect{b}^{[l]}} &= \delta^{[l]}
\end{align*}
\end{formbox}

\begin{formbox}{Optimiseurs}
\begin{align*}
\text{SGD} &: \vect{\theta} \leftarrow \vect{\theta} - \alpha\nabla L \\
\text{Momentum} &: \vect{v} = \beta\vect{v} + (1-\beta)\nabla L, \quad \vect{\theta} \leftarrow \vect{\theta} - \alpha\vect{v} \\
\text{Adam} &: \text{Combine momentum + RMSprop}
\end{align*}
\end{formbox}

\subsection{CNN}

\begin{formbox}{Convolution 2D}
\begin{align*}
\text{Output}[i,j] &= \sum_m \sum_n \text{Input}[i+m, j+n] \cdot \text{Kernel}[m,n] \\
\text{Taille sortie} &: \left\lfloor\frac{H + 2p - k}{s}\right\rfloor + 1
\end{align*}
\end{formbox}

\subsection{RNN/LSTM}

\begin{formbox}{RNN}
\begin{equation*}
\vect{h}_t = \tanh(\mat{W}_{hh}\vect{h}_{t-1} + \mat{W}_{xh}\vect{x}_t + \vect{b})
\end{equation*}
\end{formbox}

\begin{formbox}{LSTM (simplifi√©)}
\begin{align*}
\vect{f}_t &= \sigma(\mat{W}_f[\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(forget)} \\
\vect{i}_t &= \sigma(\mat{W}_i[\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(input)} \\
\vect{c}_t &= \vect{f}_t \odot \vect{c}_{t-1} + \vect{i}_t \odot \tilde{\vect{c}}_t
\end{align*}
\end{formbox}

\begin{formbox}{Attention}
\begin{equation*}
\text{Attention}(\mat{Q}, \mat{K}, \mat{V}) = \text{softmax}\left(\frac{\mat{Q}\mat{K}^T}{\sqrt{d_k}}\right)\mat{V}
\end{equation*}
\end{formbox}

\subsection{Reinforcement Learning}

\begin{formbox}{Q-Learning}
\begin{equation*}
Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma\max_{a'}Q(s',a') - Q(s,a)]
\end{equation*}
\end{formbox}

% ===== GLOSSAIRE =====
\section{Glossaire}

\begin{longtable}{p{4cm}p{11cm}}
\toprule
\textbf{Terme} & \textbf{D√©finition} \\
\midrule
\endhead

\textbf{Accuracy} & Proportion de pr√©dictions correctes \\
\textbf{Activation Function} & Fonction non-lin√©aire appliqu√©e aux neurones (ReLU, sigmoid, etc.) \\
\textbf{Adam} & Optimiseur adaptatif combinant momentum et RMSprop \\
\textbf{Backpropagation} & Algorithme pour calculer gradients dans r√©seaux de neurones \\
\textbf{Batch} & Sous-ensemble de donn√©es utilis√© pour une mise √† jour \\
\textbf{Batch Normalization} & Normalisation des activations par batch \\
\textbf{Bias} & Biais (intercept) ou biais statistique (erreur syst√©matique) \\
\textbf{Bias-Variance Tradeoff} & Compromis entre underfitting et overfitting \\
\textbf{BPTT} & Backpropagation Through Time (pour RNN) \\
\textbf{CNN} & Convolutional Neural Network (r√©seau convolutif) \\
\textbf{Confusion Matrix} & Tableau TP/TN/FP/FN pour classification \\
\textbf{Cross-Entropy} & Fonction de perte pour classification \\
\textbf{Cross-Validation} & Validation crois√©e (K-Fold) \\
\textbf{Data Augmentation} & Augmentation artificielle du dataset \\
\textbf{Data Leakage} & Fuite d'info du test vers train (erreur grave) \\
\textbf{Dropout} & D√©sactivation al√©atoire de neurones (r√©gularisation) \\
\textbf{Early Stopping} & Arr√™t si validation loss n'am√©liore plus \\
\textbf{Embedding} & Repr√©sentation vectorielle dense (mots, etc.) \\
\textbf{Ensemble} & Combinaison de plusieurs mod√®les \\
\textbf{Epoch} & Une passe compl√®te sur le dataset d'entra√Ænement \\
\textbf{F1-Score} & Moyenne harmonique de Precision et Recall \\
\textbf{Feature Engineering} & Cr√©ation/transformation de features \\
\textbf{Fine-Tuning} & Ajustement d'un mod√®le pr√©-entra√Æn√© \\
\textbf{Gradient Descent} & Optimisation it√©rative par descente de gradient \\
\textbf{GRU} & Gated Recurrent Unit (variante LSTM) \\
\textbf{Hyperparameter} & Param√®tre fix√© avant entra√Ænement (LR, etc.) \\
\textbf{L1/L2 Regularization} & P√©nalisation sur poids (Lasso/Ridge) \\
\textbf{Learning Rate} & Taille du pas de gradient descent \\
\textbf{LSTM} & Long Short-Term Memory (RNN avec m√©moire) \\
\textbf{MLP} & Multi-Layer Perceptron (r√©seau fully-connected) \\
\textbf{MSE} & Mean Squared Error \\
\textbf{Overfitting} & Mod√®le trop complexe, bon sur train, mauvais sur test \\
\textbf{PCA} & Principal Component Analysis (r√©duction dim) \\
\textbf{Pooling} & Sous-√©chantillonnage (Max/Average) \\
\textbf{Precision} & TP / (TP + FP) \\
\textbf{Recall} & TP / (TP + FN) \\
\textbf{Regularization} & Techniques contre overfitting (L1, L2, dropout) \\
\textbf{ReLU} & Rectified Linear Unit : $\max(0, x)$ \\
\textbf{RNN} & Recurrent Neural Network (s√©quences) \\
\textbf{SGD} & Stochastic Gradient Descent \\
\textbf{Softmax} & Fonction de sortie pour multi-classe \\
\textbf{Transfer Learning} & R√©utilisation mod√®le pr√©-entra√Æn√© \\
\textbf{Transformer} & Architecture attention pour NLP \\
\textbf{Underfitting} & Mod√®le trop simple \\
\textbf{Vanishing Gradient} & Gradients ‚Üí 0 dans r√©seaux profonds \\
\textbf{Validation Set} & Donn√©es pour tuner hyperparam√®tres \\
\bottomrule
\end{longtable}

% ===== RESSOURCES =====
\section{Ressources}

\subsection{Biblioth√®ques Python}

\subsubsection{ML Classique}
\begin{itemize}
    \item \textbf{scikit-learn} : ML complet (classification, r√©gression, clustering)
    \item \textbf{XGBoost} : Gradient Boosting haute performance
    \item \textbf{LightGBM} : Gradient Boosting rapide (Microsoft)
    \item \textbf{CatBoost} : Gradient Boosting pour cat√©gorielles (Yandex)
\end{itemize}

\subsubsection{Deep Learning}
\begin{itemize}
    \item \textbf{PyTorch} : Framework DL flexible (recherche)
    \item \textbf{TensorFlow/Keras} : Framework DL production
    \item \textbf{FastAI} : Haut niveau sur PyTorch
\end{itemize}

\subsubsection{NLP}
\begin{itemize}
    \item \textbf{Transformers (HuggingFace)} : BERT, GPT, T5, etc.
    \item \textbf{spaCy} : NLP industriel
    \item \textbf{NLTK} : NLP p√©dagogique
\end{itemize}

\subsubsection{Data Science}
\begin{itemize}
    \item \textbf{NumPy} : Calcul num√©rique
    \item \textbf{Pandas} : Manipulation donn√©es tabulaires
    \item \textbf{Matplotlib/Seaborn} : Visualisation
    \item \textbf{Plotly} : Visualisation interactive
\end{itemize}

\subsubsection{Outils}
\begin{itemize}
    \item \textbf{Jupyter} : Notebooks interactifs
    \item \textbf{MLflow} : Tracking exp√©riences
    \item \textbf{Optuna} : Hyperparameter tuning
    \item \textbf{Weights \& Biases} : Exp√©rience tracking
    \item \textbf{DVC} : Version control donn√©es
\end{itemize}

\subsection{Datasets}

\subsubsection{D√©butant}
\begin{itemize}
    \item \textbf{MNIST} : Chiffres manuscrits (10 classes, 60K images)
    \item \textbf{Iris} : Classification florale (150 samples, 4 features)
    \item \textbf{Titanic} : Survie passagers (Kaggle)
    \item \textbf{Boston Housing} : Pr√©diction prix immobilier
\end{itemize}

\subsubsection{Interm√©diaire}
\begin{itemize}
    \item \textbf{CIFAR-10/100} : Images couleur (10/100 classes)
    \item \textbf{IMDB Reviews} : Sentiment analysis
    \item \textbf{Fashion-MNIST} : V√™tements (alternative MNIST)
\end{itemize}

\subsubsection{Avanc√©}
\begin{itemize}
    \item \textbf{ImageNet} : 1.2M images, 1000 classes
    \item \textbf{COCO} : D√©tection objets, segmentation
    \item \textbf{SQuAD} : Question answering
    \item \textbf{Kaggle Competitions} : Datasets r√©els vari√©s
\end{itemize}

\subsection{Environnements RL}
\begin{itemize}
    \item \textbf{OpenAI Gym} : Environnements RL standards
    \item \textbf{Stable Baselines3} : Impl√©mentations RL
    \item \textbf{PettingZoo} : Multi-agent RL
\end{itemize}

\subsection{Cours en Ligne}

\begin{itemize}
    \item \textbf{Fast.ai} : Deep Learning pratique
    \item \textbf{CS231n (Stanford)} : CNN pour vision
    \item \textbf{CS224n (Stanford)} : NLP avec Deep Learning
    \item \textbf{Coursera - Andrew Ng} : ML et DL
    \item \textbf{DeepLearning.AI} : Sp√©cialisations DL
\end{itemize}

\subsection{Livres}

\begin{itemize}
    \item G√©ron - \textit{Hands-On Machine Learning (3rd ed, 2023)}
    \item Goodfellow et al. - \textit{Deep Learning}
    \item Bishop - \textit{Pattern Recognition and Machine Learning}
    \item Hastie et al. - \textit{The Elements of Statistical Learning}
    \item Sutton \& Barto - \textit{Reinforcement Learning}
    \item Chollet - \textit{Deep Learning with Python (2nd ed)}
\end{itemize}

% ===== FAQ =====
\section{FAQ : Pi√®ges Courants}

\subsection{Donn√©es}

\textbf{Q : Dois-je normaliser mes donn√©es ?}

A : Oui, pour la plupart des algorithmes (SVM, r√©seaux de neurones, K-Means). Non pour les arbres de d√©cision. Utiliser StandardScaler ou MinMaxScaler selon le contexte.

\vspace{0.3cm}

\textbf{Q : Comment g√©rer les valeurs manquantes ?}

A : (1) Supprimer si < 5\%, (2) Imputer m√©diane/moyenne pour num√©riques, (3) Mode pour cat√©gorielles, (4) Cr√©er indicateur "missing".

\vspace{0.3cm}

\textbf{Q : Qu'est-ce que le data leakage ?}

A : Utiliser des informations du test set pendant l'entra√Ænement. Erreurs courantes :
\begin{itemize}
    \item Scaler fit sur toutes les donn√©es
    \item Features du futur dans s√©ries temporelles
    \item Duplicatas entre train et test
\end{itemize}

\subsection{Mod√©lisation}

\textbf{Q : Mon mod√®le a 99\% d'accuracy mais ne marche pas bien ?}

A : Classes probablement d√©s√©quilibr√©es. Utiliser F1-score, Precision/Recall ou AUC-ROC au lieu d'accuracy.

\vspace{0.3cm}

\textbf{Q : Overfitting vs Underfitting ?}

A :
\begin{itemize}
    \item \textbf{Overfitting} : Train error faible, val error √©lev√© ‚Üí R√©gularisation, plus de donn√©es
    \item \textbf{Underfitting} : Train et val errors √©lev√©s ‚Üí Mod√®le plus complexe
\end{itemize}

\vspace{0.3cm}

\textbf{Q : Combien de donn√©es faut-il ?}

A : R√®gle empirique :
\begin{itemize}
    \item ML classique : 10√ó le nombre de features minimum
    \item Deep Learning : 1000+ par classe minimum
    \item Transfer Learning : 100+ par classe peut suffire
\end{itemize}

\subsection{Entra√Ænement}

\textbf{Q : Mon loss est NaN ?}

A : (1) Learning rate trop √©lev√©, (2) Explosion gradients ‚Üí gradient clipping, (3) Donn√©es non normalis√©es.

\vspace{0.3cm}

\textbf{Q : Mon mod√®le n'apprend pas (loss stagne) ?}

A : (1) Learning rate trop faible, (2) Mauvaise initialisation, (3) Dead ReLU (tous neurones inactifs).

\vspace{0.3cm}

\textbf{Q : Quelle taille de batch ?}

A : 32-128 g√©n√©ralement. Plus grand = plus rapide mais moins de g√©n√©ralisation. Plus petit = meilleure g√©n√©ralisation mais plus lent.

\subsection{√âvaluation}

\textbf{Q : Puis-je utiliser le test set plusieurs fois ?}

A : \textbf{NON !} Le test set ne doit √™tre utilis√© qu'\textbf{une seule fois} pour l'√©valuation finale. Utiliser validation set pour tuning.

\vspace{0.3cm}

\textbf{Q : Cross-validation ou simple train/val/test ?}

A :
\begin{itemize}
    \item CV : Petites donn√©es (< 10K), √©valuation robuste
    \item Train/Val/Test : Grandes donn√©es, DL
\end{itemize}

\subsection{Production}

\textbf{Q : Comment d√©ployer mon mod√®le ?}

A : (1) Sauvegarder avec joblib/pickle, (2) API REST (FastAPI/Flask), (3) Containeriser (Docker), (4) Monitoring.

\vspace{0.3cm}

\textbf{Q : Mon mod√®le se d√©grade en production ?}

A : Distribution shift (data drift). Solution : Monitoring, r√©entra√Ænement p√©riodique, d√©tection d'anomalies.

% ===== COMMANDES PYTHON ESSENTIELLES =====
\section{Commandes Python Essentielles}

\subsection{scikit-learn Workflow}

\begin{lstlisting}
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Pas de fit!

# Train
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# Evaluate
y_pred = model.predict(X_test_scaled)
print(classification_report(y_test, y_pred))
\end{lstlisting}

\subsection{PyTorch Workflow}

\begin{lstlisting}
import torch
import torch.nn as nn
import torch.optim as optim

# Model
model = nn.Sequential(
    nn.Linear(input_dim, 128),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(128, output_dim)
)

# Loss et optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(num_epochs):
    model.train()
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
\end{lstlisting}

% ===== ANNEXE D : CHEAT SHEETS PAR FRAMEWORK =====
\section{Annexe D : Cheat Sheets par Framework}

\subsection{scikit-learn : Workflow Complet}

\begin{lstlisting}
# Pipeline scikit-learn classique
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# 1. Split des donn√©es
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. Pipeline avec preprocessing
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', RandomForestClassifier(random_state=42))
])

# 3. Grid Search pour hyperparameters
param_grid = {
    'clf__n_estimators': [100, 200, 300],
    'clf__max_depth': [10, 20, None],
    'clf__min_samples_split': [2, 5, 10]
}

grid = GridSearchCV(pipeline, param_grid, cv=5,
                    scoring='f1_weighted', n_jobs=-1)
grid.fit(X_train, y_train)

# 4. Evaluation
print(f"Best params: {grid.best_params_}")
print(f"Best CV score: {grid.best_score_:.3f}")
y_pred = grid.predict(X_test)
print(classification_report(y_test, y_pred))
\end{lstlisting}

\subsection{PyTorch : Training Loop Standard}

\begin{lstlisting}
# Training loop PyTorch typique
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 1. Pr√©paration des donn√©es
train_dataset = TensorDataset(
    torch.tensor(X_train, dtype=torch.float32),
    torch.tensor(y_train, dtype=torch.long)
)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# 2. D√©finition du mod√®le
model = nn.Sequential(
    nn.Linear(input_dim, 128),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, num_classes)
)

# 3. Loss et optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)

# 4. Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # Validation
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val)
        val_loss = criterion(val_outputs, y_val)

    scheduler.step(val_loss)
    print(f"Epoch {epoch+1}/{num_epochs} | "
          f"Train Loss: {total_loss/len(train_loader):.4f} | "
          f"Val Loss: {val_loss:.4f}")
\end{lstlisting}

\subsection{Hugging Face Transformers : Fine-tuning}

\begin{lstlisting}
# Fine-tuning BERT avec Hugging Face
from transformers import (BertTokenizer, BertForSequenceClassification,
                          Trainer, TrainingArguments)

# 1. Tokenizer et mod√®le
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', num_labels=2
)

# 2. Tokenization
train_encodings = tokenizer(train_texts, truncation=True,
                            padding=True, max_length=128)
val_encodings = tokenizer(val_texts, truncation=True,
                          padding=True, max_length=128)

# 3. Dataset PyTorch
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx])
                for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = CustomDataset(train_encodings, train_labels)
val_dataset = CustomDataset(val_encodings, val_labels)

# 4. Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True
)

# 5. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# 6. Fine-tuning
trainer.train()
\end{lstlisting}

% ===== ANNEXE E : GUIDE HARDWARE ET CLOUD =====
\section{Annexe E : Guide Hardware et Cloud}

\subsection{Choix de GPU pour ML/DL}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{GPU} & \textbf{VRAM} & \textbf{Prix} & \textbf{Use Case} & \textbf{Performance} \\
\hline
GTX 1660 Ti & 6 GB & \$280 & Apprentissage & Entr√©e de gamme \\
RTX 3060 & 12 GB & \$330 & ML l√©ger & Bon rapport qualit√©/prix \\
RTX 3080 & 10 GB & \$700 & DL moyen & Tr√®s bon pour entra√Ænement \\
RTX 4090 & 24 GB & \$1600 & DL lourd & Top performance \\
A100 (40GB) & 40 GB & \$10k+ & Production & Datacenters \\
\hline
\end{tabular}
\caption{Comparaison GPUs pour Machine Learning (2024)}
\end{table}

\textbf{Recommandations selon budget :}
\begin{itemize}
    \item \textbf{Budget < 500\$} : RTX 3060 (12GB) - Excellent pour d√©buter
    \item \textbf{Budget 500-1000\$} : RTX 3080/4070 - Bon pour projets s√©rieux
    \item \textbf{Budget 1000-2000\$} : RTX 4090 - Top pour recherche/production
    \item \textbf{Budget illimit√©} : Multiple A100 ou H100 - Datacenters
\end{itemize}

\textbf{VRAM n√©cessaire selon mod√®le :}
\begin{itemize}
    \item ResNet-50 : 4-6 GB
    \item BERT-base : 8-12 GB
    \item BERT-large : 16-20 GB
    \item GPT-2 (1.5B) : 20-24 GB
    \item Fine-tuning LLaMA 7B : 40+ GB (ou quantization)
\end{itemize}

\subsection{Plateformes Cloud pour ML}

\begin{table}[h]
\centering
\small
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Plateforme} & \textbf{GPU disponibles} & \textbf{Prix/h} & \textbf{Avantages} \\
\hline
Google Colab Pro+ & T4, A100 & Gratuit-\$50/mois & Simple, notebooks \\
AWS SageMaker & P3, P4 & \$3-10/h & Complet, scalable \\
Google Cloud AI & V100, A100, TPU & \$2-8/h & TPUs puissants \\
Azure ML & NC, ND series & \$2-10/h & Int√©gration Microsoft \\
Lambda Labs & A100, H100 & \$1-3/h & Moins cher \\
Paperspace Gradient & RTX 4000-A100 & \$0.5-3/h & Simple, abordable \\
\hline
\end{tabular}
\caption{Comparaison plateformes Cloud ML (2024)}
\end{table}

\textbf{Recommandations :}
\begin{itemize}
    \item \textbf{D√©butants} : Google Colab Pro (gratuit tier g√©n√©reux)
    \item \textbf{Prototyping} : Paperspace Gradient (rapport qualit√©/prix)
    \item \textbf{Production} : AWS SageMaker ou GCP AI Platform (robustesse)
    \item \textbf{Recherche} : Lambda Labs (GPUs puissants, moins cher)
\end{itemize}

% ===== ANNEXE F : CARRI√àRES ET CERTIFICATIONS =====
\section{Annexe F : Carri√®res en Machine Learning}

\subsection{Parcours Professionnels}

\textbf{1. ML Engineer}
\begin{itemize}
    \item \textbf{R√¥le} : D√©velopper et d√©ployer des mod√®les ML en production
    \item \textbf{Comp√©tences} : Python, ML libs, MLOps, CI/CD, cloud
    \item \textbf{Salaire (France)} : 45k-70k‚Ç¨ (junior), 70k-110k‚Ç¨ (senior)
    \item \textbf{Demande} : Tr√®s forte (croissance 40\%/an)
\end{itemize}

\textbf{2. Data Scientist}
\begin{itemize}
    \item \textbf{R√¥le} : Analyser donn√©es, cr√©er mod√®les pr√©dictifs, insights business
    \item \textbf{Comp√©tences} : Stats, Python/R, ML, visualisation, communication
    \item \textbf{Salaire} : 40k-65k‚Ç¨ (junior), 65k-100k‚Ç¨ (senior)
    \item \textbf{Demande} : Forte mais saturation dans certaines r√©gions
\end{itemize}

\textbf{3. Research Scientist (ML/DL)}
\begin{itemize}
    \item \textbf{R√¥le} : Recherche fondamentale, publications, nouveaux algorithmes
    \item \textbf{Comp√©tences} : PhD, math√©matiques avanc√©es, PyTorch, publications
    \item \textbf{Salaire} : 60k-90k‚Ç¨ (postdoc), 90k-150k‚Ç¨+ (senior, FAANG)
    \item \textbf{Demande} : Mod√©r√©e mais tr√®s comp√©titive
\end{itemize}

\textbf{4. MLOps Engineer}
\begin{itemize}
    \item \textbf{R√¥le} : Infrastructure ML, CI/CD, monitoring, scalabilit√©
    \item \textbf{Comp√©tences} : DevOps, Kubernetes, Docker, ML frameworks
    \item \textbf{Salaire} : 50k-75k‚Ç¨ (junior), 75k-120k‚Ç¨ (senior)
    \item \textbf{Demande} : En forte croissance (nouveau m√©tier)
\end{itemize}

\subsection{Certifications Recommand√©es}

\textbf{Certifications Cloud ML :}
\begin{itemize}
    \item \textbf{AWS Certified Machine Learning - Specialty} (difficult√© : √©lev√©e)
    \item \textbf{Google Cloud Professional ML Engineer} (difficult√© : √©lev√©e)
    \item \textbf{Microsoft Azure AI Engineer Associate} (difficult√© : moyenne)
\end{itemize}

\textbf{Certifications Acad√©miques :}
\begin{itemize}
    \item \textbf{deeplearning.ai Specializations} (Coursera) : 5 sp√©cialisations excellentes
    \item \textbf{fast.ai Practical Deep Learning} : Gratuit, tr√®s pratique
    \item \textbf{Stanford CS229 (Machine Learning)} : Th√©orique et rigoureux
\end{itemize}

\subsection{Roadmap de Progression}

\textbf{Niveau D√©butant (0-6 mois) :}
\begin{enumerate}
    \item Ma√Ætriser Python et NumPy/Pandas
    \item Comprendre ML supervis√© (regression, classification)
    \item Impl√©menter algorithmes from scratch
    \item Compl√©ter ce cours ML (chapitres 00-05)
\end{enumerate}

\textbf{Niveau Interm√©diaire (6-18 mois) :}
\begin{enumerate}
    \item Deep Learning (PyTorch ou TensorFlow)
    \item Projets Kaggle (top 25\%)
    \item Compl√©ter chapitres 06-10 de ce cours
    \item Contribuer √† projets open-source
\end{enumerate}

\textbf{Niveau Avanc√© (18+ mois) :}
\begin{enumerate}
    \item Sp√©cialisation (NLP, CV, RL, etc.)
    \item Publications ou projets significatifs
    \item MLOps et d√©ploiement production
    \item Compl√©ter chapitres 11-14 + applications avanc√©es
\end{enumerate}

% ===== ANNEXE G : OUTILS ET LIBRAIRIES ESSENTIELS =====
\section{Annexe G : Outils et Librairies Essentiels}

\subsection{Stack ML Compl√®te}

\textbf{Data Processing :}
\begin{itemize}
    \item \textbf{Pandas} : Manipulation de DataFrames
    \item \textbf{Polars} : Alternative ultra-rapide √† Pandas
    \item \textbf{Dask} : DataFrames distribu√©s pour big data
    \item \textbf{PySpark} : Processing distribu√© √† grande √©chelle
\end{itemize}

\textbf{Machine Learning :}
\begin{itemize}
    \item \textbf{scikit-learn} : ML classique (must-have)
    \item \textbf{XGBoost, LightGBM, CatBoost} : Gradient boosting
    \item \textbf{PyTorch} : Deep Learning (recommand√©)
    \item \textbf{TensorFlow/Keras} : Deep Learning (alternative)
    \item \textbf{Hugging Face Transformers} : NLP state-of-the-art
\end{itemize}

\textbf{Visualisation :}
\begin{itemize}
    \item \textbf{Matplotlib} : Base, flexible
    \item \textbf{Seaborn} : Statistical plots, esth√©tique
    \item \textbf{Plotly} : Interactif, dashboards
    \item \textbf{Weights \& Biases} : Experiment tracking
\end{itemize}

\textbf{MLOps :}
\begin{itemize}
    \item \textbf{MLflow} : Experiment tracking, model registry
    \item \textbf{DVC} : Version control pour donn√©es/mod√®les
    \item \textbf{Docker} : Conteneurisation
    \item \textbf{Kubernetes} : Orchestration √† grande √©chelle
    \item \textbf{FastAPI} : APIs REST performantes
\end{itemize}

\subsection{Commandes Essentielles}

\subsubsection{Installation Rapide}

\begin{lstlisting}[language=bash]
# ML Stack minimal
pip install numpy pandas scikit-learn matplotlib

# Deep Learning (PyTorch)
pip install torch torchvision torchaudio

# Deep Learning (TensorFlow)
pip install tensorflow

# NLP moderne
pip install transformers datasets tokenizers

# MLOps
pip install mlflow dvc fastapi uvicorn

# Gradient Boosting
pip install xgboost lightgbm catboost
\end{lstlisting}

\subsubsection{V√©rifier GPU PyTorch}

\begin{lstlisting}
import torch
print(f"CUDA disponible : {torch.cuda.is_available()}")
print(f"GPU : {torch.cuda.get_device_name(0)}")
print(f"VRAM : {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
\end{lstlisting}

% ===== CONCLUSION =====
\section*{Conclusion}

Ce document d'annexes compl√®te le cours complet de Machine Learning. Utilisez-le comme r√©f√©rence rapide pendant vos projets.

\textbf{Points cl√©s √† retenir :}
\begin{itemize}
    \item Toujours commencer simple (baseline)
    \item √âviter le data leakage √† tout prix
    \item Choisir la bonne m√©trique selon le probl√®me
    \item Valider rigoureusement (CV ou train/val/test)
    \item Test set = une seule utilisation finale
    \item Documenter et versionner (code, donn√©es, mod√®les)
\end{itemize}

\vspace{1cm}

\begin{center}
\Large\textbf{Bon apprentissage et bons projets ML !}
\end{center}

\end{document}
