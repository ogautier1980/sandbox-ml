{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/XX_CHAPTER/XX_NOTEBOOK.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '08_demo_rag_llm.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 08 - RAG et LLMs : Retrieval-Augmented Generation\n",
    "\n",
    "Ce notebook explore les concepts avanc√©s de **RAG (Retrieval-Augmented Generation)**, une technique qui combine la recherche d'information avec la g√©n√©ration de texte par LLMs.\n",
    "\n",
    "## Objectifs\n",
    "1. Comprendre l'architecture RAG\n",
    "2. Impl√©menter des embeddings et vector search avec FAISS\n",
    "3. Construire un pipeline RAG complet\n",
    "4. Techniques avanc√©es : chunking, reranking, hybrid search\n",
    "5. √âvaluer la qualit√© du RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction au RAG\n",
    "\n",
    "### Qu'est-ce que le RAG ?\n",
    "\n",
    "Le **Retrieval-Augmented Generation** est une technique qui am√©liore les LLMs en leur fournissant des informations contextuelles pertinentes r√©cup√©r√©es d'une base de connaissances.\n",
    "\n",
    "**Architecture RAG** :\n",
    "```\n",
    "Query ‚Üí Retrieval (recherche) ‚Üí Context ‚Üí LLM ‚Üí Response\n",
    "         ‚Üì\n",
    "    Knowledge Base\n",
    "    (documents vectoris√©s)\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- R√©duit les hallucinations du LLM\n",
    "- Permet d'utiliser des connaissances √† jour sans r√©entra√Ænement\n",
    "- Transparence : les sources peuvent √™tre cit√©es\n",
    "\n",
    "**Cas d'usage** :\n",
    "- Chatbots de support client\n",
    "- Syst√®mes de Q&A sur documentation\n",
    "- Assistants de recherche acad√©mique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des biblioth√®ques n√©cessaires\n",
    "!pip install -q sentence-transformers faiss-cpu rank-bm25 scikit-learn matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss\n",
    "from sklearn.decomposition import PCA\n",
    "from rank_bm25 import BM25Okapi\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings et Vector Search\n",
    "\n",
    "### 2.1 Cr√©ation d'Embeddings avec Sentence-BERT\n",
    "\n",
    "Les **embeddings** sont des repr√©sentations vectorielles denses qui capturent le sens s√©mantique du texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un mod√®le d'embeddings pr√©-entra√Æn√©\n",
    "# all-MiniLM-L6-v2 : l√©ger (80 MB), rapide, 384 dimensions\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"Mod√®le charg√© : {model.get_sentence_embedding_dimension()} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exemple : corpus de connaissances ML\n",
    "documents = [\n",
    "    \"Le Machine Learning est une branche de l'intelligence artificielle qui permet aux syst√®mes d'apprendre automatiquement √† partir des donn√©es sans √™tre explicitement programm√©s.\",\n",
    "    \"Les r√©seaux de neurones profonds sont compos√©s de plusieurs couches de neurones artificiels interconnect√©s qui transforment progressivement les donn√©es d'entr√©e.\",\n",
    "    \"LSTM signifie Long Short-Term Memory. C'est une architecture de r√©seau de neurones r√©current con√ßue pour r√©soudre le probl√®me du gradient qui dispara√Æt.\",\n",
    "    \"L'attention est un m√©canisme qui permet au mod√®le de se concentrer sur diff√©rentes parties de l'entr√©e lors de la g√©n√©ration de chaque √©l√©ment de sortie.\",\n",
    "    \"Les Transformers utilisent uniquement des m√©canismes d'attention et se passent compl√®tement de r√©currence, ce qui permet une meilleure parall√©lisation.\",\n",
    "    \"BERT (Bidirectional Encoder Representations from Transformers) est pr√©-entra√Æn√© avec une t√¢che de Masked Language Modeling sur de grands corpus.\",\n",
    "    \"GPT (Generative Pre-trained Transformer) est un mod√®le autor√©gressif entra√Æn√© √† pr√©dire le mot suivant dans une s√©quence.\",\n",
    "    \"Le fine-tuning consiste √† adapter un mod√®le pr√©-entra√Æn√© √† une t√¢che sp√©cifique en continuant l'entra√Ænement sur des donn√©es de cette t√¢che.\",\n",
    "    \"La validation crois√©e permet d'√©valuer la performance d'un mod√®le en divisant les donn√©es en plusieurs folds et en entra√Ænant/testant sur diff√©rentes combinaisons.\",\n",
    "    \"L'overfitting se produit quand un mod√®le apprend trop bien les donn√©es d'entra√Ænement au point de ne plus g√©n√©raliser sur de nouvelles donn√©es.\",\n",
    "    \"La r√©gularisation L2 (Ridge) ajoute une p√©nalit√© proportionnelle au carr√© des poids pour √©viter l'overfitting.\",\n",
    "    \"Le dropout est une technique de r√©gularisation qui d√©sactive al√©atoirement des neurones pendant l'entra√Ænement pour forcer le r√©seau √† √™tre plus robuste.\",\n",
    "    \"Le gradient descent est un algorithme d'optimisation qui ajuste it√©rativement les param√®tres dans la direction oppos√©e au gradient de la fonction de perte.\",\n",
    "    \"Adam (Adaptive Moment Estimation) est un optimiseur qui combine les avantages de RMSProp et momentum avec des taux d'apprentissage adaptatifs.\",\n",
    "    \"La backpropagation est l'algorithme utilis√© pour calculer les gradients dans les r√©seaux de neurones en propageant l'erreur de la sortie vers l'entr√©e.\"\n",
    "]\n",
    "\n",
    "print(f\"Corpus de {len(documents)} documents charg√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer les embeddings pour tous les documents\n",
    "print(\"G√©n√©ration des embeddings...\")\n",
    "embeddings = model.encode(documents, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nShape des embeddings : {embeddings.shape}\")\n",
    "print(f\"Type : {embeddings.dtype}\")\n",
    "print(f\"Exemple (5 premi√®res dimensions du doc 0) : {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vector Search avec FAISS\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) est une biblioth√®que optimis√©e pour la recherche de similarit√© dans des espaces vectoriels de haute dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un index FAISS pour recherche rapide\n",
    "dimension = embeddings.shape[1]  # 384 pour all-MiniLM-L6-v2\n",
    "\n",
    "# IndexFlatL2 : recherche exhaustive avec distance L2 (Euclidienne)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Ajouter les embeddings √† l'index\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(f\"Index FAISS cr√©√© avec {index.ntotal} vecteurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de recherche\n",
    "def search(query, k=3):\n",
    "    \"\"\"\n",
    "    Recherche les k documents les plus similaires √† la query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Question/requ√™te de l'utilisateur\n",
    "        k (int): Nombre de r√©sultats √† retourner\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: Liste de (document, distance)\n",
    "    \"\"\"\n",
    "    # Encoder la query\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Rechercher dans l'index\n",
    "    distances, indices = index.search(query_embedding.astype('float32'), k)\n",
    "    \n",
    "    # Retourner les r√©sultats\n",
    "    results = []\n",
    "    for j, i in enumerate(indices[0]):\n",
    "        results.append((documents[i], float(distances[0][j])))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de recherche\n",
    "query = \"Comment fonctionnent les LSTM ?\"\n",
    "results = search(query, k=3)\n",
    "\n",
    "print(f\"Query : '{query}'\\n\")\n",
    "print(\"Top 3 r√©sultats :\")\n",
    "print(\"=\" * 80)\n",
    "for i, (doc, dist) in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}] Distance L2: {dist:.3f}\")\n",
    "    print(f\"Document : {doc}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester plusieurs queries\n",
    "test_queries = [\n",
    "    \"Qu'est-ce que l'attention dans les r√©seaux de neurones ?\",\n",
    "    \"Comment √©viter l'overfitting ?\",\n",
    "    \"Quelle est la diff√©rence entre BERT et GPT ?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query : '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    results = search(query, k=2)\n",
    "    for i, (doc, dist) in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] Distance: {dist:.3f}\")\n",
    "        print(f\"{doc[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline RAG Complet\n",
    "\n",
    "Construisons un syst√®me RAG end-to-end qui int√®gre retrieval et g√©n√©ration de contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    Pipeline RAG simple : Retrieval + Context Generation.\n",
    "    \n",
    "    Note : Cette impl√©mentation ne fait pas appel √† un LLM r√©el (OpenAI/GPT)\n",
    "    mais g√©n√®re un prompt format√© qui pourrait √™tre envoy√© √† un LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents, model_name='all-MiniLM-L6-v2'):\n",
    "        self.documents = documents\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.embeddings = self.model.encode(documents, show_progress_bar=False)\n",
    "        self.index = self._create_index()\n",
    "    \n",
    "    def _create_index(self):\n",
    "        \"\"\"Cr√©e un index FAISS pour la recherche vectorielle.\"\"\"\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(self.embeddings.astype('float32'))\n",
    "        return index\n",
    "    \n",
    "    def retrieve(self, query, k=3):\n",
    "        \"\"\"\n",
    "        R√©cup√®re les k documents les plus pertinents.\n",
    "        \n",
    "        Args:\n",
    "            query (str): Question de l'utilisateur\n",
    "            k (int): Nombre de documents √† r√©cup√©rer\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: Liste des documents pertinents\n",
    "        \"\"\"\n",
    "        query_emb = self.model.encode([query])\n",
    "        distances, indices = self.index.search(query_emb.astype('float32'), k)\n",
    "        return [self.documents[i] for i in indices[0]]\n",
    "    \n",
    "    def generate_context(self, query, k=3):\n",
    "        \"\"\"\n",
    "        G√©n√®re un contexte format√© √† partir des documents r√©cup√©r√©s.\n",
    "        \n",
    "        Args:\n",
    "            query (str): Question de l'utilisateur\n",
    "            k (int): Nombre de documents √† utiliser\n",
    "        \n",
    "        Returns:\n",
    "            str: Contexte format√©\n",
    "        \"\"\"\n",
    "        retrieved_docs = self.retrieve(query, k=k)\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document {i+1}: {doc}\" \n",
    "            for i, doc in enumerate(retrieved_docs)\n",
    "        ])\n",
    "        return context\n",
    "    \n",
    "    def answer(self, query, k=3):\n",
    "        \"\"\"\n",
    "        G√©n√®re un prompt complet pour un LLM avec contexte r√©cup√©r√©.\n",
    "        \n",
    "        Args:\n",
    "            query (str): Question de l'utilisateur\n",
    "            k (int): Nombre de documents √† utiliser\n",
    "        \n",
    "        Returns:\n",
    "            str: Prompt format√© pour LLM\n",
    "        \"\"\"\n",
    "        context = self.generate_context(query, k=k)\n",
    "        \n",
    "        prompt = f\"\"\"Vous √™tes un assistant IA expert en Machine Learning.\n",
    "\n",
    "Contexte r√©cup√©r√© de la base de connaissances :\n",
    "{context}\n",
    "\n",
    "Question de l'utilisateur : {query}\n",
    "\n",
    "Instructions :\n",
    "- R√©pondez uniquement en vous basant sur le contexte fourni\n",
    "- Si l'information n'est pas dans le contexte, dites-le clairement\n",
    "- Citez les documents pertinents (Document 1, 2, etc.)\n",
    "\n",
    "R√©ponse :\"\"\"\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancier le syst√®me RAG\n",
    "rag = SimpleRAG(documents)\n",
    "\n",
    "print(\"Syst√®me RAG initialis√© avec succ√®s !\")\n",
    "print(f\"Nombre de documents index√©s : {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du pipeline RAG\n",
    "query = \"Qu'est-ce qu'un LSTM et pourquoi est-il utilis√© ?\"\n",
    "prompt = rag.answer(query, k=3)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester avec diff√©rentes questions\n",
    "test_queries = [\n",
    "    \"Comment fonctionne le m√©canisme d'attention ?\",\n",
    "    \"Quelle est la diff√©rence entre BERT et GPT ?\",\n",
    "    \"Quelles sont les techniques pour √©viter l'overfitting ?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUERY : {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # R√©cup√©rer seulement les documents (sans le prompt complet)\n",
    "    retrieved = rag.retrieve(query, k=2)\n",
    "    for i, doc in enumerate(retrieved, 1):\n",
    "        print(f\"\\n[Document {i}]\")\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunking et Preprocessing\n",
    "\n",
    "Pour des documents longs, il faut les **d√©couper en chunks** (morceaux) g√©rables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    D√©coupe un texte en chunks avec overlap.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte √† d√©couper\n",
    "        chunk_size (int): Nombre de mots par chunk\n",
    "        overlap (int): Nombre de mots de chevauchement entre chunks\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Liste de chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk:  # √âviter les chunks vides\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple avec un document long (simulation)\n",
    "long_document = \"\"\"Le Machine Learning est une discipline passionnante. \n",
    "Elle permet de cr√©er des syst√®mes intelligents. Les algorithmes apprennent des donn√©es. \n",
    "Il existe plusieurs types d'apprentissage : supervis√©, non-supervis√© et par renforcement. \n",
    "Les r√©seaux de neurones sont des mod√®les tr√®s puissants. Ils sont inspir√©s du cerveau humain.\n",
    "\"\"\" * 20  # R√©p√©ter pour simuler un long document\n",
    "\n",
    "print(f\"Document original : {len(long_document.split())} mots\\n\")\n",
    "\n",
    "# D√©couper avec diff√©rents param√®tres\n",
    "chunks = chunk_text(long_document, chunk_size=50, overlap=10)\n",
    "\n",
    "print(f\"Document d√©coup√© en {len(chunks)} chunks\")\n",
    "print(f\"\\nExemple de chunks :\")\n",
    "for i, chunk in enumerate(chunks[:3], 1):\n",
    "    print(f\"\\n[Chunk {i}] ({len(chunk.split())} mots)\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunk(text, separator='\\n\\n'):\n",
    "    \"\"\"\n",
    "    D√©coupe un texte par s√©parateur s√©mantique (paragraphes).\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte √† d√©couper\n",
    "        separator (str): S√©parateur (d√©faut : double saut de ligne)\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: Liste de chunks\n",
    "    \"\"\"\n",
    "    paragraphs = text.split(separator)\n",
    "    return [p.strip() for p in paragraphs if p.strip()]\n",
    "\n",
    "# Exemple\n",
    "text_with_paragraphs = \"\"\"Paragraphe 1 : Introduction au ML.\n",
    "Le Machine Learning transforme les donn√©es en connaissances.\n",
    "\n",
    "Paragraphe 2 : Les algorithmes.\n",
    "Il existe de nombreux algorithmes diff√©rents.\n",
    "\n",
    "Paragraphe 3 : Applications.\n",
    "Le ML est utilis√© dans de nombreux domaines.\"\"\"\n",
    "\n",
    "semantic_chunks = semantic_chunk(text_with_paragraphs)\n",
    "print(f\"Chunks s√©mantiques : {len(semantic_chunks)}\\n\")\n",
    "for i, chunk in enumerate(semantic_chunks, 1):\n",
    "    print(f\"[Chunk {i}]\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. √âvaluation du RAG\n",
    "\n",
    "Mesurer la qualit√© du retrieval avec **Precision@K** et **Recall@K**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(queries, expected_indices, rag_system, k=3):\n",
    "    \"\"\"\n",
    "    √âvalue la qualit√© du retrieval avec Precision@K et Recall@K.\n",
    "    \n",
    "    Args:\n",
    "        queries (List[str]): Liste de questions\n",
    "        expected_indices (List[List[int]]): Indices des documents pertinents pour chaque query\n",
    "        rag_system (SimpleRAG): Syst√®me RAG √† √©valuer\n",
    "        k (int): Nombre de documents √† r√©cup√©rer\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, float]: (mean_precision, mean_recall)\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for query, expected in zip(queries, expected_indices):\n",
    "        # R√©cup√©rer les documents\n",
    "        retrieved_docs = rag_system.retrieve(query, k=k)\n",
    "        \n",
    "        # Trouver les indices des documents r√©cup√©r√©s\n",
    "        retrieved_indices = []\n",
    "        for doc in retrieved_docs:\n",
    "            try:\n",
    "                idx = rag_system.documents.index(doc)\n",
    "                retrieved_indices.append(idx)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Calculer Precision et Recall\n",
    "        retrieved_set = set(retrieved_indices)\n",
    "        expected_set = set(expected)\n",
    "        \n",
    "        true_positives = len(retrieved_set & expected_set)\n",
    "        \n",
    "        precision = true_positives / k if k > 0 else 0\n",
    "        recall = true_positives / len(expected_set) if expected_set else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return np.mean(precisions), np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset d'√©valuation\n",
    "eval_queries = [\n",
    "    \"Qu'est-ce que LSTM ?\",\n",
    "    \"Comment √©viter l'overfitting ?\",\n",
    "    \"Qu'est-ce que l'attention ?\"\n",
    "]\n",
    "\n",
    "# Indices des documents pertinents (ground truth)\n",
    "# Bas√© sur notre corpus de 15 documents\n",
    "expected_indices = [\n",
    "    [2],        # Query 1 : doc sur LSTM (index 2)\n",
    "    [9, 11],    # Query 2 : docs sur overfitting et r√©gularisation (index 9, 11)\n",
    "    [3, 4]      # Query 3 : docs sur attention (index 3, 4)\n",
    "]\n",
    "\n",
    "# √âvaluation\n",
    "precision, recall = evaluate_retrieval(eval_queries, expected_indices, rag, k=3)\n",
    "\n",
    "print(f\"Precision@3 : {precision:.3f}\")\n",
    "print(f\"Recall@3    : {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer pour diff√©rentes valeurs de K\n",
    "k_values = [1, 2, 3, 5]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    prec, rec = evaluate_retrieval(eval_queries, expected_indices, rag, k=k)\n",
    "    results.append((k, prec, rec))\n",
    "    print(f\"K={k} | Precision: {prec:.3f} | Recall: {rec:.3f}\")\n",
    "\n",
    "# Visualisation\n",
    "k_vals, precs, recs = zip(*results)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_vals, precs, marker='o', label='Precision@K', linewidth=2)\n",
    "plt.plot(k_vals, recs, marker='s', label='Recall@K', linewidth=2)\n",
    "plt.xlabel('K (nombre de documents r√©cup√©r√©s)', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Precision@K et Recall@K en fonction de K', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_vals)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Techniques Avanc√©es\n",
    "\n",
    "### 6.1 Reranking : Am√©liorer les R√©sultats\n",
    "\n",
    "Le **reranking** utilise un mod√®le plus pr√©cis (CrossEncoder) pour r√©ordonner les r√©sultats de la premi√®re recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un mod√®le de reranking\n",
    "print(\"Chargement du mod√®le de reranking...\")\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"Mod√®le charg√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(query, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    R√©ordonne les documents avec un CrossEncoder.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Question de l'utilisateur\n",
    "        documents (List[str]): Documents √† r√©ordonner\n",
    "        top_k (int): Nombre de r√©sultats √† retourner\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: Documents r√©ordonn√©s avec scores\n",
    "    \"\"\"\n",
    "    # Cr√©er des paires (query, document)\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    \n",
    "    # Pr√©dire les scores de pertinence\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # R√©ordonner par score d√©croissant\n",
    "    ranked_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    return [(documents[i], float(scores[i])) for i in ranked_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison : Retrieval seul vs Retrieval + Reranking\n",
    "query = \"Expliquez le m√©canisme d'attention dans les Transformers\"\n",
    "\n",
    "# 1. Retrieval classique (top 5)\n",
    "retrieved_docs = rag.retrieve(query, k=5)\n",
    "\n",
    "print(\"RETRIEVAL SEUL (FAISS) - Top 5\")\n",
    "print(\"=\"*80)\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"[{i}] {doc[:100]}...\\n\")\n",
    "\n",
    "# 2. Reranking des r√©sultats\n",
    "reranked = rerank(query, retrieved_docs, top_k=3)\n",
    "\n",
    "print(\"\\nAPR√àS RERANKING - Top 3\")\n",
    "print(\"=\"*80)\n",
    "for i, (doc, score) in enumerate(reranked, 1):\n",
    "    print(f\"[{i}] Score: {score:.3f}\")\n",
    "    print(f\"{doc[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Hybrid Search : BM25 + Dense Embeddings\n",
    "\n",
    "Combine **recherche par mots-cl√©s (BM25)** et **recherche s√©mantique (embeddings)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, documents, embeddings, index, model, alpha=0.5, k=3):\n",
    "    \"\"\"\n",
    "    Recherche hybride : BM25 + Dense embeddings.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Question de l'utilisateur\n",
    "        documents (List[str]): Corpus de documents\n",
    "        embeddings (np.ndarray): Embeddings des documents\n",
    "        index (faiss.Index): Index FAISS\n",
    "        model (SentenceTransformer): Mod√®le d'embeddings\n",
    "        alpha (float): Poids de BM25 (0=dense only, 1=BM25 only)\n",
    "        k (int): Nombre de r√©sultats\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: Documents avec scores hybrides\n",
    "    \"\"\"\n",
    "    # 1. BM25 (keyword-based)\n",
    "    tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    bm25_scores = bm25.get_scores(query.lower().split())\n",
    "    \n",
    "    # Normaliser scores BM25 (0-1)\n",
    "    if bm25_scores.max() > 0:\n",
    "        bm25_scores = bm25_scores / bm25_scores.max()\n",
    "    \n",
    "    # 2. Dense search (embedding-based)\n",
    "    query_emb = model.encode([query])\n",
    "    distances, indices = index.search(query_emb.astype('float32'), len(documents))\n",
    "    \n",
    "    # Convertir distances L2 en scores de similarit√© (inverse)\n",
    "    dense_scores = np.zeros(len(documents))\n",
    "    max_dist = distances[0].max() if distances[0].max() > 0 else 1\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        dense_scores[idx] = 1 - (distances[0][i] / max_dist)\n",
    "    \n",
    "    # 3. Combiner les scores\n",
    "    combined_scores = alpha * bm25_scores + (1 - alpha) * dense_scores\n",
    "    \n",
    "    # 4. S√©lectionner top-k\n",
    "    top_indices = np.argsort(combined_scores)[::-1][:k]\n",
    "    \n",
    "    return [(documents[i], float(combined_scores[i])) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des strat√©gies de recherche\n",
    "query = \"gradient descent optimisation\"\n",
    "\n",
    "print(f\"Query : '{query}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# BM25 only (alpha=1)\n",
    "bm25_results = hybrid_search(query, documents, embeddings, index, model, alpha=1.0, k=3)\n",
    "print(\"\\nBM25 ONLY (Keyword-based)\")\n",
    "for i, (doc, score) in enumerate(bm25_results, 1):\n",
    "    print(f\"[{i}] Score: {score:.3f} | {doc[:80]}...\")\n",
    "\n",
    "# Dense only (alpha=0)\n",
    "dense_results = hybrid_search(query, documents, embeddings, index, model, alpha=0.0, k=3)\n",
    "print(\"\\nDENSE ONLY (Semantic)\")\n",
    "for i, (doc, score) in enumerate(dense_results, 1):\n",
    "    print(f\"[{i}] Score: {score:.3f} | {doc[:80]}...\")\n",
    "\n",
    "# Hybrid (alpha=0.5)\n",
    "hybrid_results = hybrid_search(query, documents, embeddings, index, model, alpha=0.5, k=3)\n",
    "print(\"\\nHYBRID (BM25 + Dense, alpha=0.5)\")\n",
    "for i, (doc, score) in enumerate(hybrid_results, 1):\n",
    "    print(f\"[{i}] Score: {score:.3f} | {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des Embeddings\n",
    "\n",
    "Visualiser les embeddings en 2D avec **PCA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©duction de dimensionnalit√© avec PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "print(f\"Variance expliqu√©e par les 2 composantes : {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100, alpha=0.6, c='steelblue', edgecolors='black')\n",
    "\n",
    "# Annoter avec les premiers mots de chaque document\n",
    "for i, doc in enumerate(documents):\n",
    "    label = ' '.join(doc.split()[:4]) + '...'  # 4 premiers mots\n",
    "    plt.annotate(label, \n",
    "                 (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                 fontsize=8,\n",
    "                 alpha=0.8,\n",
    "                 bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "plt.title('Visualisation des Embeddings (PCA 2D)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Composante Principale 1', fontsize=12)\n",
    "plt.ylabel('Composante Principale 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser une query et ses r√©sultats\n",
    "query_test = \"Qu'est-ce que le dropout ?\"\n",
    "query_emb = model.encode([query_test])\n",
    "query_2d = pca.transform(query_emb)\n",
    "\n",
    "# R√©cup√©rer les documents pertinents\n",
    "retrieved_indices = []\n",
    "retrieved_docs = rag.retrieve(query_test, k=3)\n",
    "for doc in retrieved_docs:\n",
    "    retrieved_indices.append(documents.index(doc))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Documents (gris)\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100, alpha=0.3, c='gray', label='Autres documents')\n",
    "\n",
    "# Documents r√©cup√©r√©s (vert)\n",
    "retrieved_2d = embeddings_2d[retrieved_indices]\n",
    "plt.scatter(retrieved_2d[:, 0], retrieved_2d[:, 1], s=150, alpha=0.8, c='green', edgecolors='black', label='Docs r√©cup√©r√©s')\n",
    "\n",
    "# Query (rouge)\n",
    "plt.scatter(query_2d[:, 0], query_2d[:, 1], s=200, c='red', marker='*', edgecolors='black', label='Query', zorder=5)\n",
    "\n",
    "# Annotations\n",
    "plt.annotate('QUERY', (query_2d[0, 0], query_2d[0, 1]), fontsize=12, fontweight='bold', color='red')\n",
    "for i in retrieved_indices:\n",
    "    label = ' '.join(documents[i].split()[:4]) + '...'\n",
    "    plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=9, color='green')\n",
    "\n",
    "plt.title(f'Query : \"{query_test}\"', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Composante Principale 1', fontsize=12)\n",
    "plt.ylabel('Composante Principale 2', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Cr√©er un RAG pour un Corpus Personnalis√©\n",
    "\n",
    "**T√¢che** : Cr√©ez un syst√®me RAG sur un corpus de votre choix (ex: articles scientifiques, documentation technique, etc.).\n",
    "\n",
    "**Instructions** :\n",
    "1. Pr√©parez un corpus d'au moins 20 documents\n",
    "2. Impl√©mentez le chunking si n√©cessaire\n",
    "3. Cr√©ez l'index FAISS\n",
    "4. Testez avec 5 queries\n",
    "5. √âvaluez avec Precision@3 et Recall@3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION EXERCICE 1\n",
    "\n",
    "# Corpus exemple : concepts Python avanc√©s\n",
    "python_docs = [\n",
    "    \"Les d√©corateurs en Python sont des fonctions qui modifient le comportement d'autres fonctions ou classes.\",\n",
    "    \"Les g√©n√©rateurs permettent de cr√©er des it√©rateurs de mani√®re √©l√©gante en utilisant le mot-cl√© yield.\",\n",
    "    \"Le context manager (with statement) g√®re automatiquement l'acquisition et la lib√©ration de ressources.\",\n",
    "    \"Les list comprehensions offrent une syntaxe concise pour cr√©er des listes √† partir d'it√©rables.\",\n",
    "    \"Les lambda functions sont des fonctions anonymes d√©finies en une seule ligne avec le mot-cl√© lambda.\",\n",
    "    \"Le Global Interpreter Lock (GIL) emp√™che plusieurs threads d'ex√©cuter du bytecode Python simultan√©ment.\",\n",
    "    \"Les metaclasses sont des classes de classes qui contr√¥lent la cr√©ation et le comportement des classes.\",\n",
    "    \"Le duck typing permet d'utiliser des objets bas√©s sur leurs m√©thodes plut√¥t que sur leur type explicite.\",\n",
    "    \"Les asyncio et await permettent la programmation asynchrone pour g√©rer les op√©rations I/O efficacement.\",\n",
    "    \"Le property decorator transforme une m√©thode en attribut accessible avec une syntaxe simplifi√©e.\",\n",
    "    \"Les dataclasses (depuis Python 3.7) r√©duisent le boilerplate pour cr√©er des classes de donn√©es.\",\n",
    "    \"Le module collections fournit des conteneurs sp√©cialis√©s comme defaultdict, Counter et deque.\",\n",
    "    \"Les type hints (PEP 484) permettent d'annoter le code avec des types pour am√©liorer la lisibilit√©.\",\n",
    "    \"Le pattern matching (Python 3.10+) permet de comparer des structures de donn√©es avec match/case.\",\n",
    "    \"Les f-strings offrent une syntaxe moderne et performante pour le formatage de cha√Ænes.\",\n",
    "    \"Le module functools fournit des outils pour la programmation fonctionnelle comme partial et reduce.\",\n",
    "    \"Les slots optimisent la m√©moire en d√©finissant explicitement les attributs d'une classe.\",\n",
    "    \"Le module abc (Abstract Base Classes) permet de d√©finir des interfaces en Python.\",\n",
    "    \"Les coroutines sont des fonctions qui peuvent √™tre suspendues et reprises, essentielles pour asyncio.\",\n",
    "    \"Le module itertools fournit des it√©rateurs efficaces pour des op√©rations combinatoires et de permutation.\"\n",
    "]\n",
    "\n",
    "# Cr√©er le syst√®me RAG\n",
    "python_rag = SimpleRAG(python_docs)\n",
    "\n",
    "# Test avec queries\n",
    "test_queries = [\n",
    "    \"Comment cr√©er des it√©rateurs en Python ?\",\n",
    "    \"Qu'est-ce que le GIL ?\",\n",
    "    \"Comment formater des cha√Ænes en Python ?\",\n",
    "    \"Qu'est-ce qu'une fonction lambda ?\",\n",
    "    \"Comment faire de la programmation asynchrone ?\"\n",
    "]\n",
    "\n",
    "print(\"Test du RAG sur documentation Python\\n\")\n",
    "for query in test_queries:\n",
    "    print(f\"Query : {query}\")\n",
    "    results = python_rag.retrieve(query, k=2)\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"  [{i}] {doc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Comparer Diff√©rents Mod√®les d'Embeddings\n",
    "\n",
    "**T√¢che** : Comparez les performances de diff√©rents mod√®les d'embeddings.\n",
    "\n",
    "**Mod√®les √† tester** :\n",
    "- `all-MiniLM-L6-v2` (rapide, 384 dim)\n",
    "- `all-mpnet-base-v2` (meilleur qualit√©, 768 dim)\n",
    "\n",
    "**M√©trique** : Precision@3 sur un ensemble de queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION EXERCICE 2\n",
    "\n",
    "import time\n",
    "\n",
    "models_to_test = [\n",
    "    'all-MiniLM-L6-v2',      # 384 dimensions\n",
    "    'all-mpnet-base-v2'      # 768 dimensions\n",
    "]\n",
    "\n",
    "# Queries et ground truth (bas√© sur notre corpus ML original)\n",
    "eval_queries = [\n",
    "    \"Qu'est-ce que LSTM ?\",\n",
    "    \"Comment √©viter l'overfitting ?\",\n",
    "    \"Qu'est-ce que l'attention ?\"\n",
    "]\n",
    "expected_indices = [[2], [9, 11], [3, 4]]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\nTest du mod√®le : {model_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Cr√©er RAG avec ce mod√®le\n",
    "    start = time.time()\n",
    "    rag_test = SimpleRAG(documents, model_name=model_name)\n",
    "    init_time = time.time() - start\n",
    "    \n",
    "    # √âvaluer\n",
    "    prec, rec = evaluate_retrieval(eval_queries, expected_indices, rag_test, k=3)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'model': model_name,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'init_time': init_time,\n",
    "        'dimensions': rag_test.embeddings.shape[1]\n",
    "    })\n",
    "    \n",
    "    print(f\"Dimensions      : {rag_test.embeddings.shape[1]}\")\n",
    "    print(f\"Temps init      : {init_time:.2f}s\")\n",
    "    print(f\"Precision@3     : {prec:.3f}\")\n",
    "    print(f\"Recall@3        : {rec:.3f}\")\n",
    "\n",
    "# Tableau comparatif\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON DES MOD√àLES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Mod√®le':<30} {'Dim':<8} {'P@3':<8} {'R@3':<8} {'Temps (s)':<10}\")\n",
    "print(\"-\"*80)\n",
    "for res in comparison_results:\n",
    "    print(f\"{res['model']:<30} {res['dimensions']:<8} {res['precision']:<8.3f} {res['recall']:<8.3f} {res['init_time']:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : Impl√©menter un Syst√®me de Reranking\n",
    "\n",
    "**T√¢che** : Impl√©mentez un pipeline complet avec retrieval initial (k=10) puis reranking (top-3).\n",
    "\n",
    "**√âtapes** :\n",
    "1. R√©cup√©rer 10 documents avec FAISS\n",
    "2. Reranker avec CrossEncoder\n",
    "3. Garder les 3 meilleurs\n",
    "4. Comparer avec retrieval direct (k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION EXERCICE 3\n",
    "\n",
    "class RAGWithReranking:\n",
    "    \"\"\"\n",
    "    Pipeline RAG avec √©tape de reranking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents, embedding_model='all-MiniLM-L6-v2', \n",
    "                 reranker_model='cross-encoder/ms-marco-MiniLM-L-6-v2'):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.reranker = CrossEncoder(reranker_model)\n",
    "        self.embeddings = self.embedding_model.encode(documents, show_progress_bar=False)\n",
    "        self.index = self._create_index()\n",
    "    \n",
    "    def _create_index(self):\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(self.embeddings.astype('float32'))\n",
    "        return index\n",
    "    \n",
    "    def retrieve_and_rerank(self, query, retrieve_k=10, final_k=3):\n",
    "        \"\"\"\n",
    "        Pipeline : Retrieval (k=retrieve_k) ‚Üí Reranking ‚Üí Top final_k.\n",
    "        \"\"\"\n",
    "        # √âtape 1 : Retrieval initial\n",
    "        query_emb = self.embedding_model.encode([query])\n",
    "        distances, indices = self.index.search(query_emb.astype('float32'), retrieve_k)\n",
    "        candidate_docs = [self.documents[i] for i in indices[0]]\n",
    "        \n",
    "        # √âtape 2 : Reranking\n",
    "        pairs = [[query, doc] for doc in candidate_docs]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # √âtape 3 : Top final_k\n",
    "        ranked_indices = np.argsort(scores)[::-1][:final_k]\n",
    "        final_docs = [(candidate_docs[i], float(scores[i])) for i in ranked_indices]\n",
    "        \n",
    "        return final_docs\n",
    "\n",
    "# Cr√©er le syst√®me\n",
    "print(\"Initialisation du syst√®me avec reranking...\")\n",
    "rag_rerank = RAGWithReranking(documents)\n",
    "print(\"Syst√®me pr√™t !\\n\")\n",
    "\n",
    "# Test\n",
    "query = \"Expliquez la backpropagation dans les r√©seaux de neurones\"\n",
    "\n",
    "print(f\"Query : '{query}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# R√©sultats avec reranking\n",
    "results_rerank = rag_rerank.retrieve_and_rerank(query, retrieve_k=10, final_k=3)\n",
    "print(\"AVEC RERANKING (retrieve 10 ‚Üí rerank ‚Üí top 3)\")\n",
    "for i, (doc, score) in enumerate(results_rerank, 1):\n",
    "    print(f\"\\n[{i}] Score: {score:.3f}\")\n",
    "    print(f\"{doc}\")\n",
    "\n",
    "# Comparaison avec retrieval direct\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "results_direct = rag.retrieve(query, k=3)\n",
    "print(\"SANS RERANKING (retrieval direct top 3)\")\n",
    "for i, doc in enumerate(results_direct, 1):\n",
    "    print(f\"\\n[{i}] {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### R√©capitulatif\n",
    "\n",
    "Dans ce notebook, nous avons explor√© :\n",
    "\n",
    "1. **Architecture RAG** : Retrieval + Generation pour am√©liorer les LLMs\n",
    "2. **Embeddings** : Repr√©sentations vectorielles avec Sentence-BERT\n",
    "3. **Vector Search** : Recherche rapide avec FAISS (IndexFlatL2)\n",
    "4. **Pipeline RAG** : Syst√®me complet de retrieval et g√©n√©ration de contexte\n",
    "5. **Chunking** : Techniques de d√©coupage de documents longs\n",
    "6. **√âvaluation** : Precision@K et Recall@K pour mesurer la qualit√©\n",
    "7. **Techniques avanc√©es** :\n",
    "   - **Reranking** : CrossEncoder pour am√©liorer les r√©sultats\n",
    "   - **Hybrid Search** : Combinaison BM25 (keyword) + Dense (semantic)\n",
    "8. **Visualisation** : PCA pour explorer l'espace des embeddings\n",
    "\n",
    "### Points Cl√©s\n",
    "\n",
    "- **RAG r√©duit les hallucinations** en fournissant un contexte factuel au LLM\n",
    "- **FAISS est essentiel** pour la recherche vectorielle √† grande √©chelle\n",
    "- **Reranking am√©liore significativement** la qualit√© des r√©sultats (mais co√ªt computationnel)\n",
    "- **Hybrid search combine** les avantages de la recherche par mots-cl√©s et s√©mantique\n",
    "- **L'√©valuation est cruciale** : toujours mesurer Precision/Recall\n",
    "\n",
    "### Pour Aller Plus Loin\n",
    "\n",
    "- **LlamaIndex** : framework complet pour RAG\n",
    "- **LangChain** : orchestration de LLMs avec RAG\n",
    "- **ChromaDB, Pinecone, Weaviate** : bases de donn√©es vectorielles\n",
    "- **ColBERT** : late interaction pour retrieval ultra-performant\n",
    "- **Dense Passage Retrieval (DPR)** : mod√®le bi-encoder entra√Æn√© end-to-end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}