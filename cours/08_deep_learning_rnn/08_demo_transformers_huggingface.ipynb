{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/08_deep_learning_rnn/08_demo_transformers_huggingface.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '08_demo_transformers_huggingface.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 08 - Transformers avec Hugging Face\n",
    "\n",
    "Ce notebook d√©montre l'utilisation de Transformers pr√©-entra√Æn√©s via Hugging Face.\n",
    "\n",
    "## Objectifs\n",
    "- Charger et utiliser des mod√®les pr√©-entra√Æn√©s (BERT, GPT)\n",
    "- Fine-tuner un mod√®le pour une t√¢che sp√©cifique\n",
    "- Comprendre les tokenizers et le pipeline Hugging Face\n",
    "- √âvaluer les performances sur des t√¢ches NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, pipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline Simple - Sentiment Analysis\n",
    "\n",
    "Utilisation du pipeline Hugging Face pour une t√¢che rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un pipeline de sentiment analysis\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# Tester sur quelques phrases\n",
    "test_sentences = [\n",
    "    \"This movie is absolutely fantastic!\",\n",
    "    \"I hated this film, it was terrible.\",\n",
    "    \"The acting was great but the plot was boring.\",\n",
    "    \"Best movie I've ever seen, highly recommended!\",\n",
    "    \"Waste of time and money, very disappointing.\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Analysis with Pre-trained Model:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    result = sentiment_analyzer(sentence)[0]\n",
    "    print(f\"Text: {sentence}\")\n",
    "    print(f\"  Label: {result['label']}, Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pr√©paration des Donn√©es pour Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un dataset synth√©tique (remplacer par vrai dataset en production)\n",
    "positive_samples = [\n",
    "    \"excellent product highly recommended\",\n",
    "    \"amazing quality great value for money\",\n",
    "    \"best purchase ever very satisfied\",\n",
    "    \"outstanding service quick delivery\",\n",
    "    \"love it perfect exactly what i needed\"\n",
    "] * 50\n",
    "\n",
    "negative_samples = [\n",
    "    \"terrible quality waste of money\",\n",
    "    \"disappointed poor service never again\",\n",
    "    \"worst purchase broke immediately\",\n",
    "    \"horrible experience bad quality\",\n",
    "    \"not recommended cheap materials poor design\"\n",
    "] * 50\n",
    "\n",
    "# Combiner\n",
    "texts = positive_samples + negative_samples\n",
    "labels = [1] * len(positive_samples) + [0] * len(negative_samples)\n",
    "\n",
    "# Split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenizer les donn√©es\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Cr√©er des datasets Hugging Face\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': train_labels\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask'],\n",
    "    'labels': test_labels\n",
    "})\n",
    "\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Test dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Charger le Mod√®le et Fine-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le mod√®le pr√©-entra√Æn√©\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de m√©triques\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Arguments d'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Cr√©er le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuner le mod√®le\n",
    "print(\"Starting fine-tuning...\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nFine-tuning completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer le mod√®le\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions sur le test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, pred_labels, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, pred_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.title('Confusion Matrix - Fine-tuned Model', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation de l'Historique d'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les logs\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# S√©parer train et eval logs\n",
    "train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "if train_logs and eval_logs:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    train_steps = [log['step'] for log in train_logs]\n",
    "    train_loss = [log['loss'] for log in train_logs]\n",
    "    axes[0].plot(train_steps, train_loss, linewidth=2, marker='o')\n",
    "    axes[0].set_xlabel('Steps', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    eval_epochs = [log['epoch'] for log in eval_logs]\n",
    "    eval_acc = [log['eval_accuracy'] for log in eval_logs]\n",
    "    axes[1].plot(eval_epochs, eval_acc, linewidth=2, marker='s', color='green')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[1].set_title('Evaluation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training logs available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Utilisation du Mod√®le Fine-tun√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un pipeline avec le mod√®le fine-tun√©\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Tester sur de nouveaux textes\n",
    "new_texts = [\n",
    "    \"This product exceeded my expectations, absolutely wonderful!\",\n",
    "    \"Complete waste of money, very poor quality.\",\n",
    "    \"Good value for the price, would buy again.\",\n",
    "    \"Terrible experience, broken on arrival.\",\n",
    "    \"Perfect gift, everyone loved it!\"\n",
    "]\n",
    "\n",
    "print(\"Predictions on New Texts:\\n\")\n",
    "for text in new_texts:\n",
    "    result = classifier(text)[0]\n",
    "    sentiment = \"Positive\" if result['label'] == 'LABEL_1' else \"Negative\"\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  Prediction: {sentiment} (score: {result['score']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse des Embeddings (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les embeddings pour quelques exemples\n",
    "sample_texts = [\n",
    "    \"excellent product\",\n",
    "    \"terrible quality\",\n",
    "    \"good value\",\n",
    "    \"waste of money\"\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in sample_texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model.distilbert(**inputs)\n",
    "        # Prendre la moyenne du dernier hidden state\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.append(embedding[0])\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# PCA pour visualisation\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "colors = ['green', 'red', 'green', 'red']\n",
    "for i, text in enumerate(sample_texts):\n",
    "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], \n",
    "                c=colors[i], s=200, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "    plt.annotate(text, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                 fontsize=11, fontweight='bold', ha='center')\n",
    "\n",
    "plt.xlabel('PC1', fontsize=12)\n",
    "plt.ylabel('PC2', fontsize=12)\n",
    "plt.title('BERT Embeddings Visualization (PCA)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. G√©n√©ration de Texte avec GPT-2 (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de g√©n√©ration de texte\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Prompts\n",
    "prompts = [\n",
    "    \"Once upon a time in a distant galaxy\",\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In the year 2050, technology will\"\n",
    "]\n",
    "\n",
    "print(\"Text Generation with GPT-2:\\n\")\n",
    "for prompt in prompts:\n",
    "    generated = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated[0]['generated_text']}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Ce que nous avons appris:\n",
    "1. Utiliser les pipelines Hugging Face pour des t√¢ches rapides\n",
    "2. Fine-tuner un mod√®le BERT pr√©-entra√Æn√©\n",
    "3. Utiliser le Trainer API pour l'entra√Ænement\n",
    "4. √âvaluer et d√©ployer des mod√®les Transformers\n",
    "5. Visualiser les embeddings et g√©n√©rer du texte\n",
    "\n",
    "### Pour aller plus loin:\n",
    "- Essayer d'autres mod√®les (RoBERTa, ALBERT, ELECTRA)\n",
    "- Fine-tuner pour d'autres t√¢ches (NER, QA, summarization)\n",
    "- Utiliser des datasets r√©els (GLUE, SuperGLUE)\n",
    "- Optimiser avec quantization et distillation\n",
    "- D√©ployer en production avec FastAPI/TensorFlow Serving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}