{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/XX_CHAPTER/XX_NOTEBOOK.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '08_demo_lstm_sentiment.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 08 - LSTM pour Analyse de Sentiment\n",
    "\n",
    "Ce notebook d√©montre l'utilisation de LSTM pour classifier des critiques de films (positif/n√©gatif).\n",
    "\n",
    "## Objectifs\n",
    "- Pr√©parer des donn√©es textuelles (tokenization, padding)\n",
    "- Impl√©menter un LSTM bidirectionnel avec PyTorch\n",
    "- Entra√Æner et √©valuer un mod√®le d'analyse de sentiment\n",
    "- Visualiser l'apprentissage et les pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Pr√©paration des Donn√©es\n",
    "\n",
    "Nous utilisons le dataset IMDB (via PyTorch/torchtext ou une version simplifi√©e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset simplifi√© IMDB (version de d√©monstration)\n",
    "# Pour un dataset complet, utiliser: from torchtext.datasets import IMDB\n",
    "\n",
    "positive_reviews = [\n",
    "    \"this movie is absolutely fantastic and amazing\",\n",
    "    \"i loved this film it was brilliant and well made\",\n",
    "    \"great performance outstanding cinematography highly recommended\",\n",
    "    \"excellent storyline wonderful acting must see\",\n",
    "    \"best movie ever incredible experience enjoyed every minute\",\n",
    "    \"superb direction beautiful visuals truly masterpiece\",\n",
    "    \"amazing cast perfect script loved everything about it\",\n",
    "    \"wonderful film highly entertaining great fun\",\n",
    "    \"outstanding performances breathtaking scenes absolute gem\",\n",
    "    \"brilliant movie loved the plot and characters\"\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"this movie is terrible waste of time awful\",\n",
    "    \"horrible film boring script poor acting disappointing\",\n",
    "    \"worst movie ever seen complete disaster terrible\",\n",
    "    \"bad storyline weak performances not recommended\",\n",
    "    \"dreadful film poor quality waste of money\",\n",
    "    \"awful direction terrible acting horrible experience\",\n",
    "    \"boring plot uninteresting characters complete waste\",\n",
    "    \"terrible movie poorly executed bad script\",\n",
    "    \"disappointing performances weak storyline not worth watching\",\n",
    "    \"horrible film terrible acting waste of time\"\n",
    "]\n",
    "\n",
    "# R√©pliquer pour avoir plus de donn√©es\n",
    "reviews = (positive_reviews * 100) + (negative_reviews * 100)\n",
    "labels = [1] * (len(positive_reviews) * 100) + [0] * (len(negative_reviews) * 100)\n",
    "\n",
    "print(f\"Total reviews: {len(reviews)}\")\n",
    "print(f\"Positive: {sum(labels)}, Negative: {len(labels) - sum(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization et Vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, min_freq=1):\n",
    "    \"\"\"Construit un vocabulaire √† partir des textes.\"\"\"\n",
    "    word_freq = Counter()\n",
    "    for text in texts:\n",
    "        words = text.lower().split()\n",
    "        word_freq.update(words)\n",
    "    \n",
    "    # Filtrer par fr√©quence minimum\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word, freq in word_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def text_to_sequence(text, vocab, max_len=50):\n",
    "    \"\"\"Convertit un texte en s√©quence d'indices.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    sequence = [vocab.get(word, vocab['<UNK>']) for word in words]\n",
    "    \n",
    "    # Padding\n",
    "    if len(sequence) < max_len:\n",
    "        sequence += [vocab['<PAD>']] * (max_len - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_len]\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Construire le vocabulaire\n",
    "vocab = build_vocab(reviews)\n",
    "vocab_size = len(vocab)\n",
    "max_len = 50\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample words: {list(vocab.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir tous les textes en s√©quences\n",
    "sequences = [text_to_sequence(text, vocab, max_len) for text in reviews]\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Cr√©er les datasets\n",
    "train_dataset = SentimentDataset(X_train, y_train)\n",
    "test_dataset = SentimentDataset(X_test, y_test)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mod√®le LSTM Bidirectionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Sentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super(BiLSTM_Sentiment, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM bidirectionnel\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected (x2 pour bidirectionnel)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        # lstm_out: (batch_size, seq_len, hidden_dim*2)\n",
    "        # hidden: (num_layers*2, batch_size, hidden_dim)\n",
    "        \n",
    "        # Prendre les derniers √©tats cach√©s (forward et backward)\n",
    "        hidden_fwd = hidden[-2, :, :]  # (batch_size, hidden_dim)\n",
    "        hidden_bwd = hidden[-1, :, :]  # (batch_size, hidden_dim)\n",
    "        hidden_concat = torch.cat([hidden_fwd, hidden_bwd], dim=1)\n",
    "        \n",
    "        # Dropout et classification\n",
    "        hidden_dropout = self.dropout(hidden_concat)\n",
    "        output = self.fc(hidden_dropout)  # (batch_size, 1)\n",
    "        \n",
    "        return output.squeeze(1)  # (batch_size,)\n",
    "\n",
    "# Hyperparam√®tres\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "model = BiLSTM_Sentiment(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sequences, labels in loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # M√©triques\n",
    "        total_loss += loss.item()\n",
    "        predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration entra√Ænement\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 20\n",
    "\n",
    "# Historique\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'test_loss': [], 'test_acc': []\n",
    "}\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"Starting training...\\n\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['test_loss'], label='Test Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['test_acc'], label='Test Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Training and Test Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. √âvaluation Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation compl√®te\n",
    "test_loss, test_acc, predictions, true_labels = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(\"Final Test Results:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pr√©dictions sur Nouveaux Textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, vocab, device, max_len=50):\n",
    "    \"\"\"Pr√©dit le sentiment d'un texte.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Pr√©traitement\n",
    "    sequence = text_to_sequence(text, vocab, max_len)\n",
    "    sequence_tensor = torch.LongTensor([sequence]).to(device)\n",
    "    \n",
    "    # Pr√©diction\n",
    "    with torch.no_grad():\n",
    "        output = model(sequence_tensor)\n",
    "        probability = torch.sigmoid(output).item()\n",
    "    \n",
    "    sentiment = \"Positive\" if probability > 0.5 else \"Negative\"\n",
    "    return sentiment, probability\n",
    "\n",
    "# Tests\n",
    "test_texts = [\n",
    "    \"this movie is absolutely brilliant and amazing\",\n",
    "    \"terrible waste of time horrible acting\",\n",
    "    \"great film loved every minute of it\",\n",
    "    \"boring and disappointing not recommended\",\n",
    "    \"outstanding performances wonderful experience\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Predictions:\\n\")\n",
    "for text in test_texts:\n",
    "    sentiment, prob = predict_sentiment(text, model, vocab, device)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  Sentiment: {sentiment} (confidence: {prob:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Ce que nous avons appris:\n",
    "1. Pr√©traitement de texte: tokenization, vocabulaire, padding\n",
    "2. Architecture LSTM bidirectionnelle pour NLP\n",
    "3. Entra√Ænement et √©valuation d'un mod√®le de sentiment\n",
    "4. Utilisation d'embeddings et de couches r√©currentes\n",
    "\n",
    "### Pour aller plus loin:\n",
    "- Utiliser des embeddings pr√©-entra√Æn√©s (GloVe, Word2Vec)\n",
    "- Essayer des architectures plus complexes (attention, Transformers)\n",
    "- Fine-tuner des mod√®les pr√©-entra√Æn√©s (BERT, RoBERTa)\n",
    "- Travailler avec des datasets r√©els (IMDB complet, Twitter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}