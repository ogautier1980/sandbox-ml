{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/08_deep_learning_rnn/08_exercices.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '08_exercices.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 08 - Exercices : RNN, LSTM et Transformers\n",
    "\n",
    "Ce notebook contient des exercices pratiques sur les r√©seaux r√©currents et les Transformers.\n",
    "\n",
    "## Exercices\n",
    "1. Seq2Seq avec Attention pour traduction\n",
    "2. G√©n√©ration de texte avec LSTM\n",
    "3. Pr√©diction de s√©ries temporelles avec GRU\n",
    "4. Classification multi-classe avec BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: Seq2Seq avec Attention\n",
    "\n",
    "### Objectif\n",
    "Impl√©menter un mod√®le Sequence-to-Sequence avec m√©canisme d'attention pour traduire des nombres en mots.\n",
    "\n",
    "### Instructions\n",
    "1. Cr√©er un dataset de paires (nombre, mot): (1, \"one\"), (2, \"two\"), etc.\n",
    "2. Impl√©menter un Encoder LSTM\n",
    "3. Impl√©menter un Decoder LSTM avec Attention\n",
    "4. Entra√Æner le mod√®le\n",
    "5. Tester sur de nouveaux nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dataset nombre -> mot\n",
    "number_to_word = {\n",
    "    '1': 'one', '2': 'two', '3': 'three', '4': 'four', '5': 'five',\n",
    "    '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine', '0': 'zero'\n",
    "}\n",
    "\n",
    "# G√©n√©rer des exemples: \"123\" -> \"one two three\"\n",
    "def generate_samples(num_samples=1000, max_len=5):\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        length = np.random.randint(1, max_len + 1)\n",
    "        number_str = ''.join([str(np.random.randint(0, 10)) for _ in range(length)])\n",
    "        word_str = ' '.join([number_to_word[d] for d in number_str])\n",
    "        samples.append((number_str, word_str))\n",
    "    return samples\n",
    "\n",
    "samples = generate_samples(1000)\n",
    "print(\"Sample pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {samples[i][0]} -> {samples[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Vocabulaire\n",
    "# Cr√©er des vocabulaires pour input (nombres) et output (mots)\n",
    "# Indices sp√©ciaux: <PAD>=0, <SOS>=1, <EOS>=2\n",
    "\n",
    "input_vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2}\n",
    "for digit in '0123456789':\n",
    "    input_vocab[digit] = len(input_vocab)\n",
    "\n",
    "output_vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2}\n",
    "for word in number_to_word.values():\n",
    "    if word not in output_vocab:\n",
    "        output_vocab[word] = len(output_vocab)\n",
    "\n",
    "# Vocabulaires invers√©s\n",
    "inv_output_vocab = {v: k for k, v in output_vocab.items()}\n",
    "\n",
    "print(f\"Input vocab size: {len(input_vocab)}\")\n",
    "print(f\"Output vocab size: {len(output_vocab)}\")\n",
    "print(f\"Output vocab: {output_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encoder LSTM\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# TODO: Attention Layer\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (1, batch_size, hidden_dim)\n",
    "        # encoder_outputs: (batch_size, src_len, hidden_dim)\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # R√©p√©ter hidden pour chaque timestep\n",
    "        hidden = hidden.repeat(src_len, 1, 1)  # (src_len, batch_size, hidden_dim)\n",
    "        hidden = hidden.permute(1, 0, 2)  # (batch_size, src_len, hidden_dim)\n",
    "        \n",
    "        # Calcul des scores d'attention\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)  # (batch_size, src_len)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "# TODO: Decoder LSTM avec Attention\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        # x: (batch_size, 1)\n",
    "        embedded = self.embedding(x)  # (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # Attention\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)  # (batch_size, src_len)\n",
    "        attn_weights = attn_weights.unsqueeze(1)  # (batch_size, 1, src_len)\n",
    "        \n",
    "        # Context vector\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        # Combiner embedding et context\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)  # (batch_size, 1, emb+hidden)\n",
    "        \n",
    "        # LSTM\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        \n",
    "        # Pr√©diction\n",
    "        prediction = self.fc(output.squeeze(1))  # (batch_size, output_size)\n",
    "        \n",
    "        return prediction, hidden, cell, attn_weights\n",
    "\n",
    "print(\"Seq2Seq architecture defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Impl√©menter l'entra√Ænement et tester sur de nouveaux exemples\n",
    "# ASTUCE: Utiliser teacher forcing pendant l'entra√Ænement\n",
    "# ASTUCE: Pour l'inf√©rence, g√©n√©rer mot par mot jusqu'√† <EOS>\n",
    "\n",
    "print(\"\\n=== EXERCISE 1: Implement training and inference ===\")\n",
    "print(\"Hint: Use teacher forcing ratio of 0.5\")\n",
    "print(\"Hint: Train for ~50 epochs with batch_size=32\")\n",
    "print(\"Expected accuracy: >95% on test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: G√©n√©ration de Texte avec LSTM\n",
    "\n",
    "### Objectif\n",
    "Cr√©er un g√©n√©rateur de texte caract√®re par caract√®re.\n",
    "\n",
    "### Instructions\n",
    "1. Utiliser un texte simple (ex: Shakespeare, Lorem Ipsum)\n",
    "2. Cr√©er un vocabulaire de caract√®res\n",
    "3. G√©n√©rer des s√©quences (input: N caract√®res, target: N+1√®me caract√®re)\n",
    "4. Entra√Æner un LSTM\n",
    "5. G√©n√©rer du nouveau texte avec sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Texte d'exemple\n",
    "text = \"\"\"\n",
    "To be or not to be, that is the question.\n",
    "Whether tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them.\n",
    "\"\"\" * 20  # R√©p√©ter pour avoir plus de donn√©es\n",
    "\n",
    "# TODO: Cr√©er vocabulaire de caract√®res\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cr√©er s√©quences\n",
    "seq_length = 40\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(len(text) - seq_length):\n",
    "    sequences.append(text[i:i+seq_length])\n",
    "    next_chars.append(text[i+seq_length])\n",
    "\n",
    "print(f\"Number of sequences: {len(sequences)}\")\n",
    "print(f\"Example: '{sequences[0]}' -> '{next_chars[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Mod√®le LSTM pour g√©n√©ration de caract√®res\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        if hidden is None:\n",
    "            output, hidden = self.lstm(embedded)\n",
    "        else:\n",
    "            output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "# TODO: Fonction de g√©n√©ration\n",
    "def generate_text(model, start_str, length, temperature=1.0):\n",
    "    \"\"\"G√©n√®re du texte √† partir d'un seed.\n",
    "    \n",
    "    temperature: contr√¥le la cr√©ativit√© (0.5=conservateur, 1.5=cr√©atif)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    chars_generated = [ch for ch in start_str]\n",
    "    input_seq = torch.LongTensor([[char_to_idx[ch] for ch in start_str]]).to(device)\n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            output = output[0, -1, :] / temperature\n",
    "            probs = torch.softmax(output, dim=0).cpu().numpy()\n",
    "            char_idx = np.random.choice(len(probs), p=probs)\n",
    "            next_char = idx_to_char[char_idx]\n",
    "            chars_generated.append(next_char)\n",
    "            input_seq = torch.LongTensor([[char_idx]]).to(device)\n",
    "    \n",
    "    return ''.join(chars_generated)\n",
    "\n",
    "print(\"\\n=== EXERCISE 2: Implement training and generate text ===\")\n",
    "print(\"Hint: Train for ~100 epochs\")\n",
    "print(\"Hint: Try different temperatures (0.5, 1.0, 1.5)\")\n",
    "print(\"Expected: Generated text should resemble Shakespeare style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3: Pr√©diction de S√©ries Temporelles avec GRU\n",
    "\n",
    "### Objectif\n",
    "Pr√©dire les valeurs futures d'une s√©rie temporelle (ex: sinuso√Øde + bruit).\n",
    "\n",
    "### Instructions\n",
    "1. G√©n√©rer une s√©rie temporelle synth√©tique\n",
    "2. Cr√©er des s√©quences (fen√™tre glissante)\n",
    "3. Entra√Æner un GRU\n",
    "4. Pr√©dire les valeurs futures\n",
    "5. Visualiser les pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: G√©n√©rer s√©rie temporelle\n",
    "def generate_time_series(n_points=1000):\n",
    "    t = np.linspace(0, 100, n_points)\n",
    "    # Combinaison de sinuso√Ødes + tendance + bruit\n",
    "    series = (\n",
    "        10 * np.sin(0.1 * t) + \n",
    "        5 * np.sin(0.3 * t) + \n",
    "        0.05 * t +\n",
    "        np.random.randn(n_points) * 2\n",
    "    )\n",
    "    return series\n",
    "\n",
    "time_series = generate_time_series(1000)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(time_series[:200], linewidth=2)\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.ylabel('Value', fontsize=12)\n",
    "plt.title('Time Series Data (first 200 points)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cr√©er s√©quences avec fen√™tre glissante\n",
    "def create_sequences(data, seq_length, pred_length=1):\n",
    "    \"\"\"Cr√©er des paires (X, y) pour pr√©diction multi-step.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - pred_length + 1):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length:i+seq_length+pred_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 50\n",
    "pred_length = 10  # Pr√©dire 10 pas dans le futur\n",
    "\n",
    "# TODO: Normaliser les donn√©es\n",
    "# TODO: Cr√©er train/test split\n",
    "# TODO: Cr√©er DataLoader PyTorch\n",
    "\n",
    "print(\"\\n=== EXERCISE 3: Implement GRU for time series prediction ===\")\n",
    "print(f\"Hint: Use sequence length = {seq_length}\")\n",
    "print(f\"Hint: Predict {pred_length} steps ahead\")\n",
    "print(\"Expected: MSE < 5.0 on test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Mod√®le GRU\n",
    "class TimeSeriesGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(TimeSeriesGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        output, hidden = self.gru(x)\n",
    "        # Prendre le dernier output\n",
    "        prediction = self.fc(output[:, -1, :])\n",
    "        return prediction\n",
    "\n",
    "# TODO: Entra√Æner et visualiser les pr√©dictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4: Classification Multi-classe avec BERT\n",
    "\n",
    "### Objectif\n",
    "Utiliser BERT pour classifier des textes en plusieurs cat√©gories.\n",
    "\n",
    "### Instructions\n",
    "1. Cr√©er un dataset synth√©tique de 3 cat√©gories (tech, sport, politique)\n",
    "2. Fine-tuner BERT pour classification multi-classe\n",
    "3. √âvaluer avec accuracy et confusion matrix\n",
    "4. Tester sur de nouveaux textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dataset multi-classe\n",
    "tech_texts = [\n",
    "    \"new smartphone release powerful processor\",\n",
    "    \"artificial intelligence machine learning breakthrough\",\n",
    "    \"software update bug fixes performance improvements\",\n",
    "    \"cloud computing data center expansion\",\n",
    "    \"cybersecurity threat detection prevention\"\n",
    "] * 40\n",
    "\n",
    "sport_texts = [\n",
    "    \"football match championship final victory\",\n",
    "    \"basketball team playoff tournament win\",\n",
    "    \"tennis player grand slam title\",\n",
    "    \"olympic games gold medal record\",\n",
    "    \"soccer world cup qualifier goal\"\n",
    "] * 40\n",
    "\n",
    "politics_texts = [\n",
    "    \"election campaign presidential debate vote\",\n",
    "    \"government policy reform legislation passed\",\n",
    "    \"international summit diplomatic relations treaty\",\n",
    "    \"parliament session bill proposal discussion\",\n",
    "    \"political party coalition agreement negotiation\"\n",
    "] * 40\n",
    "\n",
    "# TODO: Combiner et cr√©er labels (0=tech, 1=sport, 2=politics)\n",
    "# TODO: Utiliser transformers library pour BERT\n",
    "# TODO: Fine-tuner et √©valuer\n",
    "\n",
    "print(\"\\n=== EXERCISE 4: Implement multi-class classification with BERT ===\")\n",
    "print(\"Hint: Use 'bert-base-uncased' model\")\n",
    "print(\"Hint: Set num_labels=3\")\n",
    "print(\"Expected accuracy: >90%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions (√† d√©commenter apr√®s avoir essay√©)\n",
    "\n",
    "Les solutions sont fournies ci-dessous. Essayez d'abord de r√©soudre les exercices par vous-m√™me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION EXERCICE 1: Disponible dans les notebooks de d√©monstration\n",
    "# SOLUTION EXERCICE 2: Voir 08_demo_lstm_sentiment.ipynb pour architecture similaire\n",
    "# SOLUTION EXERCICE 3: Adapter le mod√®le LSTM avec sortie multi-step\n",
    "# SOLUTION EXERCICE 4: Voir 08_demo_transformers_huggingface.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Projets Avanc√©s\n",
    "\n",
    "### Projet 1: Chatbot Simple\n",
    "- Cr√©er un dataset de paires question-r√©ponse\n",
    "- Entra√Æner un seq2seq avec attention\n",
    "- Ajouter beam search pour g√©n√©ration\n",
    "\n",
    "### Projet 2: R√©sum√© Automatique\n",
    "- Utiliser un dataset de textes longs + r√©sum√©s\n",
    "- Fine-tuner BART ou T5\n",
    "- √âvaluer avec ROUGE score\n",
    "\n",
    "### Projet 3: Named Entity Recognition\n",
    "- Dataset avec entit√©s annot√©es (PER, LOC, ORG)\n",
    "- Fine-tuner BERT pour NER\n",
    "- Visualiser les entit√©s d√©tect√©es\n",
    "\n",
    "### Projet 4: Analyse de Sentiments Multi-aspects\n",
    "- Analyser diff√©rents aspects d'un produit (qualit√©, prix, service)\n",
    "- Multi-task learning avec BERT\n",
    "- Visualiser les sentiments par aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources Compl√©mentaires\n",
    "\n",
    "### Papers Importants\n",
    "- Seq2Seq: \"Sequence to Sequence Learning with Neural Networks\" (Sutskever et al., 2014)\n",
    "- Attention: \"Neural Machine Translation by Jointly Learning to Align and Translate\" (Bahdanau et al., 2015)\n",
    "- Transformer: \"Attention is All You Need\" (Vaswani et al., 2017)\n",
    "- BERT: \"BERT: Pre-training of Deep Bidirectional Transformers\" (Devlin et al., 2018)\n",
    "\n",
    "### Datasets Publics\n",
    "- IMDB: Sentiment analysis (50k reviews)\n",
    "- WMT: Machine translation\n",
    "- SQuAD: Question answering\n",
    "- GLUE/SuperGLUE: NLP benchmarks\n",
    "\n",
    "### Biblioth√®ques\n",
    "- Hugging Face Transformers: https://huggingface.co/transformers/\n",
    "- PyTorch: https://pytorch.org/\n",
    "- spaCy: https://spacy.io/\n",
    "- NLTK: https://www.nltk.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}