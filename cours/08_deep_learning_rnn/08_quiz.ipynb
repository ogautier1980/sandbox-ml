{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz d'Auto-√âvaluation - Chapitre 08 : Deep Learning - RNN et Transformers\n",
    "\n",
    "**Instructions** :\n",
    "- Ce quiz contient 15 questions pour tester votre compr√©hension du chapitre\n",
    "- R√©pondez aux questions par vous-m√™me avant de regarder les r√©ponses\n",
    "- Les r√©ponses sont dans une cellule masqu√©e √† la fin\n",
    "- Comptez 1 point par bonne r√©ponse\n",
    "\n",
    "**Bar√®me** :\n",
    "- 13-15 : Excellent ! Vous ma√Ætrisez le chapitre üí™\n",
    "- 10-12 : Bien, relisez les sections o√π vous avez des lacunes\n",
    "- 7-9 : Moyen, relisez le chapitre attentivement\n",
    "- < 7 : Insuffisant, reprenez le chapitre depuis le d√©but\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Question 1 : Donn√©es S√©quentielles\n",
    "Parmi ces exemples, lequel N'est PAS une donn√©e s√©quentielle typique ?\n",
    "\n",
    "A) Phrase en langage naturel  \n",
    "B) Signal audio  \n",
    "C) Image RGB statique  \n",
    "D) Cours de bourse dans le temps  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : Limitations des MLP pour les S√©quences\n",
    "Quelle est la principale limitation des MLP pour traiter des s√©quences ?\n",
    "\n",
    "A) Ils sont trop lents  \n",
    "B) Ils n√©cessitent une taille d'entr√©e fixe et n'ont pas de m√©moire du contexte  \n",
    "C) Ils ne peuvent pas utiliser de Dropout  \n",
    "D) Ils n√©cessitent trop de donn√©es  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 : Architecture RNN\n",
    "Dans un RNN vanilla, quelle est la formule de l'√©tat cach√© $h_t$ ?\n",
    "\n",
    "A) $h_t = \\text{ReLU}(W_{hh} h_{t-1} + W_{xh} x_t + b)$  \n",
    "B) $h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b)$  \n",
    "C) $h_t = \\sigma(W_{hh} h_{t-1} + W_{xh} x_t + b)$  \n",
    "D) $h_t = W_{hh} h_{t-1} + W_{xh} x_t$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 : Vanishing Gradient\n",
    "Pourquoi le RNN vanilla souffre-t-il du vanishing gradient ?\n",
    "\n",
    "A) Parce qu'il utilise trop de Dropout  \n",
    "B) Parce que le gradient se propage √† travers tous les pas de temps et s'att√©nue exponentiellement  \n",
    "C) Parce qu'il a trop de param√®tres  \n",
    "D) Parce qu'il utilise la fonction tanh  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 : LSTM - Nombre de Portes\n",
    "Combien de portes (gates) un LSTM a-t-il ?\n",
    "\n",
    "A) 1 (gate unique)  \n",
    "B) 2 (update et reset)  \n",
    "C) 3 (forget, input, output)  \n",
    "D) 4 (forget, input, output, memory)  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : LSTM - R√¥le de la Forget Gate\n",
    "Quel est le r√¥le de la forget gate $f_t$ dans un LSTM ?\n",
    "\n",
    "A) D√©cider quelle nouvelle information stocker  \n",
    "B) D√©cider quelle information de $c_{t-1}$ oublier  \n",
    "C) D√©cider quelle partie de $c_t$ exposer dans $h_t$  \n",
    "D) Cr√©er la candidate cell $\\tilde{c}_t$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 : LSTM - Avantage Principal\n",
    "Comment le LSTM r√©sout-il le probl√®me du vanishing gradient ?\n",
    "\n",
    "A) En utilisant ReLU au lieu de tanh  \n",
    "B) En ayant une cellule m√©moire $c_t$ qui agit comme une \"autoroute\" pour le gradient  \n",
    "C) En r√©duisant le nombre de param√®tres  \n",
    "D) En utilisant Batch Normalization  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 : LSTM vs GRU\n",
    "Quelle affirmation est CORRECTE concernant la diff√©rence entre LSTM et GRU ?\n",
    "\n",
    "A) LSTM a 2 portes, GRU a 3 portes  \n",
    "B) LSTM a une cellule m√©moire s√©par√©e $c_t$, GRU n'a que $h_t$  \n",
    "C) GRU a plus de param√®tres que LSTM  \n",
    "D) LSTM est toujours plus rapide que GRU  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 : Bidirectional RNN\n",
    "Pourquoi utiliser un Bidirectional RNN pour des t√¢ches comme le NER (Named Entity Recognition) ?\n",
    "\n",
    "A) Pour acc√©l√©rer l'entra√Ænement  \n",
    "B) Pour avoir acc√®s au contexte pass√© ET futur  \n",
    "C) Pour r√©duire le nombre de param√®tres  \n",
    "D) Pour √©viter l'overfitting  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 : Attention - Motivation\n",
    "Quel probl√®me le m√©canisme d'attention r√©sout-il dans les mod√®les seq2seq ?\n",
    "\n",
    "A) Le nombre de param√®tres trop √©lev√©  \n",
    "B) Le goulot d'√©tranglement du vecteur contexte fixe $c$  \n",
    "C) Le temps d'entra√Ænement trop long  \n",
    "D) L'impossibilit√© de parall√©liser  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 : Scaled Dot-Product Attention\n",
    "Dans la formule $\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$, pourquoi divise-t-on par $\\sqrt{d_k}$ ?\n",
    "\n",
    "A) Pour acc√©l√©rer le calcul  \n",
    "B) Pour normaliser et √©viter que les valeurs soient trop grandes  \n",
    "C) Pour r√©duire le nombre de param√®tres  \n",
    "D) Pour augmenter la pr√©cision  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12 : Transformers - Avantages\n",
    "Quelle est la principale diff√©rence entre Transformer et RNN ?\n",
    "\n",
    "A) Le Transformer utilise uniquement des convolutions  \n",
    "B) Le Transformer est bas√© uniquement sur l'attention, sans r√©currence  \n",
    "C) Le Transformer n√©cessite moins de donn√©es  \n",
    "D) Le Transformer est plus simple √† impl√©menter  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13 : Positional Encoding\n",
    "Pourquoi le Transformer a-t-il besoin de positional encoding ?\n",
    "\n",
    "A) Pour r√©duire le nombre de param√®tres  \n",
    "B) Parce que l'attention seule ne contient pas d'information sur l'ordre des tokens  \n",
    "C) Pour acc√©l√©rer l'entra√Ænement  \n",
    "D) Pour √©viter l'overfitting  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14 : BERT vs GPT\n",
    "Quelle est la diff√©rence principale entre BERT et GPT ?\n",
    "\n",
    "A) BERT utilise l'architecture Encoder, GPT utilise l'architecture Decoder  \n",
    "B) BERT est plus ancien que GPT  \n",
    "C) BERT a moins de param√®tres que GPT  \n",
    "D) BERT ne peut pas √™tre fine-tun√©  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15 : Multi-Head Attention\n",
    "Quel est l'avantage d'utiliser plusieurs t√™tes d'attention (multi-head) ?\n",
    "\n",
    "A) R√©duire le nombre de param√®tres  \n",
    "B) Chaque t√™te peut se concentrer sur des aspects diff√©rents (syntaxe, s√©mantique, etc.)  \n",
    "C) Acc√©l√©rer l'entra√Ænement  \n",
    "D) √âviter l'overfitting  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Auto-Correction\n",
    "\n",
    "Avant de regarder les r√©ponses, comptez combien de r√©ponses vous avez donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrez vos r√©ponses ici (ex: ['D', 'B', 'A', ...])\n",
    "mes_reponses = []  # TODO: remplir avec vos r√©ponses\n",
    "\n",
    "# R√©ponses correctes (masqu√©es)\n",
    "reponses_correctes = ['C', 'B', 'B', 'B', 'C', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B']\n",
    "\n",
    "if len(mes_reponses) == 15:\n",
    "    score = sum([1 for i, r in enumerate(mes_reponses) if r.upper() == reponses_correctes[i]])\n",
    "    print(f\"Votre score : {score}/15\")\n",
    "    \n",
    "    if score >= 13:\n",
    "        print(\"\\nüéâ Excellent ! Vous ma√Ætrisez le chapitre !\")\n",
    "    elif score >= 10:\n",
    "        print(\"\\n‚úÖ Bien ! Relisez les sections o√π vous avez des lacunes.\")\n",
    "    elif score >= 7:\n",
    "        print(\"\\n‚ö†Ô∏è  Moyen. Relisez le chapitre attentivement.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Insuffisant. Reprenez le chapitre depuis le d√©but.\")\n",
    "    \n",
    "    # Afficher les erreurs\n",
    "    print(\"\\nD√©tail :\")\n",
    "    for i, (ma_rep, bonne_rep) in enumerate(zip(mes_reponses, reponses_correctes), 1):\n",
    "        if ma_rep.upper() == bonne_rep:\n",
    "            print(f\"Q{i}: ‚úì Correct\")\n",
    "        else:\n",
    "            print(f\"Q{i}: ‚úó Votre r√©ponse: {ma_rep}, Correcte: {bonne_rep}\")\n",
    "else:\n",
    "    print(\"Veuillez remplir toutes les r√©ponses (15 lettres A, B, C ou D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Explications des R√©ponses\n",
    "\n",
    "### Q1 : C\n",
    "Une **image RGB statique** n'est PAS une s√©quence (pas d'ordre temporel). Les autres (texte, audio, bourse) sont des donn√©es s√©quentielles o√π l'ordre est crucial.\n",
    "\n",
    "### Q2 : B\n",
    "Les MLP n√©cessitent une **taille d'entr√©e fixe** et n'ont **pas de m√©moire** du contexte pr√©c√©dent, ce qui les rend inadapt√©s aux s√©quences de longueur variable.\n",
    "\n",
    "### Q3 : B\n",
    "La formule du RNN vanilla est $h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b)$. La fonction **tanh** est utilis√©e pour l'activation.\n",
    "\n",
    "### Q4 : B\n",
    "Le gradient se propage √† travers tous les pas de temps : $\\frac{\\partial h_t}{\\partial h_0} = \\prod_{k=1}^{t} W_{hh}^T \\cdot \\text{diag}(\\tanh'(z_k))$. Si $\\|W_{hh}\\| < 1$, le gradient s'att√©nue exponentiellement (vanishing).\n",
    "\n",
    "### Q5 : C\n",
    "Un LSTM a **3 portes** : forget gate ($f_t$), input gate ($i_t$), output gate ($o_t$), plus une cellule m√©moire ($c_t$).\n",
    "\n",
    "### Q6 : B\n",
    "La **forget gate** $f_t$ d√©cide quelle information de la cellule m√©moire pr√©c√©dente $c_{t-1}$ doit √™tre oubli√©e ($f_t = 0$) ou conserv√©e ($f_t = 1$).\n",
    "\n",
    "### Q7 : B\n",
    "La **cellule m√©moire** $c_t$ agit comme une \"autoroute\" permettant au gradient de se propager sans att√©nuation (si $f_t \\approx 1$), r√©solvant le vanishing gradient.\n",
    "\n",
    "### Q8 : B\n",
    "**LSTM** a une cellule m√©moire s√©par√©e $c_t$ (+ 3 portes). **GRU** n'a que $h_t$ (+ 2 portes : update et reset). GRU a **moins** de param√®tres et est plus rapide.\n",
    "\n",
    "### Q9 : B\n",
    "Un **Bidirectional RNN** lit la s√©quence dans les deux sens (forward + backward), donnant acc√®s au **contexte pass√© ET futur**, crucial pour NER, POS tagging, etc.\n",
    "\n",
    "### Q10 : B\n",
    "En seq2seq classique, le d√©codeur doit tout comprendre √† partir d'un seul vecteur contexte $c$ (goulot d'√©tranglement). L'**attention** calcule un vecteur contexte dynamique $c_t$ √† chaque pas.\n",
    "\n",
    "### Q11 : B\n",
    "On divise par $\\sqrt{d_k}$ pour **normaliser** les scores et √©viter que $QK^T$ ait des valeurs trop grandes (qui satureraient le softmax).\n",
    "\n",
    "### Q12 : B\n",
    "Le **Transformer** est bas√© uniquement sur l'**attention** (self-attention + cross-attention), sans r√©currence, ce qui permet une parall√©lisation compl√®te.\n",
    "\n",
    "### Q13 : B\n",
    "Sans r√©currence, l'attention seule ne contient **pas d'information sur l'ordre** des tokens. Le **positional encoding** injecte cette information de position.\n",
    "\n",
    "### Q14 : A\n",
    "**BERT** utilise l'architecture **Encoder** (bidirectionnel, MLM pre-training). **GPT** utilise l'architecture **Decoder** (autor√©gressif, LM pre-training).\n",
    "\n",
    "### Q15 : B\n",
    "Le **multi-head attention** permet √† chaque t√™te de se concentrer sur des **aspects diff√©rents** (syntaxe, s√©mantique, relations √† longue distance, etc.), rendant le mod√®le plus expressif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaines √âtapes\n",
    "\n",
    "- **Score < 10** : Relisez le chapitre 08 attentivement, en particulier :\n",
    "  - Section 2-4 : RNN, LSTM, GRU (architectures et probl√®me du vanishing gradient)\n",
    "  - Section 6 : M√©canisme d'attention\n",
    "  - Section 7 : Transformers (architecture, positional encoding, multi-head attention)\n",
    "\n",
    "- **Score >= 10** : Passez au Chapitre 09 (Reinforcement Learning)\n",
    "\n",
    "- **Notebooks recommand√©s** :\n",
    "  - `08_demo_lstm_sentiment.ipynb` : Analyse de sentiment avec LSTM bidirectionnel\n",
    "  - `08_demo_transformers_huggingface.ipynb` : BERT fine-tuning et GPT-2 g√©n√©ration\n",
    "  - `08_demo_rag_llm.ipynb` : RAG (Retrieval-Augmented Generation) avec embeddings\n",
    "  - `08_exercices.ipynb` : Exercices pratiques (seq2seq, attention, g√©n√©ration)\n",
    "\n",
    "- **R√©vision** : Refaites le quiz dans 2-3 jours pour ancrer les connaissances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
