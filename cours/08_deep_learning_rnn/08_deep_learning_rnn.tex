\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}

% Math√©matiques
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

% Mise en page
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}

% Graphiques et couleurs
\usepackage{graphicx}
\usepackage{xcolor}

% ===== UNICODE CHARACTERS SUPPORT =====
\usepackage{newunicodechar}

% Emojis et symboles
\newunicodechar{‚úÖ}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚ùå}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚úì}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚úó}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚ö†}{\textcolor{orange!80!black}{\textbf{/!\textbackslash}}}
\newunicodechar{üí°}{\textcolor{blue!70!black}{\textbf{(i)}}}
\newunicodechar{üéØ}{\textcolor{purple!70!black}{\textbf{$\star$}}}
\newunicodechar{üìä}{\textcolor{blue!70!black}{\textbf{[=]}}}

% √âtoiles (pour tableaux)
\newunicodechar{‚òÖ}{\textcolor{orange!80!black}{$\star$}}
\newunicodechar{‚òÜ}{\textcolor{gray!50}{$\star$}}

% Fl√®ches
\newunicodechar{‚Üí}{$\rightarrow$}
\newunicodechar{‚Üê}{$\leftarrow$}
\newunicodechar{‚Üë}{$\uparrow$}
\newunicodechar{‚Üì}{$\downarrow$}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, calc, matrix}

% Tableaux
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}

% Code et algorithmes
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Hyperliens
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 08 - Deep Learning : RNN et Transformers},
    pdfauthor={Cours ML},
}

% Boxes color√©es
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}


% ===== TCOLORBOX AVEC EMOJIS =====
\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö† Attention,
    breakable
}

\newtcolorbox{definition}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition,
    breakable
}

\newtcolorbox{astuce}{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

\newtcolorbox{remarque}{
    colback=yellow!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=üí° Remarque,
    breakable
}

\newtcolorbox{important}{
    colback=purple!5!white,
    colframe=purple!75!black,
    fonttitle=\bfseries,
    title=‚ö† Important,
    breakable
}

\newtcolorbox{exemple}{
    colback=gray!5!white,
    colframe=gray!75!black,
    fonttitle=\bfseries,
    title=Exemple,
    breakable
}

% En-t√™tes et pieds de page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 08 - Deep Learning : RNN et Transformers}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIGURATION LISTINGS =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}
\lstset{style=pythonstyle}

% ===== CONFIGURATION TCOLORBOX =====


\newtcolorbox{theoreme}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Th√©or√®me: #1,
    breakable
}







% ===== COMMANDES PERSONNALIS√âES =====
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}
% \tanh est d√©j√† d√©fini dans amsmath
\newcommand{\sigmoid}{\operatorname{sigmoid}}
\newcommand{\softmax}{\operatorname{softmax}}

\begin{document}

% ===== PAGE DE TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]

    \vspace{1cm}

    {\LARGE Chapitre 08}\\[0.3cm]
    {\LARGE\bfseries Deep Learning : RNN et Transformers}\\[2cm]

    \vfill

    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Comprendre les RNN (Recurrent Neural Networks) pour les s√©quences
        \item Ma√Ætriser LSTM et GRU pour r√©soudre les probl√®mes de vanishing gradient
        \item D√©couvrir le m√©canisme d'attention et son importance
        \item √âtudier l'architecture Transformer et son fonctionnement
        \item Appliquer ces mod√®les au NLP et aux s√©ries temporelles
    \end{itemize}
    }

    \vfill

    {\large
    \textbf{Pr√©requis :} Chapitre 06 (R√©seaux de Neurones Fondamentaux)\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 8-10 heures\\[0.3cm]
    \textbf{Notebooks :} \texttt{08\_demo\_*.ipynb}
    }

    \vfill

    {\large Cours ML - Sandbox-ML\\
    Version 1.0 - 2026}
\end{titlepage}

\tableofcontents
\newpage

% ===== SECTION 1: INTRODUCTION =====
\section{Introduction aux Donn√©es S√©quentielles}

\subsection{Qu'est-ce qu'une s√©quence ?}

Une \textbf{s√©quence} est une suite ordonn√©e d'√©l√©ments o√π l'ordre a une importance cruciale.

\begin{exemple}{Exemples de s√©quences}
\begin{itemize}
    \item \textbf{Texte} : "Le chat mange la souris" (ordre des mots ‚Üí sens)
    \item \textbf{Audio} : Signal sonore √©chantillonn√© dans le temps
    \item \textbf{Vid√©o} : S√©quence d'images (frames)
    \item \textbf{S√©ries temporelles} : Cours de bourse, temp√©rature, trafic web
    \item \textbf{ADN} : S√©quence de nucl√©otides (A, C, G, T)
\end{itemize}
\end{exemple}

\subsection{Limitations des MLP et CNN pour les s√©quences}

\textbf{MLP :}
\begin{itemize}
    \item ‚ùå Taille d'entr√©e fixe (impossible pour s√©quences de longueur variable)
    \item ‚ùå Pas de m√©moire du contexte pr√©c√©dent
    \item ‚ùå Nombre de param√®tres explose avec la longueur
\end{itemize}

\textbf{CNN :}
\begin{itemize}
    \item ‚úì Peut traiter des s√©quences avec Conv1D
    \item ‚ùå Champ r√©cepteur limit√© (contexte local uniquement)
    \item ‚ùå Pas de m√©moire √† long terme
\end{itemize}

\subsection{Types de t√¢ches s√©quentielles}

\begin{table}[h]
\centering
\caption{Architectures s√©quence-to-X}
\label{tab:seq_tasks}
\begin{tabular}{lll}
\toprule
\textbf{Type} & \textbf{Description} & \textbf{Exemple} \\
\midrule
One-to-One & Entr√©e fixe ‚Üí Sortie fixe & Classification d'image (CNN) \\
One-to-Many & Entr√©e fixe ‚Üí S√©quence & Image captioning \\
Many-to-One & S√©quence ‚Üí Sortie fixe & Sentiment analysis \\
Many-to-Many & S√©quence ‚Üí S√©quence & Traduction, g√©n√©ration texte \\
 & (m√™me longueur) & √âtiquetage de s√©quences (NER) \\
 & (longueur diff√©rente) & Traduction automatique \\
\bottomrule
\end{tabular}
\end{table}

% ===== SECTION 2: RNN =====
\section{Recurrent Neural Networks (RNN)}

\subsection{Architecture}

\begin{definition}{RNN}
Un RNN traite une s√©quence $(\vect{x}_1, \vect{x}_2, \ldots, \vect{x}_T)$ en maintenant un \textbf{√©tat cach√©} $\vect{h}_t$ qui se propage dans le temps :
\begin{align}
    \vect{h}_t &= \tanh(\mat{W}_{hh} \vect{h}_{t-1} + \mat{W}_{xh} \vect{x}_t + \vect{b}_h) \\
    \vect{y}_t &= \mat{W}_{hy} \vect{h}_t + \vect{b}_y
\end{align}
o√π :
\begin{itemize}
    \item $\vect{h}_t \in \R^h$ : √©tat cach√© au temps $t$
    \item $\vect{x}_t \in \R^d$ : entr√©e au temps $t$
    \item $\vect{y}_t \in \R^k$ : sortie au temps $t$
    \item $\mat{W}_{hh} \in \R^{h \times h}$ : poids r√©currents
    \item $\mat{W}_{xh} \in \R^{h \times d}$ : poids d'entr√©e
    \item $\mat{W}_{hy} \in \R^{k \times h}$ : poids de sortie
\end{itemize}
\end{definition}

\textbf{Caract√©ristique cl√© :} Les m√™mes poids $\mat{W}_{hh}, \mat{W}_{xh}, \mat{W}_{hy}$ sont partag√©s √† chaque pas de temps ‚Üí capacit√© √† traiter des s√©quences de longueur variable.

\subsection{D√©roulement dans le temps (Unfolding)}

On peut "d√©rouler" le RNN dans le temps :
\begin{align*}
\vect{h}_1 &= \tanh(\mat{W}_{hh} \vect{h}_0 + \mat{W}_{xh} \vect{x}_1 + \vect{b}_h) \\
\vect{h}_2 &= \tanh(\mat{W}_{hh} \vect{h}_1 + \mat{W}_{xh} \vect{x}_2 + \vect{b}_h) \\
\vect{h}_3 &= \tanh(\mat{W}_{hh} \vect{h}_2 + \mat{W}_{xh} \vect{x}_3 + \vect{b}_h) \\
&\vdots
\end{align*}

Initialisation : $\vect{h}_0 = \vect{0}$ (ou appris)

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.85, every node/.style={font=\small}]
    % Time step 0 (initial state)
    \node[draw, circle, minimum size=1.2cm, fill=gray!20] (h0) at (0,0) {$\vect{h}_0$};
    \node[below, font=\tiny] at (h0.south) {√âtat initial};

    % Time step 1
    \node[draw, rectangle, minimum width=1.5cm, minimum height=1.2cm, fill=blue!20] (rnn1) at (3,0) {RNN};
    \node[draw, circle, minimum size=1cm, fill=green!20] (x1) at (3,-2.5) {$\vect{x}_1$};
    \node[draw, circle, minimum size=1cm, fill=red!20] (y1) at (3,2.5) {$\vect{y}_1$};
    \node[draw, circle, minimum size=1.2cm, fill=gray!20] (h1) at (5.5,0) {$\vect{h}_1$};

    \draw[->, thick] (h0) -- (rnn1) node[midway, above, font=\tiny] {};
    \draw[->, thick] (x1) -- (rnn1);
    \draw[->, thick] (rnn1) -- (y1);
    \draw[->, thick] (rnn1) -- (h1);

    \node[above, font=\footnotesize] at (3, 3.2) {$t=1$};

    % Time step 2
    \node[draw, rectangle, minimum width=1.5cm, minimum height=1.2cm, fill=blue!20] (rnn2) at (8,0) {RNN};
    \node[draw, circle, minimum size=1cm, fill=green!20] (x2) at (8,-2.5) {$\vect{x}_2$};
    \node[draw, circle, minimum size=1cm, fill=red!20] (y2) at (8,2.5) {$\vect{y}_2$};
    \node[draw, circle, minimum size=1.2cm, fill=gray!20] (h2) at (10.5,0) {$\vect{h}_2$};

    \draw[->, thick] (h1) -- (rnn2);
    \draw[->, thick] (x2) -- (rnn2);
    \draw[->, thick] (rnn2) -- (y2);
    \draw[->, thick] (rnn2) -- (h2);

    \node[above, font=\footnotesize] at (8, 3.2) {$t=2$};

    % Time step 3
    \node[draw, rectangle, minimum width=1.5cm, minimum height=1.2cm, fill=blue!20] (rnn3) at (13,0) {RNN};
    \node[draw, circle, minimum size=1cm, fill=green!20] (x3) at (13,-2.5) {$\vect{x}_3$};
    \node[draw, circle, minimum size=1cm, fill=red!20] (y3) at (13,2.5) {$\vect{y}_3$};
    \node[draw, circle, minimum size=1.2cm, fill=gray!20] (h3) at (15.5,0) {$\vect{h}_3$};

    \draw[->, thick] (h2) -- (rnn3);
    \draw[->, thick] (x3) -- (rnn3);
    \draw[->, thick] (rnn3) -- (y3);
    \draw[->, thick] (rnn3) -- (h3);

    \node[above, font=\footnotesize] at (13, 3.2) {$t=3$};

    % Continuation
    \node at (17, 0) {$\cdots$};

    % Weight sharing annotation
    \draw[<->, very thick, purple, dashed] (rnn1.south) ++ (0, -1.2) -- ++ (10, 0);
    \node[below, font=\tiny, purple, align=center] at (8, -3.7) {M√™mes poids $\mat{W}_{hh}, \mat{W}_{xh}, \mat{W}_{hy}$ partag√©s};

    % Equations
    \node[draw, rectangle, fill=yellow!10, font=\tiny, align=left] at (8, -5.5) {
        $\vect{h}_t = \tanh(\mat{W}_{hh} \vect{h}_{t-1} + \mat{W}_{xh} \vect{x}_t + \vect{b}_h)$\\
        $\vect{y}_t = \mat{W}_{hy} \vect{h}_t + \vect{b}_y$
    };

    % Legend
    \node[draw, rectangle, fill=gray!10, minimum width=3cm, minimum height=2.5cm, font=\tiny, align=left] at (17.5, 1.5) {
        \textbf{L√©gende:}\\[0.1cm]
        ‚Ä¢ \textcolor{green!60!black}{Vert}: Entr√©e\\
        ‚Ä¢ \textcolor{blue!60!black}{Bleu}: RNN cell\\
        ‚Ä¢ \textcolor{red!60!black}{Rouge}: Sortie\\
        ‚Ä¢ \textcolor{gray}{Gris}: √âtat cach√©\\[0.1cm]
        \textbf{R√©currence:}\\
        $\vect{h}_t$ d√©pend de\\
        $\vect{h}_{t-1}$
    };

\end{tikzpicture}
\caption{RNN d√©roul√© dans le temps (Unfolding). Chaque cellule RNN (en bleu) partage les m√™mes poids $\mat{W}_{hh}, \mat{W}_{xh}, \mat{W}_{hy}$. √Ä chaque pas de temps $t$: (1) l'entr√©e $\vect{x}_t$ et l'√©tat cach√© pr√©c√©dent $\vect{h}_{t-1}$ sont combin√©s, (2) l'√©tat cach√© $\vect{h}_t$ est calcul√© avec $\tanh$, (3) la sortie $\vect{y}_t$ est produite, (4) $\vect{h}_t$ est propag√© au pas suivant. Cette r√©currence permet au RNN de capturer les d√©pendances temporelles dans les s√©quences.}
\label{fig:rnn_unfolded}
\end{figure}

\clearpage

\subsection{Backpropagation Through Time (BPTT)}

Pour entra√Æner un RNN, on utilise \textbf{BPTT} : backpropagation appliqu√©e au r√©seau d√©roul√©.

\textbf{Forward pass} : Calculer $\vect{h}_1, \ldots, \vect{h}_T$ et $\vect{y}_1, \ldots, \vect{y}_T$

\textbf{Backward pass} : Calculer gradients en remontant dans le temps :
\begin{equation}
    \frac{\partial L}{\partial \vect{h}_t} = \frac{\partial L}{\partial \vect{y}_t} \frac{\partial \vect{y}_t}{\partial \vect{h}_t} + \frac{\partial L}{\partial \vect{h}_{t+1}} \frac{\partial \vect{h}_{t+1}}{\partial \vect{h}_t}
\end{equation}

\subsection{Probl√®me du Vanishing/Exploding Gradient}

\begin{attention}
Le gradient se propage √† travers tous les pas de temps :
\begin{equation}
    \frac{\partial \vect{h}_t}{\partial \vect{h}_0} = \prod_{k=1}^{t} \frac{\partial \vect{h}_k}{\partial \vect{h}_{k-1}} = \prod_{k=1}^{t} \mat{W}_{hh}^T \cdot \text{diag}(\tanh'(\vect{z}_k))
\end{equation}

\textbf{Vanishing gradient} : Si $\|\mat{W}_{hh}\| < 1$, le gradient $\to 0$ exponentiellement.
\begin{itemize}
    \item Le RNN ne peut pas apprendre de d√©pendances √† long terme
\end{itemize}

\textbf{Exploding gradient} : Si $\|\mat{W}_{hh}\| > 1$, le gradient $\to \infty$.
\begin{itemize}
    \item Solution : \textbf{gradient clipping} (limiter la norme du gradient)
\end{itemize}
\end{attention}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0, every node/.style={font=\small}]
    % Timeline axis
    \draw[->, thick] (0,0) -- (13,0) node[right, font=\footnotesize] {Temps $t$};
    \foreach \x in {1,2,...,12} {
        \draw (\x, -0.1) -- (\x, 0.1);
    }
    \node[below] at (1, -0.1) {$t_0$};
    \node[below] at (12, -0.1) {$t_{T}$};

    % Vanishing gradient (exponential decay)
    \begin{scope}[yshift=3cm]
        \draw[->, thick] (0,0) -- (0,3.5) node[above, font=\footnotesize] {$\|\nabla\|$};
        \draw[red!70!black, very thick, smooth, samples=50, domain=1:12]
            plot (\x, {3*exp(-0.35*(\x-1))});

        % Annotations
        \node[red!70!black, font=\footnotesize, align=center] at (6.5, 3.0) {
            \textbf{Vanishing Gradient}\\
            $\|\mat{W}_{hh}\| < 1$
        };
        \draw[->, red!70!black, dashed, thick] (6.5, 2.6) -- (8, 0.8);

        % Gradient values annotations
        \node[red!70!black, font=\scriptsize] at (1, 3.2) {$\nabla_0$};
        \node[red!70!black, font=\scriptsize] at (4, 1.8) {$\nabla_0 \cdot 0.3$};
        \node[red!70!black, font=\scriptsize] at (8, 0.5) {$\nabla_0 \cdot 0.05$};
        \node[red!70!black, font=\scriptsize] at (12, 0.15) {$\approx 0$};

        % Danger zone
        \fill[red!10, opacity=0.5] (8,0) rectangle (13, 1.0);
        \node[red!70!black, font=\scriptsize, align=center] at (10.5, 0.5) {
            Perte de\\m√©moire
        };
    \end{scope}

    % Exploding gradient (exponential growth)
    \begin{scope}[yshift=-3cm]
        \draw[->, thick] (0,0) -- (0,3.5) node[above, font=\footnotesize] {$\|\nabla\|$};
        \draw[blue!70!black, very thick, smooth, samples=50, domain=1:10]
            plot (\x, {0.3*exp(0.35*(\x-1))});
        \draw[blue!70!black, very thick, dashed] (10, 3.0) -- (12, 3.0);

        % Annotations
        \node[blue!70!black, font=\footnotesize, align=center] at (6.5, -0.5) {
            \textbf{Exploding Gradient}\\
            $\|\mat{W}_{hh}\| > 1$
        };
        \draw[->, blue!70!black, dashed, thick] (6.5, -0.1) -- (7, 1.5);

        % Gradient values annotations
        \node[blue!70!black, font=\scriptsize] at (1, 0.1) {$\nabla_0$};
        \node[blue!70!black, font=\scriptsize] at (4, 0.8) {$\nabla_0 \cdot 3$};
        \node[blue!70!black, font=\scriptsize] at (7, 2.0) {$\nabla_0 \cdot 15$};
        \node[blue!70!black, font=\scriptsize] at (10, 3.2) {$\to \infty$};

        % Clipping solution
        \fill[green!10, opacity=0.5] (10,0) rectangle (13, 3.0);
        \node[green!50!black, font=\scriptsize, align=center] at (11.5, 1.5) {
            \textbf{Gradient}\\
            \textbf{Clipping}\\
            $\|\nabla\| \leq \tau$
        };
        \draw[green!50!black, very thick] (10, 2.5) -- (13, 2.5);
        \node[green!50!black, font=\scriptsize, right] at (13, 2.5) {$\tau$};
    \end{scope}

    % Stable gradient (LSTM) for comparison
    \begin{scope}[yshift=0cm, xshift=14.5cm]
        \draw[->, thick] (0,0) -- (0,3.5) node[above, font=\footnotesize] {$\|\nabla\|$};
        \draw[->, thick] (0,0) -- (2.5,0) node[right, font=\scriptsize] {$t$};
        \draw[green!60!black, very thick, domain=0:2, samples=30]
            plot (\x, {1.8 + 0.3*sin(\x*180*2) + 0.1*rand});

        \node[green!60!black, font=\footnotesize, align=center] at (1.2, -0.7) {
            \textbf{LSTM}\\
            (stable)
        };
        \draw[green!60!black, dashed] (0, 1.8) -- (2.5, 1.8);
    \end{scope}
\end{tikzpicture}
\caption{Probl√®me du vanishing/exploding gradient dans les RNN. \textbf{(Haut)} Avec $\|\mat{W}_{hh}\| < 1$, le gradient d√©cro√Æt exponentiellement √† travers le temps, emp√™chant l'apprentissage de d√©pendances √† long terme. \textbf{(Bas)} Avec $\|\mat{W}_{hh}\| > 1$, le gradient explose, rendant l'entra√Ænement instable ; le gradient clipping limite la norme maximale. \textbf{(Droite)} Les LSTM maintiennent un gradient stable gr√¢ce aux connexions de m√©moire.}
\label{fig:rnn_gradient_problem}
\end{figure}

\subsection{Impl√©mentation simple}

\begin{lstlisting}[language=Python, caption=RNN vanilla from scratch]
import numpy as np

class SimpleRNN:
    def __init__(self, input_dim, hidden_dim, output_dim):
        self.hidden_dim = hidden_dim

        # Initialisation Xavier
        self.Wxh = np.random.randn(hidden_dim, input_dim) * 0.01
        self.Whh = np.random.randn(hidden_dim, hidden_dim) * 0.01
        self.Why = np.random.randn(output_dim, hidden_dim) * 0.01
        self.bh = np.zeros((hidden_dim, 1))
        self.by = np.zeros((output_dim, 1))

    def forward(self, inputs):
        """
        inputs: liste de vecteurs (seq_len, input_dim)
        """
        h = np.zeros((self.hidden_dim, 1))  # h0
        self.last_inputs = inputs
        self.last_hs = {0: h}

        # Forward pass
        for t, x in enumerate(inputs):
            x = x.reshape(-1, 1)
            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)
            self.last_hs[t + 1] = h

        # Output au dernier pas de temps (many-to-one)
        y = self.Why @ h + self.by
        return y, h

    def backward(self, dy, learning_rate=0.001):
        """
        BPTT simplifi√© (output au dernier pas uniquement)
        """
        n = len(self.last_inputs)

        # Gradients accumul√©s
        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), \
                           np.zeros_like(self.Whh), \
                           np.zeros_like(self.Why)
        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)

        # Gradient output
        dWhy += dy @ self.last_hs[n].T
        dby += dy

        # Backprop through time
        dh = self.Why.T @ dy

        for t in reversed(range(n)):
            temp = (1 - self.last_hs[t + 1] ** 2) * dh  # tanh'
            dbh += temp
            dWxh += temp @ self.last_inputs[t].reshape(1, -1)
            dWhh += temp @ self.last_hs[t].T
            dh = self.Whh.T @ temp

        # Gradient clipping
        for grad in [dWxh, dWhh, dWhy, dbh, dby]:
            np.clip(grad, -1, 1, out=grad)

        # Update
        self.Wxh -= learning_rate * dWxh
        self.Whh -= learning_rate * dWhh
        self.Why -= learning_rate * dWhy
        self.bh -= learning_rate * dbh
        self.by -= learning_rate * dby
\end{lstlisting}

% ===== SECTION 3: LSTM =====
\section{Long Short-Term Memory (LSTM)}

\subsection{Motivation}

LSTM r√©sout le probl√®me du vanishing gradient en introduisant une \textbf{cellule m√©moire} $\vect{c}_t$ et des \textbf{portes (gates)} qui contr√¥lent le flux d'information.

\subsection{Architecture}

\begin{definition}{LSTM}
Un LSTM a 3 portes et 1 cellule m√©moire :
\begin{align}
    \vect{f}_t &= \sigmoid(\mat{W}_f [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_f) \quad \text{(forget gate)} \\
    \vect{i}_t &= \sigmoid(\mat{W}_i [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_i) \quad \text{(input gate)} \\
    \tilde{\vect{c}}_t &= \tanh(\mat{W}_c [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_c) \quad \text{(candidate cell)} \\
    \vect{c}_t &= \vect{f}_t \odot \vect{c}_{t-1} + \vect{i}_t \odot \tilde{\vect{c}}_t \quad \text{(cell state)} \\
    \vect{o}_t &= \sigmoid(\mat{W}_o [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_o) \quad \text{(output gate)} \\
    \vect{h}_t &= \vect{o}_t \odot \tanh(\vect{c}_t) \quad \text{(hidden state)}
\end{align}
o√π $\odot$ est le produit √©l√©ment par √©l√©ment (Hadamard).
\end{definition}

\subsection{Interpr√©tation des portes}

\begin{itemize}
    \item \textbf{Forget gate} ($\vect{f}_t$) : D√©cide quelle information de $\vect{c}_{t-1}$ oublier
    \begin{itemize}
        \item $f_t = 0$ : oublier compl√®tement
        \item $f_t = 1$ : conserver compl√®tement
    \end{itemize}

    \item \textbf{Input gate} ($\vect{i}_t$) : D√©cide quelle nouvelle information stocker dans $\vect{c}_t$
    \begin{itemize}
        \item $i_t = 0$ : ignorer la nouvelle information
        \item $i_t = 1$ : stocker compl√®tement
    \end{itemize}

    \item \textbf{Output gate} ($\vect{o}_t$) : D√©cide quelle partie de $\vect{c}_t$ exposer dans $\vect{h}_t$
\end{itemize}

\textbf{Flux d'information :}
\begin{equation}
    \vect{c}_t = \underbrace{\vect{f}_t \odot \vect{c}_{t-1}}_{\text{m√©moire pass√©e}} + \underbrace{\vect{i}_t \odot \tilde{\vect{c}}_t}_{\text{nouvelle info}}
\end{equation}

\begin{astuce}
La cellule $\vect{c}_t$ agit comme une "autoroute" permettant au gradient de se propager sans att√©nuation (si $\vect{f}_t \approx 1$). Cela r√©sout le vanishing gradient !
\end{astuce}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    scale=0.9,
    gate/.style={rectangle, draw, minimum width=1.2cm, minimum height=1cm, fill=orange!30, font=\footnotesize},
    tanh/.style={rectangle, draw, minimum width=1.2cm, minimum height=1cm, fill=green!30, font=\footnotesize},
    multiply/.style={circle, draw, minimum size=0.7cm, fill=red!20, font=\small},
    add/.style={circle, draw, minimum size=0.7cm, fill=blue!20, font=\small}
]

    % Input and previous hidden state
    \node[font=\small] (xt) at (0, 0) {$\vect{x}_t$};
    \node[font=\small] (ht_1) at (0, 4) {$\vect{h}_{t-1}$};

    % Concatenation point
    \node[draw, circle, minimum size=0.5cm, fill=gray!20] (concat) at (1.5, 2) {};
    \draw[->, thick] (xt) -| (concat);
    \draw[->, thick] (ht_1) -| (concat);
    \node[right, font=\tiny] at (concat.east) {$[\vect{h}_{t-1}, \vect{x}_t]$};

    % Forget gate
    \node[gate] (forget) at (3.5, 4.5) {$\sigma$};
    \node[above, font=\tiny] at (forget.north) {Forget Gate};
    \draw[->, thick] (concat) |- (forget);

    % Input gate
    \node[gate] (input) at (3.5, 2.5) {$\sigma$};
    \node[above, font=\tiny] at (input.north) {Input Gate};
    \draw[->, thick] (concat) -- (input);

    % Candidate cell
    \node[tanh] (candidate) at (3.5, 0.5) {$\tanh$};
    \node[above, font=\tiny] at (candidate.north) {Candidate $\tilde{\vect{c}}_t$};
    \draw[->, thick] (concat) |- (candidate);

    % Cell state path (horizontal)
    \node[font=\small] (ct_1) at (1.5, 6) {$\vect{c}_{t-1}$};
    \draw[->, very thick, blue!70!black] (ct_1) -- (5, 6);

    % Forget multiplication
    \node[multiply] (forget_mult) at (5, 6) {$\times$};
    \draw[->, thick] (forget) -| (forget_mult);

    % Input multiplication
    \node[multiply] (input_mult) at (5, 1.5) {$\times$};
    \draw[->, thick] (input) |- (input_mult);
    \draw[->, thick] (candidate) -| (input_mult);

    % Addition (cell state update)
    \node[add] (add) at (7, 6) {$+$};
    \draw[->, thick] (forget_mult) -- (add);
    \draw[->, thick] (input_mult) -| (add);

    % New cell state
    \node[font=\small] (ct) at (9, 6) {$\vect{c}_t$};
    \draw[->, very thick, blue!70!black] (add) -- (ct);
    \draw[->, thick] (ct) -- (11, 6);

    % Output gate
    \node[gate] (output_gate) at (9, 3) {$\sigma$};
    \node[above, font=\tiny] at (output_gate.north) {Output Gate};
    \draw[->, thick] (concat) -| (output_gate);

    % Tanh of cell state
    \node[tanh] (ct_tanh) at (9, 4.5) {$\tanh$};
    \draw[->, thick] (add) |- (ct_tanh);

    % Output multiplication
    \node[multiply] (output_mult) at (10.5, 3.8) {$\times$};
    \draw[->, thick] (output_gate) -| (output_mult);
    \draw[->, thick] (ct_tanh) -| (output_mult);

    % Hidden state output
    \node[font=\small] (ht) at (12.5, 3.8) {$\vect{h}_t$};
    \draw[->, very thick, red!70!black] (output_mult) -- (ht);
    \draw[->, thick] (ht) -- (12.5, 0);
    \node[below, font=\small] at (12.5, 0) {Output};

    % Loop back hidden state
    \draw[->, thick, dashed] (ht) .. controls (13, 5) and (13, 7) .. (11, 7)
        node[midway, above, font=\tiny] {to $\vect{h}_{t+1}$};

    % Gate labels with equations
    \node[below, font=\tiny, align=center] at (forget.south) {$\vect{f}_t$};
    \node[below, font=\tiny, align=center] at (input.south) {$\vect{i}_t$};
    \node[below, font=\tiny, align=center] at (output_gate.south) {$\vect{o}_t$};

    % Legend
    \node[draw, rectangle, minimum width=3.5cm, minimum height=2.5cm, dashed] (legend) at (14, 5.5) {};
    \node[above left, font=\footnotesize\bfseries] at (legend.north west) {L√©gende:};

    \node[gate, minimum width=0.8cm, minimum height=0.6cm] at (13.2, 5) {};
    \node[right, font=\tiny] at (13.6, 5) {Sigmoid $\sigma$};

    \node[tanh, minimum width=0.8cm, minimum height=0.6cm] at (13.2, 4.3) {};
    \node[right, font=\tiny] at (13.6, 4.3) {Tanh};

    \node[multiply, minimum size=0.5cm] at (12.9, 3.7) {$\times$};
    \node[right, font=\tiny] at (13.3, 3.7) {Mult. √©l√©ment};

    \node[add, minimum size=0.5cm] at (12.9, 3.2) {$+$};
    \node[right, font=\tiny] at (13.3, 3.2) {Addition};

    % Annotations
    \node[font=\tiny, align=center, fill=yellow!20, draw, dashed] at (7, 7.5) {Cellule m√©moire $\vect{c}_t$\\(autoroute du gradient)};
    \draw[->, dashed] (7, 7.3) -- (7, 6.3);

    \node[font=\tiny, align=center, fill=red!10, draw, dashed] at (5.5, -0.5) {3 portes contr√¥lent\\le flux d'information};

\end{tikzpicture}
\caption{Architecture LSTM (Long Short-Term Memory). \textbf{Portes:} (1) \textbf{Forget gate} $\vect{f}_t$: d√©cide quelle information oublier de $\vect{c}_{t-1}$; (2) \textbf{Input gate} $\vect{i}_t$: d√©cide quelle nouvelle information ajouter via $\tilde{\vect{c}}_t$; (3) \textbf{Output gate} $\vect{o}_t$: d√©cide quelle partie de $\vect{c}_t$ exposer dans $\vect{h}_t$. \textbf{Cellule m√©moire} $\vect{c}_t$ (en bleu): "autoroute" permettant au gradient de se propager sans att√©nuation, r√©solvant le vanishing gradient. Le flux $\vect{c}_t = \vect{f}_t \odot \vect{c}_{t-1} + \vect{i}_t \odot \tilde{\vect{c}}_t$ permet de conserver/oublier s√©lectivement les informations sur de longues s√©quences.}
\label{fig:lstm_architecture}
\end{figure}

\clearpage

\subsection{Nombre de param√®tres}

Pour un LSTM avec $h$ unit√©s cach√©es et $d$ inputs :
\begin{equation}
    \text{Params} = 4 \times (h \times (h + d) + h) = 4h(h + d + 1)
\end{equation}

Le facteur 4 vient des 4 matrices de poids (forget, input, candidate, output).

\begin{exemple}{LSTM(128) avec input(100)}
\begin{equation*}
    \text{Params} = 4 \times 128 \times (128 + 100 + 1) = 117{,}248
\end{equation*}
\end{exemple}

\subsection{Impl√©mentation PyTorch}

\begin{lstlisting}[language=Python, caption=LSTM avec PyTorch]
import torch
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):
        super(LSTMModel, self).__init__()

        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # LSTM layer
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,
                            batch_first=True)

        # Fully-connected output
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        """
        x: (batch, seq_len, input_dim)
        """
        # Initialiser h0, c0
        h0 = torch.zeros(self.num_layers, x.size(0),
                         self.hidden_dim).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0),
                         self.hidden_dim).to(x.device)

        # LSTM forward
        # out: (batch, seq_len, hidden_dim)
        out, (hn, cn) = self.lstm(x, (h0, c0))

        # Prendre le dernier timestep (many-to-one)
        out = self.fc(out[:, -1, :])
        return out

# Exemple d'utilisation
model = LSTMModel(input_dim=10, hidden_dim=128, output_dim=5)

# S√©quence de longueur 20
x = torch.randn(32, 20, 10)  # (batch=32, seq_len=20, input_dim=10)
output = model(x)
print(output.shape)  # (32, 5)
\end{lstlisting}

% ===== SECTION 4: GRU =====
\section{Gated Recurrent Unit (GRU)}

\subsection{Architecture}

GRU est une variante simplifi√©e du LSTM avec \textbf{2 portes} au lieu de 3.

\begin{definition}{GRU}
\begin{align}
    \vect{z}_t &= \sigmoid(\mat{W}_z [\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(update gate)} \\
    \vect{r}_t &= \sigmoid(\mat{W}_r [\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(reset gate)} \\
    \tilde{\vect{h}}_t &= \tanh(\mat{W} [\vect{r}_t \odot \vect{h}_{t-1}, \vect{x}_t]) \quad \text{(candidate)} \\
    \vect{h}_t &= (1 - \vect{z}_t) \odot \vect{h}_{t-1} + \vect{z}_t \odot \tilde{\vect{h}}_t
\end{align}
\end{definition}

\subsection{Diff√©rences avec LSTM}

\begin{table}[h]
\centering
\caption{LSTM vs GRU}
\label{tab:lstm_gru}
\begin{tabular}{lcc}
\toprule
 & \textbf{LSTM} & \textbf{GRU} \\
\midrule
Nombre de portes & 3 & 2 \\
Cellule m√©moire s√©par√©e & Oui ($\vect{c}_t$) & Non (seulement $\vect{h}_t$) \\
Param√®tres & $4h(h + d + 1)$ & $3h(h + d + 1)$ \\
Vitesse & Plus lent & Plus rapide \\
Performance & L√©g√®rement meilleure & Comparable \\
\bottomrule
\end{tabular}
\end{table}

\begin{astuce}
\textbf{R√®gle pratique :}
\begin{itemize}
    \item LSTM : S√©quences tr√®s longues, d√©pendances complexes
    \item GRU : S√©quences courtes/moyennes, plus rapide, moins de param√®tres
    \item En pratique, essayer les deux et comparer !
\end{itemize}
\end{astuce}

\subsection{Impl√©mentation PyTorch}

\begin{lstlisting}[language=Python, caption=GRU avec PyTorch]
class GRUModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):
        super(GRUModel, self).__init__()

        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.gru = nn.GRU(input_dim, hidden_dim, num_layers,
                          batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0),
                         self.hidden_dim).to(x.device)

        out, hn = self.gru(x, h0)
        out = self.fc(out[:, -1, :])
        return out
\end{lstlisting}

% ===== SECTION 5: BIDIRECTIONAL RNN =====
\section{Bidirectional RNN/LSTM/GRU}

\subsection{Motivation}

Un RNN classique ne voit que le \textbf{contexte pass√©}. Pour certaines t√¢ches (NER, POS tagging), le contexte \textbf{futur} est aussi important.

\begin{exemple}{Pr√©diction de mots}
Phrase : "Le \_\_\_ mange la souris"

\begin{itemize}
    \item Contexte pass√© : "Le"
    \item Contexte futur : "mange la souris" ‚Üí animal carnivore ‚Üí "chat"
\end{itemize}
\end{exemple}

\subsection{Architecture}

\begin{definition}{Bidirectional RNN}
Un Bi-RNN a deux RNN :
\begin{itemize}
    \item \textbf{Forward RNN} : lit la s√©quence de gauche √† droite ‚Üí $\overrightarrow{\vect{h}}_t$
    \item \textbf{Backward RNN} : lit la s√©quence de droite √† gauche ‚Üí $\overleftarrow{\vect{h}}_t$
\end{itemize}

La sortie finale est la concat√©nation :
\begin{equation}
    \vect{h}_t = [\overrightarrow{\vect{h}}_t ; \overleftarrow{\vect{h}}_t]
\end{equation}
\end{definition}

\textbf{Nombre de param√®tres :} Doubl√© (2 RNN ind√©pendants)

\subsection{Impl√©mentation}

\begin{lstlisting}[language=Python, caption=Bidirectional LSTM]
class BiLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(BiLSTM, self).__init__()

        # bidirectional=True
        self.lstm = nn.LSTM(input_dim, hidden_dim,
                            batch_first=True, bidirectional=True)

        # hidden_dim * 2 car bidirectionnel
        self.fc = nn.Linear(hidden_dim * 2, output_dim)

    def forward(self, x):
        # out: (batch, seq_len, hidden_dim * 2)
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])  # Dernier timestep
        return out
\end{lstlisting}

% ===== SECTION 6: ATTENTION MECHANISM =====
\section{M√©canisme d'Attention}

\subsection{Motivation : Probl√®me du goulot d'√©tranglement}

En seq2seq (traduction), le d√©codeur doit tout comprendre √† partir d'un seul vecteur contexte $\vect{c}$ :
\begin{equation}
    \text{Encoder} : (x_1, \ldots, x_n) \to \vect{c} \to \text{Decoder} : (y_1, \ldots, y_m)
\end{equation}

\textbf{Probl√®me :} $\vect{c}$ est un goulot d'√©tranglement, surtout pour des s√©quences longues.

\subsection{Attention de Bahdanau}

\begin{definition}{Attention Mechanism}
Au lieu d'un vecteur contexte fixe, on calcule un vecteur contexte \textbf{dynamique} $\vect{c}_t$ √† chaque pas de d√©codage :
\begin{align}
    e_{t,i} &= \text{score}(\vect{s}_{t-1}, \vect{h}_i) \quad \text{(score d'attention)} \\
    \alpha_{t,i} &= \frac{\exp(e_{t,i})}{\sum_{j=1}^n \exp(e_{t,j})} \quad \text{(poids d'attention)} \\
    \vect{c}_t &= \sum_{i=1}^n \alpha_{t,i} \vect{h}_i \quad \text{(contexte pond√©r√©)}
\end{align}
o√π :
\begin{itemize}
    \item $\vect{s}_{t-1}$ : √©tat cach√© du d√©codeur
    \item $\vect{h}_i$ : √©tats cach√©s de l'encodeur
    \item $\alpha_{t,i}$ : attention sur le mot $i$ de l'entr√©e
\end{itemize}
\end{definition}

\subsection{Fonctions de score}

\textbf{Dot product :}
\begin{equation}
    \text{score}(\vect{s}, \vect{h}) = \vect{s}^T \vect{h}
\end{equation}

\textbf{General (bilinear) :}
\begin{equation}
    \text{score}(\vect{s}, \vect{h}) = \vect{s}^T \mat{W}_a \vect{h}
\end{equation}

\textbf{Additive (Bahdanau) :}
\begin{equation}
    \text{score}(\vect{s}, \vect{h}) = \vect{v}_a^T \tanh(\mat{W}_1 \vect{s} + \mat{W}_2 \vect{h})
\end{equation}

\subsection{Scaled Dot-Product Attention}

Version utilis√©e dans les Transformers :
\begin{equation}
    \text{Attention}(\mat{Q}, \mat{K}, \mat{V}) = \softmax\left(\frac{\mat{Q}\mat{K}^T}{\sqrt{d_k}}\right) \mat{V}
\end{equation}
o√π :
\begin{itemize}
    \item $\mat{Q}$ (Queries) : ce qu'on cherche
    \item $\mat{K}$ (Keys) : ce qu'on a
    \item $\mat{V}$ (Values) : ce qu'on renvoie
    \item $d_k$ : dimension des cl√©s (facteur de normalisation)
\end{itemize}

\textbf{Interpr√©tation :}
\begin{enumerate}
    \item Calculer similarit√© entre $\mat{Q}$ et $\mat{K}$ : $\mat{Q}\mat{K}^T$
    \item Normaliser par $\sqrt{d_k}$ pour √©viter valeurs trop grandes
    \item Appliquer softmax pour obtenir poids d'attention
    \item Pond√©rer les valeurs $\mat{V}$
\end{enumerate}

% ===== SECTION 7: TRANSFORMERS =====
\section{Transformers}

\subsection{Motivation}

\textbf{Limites des RNN :}
\begin{itemize}
    \item ‚ùå Traitement s√©quentiel (pas de parall√©lisation)
    \item ‚ùå Difficult√© avec d√©pendances √† tr√®s long terme
    \item ‚ùå Lent √† entra√Æner
\end{itemize}

\textbf{Transformer (Vaswani et al., 2017) :}
\begin{itemize}
    \item ‚úÖ Uniquement bas√© sur l'attention (pas de r√©currence)
    \item ‚úÖ Compl√®tement parall√©lisable
    \item ‚úÖ Capture d√©pendances √† longue distance facilement
    \item ‚úÖ State-of-the-art en NLP (BERT, GPT, T5, etc.)
\end{itemize}

\subsection{Architecture globale}

\textbf{Encoder-Decoder} avec 6 couches chacun (original paper) :

\begin{itemize}
    \item \textbf{Encoder} : Traite la s√©quence d'entr√©e
    \item \textbf{Decoder} : G√©n√®re la s√©quence de sortie (autor√©gressif)
\end{itemize}

\subsection{Multi-Head Attention}

\begin{definition}{Multi-Head Attention}
Au lieu d'une seule attention, on calcule $h$ attentions en parall√®le avec des projections diff√©rentes :
\begin{align}
    \text{head}_i &= \text{Attention}(\mat{Q}\mat{W}_i^Q, \mat{K}\mat{W}_i^K, \mat{V}\mat{W}_i^V) \\
    \text{MultiHead}(\mat{Q}, \mat{K}, \mat{V}) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \mat{W}^O
\end{align}
o√π $\mat{W}_i^Q, \mat{W}_i^K, \mat{W}_i^V$ sont des matrices de projection apprenables.
\end{definition}

\textbf{Avantages :}
\begin{itemize}
    \item Chaque t√™te peut se concentrer sur des aspects diff√©rents (syntaxe, s√©mantique, etc.)
    \item Plus expressif qu'une seule attention
\end{itemize}

\subsection{Positional Encoding}

\textbf{Probl√®me :} Sans r√©currence, le Transformer ne conna√Æt pas l'ordre des mots !

\textbf{Solution :} Ajouter un \textbf{encodage de position} √† chaque embedding :
\begin{align}
    PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d}}\right) \\
    PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d}}\right)
\end{align}
o√π $pos$ est la position et $i$ la dimension.

\begin{equation}
    \vect{x}_{\text{input}} = \text{Embedding}(\text{token}) + \text{PositionalEncoding}(pos)
\end{equation}

\subsection{Encoder Layer}

Une couche encoder contient :
\begin{enumerate}
    \item \textbf{Multi-Head Self-Attention}
    \item \textbf{Add \& Norm} (Residual connection + Layer Normalization)
    \item \textbf{Feed-Forward Network} (2 couches FC avec ReLU)
    \item \textbf{Add \& Norm}
\end{enumerate}

\begin{align}
    \vect{z} &= \text{LayerNorm}(\vect{x} + \text{MultiHeadAttention}(\vect{x}, \vect{x}, \vect{x})) \\
    \text{output} &= \text{LayerNorm}(\vect{z} + \text{FFN}(\vect{z}))
\end{align}

\subsection{Decoder Layer}

Une couche decoder contient :
\begin{enumerate}
    \item \textbf{Masked Multi-Head Self-Attention} (ne voit que le pass√©)
    \item \textbf{Add \& Norm}
    \item \textbf{Multi-Head Cross-Attention} (attention sur l'encodeur)
    \item \textbf{Add \& Norm}
    \item \textbf{Feed-Forward Network}
    \item \textbf{Add \& Norm}
\end{enumerate}

\textbf{Masking :} En g√©n√©ration, le d√©codeur ne doit voir que les tokens pr√©c√©dents (autor√©gressif).

\subsection{Impl√©mentation simplifi√©e}

\begin{lstlisting}[language=Python, caption=Scaled Dot-Product Attention]
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Q, K, V: (batch, seq_len, d_k)
    """
    d_k = Q.size(-1)

    # Scores: (batch, seq_len, seq_len)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))

    # Masking (optionnel)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # Softmax
    attention_weights = F.softmax(scores, dim=-1)

    # Weighted sum
    output = torch.matmul(attention_weights, V)

    return output, attention_weights
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Multi-Head Attention]
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Linear projections
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def split_heads(self, x):
        """Split into multiple heads"""
        batch_size, seq_len, d_model = x.size()
        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # Linear projections
        Q = self.split_heads(self.W_q(Q))  # (batch, num_heads, seq_len, d_k)
        K = self.split_heads(self.W_k(K))
        V = self.split_heads(self.W_v(V))

        # Scaled dot-product attention
        attn_output, _ = scaled_dot_product_attention(Q, K, V, mask)

        # Concatenate heads
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )

        # Final linear
        output = self.W_o(attn_output)
        return output
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Transformer Encoder Layer]
class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerEncoderLayer, self).__init__()

        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # Self-attention + residual + norm
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # Feed-forward + residual + norm
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x
\end{lstlisting}

% ===== SECTION 8: APPLICATIONS NLP =====
\section{Applications NLP}

\subsection{Mod√®les pr√©-entra√Æn√©s}

\subsubsection{BERT (Bidirectional Encoder Representations from Transformers)}

\textbf{Architecture :} Encoder Transformer (12 ou 24 couches)

\textbf{Pr√©-entra√Ænement :}
\begin{itemize}
    \item \textbf{Masked Language Modeling (MLM)} : Pr√©dire les mots masqu√©s
    \item \textbf{Next Sentence Prediction (NSP)} : Pr√©dire si phrase B suit phrase A
\end{itemize}

\textbf{Fine-tuning :} Classification, NER, Q\&A, etc.

\subsubsection{GPT (Generative Pre-trained Transformer)}

\textbf{Architecture :} Decoder Transformer (autor√©gressif)

\textbf{Pr√©-entra√Ænement :} Language modeling (pr√©dire token suivant)

\textbf{Mod√®les :}
\begin{itemize}
    \item GPT-2 (1.5B params)
    \item GPT-3 (175B params)
    \item GPT-4 (1.76T params estim√©)
\end{itemize}

\subsubsection{T5, BART, RoBERTa, etc.}

Nombreuses variantes avec diff√©rentes strat√©gies de pr√©-entra√Ænement.

\subsection{Utilisation avec HuggingFace Transformers}

\begin{lstlisting}[language=Python, caption=BERT pour classification]
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Charger mod√®le pr√©-entra√Æn√©
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # Binary classification
)

# Texte √† classifier
text = "This movie is fantastic!"
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

# Forward
outputs = model(**inputs)
logits = outputs.logits
probs = torch.softmax(logits, dim=-1)

print(f"Positive: {probs[0, 1]:.2f}")
print(f"Negative: {probs[0, 0]:.2f}")
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=GPT-2 pour g√©n√©ration de texte]
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Prompt
prompt = "Once upon a time"
inputs = tokenizer.encode(prompt, return_tensors='pt')

# G√©n√©ration
outputs = model.generate(
    inputs,
    max_length=50,
    num_return_sequences=1,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
\end{lstlisting}

% ===== SECTION 9: S√âRIES TEMPORELLES =====
\section{S√©ries Temporelles}

\subsection{Pr√©diction}

\begin{lstlisting}[language=Python, caption=LSTM pour pr√©diction de s√©ries temporelles]
import numpy as np
import torch
import torch.nn as nn

# G√©n√©rer s√©rie temporelle synth√©tique
t = np.linspace(0, 100, 1000)
data = np.sin(t) + 0.1 * np.random.randn(1000)

# Cr√©er s√©quences (window = 50)
def create_sequences(data, window=50):
    X, y = [], []
    for i in range(len(data) - window):
        X.append(data[i:i+window])
        y.append(data[i+window])
    return np.array(X), np.array(y)

X, y = create_sequences(data, window=50)
X = torch.FloatTensor(X).unsqueeze(-1)  # (N, 50, 1)
y = torch.FloatTensor(y)

# Mod√®le LSTM
class TimeSeriesLSTM(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2):
        super(TimeSeriesLSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])  # Dernier timestep
        return out.squeeze()

model = TimeSeriesLSTM()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    outputs = model(X)
    loss = criterion(outputs, y)

    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
\end{lstlisting}

% ===== SECTION 10: BONNES PRATIQUES =====
\section{Bonnes Pratiques}

\subsection{RNN/LSTM/GRU}

\begin{itemize}
    \item \textbf{Gradient clipping} : Limiter norme du gradient (5-10)
    \item \textbf{Layer Normalization} : Stabilise l'entra√Ænement
    \item \textbf{Dropout} : Sur les connexions non-r√©currentes uniquement
    \item \textbf{Teacher Forcing} : En seq2seq, utiliser vraies sorties pendant entra√Ænement
    \item \textbf{Beam Search} : Pour g√©n√©ration (au lieu de greedy)
\end{itemize}

\subsection{Transformers}

\begin{itemize}
    \item \textbf{Learning Rate Warmup} : Augmenter LR progressivement au d√©but
    \item \textbf{Label Smoothing} : R√©gularisation pour classification
    \item \textbf{Dropout} : Sur attention et FFN
    \item \textbf{Layer Normalization} : Avant ou apr√®s chaque sous-couche
    \item \textbf{Gradient Accumulation} : Pour simuler grands batch sizes
\end{itemize}

% ===== SECTION 11: R√âSUM√â =====
\section{R√©sum√© du Chapitre}

\subsection{Points Cl√©s}

\begin{itemize}
    \item \textbf{RNN} : √âtat cach√© r√©current, vanishing/exploding gradient
    \item \textbf{LSTM} : 3 portes (forget, input, output) + cellule m√©moire
    \item \textbf{GRU} : Variante simplifi√©e avec 2 portes
    \item \textbf{Bidirectional} : Contexte pass√© + futur
    \item \textbf{Attention} : Pond√©ration dynamique des inputs
    \item \textbf{Transformer} : Uniquement attention, parall√©lisable, state-of-the-art NLP
    \item \textbf{BERT} : Encoder pr√©-entra√Æn√© (MLM)
    \item \textbf{GPT} : Decoder autor√©gressif (LM)
\end{itemize}

\subsection{Formules Essentielles}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Formules √† retenir]
\textbf{RNN :}
\begin{equation*}
    \vect{h}_t = \tanh(\mat{W}_{hh} \vect{h}_{t-1} + \mat{W}_{xh} \vect{x}_t + \vect{b})
\end{equation*}

\textbf{LSTM (simplifi√©) :}
\begin{align*}
    \vect{c}_t &= \vect{f}_t \odot \vect{c}_{t-1} + \vect{i}_t \odot \tilde{\vect{c}}_t \\
    \vect{h}_t &= \vect{o}_t \odot \tanh(\vect{c}_t)
\end{align*}

\textbf{Scaled Dot-Product Attention :}
\begin{equation*}
    \text{Attention}(\mat{Q}, \mat{K}, \mat{V}) = \softmax\left(\frac{\mat{Q}\mat{K}^T}{\sqrt{d_k}}\right) \mat{V}
\end{equation*}
\end{tcolorbox}

% ===== SECTION 12: EXERCICES =====
\section{Exercices}

\subsection{Questions de compr√©hension}

\begin{enumerate}
    \item Pourquoi le RNN vanilla souffre-t-il du vanishing gradient ?
    \item Expliquer le r√¥le de chaque porte dans un LSTM.
    \item Quelle est la diff√©rence principale entre LSTM et GRU ?
    \item Pourquoi utiliser un Bidirectional RNN pour le NER ?
    \item Comment le m√©canisme d'attention r√©sout-il le probl√®me du goulot d'√©tranglement ?
    \item Pourquoi le Transformer a-t-il besoin de positional encoding ?
\end{enumerate}

\subsection{Exercices pratiques}

\begin{enumerate}
    \item \textbf{Pr√©diction de s√©ries temporelles}
    \begin{itemize}
        \item Impl√©menter un LSTM pour pr√©dire une s√©rie temporelle
        \item Comparer LSTM vs GRU
        \item Visualiser les pr√©dictions
    \end{itemize}

    \item \textbf{Sentiment Analysis}
    \begin{itemize}
        \item Fine-tuner BERT sur un dataset de reviews (IMDB, Yelp)
        \item Comparer avec un LSTM from scratch
    \end{itemize}

    \item \textbf{G√©n√©ration de texte}
    \begin{itemize}
        \item Entra√Æner un RNN character-level sur Shakespeare
        \item G√©n√©rer du texte avec temperature sampling
    \end{itemize}

    \item \textbf{Attention Visualization}
    \begin{itemize}
        \item Impl√©menter attention de Bahdanau
        \item Visualiser les poids d'attention sur une t√¢che de traduction
    \end{itemize}
\end{enumerate}

\textit{Solutions disponibles dans} \texttt{08\_exercices.ipynb} \textit{(solutions int√©gr√©es dans le notebook)}

% ===== SECTION: NOTEBOOKS PRATIQUES =====
\section{Notebooks Pratiques}

Ce chapitre est accompagn√© des notebooks suivants :

\begin{itemize}
    \item \texttt{08\_demo\_lstm\_sentiment.ipynb} : Analyse de sentiment avec LSTM
    \begin{itemize}
        \item LSTM bidirectionnel pour classification de texte
        \item Dataset IMDB Reviews (sentiment positif/n√©gatif)
        \item Preprocessing et tokenization
        \item Training avec PyTorch et visualisation des r√©sultats
    \end{itemize}

    \item \texttt{08\_demo\_transformers\_huggingface.ipynb} : Transformers avec Hugging Face
    \begin{itemize}
        \item Fine-tuning de BERT pour classification de texte
        \item G√©n√©ration de texte avec GPT-2
        \item Utilisation de la biblioth√®que Transformers
        \item Tokenization et gestion des mod√®les pr√©-entra√Æn√©s
    \end{itemize}

    \item \texttt{08\_demo\_rag\_llm.ipynb} : RAG et LLMs avanc√©s \textbf{(NOUVEAU)}
    \begin{itemize}
        \item Introduction au Retrieval-Augmented Generation (RAG)
        \item Embeddings avec Sentence-BERT et recherche vectorielle (FAISS)
        \item Pipeline RAG complet : Retrieval + Context + Generation
        \item Chunking et preprocessing de documents longs
        \item Techniques avanc√©es : Reranking avec CrossEncoder
        \item Hybrid Search : combinaison BM25 (keyword) + Dense embeddings
        \item √âvaluation du RAG : Precision@K, Recall@K
        \item Visualisation des embeddings avec PCA
        \item Applications pratiques : chatbots, QA systems, documentation
    \end{itemize}

    \item \texttt{08\_exercices.ipynb} : Exercices pratiques avec solutions int√©gr√©es
    \begin{itemize}
        \item Impl√©mentation de RNN vanilla et LSTM from scratch
        \item Seq2Seq avec attention pour traduction
        \item G√©n√©ration de texte avec temperature sampling
        \item Attention visualization
    \end{itemize}
\end{itemize}

% ===== SECTION 13: POUR ALLER PLUS LOIN =====
\section{Pour Aller Plus Loin}

\subsection{Lectures Recommand√©es}

\begin{itemize}
    \item Hochreiter \& Schmidhuber (1997) - "Long Short-Term Memory"
    \item Bahdanau et al. (2014) - "Neural Machine Translation by Jointly Learning to Align and Translate"
    \item Vaswani et al. (2017) - "Attention Is All You Need" (Transformer)
    \item Devlin et al. (2018) - "BERT: Pre-training of Deep Bidirectional Transformers"
    \item Radford et al. (2019) - "Language Models are Unsupervised Multitask Learners" (GPT-2)
\end{itemize}

\subsection{Ressources}

\begin{itemize}
    \item The Illustrated Transformer : \url{https://jalammar.github.io/illustrated-transformer/}
    \item HuggingFace Transformers : \url{https://huggingface.co/docs/transformers/}
    \item Sequence Models (Coursera) : Andrew Ng
    \item Annotated Transformer : \url{https://nlp.seas.harvard.edu/annotated-transformer/}
\end{itemize}

\subsection{Prochaines √âtapes}

Chapitre suivant recommand√© : \textbf{Chapitre 09 - Reinforcement Learning}

Le reinforcement learning permet d'entra√Æner des agents √† prendre des d√©cisions s√©quentielles pour maximiser une r√©compense.

% ===== BIBLIOGRAPHIE =====
\section*{R√©f√©rences}

\begin{enumerate}
    \item Hochreiter, S., \& Schmidhuber, J. (1997). "Long short-term memory". \textit{Neural computation}, 9(8), 1735-1780.
    \item Cho, K., et al. (2014). "Learning phrase representations using RNN encoder-decoder for statistical machine translation". \textit{EMNLP}.
    \item Bahdanau, D., Cho, K., \& Bengio, Y. (2014). "Neural machine translation by jointly learning to align and translate". \textit{arXiv:1409.0473}.
    \item Vaswani, A., et al. (2017). "Attention is all you need". \textit{NIPS}.
    \item Devlin, J., et al. (2018). "BERT: Pre-training of deep bidirectional transformers for language understanding". \textit{arXiv:1810.04805}.
    \item Radford, A., et al. (2019). "Language models are unsupervised multitask learners". \textit{OpenAI blog}.
\end{enumerate}

\end{document}
