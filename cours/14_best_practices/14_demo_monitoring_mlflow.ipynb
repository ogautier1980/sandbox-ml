{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/XX_CHAPTER/XX_NOTEBOOK.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '14_demo_monitoring_mlflow.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D√©monstration : Monitoring et Tracking avec MLflow\n",
    "\n",
    "Ce notebook illustre l'utilisation de **MLflow** pour le tracking et la gestion d'exp√©riences ML :\n",
    "\n",
    "1. **Tracking d'Exp√©riences** : Logging de params, metrics, artifacts\n",
    "2. **Comparaison de Runs** : Comparaison de plusieurs entra√Ænements\n",
    "3. **Model Registry** : Sauvegarde et versioning de mod√®les\n",
    "4. **UI MLflow** : Interface web pour visualisation\n",
    "5. **Autologging** : Logging automatique avec scikit-learn et PyTorch\n",
    "\n",
    "**Dataset** : California Housing (r√©gression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration de visualisation\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Biblioth√®ques import√©es avec succ√®s !\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de MLflow\n",
    "import os\n",
    "\n",
    "# R√©pertoire de tracking\n",
    "mlflow_dir = '/tmp/mlflow'\n",
    "os.makedirs(mlflow_dir, exist_ok=True)\n",
    "\n",
    "# URI de tracking (filesystem local)\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_dir}\")\n",
    "\n",
    "# Nom de l'exp√©rience\n",
    "experiment_name = \"Housing_Price_Prediction\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow configur√© !\")\n",
    "print(f\"  Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"  Experiment: {experiment_name}\")\n",
    "print(f\"\\nPour lancer l'UI MLflow:\")\n",
    "print(f\"  mlflow ui --backend-store-uri {mlflow_dir} --port 5000\")\n",
    "print(f\"  Puis ouvrir: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "feature_names = housing.feature_names\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset California Housing:\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Test:  {X_test.shape}\")\n",
    "print(f\"  Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tracking d'une Exp√©rience Simple (Manuel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXP√âRIENCE 1 : Random Forest (Tracking Manuel)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# D√©marrer un run MLflow\n",
    "with mlflow.start_run(run_name=\"RF_baseline\") as run:\n",
    "    # Hyperparam√®tres\n",
    "    n_estimators = 100\n",
    "    max_depth = 20\n",
    "    min_samples_split = 5\n",
    "    \n",
    "    # Logging des param√®tres\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # M√©triques\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    # Logging des m√©triques\n",
    "    mlflow.log_metric(\"train_r2\", train_r2)\n",
    "    mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"overfit_score\", train_r2 - test_r2)\n",
    "    \n",
    "    # Validation crois√©e\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    mlflow.log_metric(\"cv_r2_mean\", cv_scores.mean())\n",
    "    mlflow.log_metric(\"cv_r2_std\", cv_scores.std())\n",
    "    \n",
    "    # Logging du mod√®le avec signature\n",
    "    signature = infer_signature(X_train, y_pred_train)\n",
    "    mlflow.sklearn.log_model(model, \"model\", signature=signature)\n",
    "    \n",
    "    # Sauvegarde d'un graphique (artifact)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.scatter(y_test, y_pred_test, alpha=0.5, s=20)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('Valeurs R√©elles')\n",
    "    ax.set_ylabel('Valeurs Pr√©dites')\n",
    "    ax.set_title(f'RF Baseline - R¬≤ = {test_r2:.4f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plot_path = '/tmp/rf_baseline_plot.png'\n",
    "    plt.savefig(plot_path, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    mlflow.log_artifact(plot_path, \"plots\")\n",
    "    \n",
    "    # Logging feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    importance_path = '/tmp/feature_importance.csv'\n",
    "    feature_importance.to_csv(importance_path, index=False)\n",
    "    mlflow.log_artifact(importance_path, \"data\")\n",
    "    \n",
    "    # Tags\n",
    "    mlflow.set_tag(\"model_type\", \"RandomForest\")\n",
    "    mlflow.set_tag(\"dataset\", \"California Housing\")\n",
    "    mlflow.set_tag(\"task\", \"regression\")\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    print(f\"\\nRun ID: {run_id}\")\n",
    "    print(f\"\\nM√©triques:\")\n",
    "    print(f\"  Train R¬≤:  {train_r2:.4f}\")\n",
    "    print(f\"  Train RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"  Test R¬≤:   {test_r2:.4f}\")\n",
    "    print(f\"  Test RMSE:  {test_rmse:.4f}\")\n",
    "    print(f\"  Test MAE:   {test_mae:.4f}\")\n",
    "    print(f\"  CV R¬≤:      {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tracking de Plusieurs Exp√©riences (Comparaison de Mod√®les)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPARAISON DE MOD√àLES (4 mod√®les diff√©rents)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# D√©finition des mod√®les √† tester\n",
    "models_config = [\n",
    "    {\n",
    "        \"name\": \"RandomForest_50trees\",\n",
    "        \"model\": RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1),\n",
    "        \"params\": {\"n_estimators\": 50, \"model_type\": \"RandomForest\"}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RandomForest_200trees\",\n",
    "        \"model\": RandomForestRegressor(n_estimators=200, max_depth=30, random_state=42, n_jobs=-1),\n",
    "        \"params\": {\"n_estimators\": 200, \"max_depth\": 30, \"model_type\": \"RandomForest\"}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GradientBoosting\",\n",
    "        \"model\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "        \"params\": {\"n_estimators\": 100, \"learning_rate\": 0.1, \"model_type\": \"GradientBoosting\"}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ridge\",\n",
    "        \"model\": Ridge(alpha=1.0),\n",
    "        \"params\": {\"alpha\": 1.0, \"model_type\": \"Ridge\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in models_config:\n",
    "    with mlflow.start_run(run_name=config[\"name\"]) as run:\n",
    "        print(f\"\\nEntra√Ænement : {config['name']}\")\n",
    "        \n",
    "        # Logging params\n",
    "        for key, value in config[\"params\"].items():\n",
    "            mlflow.log_param(key, value)\n",
    "        \n",
    "        # Entra√Ænement\n",
    "        model = config[\"model\"]\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # M√©triques\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        mlflow.log_metric(\"test_r2\", test_r2)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        mlflow.log_metric(\"test_mae\", test_mae)\n",
    "        \n",
    "        # Logging du mod√®le\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Tags\n",
    "        mlflow.set_tag(\"model_type\", config[\"params\"][\"model_type\"])\n",
    "        mlflow.set_tag(\"experiment_type\", \"model_comparison\")\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": config[\"name\"],\n",
    "            \"Run ID\": run.info.run_id,\n",
    "            \"R¬≤\": test_r2,\n",
    "            \"RMSE\": test_rmse,\n",
    "            \"MAE\": test_mae\n",
    "        })\n",
    "        \n",
    "        print(f\"  R¬≤: {test_r2:.4f}, RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}\")\n",
    "\n",
    "# DataFrame de r√©sultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"R√âSULTATS DE LA COMPARAISON\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la comparaison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "# R¬≤\n",
    "axes[0].bar(results_df['Model'], results_df['R¬≤'], color=colors, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].set_title('Comparaison R¬≤')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(results_df['R¬≤']):\n",
    "    axes[0].text(i, v + 0.02, f\"{v:.4f}\", ha='center', fontsize=9)\n",
    "\n",
    "# RMSE\n",
    "axes[1].bar(results_df['Model'], results_df['RMSE'], color=colors, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Comparaison RMSE (plus bas = mieux)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(results_df['RMSE']):\n",
    "    axes[1].text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontsize=9)\n",
    "\n",
    "# MAE\n",
    "axes[2].bar(results_df['Model'], results_df['MAE'], color=colors, edgecolor='black', alpha=0.7)\n",
    "axes[2].set_ylabel('MAE')\n",
    "axes[2].set_title('Comparaison MAE (plus bas = mieux)')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(results_df['MAE']):\n",
    "    axes[2].text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autologging avec Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"AUTOLOGGING SCIKIT-LEARN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Activation de l'autologging\n",
    "mlflow.sklearn.autolog(log_input_examples=True, log_model_signatures=True)\n",
    "\n",
    "with mlflow.start_run(run_name=\"RF_autolog\") as run:\n",
    "    # L'autologging capture automatiquement params, metrics, et le mod√®le\n",
    "    model = RandomForestRegressor(n_estimators=150, max_depth=25, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Pr√©dictions (automatiquement logg√©es)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # M√©triques additionnelles (manuelles)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"custom_r2\", test_r2)\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    print(f\"\\nRun ID: {run_id}\")\n",
    "    print(f\"Autologging activ√© : params, metrics, et mod√®le logg√©s automatiquement !\")\n",
    "    print(f\"Test R¬≤: {test_r2:.4f}\")\n",
    "\n",
    "# D√©sactivation de l'autologging\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "print(\"\\nAutologging d√©sactiv√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recherche et Chargement de Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RECHERCHE DE RUNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# R√©cup√©ration de l'exp√©rience\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Recherche de tous les runs de l'exp√©rience\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    order_by=[\"metrics.test_r2 DESC\"],\n",
    "    max_results=10\n",
    ")\n",
    "\n",
    "print(f\"\\nNombre de runs trouv√©s: {len(runs)}\")\n",
    "print(f\"\\nTop 5 runs (par test_r2):\")\n",
    "print(runs[['run_id', 'tags.mlflow.runName', 'metrics.test_r2', 'metrics.test_rmse']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du meilleur mod√®le\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHARGEMENT DU MEILLEUR MOD√àLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_run_id = runs.iloc[0]['run_id']\n",
    "best_run_name = runs.iloc[0]['tags.mlflow.runName']\n",
    "best_r2 = runs.iloc[0]['metrics.test_r2']\n",
    "\n",
    "print(f\"\\nMeilleur run:\")\n",
    "print(f\"  Run ID: {best_run_id}\")\n",
    "print(f\"  Name: {best_run_name}\")\n",
    "print(f\"  Test R¬≤: {best_r2:.4f}\")\n",
    "\n",
    "# Chargement du mod√®le\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "print(f\"\\nMod√®le charg√© depuis MLflow !\")\n",
    "print(f\"Type: {type(loaded_model).__name__}\")\n",
    "\n",
    "# Test de pr√©diction\n",
    "sample_predictions = loaded_model.predict(X_test[:5])\n",
    "print(f\"\\nPr√©dictions sur 5 √©chantillons:\")\n",
    "for i, (true_val, pred_val) in enumerate(zip(y_test[:5], sample_predictions), 1):\n",
    "    print(f\"  {i}. True: ${true_val * 100:.0f}k, Predicted: ${pred_val * 100:.0f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Registry (Versioning de Mod√®les)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL REGISTRY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: Le Model Registry n√©cessite un backend SQL (SQLite, MySQL, PostgreSQL)\n",
    "# Avec un backend filesystem local, nous pouvons seulement loguer les mod√®les\n",
    "\n",
    "model_name = \"HousingPricePredictor\"\n",
    "\n",
    "try:\n",
    "    # Enregistrement du meilleur mod√®le dans le registry\n",
    "    model_uri = f\"runs:/{best_run_id}/model\"\n",
    "    \n",
    "    # Version 1 du mod√®le\n",
    "    model_version = mlflow.register_model(model_uri, model_name)\n",
    "    \n",
    "    print(f\"\\nMod√®le enregistr√© dans le registry !\")\n",
    "    print(f\"  Name: {model_name}\")\n",
    "    print(f\"  Version: {model_version.version}\")\n",
    "    print(f\"  Run ID: {best_run_id}\")\n",
    "    \n",
    "    # Chargement depuis le registry\n",
    "    loaded_model_registry = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version.version}\")\n",
    "    print(f\"\\nMod√®le charg√© depuis le registry !\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"\\nModel Registry non disponible avec backend filesystem.\")\n",
    "    print(f\"Pour activer le Model Registry, utiliser un backend SQL:\")\n",
    "    print(f\"  mlflow.set_tracking_uri('sqlite:///mlflow.db')\")\n",
    "    print(f\"\\nErreur: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparaison Visuelle des Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'historique des runs\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALISATION DES RUNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extraction des m√©triques de tous les runs\n",
    "runs_data = []\n",
    "for _, run in runs.iterrows():\n",
    "    runs_data.append({\n",
    "        'Run Name': run.get('tags.mlflow.runName', 'Unknown'),\n",
    "        'R¬≤': run.get('metrics.test_r2', None),\n",
    "        'RMSE': run.get('metrics.test_rmse', None),\n",
    "        'MAE': run.get('metrics.test_mae', None)\n",
    "    })\n",
    "\n",
    "runs_viz_df = pd.DataFrame(runs_data).dropna()\n",
    "\n",
    "print(f\"\\nRuns avec m√©triques compl√®tes: {len(runs_viz_df)}\")\n",
    "\n",
    "if len(runs_viz_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # R¬≤ par run\n",
    "    axes[0, 0].barh(runs_viz_df['Run Name'], runs_viz_df['R¬≤'], color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('R¬≤ Score')\n",
    "    axes[0, 0].set_title('R¬≤ par Run')\n",
    "    axes[0, 0].set_xlim([0, 1])\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    \n",
    "    # RMSE par run\n",
    "    axes[0, 1].barh(runs_viz_df['Run Name'], runs_viz_df['RMSE'], color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('RMSE')\n",
    "    axes[0, 1].set_title('RMSE par Run')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    \n",
    "    # MAE par run\n",
    "    axes[1, 0].barh(runs_viz_df['Run Name'], runs_viz_df['MAE'], color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('MAE')\n",
    "    axes[1, 0].set_title('MAE par Run')\n",
    "    axes[1, 0].invert_yaxis()\n",
    "    \n",
    "    # Scatter R¬≤ vs RMSE\n",
    "    axes[1, 1].scatter(runs_viz_df['R¬≤'], runs_viz_df['RMSE'], s=100, alpha=0.7, edgecolor='black')\n",
    "    for i, txt in enumerate(runs_viz_df['Run Name']):\n",
    "        axes[1, 1].annotate(txt, (runs_viz_df['R¬≤'].iloc[i], runs_viz_df['RMSE'].iloc[i]), \n",
    "                           fontsize=8, alpha=0.7, ha='right')\n",
    "    axes[1, 1].set_xlabel('R¬≤')\n",
    "    axes[1, 1].set_ylabel('RMSE')\n",
    "    axes[1, 1].set_title('R¬≤ vs RMSE')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Pas assez de runs avec m√©triques compl√®tes pour visualisation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Grid Search avec MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GRID SEARCH AVEC MLFLOW TRACKING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Grid de param√®tres\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "print(f\"\\nNombre de combinaisons: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"GridSearch_RF\") as parent_run:\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Logging du meilleur mod√®le\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "    mlflow.log_metric(\"best_cv_score\", grid_search.best_score_)\n",
    "    \n",
    "    # Test set performance\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    \n",
    "    # Logging du mod√®le\n",
    "    mlflow.sklearn.log_model(grid_search.best_estimator_, \"best_model\")\n",
    "    \n",
    "    # Logging de tous les r√©sultats du grid search\n",
    "    cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    cv_results_path = '/tmp/grid_search_results.csv'\n",
    "    cv_results_df.to_csv(cv_results_path, index=False)\n",
    "    mlflow.log_artifact(cv_results_path, \"grid_search\")\n",
    "    \n",
    "    print(f\"\\nMeilleurs param√®tres:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"\\nMeilleur CV score (R¬≤): {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Test R¬≤: {test_r2:.4f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. R√©capitulatif et Instructions MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"R√âCAPITULATIF MLFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Statistiques de l'exp√©rience\n",
    "total_runs = len(runs)\n",
    "best_r2 = runs['metrics.test_r2'].max()\n",
    "avg_r2 = runs['metrics.test_r2'].mean()\n",
    "\n",
    "print(f\"\\nExp√©rience: {experiment_name}\")\n",
    "print(f\"  Total runs: {total_runs}\")\n",
    "print(f\"  Meilleur R¬≤: {best_r2:.4f}\")\n",
    "print(f\"  R¬≤ moyen: {avg_r2:.4f}\")\n",
    "print(f\"\\nTracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LANCEMENT DE L'INTERFACE MLFLOW UI\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPour visualiser toutes les exp√©riences dans l'UI MLflow:\")\n",
    "print(f\"\\n1. Ouvrir un terminal\")\n",
    "print(f\"2. Ex√©cuter la commande:\")\n",
    "print(f\"   mlflow ui --backend-store-uri {mlflow_dir} --port 5000\")\n",
    "print(f\"\\n3. Ouvrir dans un navigateur:\")\n",
    "print(f\"   http://localhost:5000\")\n",
    "print(f\"\\n4. Fonctionnalit√©s de l'UI:\")\n",
    "print(f\"   - Comparaison visuelle des runs\")\n",
    "   - Graphiques de m√©triques\")\n",
    "print(f\"   - T√©l√©chargement d'artifacts\")\n",
    "print(f\"   - Filtrage et recherche de runs\")\n",
    "print(f\"   - Visualisation des param√®tres et m√©triques\")\n",
    "print(f\"   - Model Registry (si backend SQL configur√©)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FICHIERS G√âN√âR√âS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nR√©pertoire MLflow: {mlflow_dir}\")\n",
    "print(f\"  - Contient tous les runs et artifacts\")\n",
    "print(f\"  - Mod√®les sauvegard√©s pour chaque run\")\n",
    "print(f\"  - Graphiques et fichiers CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "### Points Cl√©s de MLflow\n",
    "\n",
    "1. **Tracking** :\n",
    "   - Logging de params, metrics, artifacts (plots, mod√®les, fichiers)\n",
    "   - Organisation en exp√©riences et runs\n",
    "   - Tags pour cat√©gorisation\n",
    "\n",
    "2. **Autologging** :\n",
    "   - Capture automatique avec scikit-learn, PyTorch, TensorFlow\n",
    "   - Params, metrics, et mod√®les logg√©s sans code suppl√©mentaire\n",
    "\n",
    "3. **Comparaison de Mod√®les** :\n",
    "   - Recherche et tri de runs par m√©triques\n",
    "   - Visualisation comparative dans l'UI\n",
    "   - Identification du meilleur mod√®le\n",
    "\n",
    "4. **Model Registry** :\n",
    "   - Versioning de mod√®les\n",
    "   - Stages : None, Staging, Production, Archived\n",
    "   - Tra√ßabilit√© compl√®te\n",
    "\n",
    "5. **Reproductibilit√©** :\n",
    "   - Chargement de mod√®les depuis runs\n",
    "   - Signature de mod√®le (input/output schema)\n",
    "   - Environnement Python sauvegard√©\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Naming** : Utiliser des noms de runs descriptifs\n",
    "- **Tags** : Taguer les runs (task, dataset, experiment_type)\n",
    "- **Artifacts** : Sauvegarder plots, confusion matrices, feature importance\n",
    "- **Backend** : Utiliser SQLite/PostgreSQL pour production (au lieu de filesystem)\n",
    "- **Model Registry** : G√©rer le cycle de vie des mod√®les (dev ‚Üí staging ‚Üí prod)\n",
    "- **CI/CD** : Int√©grer MLflow dans les pipelines d'entra√Ænement\n",
    "\n",
    "### Alternatives √† MLflow\n",
    "\n",
    "- **Weights & Biases (W&B)** : Interface plus moderne, cloud-first\n",
    "- **Neptune.ai** : Collaboration d'√©quipe, int√©grations √©tendues\n",
    "- **TensorBoard** : Int√©gr√© avec TensorFlow/PyTorch\n",
    "- **Comet.ml** : Tracking + d√©ploiement + monitoring\n",
    "\n",
    "### Ressources\n",
    "\n",
    "- Documentation : https://mlflow.org/docs/latest/index.html\n",
    "- Tutoriels : https://mlflow.org/docs/latest/tutorials-and-examples/index.html\n",
    "- GitHub : https://github.com/mlflow/mlflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}