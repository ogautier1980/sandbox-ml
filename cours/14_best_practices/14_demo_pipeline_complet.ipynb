{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/14_best_practices/14_demo_pipeline_complet.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '14_demo_pipeline_complet.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D√©monstration : Pipeline ML Complet avec Scikit-Learn\n",
    "\n",
    "Ce notebook illustre les **best practices** pour construire un pipeline ML complet et robuste :\n",
    "\n",
    "1. **Chargement et EDA** : Exploration des donn√©es\n",
    "2. **Feature Engineering** : Cr√©ation de features pertinentes\n",
    "3. **Pipeline scikit-learn** : Preprocessing + Model\n",
    "4. **Validation Crois√©e** : √âvaluation robuste\n",
    "5. **Hyperparameter Tuning** : GridSearchCV\n",
    "6. **Persistence** : Sauvegarde du mod√®le (joblib/pickle)\n",
    "\n",
    "**Dataset** : Titanic (pr√©diction de survie) ou California Housing (r√©gression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration de visualisation\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Biblioth√®ques import√©es avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Exploration des Donn√©es (EDA)\n",
    "\n",
    "Nous utilisons le dataset **California Housing** (r√©gression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)  # type: ignore\n",
    "df['MedHouseVal'] = housing.target  # Target: prix m√©dian des maisons (en 100k$)  # type: ignore\n",
    "\n",
    "print(\"Dataset California Housing charg√© !\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nPremi√®res lignes:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations sur le dataset\n",
    "print(\"Informations sur le dataset:\")\n",
    "print(df.info())\n",
    "print(\"\\nStatistiques descriptives:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification des valeurs manquantes\n",
    "print(\"Valeurs manquantes par colonne:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal valeurs manquantes: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution de la target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['MedHouseVal'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Prix M√©dian (100k$)')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "axes[0].set_title('Distribution des Prix')\n",
    "axes[0].axvline(df['MedHouseVal'].mean(), color='red', linestyle='--', label=f'Moyenne: {df[\"MedHouseVal\"].mean():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].boxplot(df['MedHouseVal'], vert=True)\n",
    "axes[1].set_ylabel('Prix M√©dian (100k$)')\n",
    "axes[1].set_title('Boxplot des Prix')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Prix moyen: ${df['MedHouseVal'].mean() * 100:.0f}k\")\n",
    "print(f\"Prix m√©dian: ${df['MedHouseVal'].median() * 100:.0f}k\")\n",
    "print(f\"√âcart-type: ${df['MedHouseVal'].std() * 100:.0f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corr√©lation\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matrice de Corr√©lation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorr√©lations avec la target (MedHouseVal):\")\n",
    "print(corr_matrix['MedHouseVal'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots des features les plus corr√©l√©es\n",
    "top_features = corr_matrix['MedHouseVal'].abs().sort_values(ascending=False)[1:5].index\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(top_features):\n",
    "    axes[i].scatter(df[feature], df['MedHouseVal'], alpha=0.3, s=10)\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('MedHouseVal')\n",
    "    axes[i].set_title(f'{feature} vs MedHouseVal (r={corr_matrix.loc[feature, \"MedHouseVal\"]:.3f})')\n",
    "    \n",
    "    # Ligne de r√©gression simple\n",
    "    z = np.polyfit(df[feature], df['MedHouseVal'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[i].plot(df[feature], p(df[feature]), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Cr√©ation de features d√©riv√©es pour am√©liorer le mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def add_features(X):\n",
    "    \"\"\"Ajoute des features d√©riv√©es.\"\"\"\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Ratio chambres par m√©nage\n",
    "    X['RoomsPerHousehold'] = X['AveRooms'] / X['AveOccup']\n",
    "    \n",
    "    # Ratio chambres √† coucher par m√©nage\n",
    "    X['BedroomsPerHousehold'] = X['AveBedrms'] / X['AveOccup']\n",
    "    \n",
    "    # Population par m√©nage\n",
    "    X['PopulationPerHousehold'] = X['Population'] / X['AveOccup']\n",
    "    \n",
    "    # Densit√© de population (personnes par bloc)\n",
    "    X['PopulationDensity'] = X['Population'] / (X['Latitude'].abs() + X['Longitude'].abs())\n",
    "    \n",
    "    # Cat√©gorie d'√¢ge de la maison\n",
    "    X['HouseAgeCategory'] = pd.cut(X['HouseAge'], bins=[0, 10, 30, 100], labels=['New', 'Mid', 'Old'])\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Application du feature engineering\n",
    "df_engineered = add_features(df)\n",
    "\n",
    "print(\"Feature Engineering appliqu√© !\")\n",
    "print(f\"\\nNouvelles features:\")\n",
    "print(df_engineered[['RoomsPerHousehold', 'BedroomsPerHousehold', \n",
    "                      'PopulationPerHousehold', 'PopulationDensity', 'HouseAgeCategory']].head())\n",
    "\n",
    "print(f\"\\nShape apr√®s feature engineering: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Train/Test et Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration features / target\n",
    "X = df_engineered.drop('MedHouseVal', axis=1)\n",
    "y = df_engineered['MedHouseVal']\n",
    "\n",
    "# Split train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Split Train/Test:\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Test:  {X_test.shape}\")\n",
    "print(f\"\\nDistribution de la target:\")\n",
    "print(f\"  Train - Mean: {y_train.mean():.3f}, Std: {y_train.std():.3f}\")\n",
    "print(f\"  Test  - Mean: {y_test.mean():.3f}, Std: {y_test.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construction du Pipeline Scikit-Learn\n",
    "\n",
    "Un **Pipeline** encapsule preprocessing + model pour √©viter les data leaks et faciliter la production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des colonnes num√©riques et cat√©gorielles\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Features num√©riques ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"\\nFeatures cat√©gorielles ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pour features num√©riques\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Imputation valeurs manquantes\n",
    "    ('scaler', StandardScaler())  # Normalisation\n",
    "])\n",
    "\n",
    "# Preprocessing pour features cat√©gorielles\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputation valeurs manquantes\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
    "])\n",
    "\n",
    "# ColumnTransformer : applique transformations sp√©cifiques par type\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Colonnes non sp√©cifi√©es sont dropp√©es\n",
    ")\n",
    "\n",
    "print(\"Preprocessor cr√©√© avec succ√®s !\")\n",
    "print(f\"\\nTransformations:\")\n",
    "print(f\"  - Num√©riques: Imputation (m√©diane) + StandardScaler\")\n",
    "print(f\"  - Cat√©gorielles: Imputation (mode) + OneHotEncoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline complet : Preprocessing + Model\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"Pipeline cr√©√© avec succ√®s !\")\n",
    "print(f\"\\n√âtapes du pipeline:\")\n",
    "for i, (name, step) in enumerate(pipeline_rf.steps, 1):\n",
    "    print(f\"  {i}. {name}: {step.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation Crois√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation crois√©e 5-fold\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION CROIS√âE (5-FOLD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cv_scores_r2 = cross_val_score(pipeline_rf, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "cv_scores_rmse = -cross_val_score(pipeline_rf, X_train, y_train, cv=5, \n",
    "                                   scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "cv_scores_mae = -cross_val_score(pipeline_rf, X_train, y_train, cv=5, \n",
    "                                  scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "print(f\"\\nR¬≤ Scores: {cv_scores_r2}\")\n",
    "print(f\"  Mean: {cv_scores_r2.mean():.4f} (+/- {cv_scores_r2.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\nRMSE Scores: {cv_scores_rmse}\")\n",
    "print(f\"  Mean: {cv_scores_rmse.mean():.4f} (+/- {cv_scores_rmse.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\nMAE Scores: {cv_scores_mae}\")\n",
    "print(f\"  Mean: {cv_scores_mae.mean():.4f} (+/- {cv_scores_mae.std() * 2:.4f})\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].bar(range(1, 6), cv_scores_r2, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(cv_scores_r2.mean(), color='red', linestyle='--', label=f'Mean: {cv_scores_r2.mean():.4f}')\n",
    "axes[0].set_xlabel('Fold')\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].set_title('Validation Crois√©e - R¬≤')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "axes[1].bar(range(1, 6), cv_scores_rmse, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].axhline(cv_scores_rmse.mean(), color='red', linestyle='--', label=f'Mean: {cv_scores_rmse.mean():.4f}')\n",
    "axes[1].set_xlabel('Fold')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Validation Crois√©e - RMSE')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].bar(range(1, 6), cv_scores_mae, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "axes[2].axhline(cv_scores_mae.mean(), color='red', linestyle='--', label=f'Mean: {cv_scores_mae.mean():.4f}')\n",
    "axes[2].set_xlabel('Fold')\n",
    "axes[2].set_ylabel('MAE')\n",
    "axes[2].set_title('Validation Crois√©e - MAE')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning avec GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid de param√®tres √† tester\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_depth': [10, 20, None],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GRID SEARCH CV\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNombre de combinaisons: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "print(f\"Param√®tres √† tester:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline_rf,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nD√©marrage du Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nGrid Search termin√© !\")\n",
    "print(f\"\\nMeilleurs param√®tres:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nMeilleur score (R¬≤ CV): {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©ration du meilleur mod√®le\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# √âvaluation sur le test set\n",
    "y_pred_test = best_pipeline.predict(X_test)\n",
    "\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"√âVALUATION SUR LE TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"R¬≤ Score:  {r2_test:.4f}\")\n",
    "print(f\"RMSE:      {rmse_test:.4f} (${rmse_test * 100:.0f}k)\")\n",
    "print(f\"MAE:       {mae_test:.4f} (${mae_test * 100:.0f}k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des pr√©dictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot: Valeurs r√©elles vs pr√©dites\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Valeurs R√©elles')\n",
    "axes[0].set_ylabel('Valeurs Pr√©dites')\n",
    "axes[0].set_title(f'Pr√©dictions vs R√©alit√© (R¬≤ = {r2_test:.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogramme des r√©sidus\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('R√©sidus')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "axes[1].set_title(f'Distribution des R√©sidus (Mean: {residuals.mean():.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction du mod√®le RandomForest du pipeline\n",
    "rf_model = best_pipeline.named_steps['regressor']\n",
    "\n",
    "# Feature importance (apr√®s preprocessing)\n",
    "# Note: le preprocessor transforme les features, donc les noms ne correspondent plus exactement\n",
    "feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Approximation des noms de features (numeric + one-hot encoded categorical)\n",
    "feature_names_approx = numeric_features.copy()\n",
    "if len(categorical_features) > 0:\n",
    "    # OneHotEncoder cr√©e de nouvelles features\n",
    "    cat_encoder = best_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_feature_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "    feature_names_approx.extend(cat_feature_names)\n",
    "\n",
    "# Tri par importance\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "top_n = 15\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"TOP {top_n} FEATURES LES PLUS IMPORTANTES\")\n",
    "print(\"=\" * 60)\n",
    "for i, idx in enumerate(indices[:top_n], 1):\n",
    "    feature_name = feature_names_approx[idx] if idx < len(feature_names_approx) else f\"Feature_{idx}\"\n",
    "    print(f\"{i:2d}. {feature_name:30s} : {feature_importance[idx]:.4f}\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = [feature_names_approx[i] if i < len(feature_names_approx) else f\"Feature_{i}\" for i in indices[:top_n]]\n",
    "top_importance = feature_importance[indices[:top_n]]\n",
    "\n",
    "plt.barh(range(top_n), top_importance, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.yticks(range(top_n), top_features)\n",
    "plt.xlabel('Importance')\n",
    "plt.title(f'Top {top_n} Features - Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Persistence : Sauvegarde du Mod√®le\n",
    "\n",
    "Sauvegarde du pipeline complet (preprocessing + model) pour production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Cr√©ation du dossier de sauvegarde\n",
    "model_dir = '/tmp/models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Sauvegarde avec joblib (recommand√© pour scikit-learn)\n",
    "model_path_joblib = os.path.join(model_dir, 'housing_pipeline.joblib')\n",
    "joblib.dump(best_pipeline, model_path_joblib)\n",
    "print(f\"Mod√®le sauvegard√© (joblib): {model_path_joblib}\")\n",
    "print(f\"  Taille: {os.path.getsize(model_path_joblib) / 1024:.2f} KB\")\n",
    "\n",
    "# Sauvegarde avec pickle (alternative)\n",
    "model_path_pickle = os.path.join(model_dir, 'housing_pipeline.pkl')\n",
    "with open(model_path_pickle, 'wb') as f:\n",
    "    pickle.dump(best_pipeline, f)\n",
    "print(f\"\\nMod√®le sauvegard√© (pickle): {model_path_pickle}\")\n",
    "print(f\"  Taille: {os.path.getsize(model_path_pickle) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du mod√®le depuis le disque\n",
    "loaded_pipeline = joblib.load(model_path_joblib)\n",
    "\n",
    "# Test de pr√©diction avec le mod√®le charg√©\n",
    "sample_data = X_test.head(5)\n",
    "predictions = loaded_pipeline.predict(sample_data)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST DU MOD√àLE CHARG√â\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n√âchantillon de donn√©es:\")\n",
    "print(sample_data)\n",
    "print(\"\\nPr√©dictions:\")\n",
    "for i, (true_val, pred_val) in enumerate(zip(y_test.head(5), predictions), 1):\n",
    "    print(f\"  {i}. True: ${true_val * 100:.0f}k, Predicted: ${pred_val * 100:.0f}k, Error: ${(true_val - pred_val) * 100:+.0f}k\")\n",
    "\n",
    "print(\"\\nMod√®le charg√© et test√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparaison avec d'Autres Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avec Ridge et GradientBoosting\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Ridge': Ridge(alpha=1.0)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARAISON DES MOD√àLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Pipeline avec preprocessing\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # M√©triques\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'R¬≤': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  R¬≤:   {r2:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "\n",
    "# DataFrame de r√©sultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la comparaison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].bar(results_df['Model'], results_df['R¬≤'], color=['steelblue', 'coral', 'lightgreen'], \n",
    "            edgecolor='black', alpha=0.7)\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].set_title('Comparaison R¬≤')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "axes[1].bar(results_df['Model'], results_df['RMSE'], color=['steelblue', 'coral', 'lightgreen'], \n",
    "            edgecolor='black', alpha=0.7)\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Comparaison RMSE (plus bas = mieux)')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "axes[2].bar(results_df['Model'], results_df['MAE'], color=['steelblue', 'coral', 'lightgreen'], \n",
    "            edgecolor='black', alpha=0.7)\n",
    "axes[2].set_ylabel('MAE')\n",
    "axes[2].set_title('Comparaison MAE (plus bas = mieux)')\n",
    "axes[2].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "### Points Cl√©s du Pipeline\n",
    "\n",
    "1. **EDA** : Comprendre les donn√©es avant de mod√©liser\n",
    "2. **Feature Engineering** : Cr√©er des features pertinentes\n",
    "3. **Pipeline scikit-learn** : Encapsulation preprocessing + model\n",
    "   - √âvite les data leaks\n",
    "   - Facilite la production\n",
    "   - Reproductible\n",
    "4. **Validation Crois√©e** : √âvaluation robuste des performances\n",
    "5. **Hyperparameter Tuning** : Optimisation syst√©matique\n",
    "6. **Persistence** : Sauvegarde avec joblib/pickle\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Toujours faire un split train/test **avant** toute transformation\n",
    "- Utiliser des Pipelines pour encapsuler toutes les √©tapes\n",
    "- Valider avec cross-validation, pas seulement train/test\n",
    "- Sauvegarder le pipeline complet, pas seulement le mod√®le\n",
    "- Versionner les mod√®les et les donn√©es\n",
    "- Documenter les choix de preprocessing et d'hyperparam√®tres\n",
    "\n",
    "### Fichiers G√©n√©r√©s\n",
    "\n",
    "- `/tmp/models/housing_pipeline.joblib` : Pipeline complet (recommand√©)\n",
    "- `/tmp/models/housing_pipeline.pkl` : Pipeline complet (alternative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}