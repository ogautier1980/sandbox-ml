\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}
\usepackage{amsmath, amssymb}
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, pdftitle={Chapitre 14 - Best Practices ML}}
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 11 - Best Practices ML}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    basicstyle=\ttfamily\small,
    breaklines=true,
    numbers=left,
    frame=single,
}
\lstset{style=pythonstyle}

\newtcolorbox{astuce}{colback=yellow!10!white, colframe=yellow!75!black, fonttitle=\bfseries, title=üí° Bonne Pratique, breakable}
\newtcolorbox{attention}{colback=red!5!white, colframe=red!75!black, fonttitle=\bfseries, title=‚ö†Ô∏è √Ä √âviter, breakable}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]
    \vspace{1cm}
    {\LARGE Chapitre 11}\\[0.3cm]
    {\LARGE\bfseries Best Practices en Machine Learning}\\[2cm]
    \vfill
    {\large
    \textbf{Objectifs :}\\[0.5cm]
    \begin{itemize}
        \item Ma√Ætriser le workflow ML de bout en bout
        \item √âviter les pi√®ges courants
        \item Optimiser performances et reproductibilit√©
        \item D√©ployer des mod√®les en production
    \end{itemize}
    }
    \vfill
    {\large Cours ML - Sandbox-ML\\ Version 1.0 - 2026}
\end{titlepage}

\tableofcontents
\newpage

\section{Workflow ML}

\subsection{Pipeline standard}

\begin{enumerate}
    \item \textbf{D√©finir le probl√®me} : Classification, r√©gression, clustering ?
    \item \textbf{Collecter les donn√©es} : Qualit√© > Quantit√©
    \item \textbf{EDA (Exploratory Data Analysis)} : Visualisations, statistiques
    \item \textbf{Preprocessing} : Nettoyage, features engineering
    \item \textbf{Train/Val/Test Split} : 60/20/20 ou 70/15/15
    \item \textbf{Baseline Model} : Mod√®le simple de r√©f√©rence
    \item \textbf{It√©ration} : Mod√®les plus complexes, hyperparameter tuning
    \item \textbf{√âvaluation finale} : Test set (une seule fois !)
    \item \textbf{D√©ploiement} : API, monitoring
\end{enumerate}

\subsection{Checklist avant entra√Ænement}

\begin{itemize}
    \item[$\square$] Donn√©es √©quilibr√©es (ou stratifi√©es) ?
    \item[$\square$] Features normalis√©es/standardis√©es ?
    \item[$\square$] Train/val/test splits fix√©s avec random seed ?
    \item[$\square$] Baseline d√©fini ?
    \item[$\square$] M√©trique d'√©valuation choisie ?
\end{itemize}

\section{Traitement des Donn√©es}

\subsection{Nettoyage}

\begin{astuce}
\textbf{Valeurs manquantes :}
\begin{itemize}
    \item Num√©riques : Imputation m√©diane/moyenne, ou indicateur manquant
    \item Cat√©gorielles : Mode ou cat√©gorie "Unknown"
    \item Supprimer si < 5\% de donn√©es manquantes
\end{itemize}
\end{astuce}

\begin{lstlisting}[caption=Gestion des NaN]
import pandas as pd
from sklearn.impute import SimpleImputer

# Imputation num√©rique
imputer_num = SimpleImputer(strategy='median')
df[num_cols] = imputer_num.fit_transform(df[num_cols])

# Imputation cat√©gorielle
imputer_cat = SimpleImputer(strategy='most_frequent')
df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])
\end{lstlisting}

\subsection{Feature Scaling}

\begin{table}[h]
\centering
\caption{M√©thodes de scaling}
\begin{tabular}{lll}
\toprule
\textbf{M√©thode} & \textbf{Formule} & \textbf{Quand} \\
\midrule
StandardScaler & $(x - \mu) / \sigma$ & Gaussian-like, SVM, NN \\
MinMaxScaler & $(x - \min) / (\max - \min)$ & Features born√©es [0,1] \\
RobustScaler & $(x - \text{median}) / \text{IQR}$ & Pr√©sence outliers \\
\bottomrule
\end{tabular}
\end{table}

\begin{attention}
\textbf{Erreur Data Leakage :} Toujours fit le scaler sur le train set uniquement !
\begin{lstlisting}
# MAUVAIS
scaler.fit(X)  # Contient test set !
X_train_scaled = scaler.transform(X_train)

# BON
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
\end{lstlisting}
\end{attention}

\subsection{Feature Engineering}

\begin{itemize}
    \item \textbf{Encoding cat√©gorielles}
    \begin{itemize}
        \item One-Hot : $< 10$ cat√©gories
        \item Label Encoding : Arbres de d√©cision
        \item Target Encoding : High cardinality
    \end{itemize}

    \item \textbf{Interactions} : $x_1 \times x_2$, $x^2$

    \item \textbf{Features temporelles} : Jour semaine, mois, heure

    \item \textbf{Aggregations} : Mean, std, count par groupe
\end{itemize}

\section{S√©lection de Mod√®le}

\subsection{Guide de choix}

\begin{table}[h]
\centering
\caption{Quel algorithme choisir ?}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Contexte} & \textbf{Algorithme recommand√©} \\
\midrule
Petites donn√©es (< 1K) & Logistic Regression, SVM \\
Donn√©es tabulaires moyennes & Random Forest, XGBoost \\
Images & CNN (ResNet, EfficientNet) \\
Texte & Transformers (BERT, GPT) \\
S√©ries temporelles & LSTM, Transformers \\
Interpr√©tabilit√© requise & Arbres de d√©cision, R√©gression lin√©aire \\
Temps r√©el critique & Mod√®les l√©gers (LR, Small NN) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Baseline Models}

Toujours commencer par un mod√®le simple :
\begin{itemize}
    \item \textbf{Classification} : Logistic Regression, Dummy Classifier
    \item \textbf{R√©gression} : Linear Regression, Mean Predictor
\end{itemize}

\begin{lstlisting}[caption=Baseline simple]
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Baseline
baseline = LogisticRegression(max_iter=1000)
baseline.fit(X_train, y_train)
y_pred = baseline.predict(X_test)
print(f"Baseline Accuracy: {accuracy_score(y_test, y_pred):.3f}")
\end{lstlisting}

\section{Validation et M√©triques}

\subsection{Cross-Validation}

\begin{astuce}
Utiliser \textbf{StratifiedKFold} pour classification d√©s√©quilibr√©e.
\begin{lstlisting}
from sklearn.model_selection import cross_val_score, StratifiedKFold

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')
print(f"CV F1: {scores.mean():.3f} +/- {scores.std():.3f}")
\end{lstlisting}
\end{astuce}

\subsection{Choix de m√©trique}

\begin{table}[h]
\centering
\caption{M√©triques selon le contexte}
\begin{tabular}{lp{7cm}}
\toprule
\textbf{Contexte} & \textbf{M√©trique} \\
\midrule
Classes √©quilibr√©es & Accuracy \\
Classes d√©s√©quilibr√©es & F1-score, AUC-ROC \\
Faux n√©gatifs co√ªteux & Recall \\
Faux positifs co√ªteux & Precision \\
R√©gression & MSE, MAE, $R^2$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Hyperparameter Tuning}

\subsection{Strat√©gies}

\begin{enumerate}
    \item \textbf{Grid Search} : Exhaustif, lent
    \item \textbf{Random Search} : Plus efficace (Bergstra \& Bengio 2012)
    \item \textbf{Bayesian Optimization} : Optuna, Hyperopt
\end{enumerate}

\begin{lstlisting}[caption=Random Search]
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

param_dist = {
    'max_depth': randint(3, 20),
    'n_estimators': randint(50, 500),
    'learning_rate': uniform(0.01, 0.3)
}

search = RandomizedSearchCV(
    model, param_dist, n_iter=50, cv=3,
    scoring='f1', random_state=42, n_jobs=-1
)

search.fit(X_train, y_train)
print(f"Best params: {search.best_params_}")
print(f"Best CV score: {search.best_score_:.3f}")
\end{lstlisting}

\section{Overfitting et R√©gularisation}

\subsection{Diagnostic}

\begin{itemize}
    \item \textbf{Underfitting} : Train error ET val error √©lev√©s ‚Üí Mod√®le trop simple
    \item \textbf{Overfitting} : Train error faible, val error √©lev√© ‚Üí Mod√®le trop complexe
    \item \textbf{Good fit} : Train error $\approx$ val error, tous deux faibles
\end{itemize}

\subsection{Solutions overfitting}

\begin{astuce}
\textbf{Arsenal anti-overfitting :}
\begin{enumerate}
    \item Plus de donn√©es (data augmentation)
    \item R√©gularisation L1/L2
    \item Dropout (NN)
    \item Early stopping
    \item R√©duire complexit√© mod√®le
    \item Ensembling
\end{enumerate}
\end{astuce}

\section{D√©ploiement}

\subsection{Sauvegarder le mod√®le}

\begin{lstlisting}[caption=Persistence avec joblib]
import joblib

# Sauvegarder
joblib.dump(model, 'model.pkl')
joblib.dump(scaler, 'scaler.pkl')

# Charger
model = joblib.load('model.pkl')
scaler = joblib.load('scaler.pkl')
\end{lstlisting}

\subsection{API REST avec FastAPI}

\begin{lstlisting}[caption=API simple]
from fastapi import FastAPI
import joblib
import numpy as np

app = FastAPI()
model = joblib.load('model.pkl')

@app.post("/predict")
def predict(features: list[float]):
    X = np.array(features).reshape(1, -1)
    prediction = model.predict(X)[0]
    proba = model.predict_proba(X)[0].tolist()
    return {"prediction": int(prediction), "probabilities": proba}

# Lancer : uvicorn main:app --reload
\end{lstlisting}

\subsection{Monitoring}

\begin{itemize}
    \item Tracker performance en production (drift)
    \item Logger pr√©dictions et features
    \item Alertes si d√©gradation
    \item R√©entra√Æner p√©riodiquement
\end{itemize}

\section{Reproductibilit√©}

\subsection{Random seeds}

\begin{lstlisting}[caption=Fixer tous les seeds]
import random
import numpy as np
import torch

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
\end{lstlisting}

\subsection{Version control}

\begin{itemize}
    \item Code : Git
    \item Donn√©es : DVC (Data Version Control)
    \item Mod√®les : MLflow, Weights \& Biases
    \item Environnement : Docker, requirements.txt, conda env
\end{itemize}

\section{Checklist Production}

\begin{itemize}
    \item[$\square$] Mod√®le valid√© sur test set ind√©pendant
    \item[$\square$] Pipeline complet (preprocessing + model) sauvegard√©
    \item[$\square$] Tests unitaires sur preprocessing
    \item[$\square$] Logging configur√©
    \item[$\square$] Monitoring des performances
    \item[$\square$] Documentation : features, m√©triques, limites
    \item[$\square$] Fallback si mod√®le √©choue
    \item[$\square$] Plan de r√©entra√Ænement
\end{itemize}

\section{Pi√®ges courants}

\subsection{Data Leakage}

\begin{attention}
\textbf{Exemples de leakage :}
\begin{itemize}
    \item Scaler fit sur toutes les donn√©es (inclut test)
    \item Features d√©riv√©es du target
    \item Features du futur dans s√©ries temporelles
    \item Duplicatas entre train et test
\end{itemize}
\end{attention}

\subsection{Optimisation sur test set}

\begin{attention}
\textbf{NE JAMAIS} :
\begin{itemize}
    \item Tuner hyperparam√®tres sur test set
    \item Regarder test set avant validation finale
    \item R√©entra√Æner apr√®s avoir vu test performance
\end{itemize}

Le test set doit rester \textbf{inviol√©} jusqu'√† l'√©valuation finale !
\end{attention}

\subsection{S√©lection de features sur tout le dataset}

S√©lectionner features UNIQUEMENT sur train set, puis appliquer √† val/test.

\section{Ressources}

\subsection{Outils}

\begin{itemize}
    \item \textbf{scikit-learn} : ML classique
    \item \textbf{PyTorch/TensorFlow} : Deep Learning
    \item \textbf{XGBoost/LightGBM/CatBoost} : Gradient Boosting
    \item \textbf{Optuna} : Hyperparameter tuning
    \item \textbf{MLflow} : Experiment tracking
    \item \textbf{FastAPI} : API deployment
    \item \textbf{Docker} : Containerization
\end{itemize}

\subsection{Lectures}

\begin{itemize}
    \item G√©ron - \textit{Hands-On Machine Learning}
    \item Hastie et al. - \textit{The Elements of Statistical Learning}
    \item Chollet - \textit{Deep Learning with Python}
\end{itemize}

\section{R√©sum√©}

\subsection{R√®gles d'or}

\begin{enumerate}
    \item Toujours commencer simple (baseline)
    \item Train/val/test splits stricts
    \item Pas de data leakage
    \item Cross-validation syst√©matique
    \item M√©triques adapt√©es au probl√®me
    \item Test set : une seule √©valuation finale
    \item Reproductibilit√© (seeds, versions)
    \item Monitoring en production
\end{enumerate}

\subsection{Workflow r√©sum√©}

\begin{equation*}
\boxed{\text{EDA}} \to \boxed{\text{Preprocessing}} \to \boxed{\text{Baseline}} \to \boxed{\text{Tuning}} \to \boxed{\text{Test}} \to \boxed{\text{Deploy}}
\end{equation*}

\section{Notebooks Pratiques}

Ce chapitre est accompagn√© des notebooks suivants :

\begin{itemize}
    \item \texttt{14_demo_pipeline_complet.ipynb} : Pipeline ML de bout en bout
    \begin{itemize}
        \item Exploration des donn√©es (EDA)
        \item Feature engineering et preprocessing
        \item S√©lection de mod√®le avec validation crois√©e
        \item Pipeline scikit-learn complet
    \end{itemize}

    \item \texttt{14_demo_deployment_fastapi.ipynb} : D√©ploiement avec FastAPI
    \begin{itemize}
        \item Cr√©ation d'une API REST
        \item Validation avec Pydantic
        \item Conteneurisation Docker
        \item Tests et documentation automatique
    \end{itemize}

    \item \texttt{14_demo_monitoring_mlflow.ipynb} : Tracking et monitoring avec MLflow
    \begin{itemize}
        \item Configuration MLflow Tracking Server
        \item Logging des hyperparam√®tres et m√©triques
        \item Model Registry
        \item Comparaison de runs et reproductibilit√©
    \end{itemize}
\end{itemize}

\vfill

\begin{center}
\Large\textbf{Fin du Cours Machine Learning}\\[0.5cm]
\large F√©licitations ! Vous ma√Ætrisez maintenant les fondamentaux et techniques avanc√©es du ML.\\[0.3cm]
Continuez √† pratiquer sur des projets r√©els et √† vous tenir √† jour avec les derni√®res avanc√©es.
\end{center}

\end{document}
