{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/01_fondamentaux_mathematiques/01_exercices_solutions.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('üì¶ Installation des packages...')\n",
    "    \n",
    "    # Packages ML de base\n",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn\n",
    "    \n",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques\n",
    "    notebook_name = '01_exercices.ipynb'  # Sera remplac√© automatiquement\n",
    "    \n",
    "    # Ch 06-08 : Deep Learning\n",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):\n",
    "        !pip install -q torch torchvision torchaudio\n",
    "    \n",
    "    # Ch 08 : NLP\n",
    "    if '08_' in notebook_name:\n",
    "        !pip install -q transformers datasets tokenizers\n",
    "        if 'rag' in notebook_name:\n",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25\n",
    "    \n",
    "    # Ch 09 : Reinforcement Learning\n",
    "    if '09_' in notebook_name:\n",
    "        !pip install -q gymnasium[classic-control]\n",
    "    \n",
    "    # Ch 04 : Boosting\n",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:\n",
    "        !pip install -q xgboost lightgbm catboost\n",
    "    \n",
    "    # Ch 05 : Clustering avanc√©\n",
    "    if '05_' in notebook_name:\n",
    "        !pip install -q umap-learn\n",
    "    \n",
    "    # Ch 11 : S√©ries temporelles\n",
    "    if '11_' in notebook_name:\n",
    "        !pip install -q statsmodels prophet\n",
    "    \n",
    "    # Ch 12 : Vision avanc√©e\n",
    "    if '12_' in notebook_name:\n",
    "        !pip install -q ultralytics timm segmentation-models-pytorch\n",
    "    \n",
    "    # Ch 13 : Recommandation\n",
    "    if '13_' in notebook_name:\n",
    "        !pip install -q scikit-surprise implicit\n",
    "    \n",
    "    # Ch 14 : MLOps\n",
    "    if '14_' in notebook_name:\n",
    "        !pip install -q mlflow fastapi pydantic\n",
    "    \n",
    "    print('‚úÖ Installation termin√©e !')\n",
    "else:\n",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 01 - Solutions : Fondamentaux Math√©matiques\n",
    "\n",
    "Ces exercices couvrent l'alg√®bre lin√©aire, les probabilit√©s et l'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Alg√®bre Lin√©aire\n",
    "\n",
    "### Exercice 1.1 : Produit Scalaire et Orthogonalit√©\n",
    "\n",
    "Calculer le produit scalaire de u = (1, 2, 3) et v = (4, -1, 2). Les vecteurs sont-ils orthogonaux ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "u = np.array([1, 2, 3])\n",
    "v = np.array([4, -1, 2])\n",
    "\n",
    "# Calculer le produit scalaire : u ¬∑ v = u1*v1 + u2*v2 + u3*v3\n",
    "dot_product = np.dot(u, v)  # Ou u @ v\n",
    "\n",
    "print(f\"Produit scalaire: {dot_product}\")\n",
    "print(f\"Orthogonaux? {dot_product == 0}\")\n",
    "\n",
    "# Calcul manuel : 1*4 + 2*(-1) + 3*2 = 4 - 2 + 6 = 8\n",
    "# Les vecteurs ne sont PAS orthogonaux car leur produit scalaire ‚â† 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.2 : Valeurs Propres et Diagonalisation\n",
    "\n",
    "Pour la matrice A = [[2, 1], [1, 3]], calculer :\n",
    "1. Les valeurs propres et vecteurs propres\n",
    "2. La d√©composition spectrale A = P @ D @ P^(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "A = np.array([[2, 1],\n",
    "              [1, 3]])\n",
    "\n",
    "# Calculer valeurs/vecteurs propres\n",
    "# np.linalg.eig retourne (valeurs_propres, vecteurs_propres)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Valeurs propres:\", eigenvalues)\n",
    "print(\"Vecteurs propres:\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# V√©rifier la d√©composition A = P @ D @ P^(-1)\n",
    "P = eigenvectors  # Matrice des vecteurs propres (colonnes)\n",
    "D = np.diag(eigenvalues)  # Matrice diagonale des valeurs propres\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Reconstruction de A\n",
    "A_reconstructed = P @ D @ P_inv\n",
    "\n",
    "print(\"\\nMatrice A originale:\")\n",
    "print(A)\n",
    "print(\"\\nMatrice A reconstruite:\")\n",
    "print(A_reconstructed)\n",
    "print(\"\\nErreur de reconstruction:\", np.linalg.norm(A - A_reconstructed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.3 : SVD\n",
    "\n",
    "Appliquer la SVD √† la matrice A = [[3, 1], [1, 3], [1, 1]] et reconstruire A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "A = np.array([[3, 1],\n",
    "              [1, 3],\n",
    "              [1, 1]])\n",
    "\n",
    "# SVD : A = U @ Œ£ @ V^T\n",
    "# Pour une matrice m√ón : U est m√óm, Œ£ est m√ón (diagonale), V^T est n√ón\n",
    "U, Sigma, VT = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "print(\"U shape:\", U.shape)  # (3, 3)\n",
    "print(\"Sigma:\", Sigma)  # Valeurs singuli√®res (vecteur de taille min(m,n))\n",
    "print(\"VT shape:\", VT.shape)  # (2, 2)\n",
    "\n",
    "# Pour reconstruire A, il faut cr√©er la matrice Sigma compl√®te (3x2)\n",
    "Sigma_full = np.zeros((U.shape[0], VT.shape[0]))\n",
    "Sigma_full[:len(Sigma), :len(Sigma)] = np.diag(Sigma)\n",
    "\n",
    "print(\"\\nSigma_full shape:\", Sigma_full.shape)\n",
    "print(\"Sigma_full:\")\n",
    "print(Sigma_full)\n",
    "\n",
    "# Reconstruire A\n",
    "A_reconstructed = U @ Sigma_full @ VT\n",
    "\n",
    "print(\"\\nMatrice A originale:\")\n",
    "print(A)\n",
    "print(\"\\nMatrice A reconstruite:\")\n",
    "print(A_reconstructed)\n",
    "print(\"\\nErreur de reconstruction:\", np.linalg.norm(A - A_reconstructed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.4 : Moindres Carr√©s\n",
    "\n",
    "R√©soudre le syst√®me lin√©aire surd√©termin√© au sens des moindres carr√©s :\n",
    "```\n",
    "[[1, 1],     [[a],      [[2],\n",
    " [1, 2],  @   [b]]  ‚âà    [3],\n",
    " [1, 3]]                 [5]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "X = np.array([[1, 1],\n",
    "              [1, 2],\n",
    "              [1, 3]])\n",
    "y = np.array([2, 3, 5])\n",
    "\n",
    "# Solution par √©quation normale : w = (X^T X)^(-1) X^T y\n",
    "# Cette formule minimise ||Xw - y||¬≤\n",
    "w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "# Alternative : utiliser np.linalg.lstsq (plus stable num√©riquement)\n",
    "# w, residuals, rank, s = np.linalg.lstsq(X, y, rcond=None)\n",
    "\n",
    "print(\"Solution [a, b]:\", w)\n",
    "\n",
    "# V√©rifier\n",
    "y_pred = X @ w\n",
    "print(\"\\nValeurs r√©elles y:\", y)\n",
    "print(\"Pr√©dictions y_pred:\", y_pred)\n",
    "print(\"\\nErreur MSE:\", np.mean((y_pred - y)**2))\n",
    "print(\"R√©sidus:\", y - y_pred)\n",
    "\n",
    "# Interpr√©tation : on cherche la droite y ‚âà a + b*x qui passe le plus proche des points\n",
    "# (1, 2), (2, 3), (3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Probabilit√©s et Statistiques\n",
    "\n",
    "### Exercice 2.1 : Th√©or√®me de Bayes\n",
    "\n",
    "Un test m√©dical d√©tecte une maladie avec 99% de pr√©cision (vrai positif).  \n",
    "La pr√©valence de la maladie est 0.1%.  \n",
    "Si le test est positif, quelle est la probabilit√© d'√™tre r√©ellement malade ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "P_disease = 0.001  # Pr√©valence P(Maladie)\n",
    "P_pos_given_disease = 0.99  # Sensibilit√© P(Test+ | Maladie)\n",
    "P_pos_given_healthy = 0.01  # Faux positif P(Test+ | Sain) = 1 - sp√©cificit√©\n",
    "\n",
    "# Calculer P(Test+) par la loi de probabilit√© totale\n",
    "# P(Test+) = P(Test+ | Maladie) * P(Maladie) + P(Test+ | Sain) * P(Sain)\n",
    "P_healthy = 1 - P_disease\n",
    "P_pos = P_pos_given_disease * P_disease + P_pos_given_healthy * P_healthy\n",
    "\n",
    "# Th√©or√®me de Bayes : P(Maladie | Test+) = P(Test+ | Maladie) * P(Maladie) / P(Test+)\n",
    "P_disease_given_pos = (P_pos_given_disease * P_disease) / P_pos\n",
    "\n",
    "print(f\"P(Test+) = {P_pos*100:.3f}%\")\n",
    "print(f\"P(Maladie | Test+) = {P_disease_given_pos*100:.2f}%\")\n",
    "print(f\"\\nContre-intuitif ! Malgr√© un test tr√®s pr√©cis (99%), il n'y a que ~9% de chance\")\n",
    "print(f\"d'√™tre r√©ellement malade si le test est positif, √† cause de la faible pr√©valence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.2 : Loi Normale\n",
    "\n",
    "Pour X ~ N(10, 4), calculer P(8 ‚â§ X ‚â§ 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "mu, sigma = 10, 2  # sigma = sqrt(variance) = sqrt(4) = 2\n",
    "rv = stats.norm(mu, sigma)\n",
    "\n",
    "# Calculer P(8 <= X <= 12) = CDF(12) - CDF(8)\n",
    "prob = rv.cdf(12) - rv.cdf(8)\n",
    "\n",
    "print(f\"P(8 ‚â§ X ‚â§ 12) = {prob:.4f}\")\n",
    "print(f\"\\nIntervalle [Œº-œÉ, Œº+œÉ] = [8, 12] contient environ 68% de la masse (r√®gle empirique)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.3 : G√©n√©ration et Statistiques\n",
    "\n",
    "G√©n√©rer 1000 √©chantillons d'une loi normale N(5, 2).  \n",
    "Calculer la moyenne et variance empiriques.  \n",
    "Tracer l'histogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "mu_true, sigma_true = 5, np.sqrt(2)\n",
    "\n",
    "# G√©n√©rer √©chantillons\n",
    "samples = np.random.normal(mu_true, sigma_true, size=1000)\n",
    "\n",
    "# Statistiques empiriques\n",
    "mean_empirical = np.mean(samples)\n",
    "var_empirical = np.var(samples, ddof=1)  # ddof=1 pour variance non biais√©e\n",
    "\n",
    "print(f\"Moyenne empirique: {mean_empirical:.3f} (th√©orique: {mu_true})\")\n",
    "print(f\"Variance empirique: {var_empirical:.3f} (th√©orique: {sigma_true**2:.3f})\")\n",
    "print(f\"\\nLa loi des grands nombres garantit que les estimations convergent\")\n",
    "print(f\"vers les vraies valeurs quand le nombre d'√©chantillons augmente.\")\n",
    "\n",
    "# Histogramme\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(samples, bins=40, density=True, alpha=0.7, edgecolor='black', label='Histogramme empirique')\n",
    "\n",
    "# Superposer la PDF th√©orique\n",
    "x = np.linspace(samples.min(), samples.max(), 100)\n",
    "plt.plot(x, stats.norm(mu_true, sigma_true).pdf(x), 'r-', linewidth=2, label='PDF th√©orique')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Densit√©')\n",
    "plt.title('Histogramme vs PDF Th√©orique')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.4 : Matrice de Covariance\n",
    "\n",
    "Calculer la matrice de covariance pour les donn√©es :\n",
    "```\n",
    "X = [[1, 2],\n",
    "     [2, 4],\n",
    "     [3, 5]]\n",
    "```\n",
    "Interpr√©ter la corr√©lation entre les deux variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "X = np.array([[1, 2],\n",
    "              [2, 4],\n",
    "              [3, 5]])\n",
    "\n",
    "# Matrice de covariance\n",
    "# rowvar=False car les colonnes repr√©sentent les variables\n",
    "cov_matrix = np.cov(X, rowvar=False)\n",
    "\n",
    "print(\"Matrice de covariance:\")\n",
    "print(cov_matrix)\n",
    "print(f\"\\nCov(X1, X1) = Var(X1) = {cov_matrix[0, 0]:.3f}\")\n",
    "print(f\"Cov(X2, X2) = Var(X2) = {cov_matrix[1, 1]:.3f}\")\n",
    "print(f\"Cov(X1, X2) = {cov_matrix[0, 1]:.3f}\")\n",
    "\n",
    "# Matrice de corr√©lation (normalise par les √©carts-types)\n",
    "corr_matrix = np.corrcoef(X, rowvar=False)\n",
    "print(\"\\nMatrice de corr√©lation:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "print(f\"\\nCorr√©lation entre var1 et var2: {corr_matrix[0, 1]:.3f}\")\n",
    "print(\"\\nInterpr√©tation:\")\n",
    "print(\"- Corr√©lation tr√®s proche de 1 : forte corr√©lation lin√©aire positive\")\n",
    "print(\"- Les deux variables √©voluent ensemble de mani√®re quasi-parfaite\")\n",
    "print(\"- Quand X1 augmente, X2 augmente proportionnellement\")\n",
    "print(\"- Cela sugg√®re une relation lin√©aire forte (ici, environ X2 ‚âà 2*X1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : Calcul Diff√©rentiel et Optimisation\n",
    "\n",
    "### Exercice 3.1 : Calcul de Gradient\n",
    "\n",
    "Calculer le gradient de f(x‚ÇÅ, x‚ÇÇ) = x‚ÇÅ¬≤ + 2x‚ÇÅx‚ÇÇ + 3x‚ÇÇ¬≤  \n",
    "√âvaluer en (1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def f(x1, x2):\n",
    "    \"\"\"Fonction √† optimiser\"\"\"\n",
    "    return x1**2 + 2*x1*x2 + 3*x2**2\n",
    "\n",
    "def grad_f(x1, x2):\n",
    "    \"\"\"Gradient analytique de f\"\"\"\n",
    "    # ‚àÇf/‚àÇx1 = 2x1 + 2x2\n",
    "    # ‚àÇf/‚àÇx2 = 2x1 + 6x2\n",
    "    grad_x1 = 2*x1 + 2*x2\n",
    "    grad_x2 = 2*x1 + 6*x2\n",
    "    return np.array([grad_x1, grad_x2])\n",
    "\n",
    "# √âvaluer en (1, 1)\n",
    "grad_at_1_1 = grad_f(1, 1)\n",
    "print(f\"‚àáf(1, 1) = {grad_at_1_1}\")\n",
    "print(f\"\\nLe gradient pointe dans la direction de plus forte augmentation de f.\")\n",
    "print(f\"Pour minimiser f, on se d√©place dans la direction oppos√©e : -‚àáf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.2 : Convexit√©\n",
    "\n",
    "Montrer que f(x) = ||Ax - b||‚ÇÇ¬≤ est convexe.  \n",
    "Calculer son gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution th√©orique et impl√©mentation\n",
    "\n",
    "# TH√âORIE :\n",
    "# f(x) = ||Ax - b||‚ÇÇ¬≤ = (Ax - b)^T (Ax - b)\n",
    "#      = x^T A^T A x - 2b^T A x + b^T b\n",
    "#\n",
    "# Gradient : ‚àáf(x) = 2 A^T (Ax - b)\n",
    "# \n",
    "# Hessienne : H = ‚àá¬≤f(x) = 2 A^T A\n",
    "#\n",
    "# Pour toute matrice A, A^T A est semi-d√©finie positive car :\n",
    "# ‚àÄz : z^T (A^T A) z = (Az)^T (Az) = ||Az||¬≤ ‚â• 0\n",
    "#\n",
    "# Donc la Hessienne est semi-d√©finie positive ‚üπ f est convexe !\n",
    "\n",
    "def f_least_squares(x, A, b):\n",
    "    \"\"\"Fonction des moindres carr√©s\"\"\"\n",
    "    residual = A @ x - b\n",
    "    return np.dot(residual, residual)\n",
    "\n",
    "def grad_f_least_squares(x, A, b):\n",
    "    \"\"\"Gradient de la fonction des moindres carr√©s\"\"\"\n",
    "    return 2 * A.T @ (A @ x - b)\n",
    "\n",
    "# Tester\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "b = np.array([1, 2, 3])\n",
    "x = np.array([0.5, 0.5])\n",
    "\n",
    "print(\"f(x):\", f_least_squares(x, A, b))\n",
    "print(\"‚àáf(x):\", grad_f_least_squares(x, A, b))\n",
    "\n",
    "# V√©rifier la convexit√© en calculant la Hessienne\n",
    "H = 2 * A.T @ A\n",
    "print(\"\\nHessienne H = 2 A^T A:\")\n",
    "print(H)\n",
    "\n",
    "# V√©rifier que H est semi-d√©finie positive (valeurs propres ‚â• 0)\n",
    "eigenvalues = np.linalg.eigvalsh(H)\n",
    "print(\"\\nValeurs propres de H:\", eigenvalues)\n",
    "print(\"Toutes positives ?\", np.all(eigenvalues >= -1e-10))  # Tol√©rance num√©rique\n",
    "print(\"\\n‚úì La fonction est bien convexe !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.3 : Descente de Gradient Simple\n",
    "\n",
    "Minimiser f(x) = (x - 3)¬≤ + 5 en partant de x‚ÇÄ = 0 par descente de gradient.  \n",
    "Tester diff√©rents learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def f(x):\n",
    "    \"\"\"Fonction √† minimiser : parabole centr√©e en x=3\"\"\"\n",
    "    return (x - 3)**2 + 5\n",
    "\n",
    "def grad_f(x):\n",
    "    \"\"\"Gradient : ‚àÇf/‚àÇx = 2(x - 3)\"\"\"\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "def gradient_descent(x0, lr, n_iter):\n",
    "    \"\"\"Descente de gradient\"\"\"\n",
    "    x = x0\n",
    "    history = [x]\n",
    "    for _ in range(n_iter):\n",
    "        # Mise √† jour : x_{t+1} = x_t - lr * ‚àáf(x_t)\n",
    "        x = x - lr * grad_f(x)\n",
    "        history.append(x)\n",
    "    return np.array(history)\n",
    "\n",
    "# Tester diff√©rents learning rates\n",
    "x0 = 0.0\n",
    "learning_rates = [0.1, 0.5, 1.0]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for lr in learning_rates:\n",
    "    history = gradient_descent(x0, lr, 20)\n",
    "    plt.plot(history, label=f'lr={lr}', marker='o', markersize=4)\n",
    "\n",
    "plt.axhline(3, color='r', linestyle='--', linewidth=2, label='Minimum x=3')\n",
    "plt.xlabel('It√©ration')\n",
    "plt.ylabel('x')\n",
    "plt.title('Convergence de la Descente de Gradient')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations :\")\n",
    "print(\"- lr=0.1 : Convergence lente mais stable\")\n",
    "print(\"- lr=0.5 : Convergence plus rapide\")\n",
    "print(\"- lr=1.0 : Convergence en une seule √©tape (optimal pour cette fonction quadratique)\")\n",
    "print(\"\\nPour lr > 1.0, la descente divergerait (oscillations croissantes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.4 : Descente de Gradient 2D\n",
    "\n",
    "Minimiser f(x‚ÇÅ, x‚ÇÇ) = x‚ÇÅ¬≤ + 4x‚ÇÇ¬≤ par descente de gradient.  \n",
    "Visualiser la trajectoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def f(x):\n",
    "    \"\"\"Fonction √† minimiser : ellipse centr√©e √† l'origine\"\"\"\n",
    "    return x[0]**2 + 4*x[1]**2\n",
    "\n",
    "def grad_f(x):\n",
    "    \"\"\"Gradient de f\n",
    "    ‚àÇf/‚àÇx1 = 2*x1\n",
    "    ‚àÇf/‚àÇx2 = 8*x2\n",
    "    \"\"\"\n",
    "    return np.array([2*x[0], 8*x[1]])\n",
    "\n",
    "# Descente de gradient\n",
    "x0 = np.array([2.0, 2.0])\n",
    "lr = 0.1\n",
    "n_iter = 50\n",
    "\n",
    "x = x0.copy()\n",
    "history = [x.copy()]\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    # Mise √† jour : x_{t+1} = x_t - lr * ‚àáf(x_t)\n",
    "    x = x - lr * grad_f(x)\n",
    "    history.append(x.copy())\n",
    "\n",
    "history = np.array(history)\n",
    "\n",
    "# Visualisation\n",
    "x1_range = np.linspace(-3, 3, 100)\n",
    "x2_range = np.linspace(-3, 3, 100)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "Z = X1**2 + 4*X2**2\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Courbes de niveau de la fonction\n",
    "contour = plt.contour(X1, X2, Z, levels=30, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(contour, label='f(x‚ÇÅ, x‚ÇÇ)')\n",
    "\n",
    "# Trajectoire de la descente de gradient\n",
    "plt.plot(history[:, 0], history[:, 1], 'r-o', markersize=5, linewidth=2, label='Trajectoire', alpha=0.7)\n",
    "plt.plot(x0[0], x0[1], 'g*', markersize=15, label='D√©part')\n",
    "plt.plot(history[-1, 0], history[-1, 1], 'r*', markersize=15, label='Arriv√©e')\n",
    "plt.plot(0, 0, 'b*', markersize=15, label='Minimum th√©orique (0,0)')\n",
    "\n",
    "plt.xlabel('x‚ÇÅ')\n",
    "plt.ylabel('x‚ÇÇ')\n",
    "plt.title('Descente de Gradient 2D sur une Fonction Quadratique')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Solution finale: {history[-1]}\")\n",
    "print(f\"Valeur f: {f(history[-1]):.6f}\")\n",
    "print(f\"\\nDistance au minimum: {np.linalg.norm(history[-1]):.6f}\")\n",
    "print(f\"\\nObservation : La convergence est plus rapide selon x‚ÇÅ que selon x‚ÇÇ\")\n",
    "print(f\"car la courbure est diff√©rente (coefficient 1 vs 4).\")\n",
    "print(f\"C'est un probl√®me mal conditionn√© qui pourrait b√©n√©ficier d'un pr√©conditionneur.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : Projet Int√©gratif\n",
    "\n",
    "### R√©gression Lin√©aire Compl√®te\n",
    "\n",
    "1. G√©n√©rer des donn√©es synth√©tiques y = 3x + 2 + bruit\n",
    "2. R√©soudre par moindres carr√©s (solution analytique)\n",
    "3. R√©soudre par descente de gradient\n",
    "4. Comparer les deux solutions\n",
    "5. Visualiser la surface de co√ªt et la trajectoire GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution compl√®te du projet int√©gratif\n",
    "\n",
    "# 1. G√©n√©rer des donn√©es synth√©tiques y = 3x + 2 + bruit\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "x = np.linspace(0, 10, n_samples)\n",
    "y_true = 3 * x + 2\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "y = y_true + noise\n",
    "\n",
    "# Pr√©parer X pour la r√©gression (ajouter colonne de 1 pour le biais)\n",
    "X = np.column_stack([np.ones(n_samples), x])  # [1, x]\n",
    "\n",
    "# 2. Solution analytique par moindres carr√©s : w = (X^T X)^(-1) X^T y\n",
    "w_analytical = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(\"Solution analytique (moindres carr√©s):\")\n",
    "print(f\"  Biais (intercept): {w_analytical[0]:.4f} (vrai: 2)\")\n",
    "print(f\"  Pente (slope): {w_analytical[1]:.4f} (vrai: 3)\")\n",
    "\n",
    "# 3. Solution par descente de gradient\n",
    "def cost_function(w, X, y):\n",
    "    \"\"\"MSE = 1/(2n) * ||Xw - y||¬≤\"\"\"\n",
    "    n = len(y)\n",
    "    residual = X @ w - y\n",
    "    return (1/(2*n)) * np.dot(residual, residual)\n",
    "\n",
    "def gradient(w, X, y):\n",
    "    \"\"\"Gradient du MSE : (1/n) * X^T (Xw - y)\"\"\"\n",
    "    n = len(y)\n",
    "    return (1/n) * X.T @ (X @ w - y)\n",
    "\n",
    "# Descente de gradient\n",
    "w0 = np.array([0.0, 0.0])  # Initialisation\n",
    "lr = 0.1\n",
    "n_iter = 100\n",
    "\n",
    "w = w0.copy()\n",
    "w_history = [w.copy()]\n",
    "cost_history = [cost_function(w, X, y)]\n",
    "\n",
    "for i in range(n_iter):\n",
    "    w = w - lr * gradient(w, X, y)\n",
    "    w_history.append(w.copy())\n",
    "    cost_history.append(cost_function(w, X, y))\n",
    "\n",
    "w_history = np.array(w_history)\n",
    "\n",
    "w_gd = w_history[-1]\n",
    "print(\"\\nSolution par descente de gradient:\")\n",
    "print(f\"  Biais (intercept): {w_gd[0]:.4f}\")\n",
    "print(f\"  Pente (slope): {w_gd[1]:.4f}\")\n",
    "\n",
    "# 4. Comparer les solutions\n",
    "print(\"\\nDiff√©rence entre les deux m√©thodes:\")\n",
    "print(f\"  Œî biais: {abs(w_analytical[0] - w_gd[0]):.6f}\")\n",
    "print(f\"  Œî pente: {abs(w_analytical[1] - w_gd[1]):.6f}\")\n",
    "\n",
    "# 5. Visualisations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# a) Donn√©es et droites de r√©gression\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(x, y, alpha=0.5, label='Donn√©es bruit√©es')\n",
    "ax1.plot(x, y_true, 'g-', linewidth=2, label='Vraie relation (y=3x+2)')\n",
    "ax1.plot(x, X @ w_analytical, 'r--', linewidth=2, label='Moindres carr√©s')\n",
    "ax1.plot(x, X @ w_gd, 'b:', linewidth=2, label='Descente de gradient')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('R√©gression Lin√©aire : Donn√©es et Ajustements')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# b) Convergence du co√ªt\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(cost_history, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('It√©ration')\n",
    "ax2.set_ylabel('Co√ªt (MSE)')\n",
    "ax2.set_title('Convergence de la Descente de Gradient')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# c) Surface de co√ªt 3D (contour plot)\n",
    "ax3 = axes[1, 0]\n",
    "w0_range = np.linspace(-2, 6, 100)\n",
    "w1_range = np.linspace(0, 6, 100)\n",
    "W0, W1 = np.meshgrid(w0_range, w1_range)\n",
    "Z = np.zeros_like(W0)\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        w_temp = np.array([W0[i, j], W1[i, j]])\n",
    "        Z[i, j] = cost_function(w_temp, X, y)\n",
    "\n",
    "contour = ax3.contour(W0, W1, Z, levels=30, cmap='viridis')\n",
    "ax3.plot(w_history[:, 0], w_history[:, 1], 'r-o', markersize=3, linewidth=2, alpha=0.7, label='Trajectoire GD')\n",
    "ax3.plot(w_analytical[0], w_analytical[1], 'g*', markersize=15, label='Minimum analytique')\n",
    "ax3.plot(w0[0], w0[1], 'b*', markersize=15, label='D√©part')\n",
    "ax3.set_xlabel('w‚ÇÄ (biais)')\n",
    "ax3.set_ylabel('w‚ÇÅ (pente)')\n",
    "ax3.set_title('Surface de Co√ªt et Trajectoire de la Descente de Gradient')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "plt.colorbar(contour, ax=ax3, label='Co√ªt')\n",
    "\n",
    "# d) √âvolution des param√®tres\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(w_history[:, 0], 'b-', linewidth=2, label='w‚ÇÄ (biais)')\n",
    "ax4.plot(w_history[:, 1], 'r-', linewidth=2, label='w‚ÇÅ (pente)')\n",
    "ax4.axhline(w_analytical[0], color='b', linestyle='--', alpha=0.5, label='w‚ÇÄ optimal')\n",
    "ax4.axhline(w_analytical[1], color='r', linestyle='--', alpha=0.5, label='w‚ÇÅ optimal')\n",
    "ax4.set_xlabel('It√©ration')\n",
    "ax4.set_ylabel('Valeur du param√®tre')\n",
    "ax4.set_title('√âvolution des Param√®tres durant la Descente de Gradient')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION DU PROJET\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. Les deux m√©thodes (analytique et GD) convergent vers la m√™me solution\")\n",
    "print(\"\\n2. La solution analytique est exacte et imm√©diate (O(n¬≤) pour l'inversion)\")\n",
    "print(\"\\n3. La descente de gradient est it√©rative mais scalable pour les grandes donn√©es\")\n",
    "print(\"\\n4. Le learning rate influence la vitesse de convergence :\")\n",
    "print(\"   - Trop petit : convergence lente\")\n",
    "print(\"   - Trop grand : divergence ou oscillations\")\n",
    "print(\"\\n5. Pour la r√©gression lin√©aire, la fonction de co√ªt est convexe (surface en bol)\")\n",
    "print(\"   donc la descente de gradient garantit de trouver le minimum global.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fin des solutions du Chapitre 01**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
