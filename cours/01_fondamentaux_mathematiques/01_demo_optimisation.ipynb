{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/01_fondamentaux_mathematiques/01_demo_optimisation.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '01_demo_optimisation.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 01 - D√©monstration : Calcul Diff√©rentiel et Optimisation\n",
    "\n",
    "Ce notebook illustre les concepts d'optimisation fondamentaux au ML :\n",
    "- Gradients et d√©riv√©es partielles\n",
    "- Descente de gradient\n",
    "- Convexit√©\n",
    "- M√©thodes d'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from scipy import optimize\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient et D√©riv√©es Partielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction 2D : f(x, y) = x^2 + 2y^2\n",
    "def f(x, y):\n",
    "    return x**2 + 2*y**2\n",
    "\n",
    "# Gradient : ‚àáf = [‚àÇf/‚àÇx, ‚àÇf/‚àÇy] = [2x, 4y]\n",
    "def grad_f(x, y):\n",
    "    return np.array([2*x, 4*y])\n",
    "\n",
    "# Cr√©er une grille\n",
    "x_range = np.linspace(-3, 3, 50)\n",
    "y_range = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# Visualisation 3D\n",
    "fig = plt.figure(figsize=(16, 7))\n",
    "\n",
    "# Surface 3D\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap=cm.viridis, alpha=0.7)\n",
    "ax1.set_xlabel('x', fontsize=12)\n",
    "ax1.set_ylabel('y', fontsize=12)\n",
    "ax1.set_zlabel('f(x, y)', fontsize=12)\n",
    "ax1.set_title('Surface f(x, y) = x¬≤ + 2y¬≤', fontsize=14)\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Contour + vecteurs gradient\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Tracer quelques vecteurs gradient\n",
    "points = [(2, 1), (1, 2), (-1.5, 1), (0, -2)]\n",
    "for px, py in points:\n",
    "    grad = grad_f(px, py)\n",
    "    # Normaliser pour la visualisation\n",
    "    grad_norm = grad / np.linalg.norm(grad) * 0.5\n",
    "    ax2.arrow(px, py, grad_norm[0], grad_norm[1],\n",
    "              head_width=0.15, head_length=0.2, fc='red', ec='red', linewidth=2)\n",
    "    ax2.plot(px, py, 'ro', markersize=8)\n",
    "\n",
    "ax2.set_xlabel('x', fontsize=12)\n",
    "ax2.set_ylabel('y', fontsize=12)\n",
    "ax2.set_title('Contours + Gradients (fl√®ches rouges)', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Gradient en quelques points:\")\n",
    "for px, py in points:\n",
    "    print(f\"  ‚àáf({px:4.1f}, {py:4.1f}) = {grad_f(px, py)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descente de Gradient - Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(grad_func, x0, learning_rate=0.1, n_iterations=50):\n",
    "    \"\"\"\n",
    "    Descente de gradient simple.\n",
    "    \n",
    "    Args:\n",
    "        grad_func: fonction qui calcule le gradient\n",
    "        x0: point de d√©part\n",
    "        learning_rate: taux d'apprentissage\n",
    "        n_iterations: nombre d'it√©rations\n",
    "    \n",
    "    Returns:\n",
    "        history: liste des points visit√©s\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    history = [x.copy()]\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        grad = grad_func(x[0], x[1])\n",
    "        x = x - learning_rate * grad\n",
    "        history.append(x.copy())\n",
    "    \n",
    "    return np.array(history)\n",
    "\n",
    "# Point de d√©part\n",
    "x0 = np.array([2.5, 2.0])\n",
    "\n",
    "# Tester diff√©rents learning rates\n",
    "learning_rates = [0.05, 0.2, 0.5]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for ax, lr in zip(axes, learning_rates):\n",
    "    # Descente de gradient\n",
    "    history = gradient_descent(grad_f, x0, learning_rate=lr, n_iterations=30)\n",
    "    \n",
    "    # Contours\n",
    "    contour = ax.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)\n",
    "    \n",
    "    # Trajectoire\n",
    "    ax.plot(history[:, 0], history[:, 1], 'r-o', linewidth=2, markersize=6,\n",
    "            label='Trajectoire')\n",
    "    ax.plot(x0[0], x0[1], 'g*', markersize=20, label='D√©part')\n",
    "    ax.plot(history[-1, 0], history[-1, 1], 'r*', markersize=20, label='Arriv√©e')\n",
    "    \n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel('y', fontsize=12)\n",
    "    ax.set_title(f'Learning Rate Œ± = {lr}', fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Afficher valeur finale\n",
    "    final_val = f(history[-1, 0], history[-1, 1])\n",
    "    ax.text(0.05, 0.95, f'f final = {final_val:.4f}',\n",
    "            transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Descente de Gradient - Impact du Learning Rate', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convergence de la Descente de Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer la convergence pour diff√©rents learning rates\n",
    "x0 = np.array([2.5, 2.0])\n",
    "learning_rates = [0.01, 0.1, 0.3, 0.5]\n",
    "n_iterations = 50\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for lr in learning_rates:\n",
    "    history = gradient_descent(grad_f, x0, learning_rate=lr, n_iterations=n_iterations)\n",
    "    \n",
    "    # Valeurs de la fonction\n",
    "    f_values = [f(x[0], x[1]) for x in history]\n",
    "    \n",
    "    # Norme du gradient\n",
    "    grad_norms = [np.linalg.norm(grad_f(x[0], x[1])) for x in history]\n",
    "    \n",
    "    ax1.plot(f_values, '-o', label=f'Œ± = {lr}', markersize=4)\n",
    "    ax2.semilogy(grad_norms, '-o', label=f'Œ± = {lr}', markersize=4)\n",
    "\n",
    "ax1.set_xlabel('It√©ration', fontsize=12)\n",
    "ax1.set_ylabel('f(x)', fontsize=12)\n",
    "ax1.set_title('Convergence de la Fonction Objectif', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('It√©ration', fontsize=12)\n",
    "ax2.set_ylabel('||‚àáf||', fontsize=12)\n",
    "ax2.set_title('Convergence de la Norme du Gradient', fontsize=14)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonction Convexe vs Non-Convexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction convexe : f(x) = x^2\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "f_convex = x_range**2\n",
    "\n",
    "# Fonction non-convexe : f(x) = x^4 - 5x^2 + 4\n",
    "f_nonconvex = x_range**4 - 5*x_range**2 + 4\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Convexe\n",
    "ax1.plot(x_range, f_convex, 'b-', linewidth=2)\n",
    "ax1.plot(0, 0, 'ro', markersize=15, label='Minimum global unique')\n",
    "ax1.set_xlabel('x', fontsize=12)\n",
    "ax1.set_ylabel('f(x)', fontsize=12)\n",
    "ax1.set_title('Fonction Convexe: f(x) = x¬≤', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Non-convexe\n",
    "ax2.plot(x_range, f_nonconvex, 'r-', linewidth=2)\n",
    "# Minima locaux approximatifs\n",
    "minima = [(-1.58, -2.25), (1.58, -2.25)]\n",
    "for xm, ym in minima:\n",
    "    ax2.plot(xm, ym, 'go', markersize=12, label='Minimum local')\n",
    "ax2.plot(0, 4, 'yo', markersize=12, label='Maximum local')\n",
    "ax2.set_xlabel('x', fontsize=12)\n",
    "ax2.set_ylabel('f(x)', fontsize=12)\n",
    "ax2.set_title('Fonction Non-Convexe: f(x) = x‚Å¥ - 5x¬≤ + 4', fontsize=14)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Propri√©t√©s:\")\n",
    "print(\"  Fonction convexe: Tout minimum local est global\")\n",
    "print(\"  Fonction non-convexe: Peut avoir plusieurs minima locaux\")\n",
    "print(\"                       La descente de gradient peut converger vers un minimum local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison : Gradient Descent vs Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction 1D : f(x) = 0.5 * x^2 - 2*x + 3\n",
    "def f_1d(x):\n",
    "    return 0.5 * x**2 - 2*x + 3\n",
    "\n",
    "# D√©riv√©e : f'(x) = x - 2\n",
    "def df_1d(x):\n",
    "    return x - 2\n",
    "\n",
    "# D√©riv√©e seconde : f''(x) = 1\n",
    "def d2f_1d(x):\n",
    "    return 1.0\n",
    "\n",
    "# Descente de gradient\n",
    "def gd_1d(x0, lr=0.3, n_iter=20):\n",
    "    x = x0\n",
    "    history = [x]\n",
    "    for _ in range(n_iter):\n",
    "        x = x - lr * df_1d(x)\n",
    "        history.append(x)\n",
    "    return np.array(history)\n",
    "\n",
    "# M√©thode de Newton\n",
    "def newton_1d(x0, n_iter=20):\n",
    "    x = x0\n",
    "    history = [x]\n",
    "    for _ in range(n_iter):\n",
    "        x = x - df_1d(x) / d2f_1d(x)\n",
    "        history.append(x)\n",
    "    return np.array(history)\n",
    "\n",
    "# Initialisation\n",
    "x0 = 5.0\n",
    "\n",
    "# Optimisation\n",
    "gd_history = gd_1d(x0, lr=0.5, n_iter=15)\n",
    "newton_history = newton_1d(x0, n_iter=5)  # Newton converge beaucoup plus vite\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Trajectoire sur la fonction\n",
    "x_range = np.linspace(0, 6, 200)\n",
    "ax1.plot(x_range, f_1d(x_range), 'b-', linewidth=2, label='f(x)')\n",
    "ax1.plot(gd_history, f_1d(gd_history), 'ro-', markersize=8, linewidth=2, label='Gradient Descent')\n",
    "ax1.plot(newton_history, f_1d(newton_history), 'gs-', markersize=10, linewidth=2, label='Newton')\n",
    "ax1.plot(2, f_1d(2), 'k*', markersize=20, label='Minimum x=2')\n",
    "ax1.set_xlabel('x', fontsize=12)\n",
    "ax1.set_ylabel('f(x)', fontsize=12)\n",
    "ax1.set_title('Trajectoires d\\'Optimisation', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence\n",
    "gd_errors = np.abs(gd_history - 2)  # Distance au minimum\n",
    "newton_errors = np.abs(newton_history - 2)\n",
    "\n",
    "ax2.semilogy(range(len(gd_errors)), gd_errors, 'ro-', markersize=8, linewidth=2, label='Gradient Descent')\n",
    "ax2.semilogy(range(len(newton_errors)), newton_errors, 'gs-', markersize=10, linewidth=2, label='Newton')\n",
    "ax2.set_xlabel('It√©ration', fontsize=12)\n",
    "ax2.set_ylabel('|x - x*|', fontsize=12)\n",
    "ax2.set_title('Convergence (√©chelle log)', fontsize=14)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Gradient Descent: {len(gd_errors)} it√©rations, erreur finale = {gd_errors[-1]:.6f}\")\n",
    "print(f\"Newton: {len(newton_errors)} it√©rations, erreur finale = {newton_errors[-1]:.2e}\")\n",
    "print(\"\\nNewton converge BEAUCOUP plus vite (convergence quadratique)\")\n",
    "print(\"Mais co√ªteux en calcul (n√©cessite la hessienne)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Application : R√©gression Lin√©aire par Descente de Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des donn√©es synth√©tiques\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = 2 * np.random.rand(n_samples)\n",
    "y = 4 + 3 * X + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Fonction de co√ªt : MSE = 1/n * sum((y_pred - y)^2)\n",
    "def mse_cost(X, y, w, b):\n",
    "    y_pred = w * X + b\n",
    "    return np.mean((y_pred - y)**2)\n",
    "\n",
    "# Gradient de MSE\n",
    "def mse_gradient(X, y, w, b):\n",
    "    y_pred = w * X + b\n",
    "    error = y_pred - y\n",
    "    grad_w = 2 * np.mean(error * X)\n",
    "    grad_b = 2 * np.mean(error)\n",
    "    return grad_w, grad_b\n",
    "\n",
    "# Descente de gradient\n",
    "w, b = 0.0, 0.0  # Initialisation\n",
    "learning_rate = 0.1\n",
    "n_iterations = 100\n",
    "\n",
    "w_history = [w]\n",
    "b_history = [b]\n",
    "cost_history = [mse_cost(X, y, w, b)]\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    grad_w, grad_b = mse_gradient(X, y, w, b)\n",
    "    w = w - learning_rate * grad_w\n",
    "    b = b - learning_rate * grad_b\n",
    "    \n",
    "    w_history.append(w)\n",
    "    b_history.append(b)\n",
    "    cost_history.append(mse_cost(X, y, w, b))\n",
    "\n",
    "print(f\"Param√®tres finaux apr√®s {n_iterations} it√©rations:\")\n",
    "print(f\"  w (pente) = {w:.4f} (vrai: 3.0)\")\n",
    "print(f\"  b (biais) = {b:.4f} (vrai: 4.0)\")\n",
    "print(f\"  MSE final = {cost_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Donn√©es + ligne de r√©gression finale\n",
    "ax1.scatter(X, y, alpha=0.6, label='Donn√©es')\n",
    "X_line = np.array([0, 2])\n",
    "y_line = w * X_line + b\n",
    "ax1.plot(X_line, y_line, 'r-', linewidth=2, label=f'y = {w:.2f}x + {b:.2f}')\n",
    "ax1.set_xlabel('X', fontsize=12)\n",
    "ax1.set_ylabel('y', fontsize=12)\n",
    "ax1.set_title('R√©gression Lin√©aire Finale', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Convergence du co√ªt\n",
    "ax2.plot(cost_history, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('It√©ration', fontsize=12)\n",
    "ax2.set_ylabel('MSE', fontsize=12)\n",
    "ax2.set_title('Convergence du Co√ªt (MSE)', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. √âvolution de w\n",
    "ax3.plot(w_history, 'g-', linewidth=2)\n",
    "ax3.axhline(3, color='r', linestyle='--', linewidth=2, label='Valeur vraie')\n",
    "ax3.set_xlabel('It√©ration', fontsize=12)\n",
    "ax3.set_ylabel('w (pente)', fontsize=12)\n",
    "ax3.set_title('Convergence de w', fontsize=14)\n",
    "ax3.legend(fontsize=11)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. √âvolution de b\n",
    "ax4.plot(b_history, 'purple', linewidth=2)\n",
    "ax4.axhline(4, color='r', linestyle='--', linewidth=2, label='Valeur vraie')\n",
    "ax4.set_xlabel('It√©ration', fontsize=12)\n",
    "ax4.set_ylabel('b (biais)', fontsize=12)\n",
    "ax4.set_title('Convergence de b', fontsize=14)\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Paysage d'Optimisation (Surface de Co√ªt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une grille pour visualiser la surface de co√ªt\n",
    "w_range = np.linspace(0, 6, 100)\n",
    "b_range = np.linspace(0, 8, 100)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "\n",
    "# Calculer le co√ªt pour chaque (w, b)\n",
    "Cost = np.zeros_like(W)\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        Cost[i, j] = mse_cost(X, y, W[i, j], B[i, j])\n",
    "\n",
    "# Visualisation\n",
    "fig = plt.figure(figsize=(18, 7))\n",
    "\n",
    "# Surface 3D\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(W, B, Cost, cmap=cm.viridis, alpha=0.7)\n",
    "\n",
    "# Trajectoire de la descente de gradient\n",
    "ax1.plot(w_history, b_history, cost_history, 'r-o', linewidth=2, markersize=3)\n",
    "ax1.scatter([w_history[0]], [b_history[0]], [cost_history[0]], \n",
    "            color='green', s=200, marker='*', label='D√©part')\n",
    "ax1.scatter([w_history[-1]], [b_history[-1]], [cost_history[-1]], \n",
    "            color='red', s=200, marker='*', label='Arriv√©e')\n",
    "\n",
    "ax1.set_xlabel('w (pente)', fontsize=12)\n",
    "ax1.set_ylabel('b (biais)', fontsize=12)\n",
    "ax1.set_zlabel('MSE', fontsize=12)\n",
    "ax1.set_title('Surface de Co√ªt 3D', fontsize=14)\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "# Contours 2D + trajectoire\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W, B, Cost, levels=30, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Trajectoire\n",
    "ax2.plot(w_history, b_history, 'r-o', linewidth=2, markersize=5, label='Descente de gradient')\n",
    "ax2.plot(w_history[0], b_history[0], 'g*', markersize=20, label='D√©part')\n",
    "ax2.plot(w_history[-1], b_history[-1], 'r*', markersize=20, label='Arriv√©e')\n",
    "\n",
    "ax2.set_xlabel('w (pente)', fontsize=12)\n",
    "ax2.set_ylabel('b (biais)', fontsize=12)\n",
    "ax2.set_title('Contours de Co√ªt + Trajectoire GD', fontsize=14)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimisation avec SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction √† minimiser : Rosenbrock (fonction test classique)\n",
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "# Gradient de Rosenbrock\n",
    "def rosenbrock_grad(x):\n",
    "    grad_x0 = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
    "    grad_x1 = 200*(x[1] - x[0]**2)\n",
    "    return np.array([grad_x0, grad_x1])\n",
    "\n",
    "# Point initial\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "# M√©thodes d'optimisation\n",
    "methods = ['BFGS', 'CG', 'Nelder-Mead']\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    if method in ['BFGS', 'CG']:\n",
    "        result = optimize.minimize(rosenbrock, x0, method=method, jac=rosenbrock_grad)\n",
    "    else:\n",
    "        result = optimize.minimize(rosenbrock, x0, method=method)\n",
    "    \n",
    "    results[method] = result\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Solution: {result.x}\")\n",
    "    print(f\"  Valeur: {result.fun:.6e}\")\n",
    "    print(f\"  It√©rations: {result.nit}\")\n",
    "    print(f\"  Succ√®s: {result.success}\")\n",
    "\n",
    "# Le minimum global est √† (1, 1)\n",
    "print(\"\\nMinimum th√©orique: [1.0, 1.0], f = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la fonction de Rosenbrock\n",
    "x_range = np.linspace(-1.5, 1.5, 200)\n",
    "y_range = np.linspace(-0.5, 1.5, 200)\n",
    "X_rose, Y_rose = np.meshgrid(x_range, y_range)\n",
    "Z_rose = (1 - X_rose)**2 + 100*(Y_rose - X_rose**2)**2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Contours (√©chelle log pour mieux voir)\n",
    "levels = np.logspace(-1, 3.5, 25)\n",
    "contour = ax.contour(X_rose, Y_rose, Z_rose, levels=levels, \n",
    "                      cmap='viridis', linewidths=1)\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Solutions des diff√©rentes m√©thodes\n",
    "colors = {'BFGS': 'red', 'CG': 'blue', 'Nelder-Mead': 'green'}\n",
    "for method, result in results.items():\n",
    "    ax.plot(result.x[0], result.x[1], 'o', color=colors[method], \n",
    "            markersize=12, label=f'{method}: ({result.x[0]:.3f}, {result.x[1]:.3f})')\n",
    "\n",
    "# Minimum th√©orique\n",
    "ax.plot(1, 1, 'k*', markersize=20, label='Minimum global (1, 1)')\n",
    "\n",
    "# Point de d√©part\n",
    "ax.plot(x0[0], x0[1], 'wo', markersize=15, markeredgecolor='black', \n",
    "        markeredgewidth=2, label='D√©part (0, 0)')\n",
    "\n",
    "ax.set_xlabel('x‚ÇÄ', fontsize=12)\n",
    "ax.set_ylabel('x‚ÇÅ', fontsize=12)\n",
    "ax.set_title('Fonction de Rosenbrock - Comparaison des M√©thodes', fontsize=14)\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©sum√©\n",
    "\n",
    "Dans ce notebook, nous avons explor√© :\n",
    "\n",
    "1. **Gradient** : Direction de plus forte croissance, calcul de d√©riv√©es partielles\n",
    "2. **Descente de gradient** : Algorithme d'optimisation de base en ML\n",
    "3. **Learning rate** : Impact crucial sur la convergence\n",
    "4. **Convexit√©** : Garantie de convergence vers le minimum global\n",
    "5. **M√©thode de Newton** : Convergence quadratique mais co√ªteuse\n",
    "6. **Application pratique** : R√©gression lin√©aire par gradient descent\n",
    "7. **Paysage d'optimisation** : Visualisation de la surface de co√ªt\n",
    "8. **SciPy optimize** : M√©thodes d'optimisation avanc√©es (BFGS, CG, etc.)\n",
    "\n",
    "Ces techniques sont **fondamentales** pour :\n",
    "- Entra√Ænement de r√©seaux de neurones (backpropagation)\n",
    "- Optimisation d'hyperparam√®tres\n",
    "- R√©solution de probl√®mes ML en g√©n√©ral\n",
    "\n",
    "**Next steps** : SGD, Momentum, Adam dans le Chapitre 06 (Deep Learning) !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}