{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/XX_CHAPTER/XX_NOTEBOOK.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '01_demo_algebre_lineaire.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 01 - D√©monstration : Alg√®bre Lin√©aire\n",
    "\n",
    "Ce notebook illustre les concepts d'alg√®bre lin√©aire essentiels au Machine Learning :\n",
    "- Vecteurs, matrices et op√©rations\n",
    "- Valeurs propres et vecteurs propres\n",
    "- SVD (Singular Value Decomposition)\n",
    "- Applications pratiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vecteurs et Op√©rations de Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir des vecteurs\n",
    "u = np.array([3, 4])\n",
    "v = np.array([1, 2])\n",
    "\n",
    "print(\"Vecteur u:\", u)\n",
    "print(\"Vecteur v:\", v)\n",
    "\n",
    "# Produit scalaire\n",
    "dot_product = np.dot(u, v)\n",
    "print(f\"\\nProduit scalaire u¬∑v = {dot_product}\")\n",
    "\n",
    "# Normes\n",
    "norm_l1 = np.linalg.norm(u, ord=1)\n",
    "norm_l2 = np.linalg.norm(u, ord=2)\n",
    "norm_linf = np.linalg.norm(u, ord=np.inf)\n",
    "\n",
    "print(f\"\\nNormes de u:\")\n",
    "print(f\"  L1 (Manhattan): {norm_l1}\")\n",
    "print(f\"  L2 (Euclidienne): {norm_l2}\")\n",
    "print(f\"  L‚àû (Max): {norm_linf}\")\n",
    "\n",
    "# Distance euclidienne\n",
    "distance = np.linalg.norm(u - v)\n",
    "print(f\"\\nDistance euclidienne entre u et v: {distance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les vecteurs\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Tracer les vecteurs\n",
    "ax.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='blue', width=0.008, label='u')\n",
    "ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='red', width=0.008, label='v')\n",
    "ax.quiver(0, 0, (u+v)[0], (u+v)[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='green', width=0.008, linestyle='--', label='u+v')\n",
    "\n",
    "ax.set_xlim(-1, 6)\n",
    "ax.set_ylim(-1, 7)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_xlabel('x‚ÇÅ', fontsize=12)\n",
    "ax.set_ylabel('x‚ÇÇ', fontsize=12)\n",
    "ax.set_title('Vecteurs et Addition Vectorielle', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrices et Multiplication Matricielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des matrices\n",
    "A = np.array([[2, 1],\n",
    "              [1, 3]])\n",
    "\n",
    "B = np.array([[1, 0],\n",
    "              [0, 2]])\n",
    "\n",
    "print(\"Matrice A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrice B:\")\n",
    "print(B)\n",
    "\n",
    "# Op√©rations matricielles\n",
    "C = A @ B  # Multiplication matricielle\n",
    "print(\"\\nA @ B:\")\n",
    "print(C)\n",
    "\n",
    "# Attention : A @ B ‚â† B @ A en g√©n√©ral\n",
    "D = B @ A\n",
    "print(\"\\nB @ A:\")\n",
    "print(D)\n",
    "print(\"\\nA @ B == B @ A?\", np.allclose(C, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propri√©t√©s matricielles\n",
    "print(\"Transpos√©e de A:\")\n",
    "print(A.T)\n",
    "\n",
    "print(\"\\nD√©terminant de A:\", np.linalg.det(A))\n",
    "print(\"Trace de A:\", np.trace(A))\n",
    "print(\"Rang de A:\", np.linalg.matrix_rank(A))\n",
    "\n",
    "# Inverse\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(\"\\nInverse de A:\")\n",
    "print(A_inv)\n",
    "\n",
    "# V√©rification : A @ A_inv = I\n",
    "I = A @ A_inv\n",
    "print(\"\\nA @ A‚Åª¬π (devrait √™tre proche de I):\")\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Valeurs Propres et Vecteurs Propres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer valeurs propres et vecteurs propres\n",
    "A = np.array([[4, 2],\n",
    "              [1, 3]])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Matrice A:\")\n",
    "print(A)\n",
    "print(\"\\nValeurs propres Œª:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nVecteurs propres (colonnes):\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# V√©rification : A @ v = Œª @ v\n",
    "for i in range(len(eigenvalues)):\n",
    "    v = eigenvectors[:, i]\n",
    "    lam = eigenvalues[i]\n",
    "    \n",
    "    Av = A @ v\n",
    "    lam_v = lam * v\n",
    "    \n",
    "    print(f\"\\nV√©rification pour Œª{i+1} = {lam:.3f}:\")\n",
    "    print(f\"  A @ v = {Av}\")\n",
    "    print(f\"  Œª @ v = {lam_v}\")\n",
    "    print(f\"  √âgaux? {np.allclose(Av, lam_v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les vecteurs propres et transformation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Avant transformation\n",
    "for i in range(2):\n",
    "    v = eigenvectors[:, i]\n",
    "    ax1.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n",
    "               color=['blue', 'red'][i], width=0.01, label=f'v{i+1}')\n",
    "\n",
    "ax1.set_xlim(-1, 1)\n",
    "ax1.set_ylim(-1, 1)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.set_title('Vecteurs Propres', fontsize=14)\n",
    "\n",
    "# Apr√®s transformation A @ v = Œªv\n",
    "for i in range(2):\n",
    "    v = eigenvectors[:, i]\n",
    "    lam = eigenvalues[i]\n",
    "    Av = A @ v\n",
    "    \n",
    "    # Vecteur original\n",
    "    ax2.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n",
    "               color=['blue', 'red'][i], width=0.005, alpha=0.3)\n",
    "    # Vecteur transform√©\n",
    "    ax2.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1,\n",
    "               color=['blue', 'red'][i], width=0.01, \n",
    "               label=f'A@v{i+1} = {lam:.2f}*v{i+1}')\n",
    "\n",
    "ax2.set_xlim(-1, 3)\n",
    "ax2.set_ylim(-1, 3)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.set_title('Transformation par A (√©tirement le long des vecteurs propres)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. D√©composition SVD (Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une matrice rectangulaire\n",
    "A = np.array([[3, 1],\n",
    "              [1, 3],\n",
    "              [1, 1]])\n",
    "\n",
    "print(\"Matrice A (3√ó2):\")\n",
    "print(A)\n",
    "\n",
    "# SVD : A = U @ Sigma @ V^T\n",
    "U, Sigma, VT = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "print(\"\\nDimensions:\")\n",
    "print(f\"  U: {U.shape}\")\n",
    "print(f\"  Sigma: {Sigma.shape}\")\n",
    "print(f\"  V^T: {VT.shape}\")\n",
    "\n",
    "print(\"\\nValeurs singuli√®res:\")\n",
    "print(Sigma)\n",
    "\n",
    "# Reconstruction\n",
    "Sigma_mat = np.diag(Sigma)\n",
    "A_reconstructed = U @ Sigma_mat @ VT\n",
    "\n",
    "print(\"\\nMatrice reconstruite A = U @ Œ£ @ V^T:\")\n",
    "print(A_reconstructed)\n",
    "print(\"\\nErreur de reconstruction:\", np.linalg.norm(A - A_reconstructed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application : approximation de rang faible\n",
    "# Cr√©er une image synth√©tique\n",
    "np.random.seed(42)\n",
    "img = np.random.randn(50, 50)\n",
    "img = img + img.T  # Rendre sym√©trique pour plus de structure\n",
    "\n",
    "# SVD\n",
    "U, Sigma, VT = np.linalg.svd(img, full_matrices=False)\n",
    "\n",
    "# Approximations de rang k\n",
    "ranks = [1, 5, 10, 20, 50]\n",
    "fig, axes = plt.subplots(1, len(ranks), figsize=(18, 4))\n",
    "\n",
    "for ax, k in zip(axes, ranks):\n",
    "    # Garder seulement les k premi√®res valeurs singuli√®res\n",
    "    Sigma_k = Sigma.copy()\n",
    "    Sigma_k[k:] = 0\n",
    "    \n",
    "    # Reconstruction\n",
    "    img_k = U @ np.diag(Sigma_k) @ VT\n",
    "    \n",
    "    # Afficher\n",
    "    ax.imshow(img_k, cmap='viridis')\n",
    "    ax.set_title(f'Rang k={k}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Approximation de Rang Faible via SVD', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# √ânergie captur√©e\n",
    "total_energy = np.sum(Sigma**2)\n",
    "cumulative_energy = np.cumsum(Sigma**2) / total_energy\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cumulative_energy)+1), cumulative_energy, 'b-o', markersize=4)\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% √©nergie')\n",
    "plt.axhline(y=0.95, color='g', linestyle='--', label='95% √©nergie')\n",
    "plt.xlabel('Nombre de composantes', fontsize=12)\n",
    "plt.ylabel('√ânergie cumulative (fraction)', fontsize=12)\n",
    "plt.title('√ânergie Captur√©e par les Composantes SVD', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Application ML : R√©gression Lin√©aire avec Moindres Carr√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des donn√©es synth√©tiques\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 1) * 2\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Ajouter une colonne de 1 pour le biais\n",
    "X_with_bias = np.c_[np.ones(n_samples), X]\n",
    "\n",
    "print(\"Forme de X (avec biais):\", X_with_bias.shape)\n",
    "print(\"Forme de y:\", y.shape)\n",
    "\n",
    "# Solution analytique : w = (X^T X)^(-1) X^T y\n",
    "w_optimal = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n",
    "\n",
    "print(\"\\nParam√®tres optimaux (m√©thode normale):\")\n",
    "print(f\"  Biais (intercept): {w_optimal[0]:.3f}\")\n",
    "print(f\"  Pente (coefficient): {w_optimal[1]:.3f}\")\n",
    "\n",
    "# Alternative : utiliser la pseudo-inverse\n",
    "X_pinv = np.linalg.pinv(X_with_bias)\n",
    "w_pinv = X_pinv @ y\n",
    "print(\"\\nParam√®tres (pseudo-inverse):\")\n",
    "print(f\"  Biais: {w_pinv[0]:.3f}\")\n",
    "print(f\"  Pente: {w_pinv[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Donn√©es')\n",
    "\n",
    "# Ligne de r√©gression\n",
    "X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "X_line_with_bias = np.c_[np.ones(len(X_line)), X_line]\n",
    "y_pred = X_line_with_bias @ w_optimal\n",
    "\n",
    "plt.plot(X_line, y_pred, 'r-', linewidth=2, \n",
    "         label=f'y = {w_optimal[1]:.2f}x + {w_optimal[0]:.2f}')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('R√©gression Lin√©aire - Solution par Moindres Carr√©s', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculer l'erreur MSE\n",
    "y_pred_train = X_with_bias @ w_optimal\n",
    "mse = np.mean((y - y_pred_train)**2)\n",
    "print(f\"\\nMean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Application : PCA (Principal Component Analysis) avec SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des donn√©es 2D corr√©l√©es\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "# Donn√©es originales\n",
    "X_original = np.random.randn(n, 2)\n",
    "\n",
    "# Rotation et √©tirement\n",
    "angle = np.pi / 4\n",
    "rotation = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "                      [np.sin(angle), np.cos(angle)]])\n",
    "scaling = np.array([[3, 0], [0, 1]])\n",
    "transform = rotation @ scaling\n",
    "\n",
    "X = (transform @ X_original.T).T\n",
    "\n",
    "# Centrer les donn√©es\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# PCA via SVD\n",
    "U, Sigma, VT = np.linalg.svd(X_centered, full_matrices=False)\n",
    "\n",
    "# Les composantes principales sont dans VT\n",
    "principal_components = VT.T\n",
    "\n",
    "print(\"Composantes principales:\")\n",
    "print(principal_components)\n",
    "print(\"\\nVariance expliqu√©e (valeurs singuli√®res au carr√©):\")\n",
    "variance_explained = (Sigma**2) / (n - 1)\n",
    "print(variance_explained)\n",
    "print(\"\\nProportion de variance expliqu√©e:\")\n",
    "print(variance_explained / variance_explained.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser PCA\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Donn√©es originales + composantes principales\n",
    "ax1.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.5)\n",
    "\n",
    "# Tracer les composantes principales\n",
    "mean = X.mean(axis=0)\n",
    "for i in range(2):\n",
    "    pc = principal_components[:, i] * Sigma[i] / 2\n",
    "    ax1.arrow(0, 0, pc[0], pc[1], head_width=0.3, head_length=0.4,\n",
    "              fc=['red', 'blue'][i], ec=['red', 'blue'][i], linewidth=2,\n",
    "              label=f'PC{i+1} ({variance_explained[i]/variance_explained.sum()*100:.1f}%)')\n",
    "\n",
    "ax1.set_aspect('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_title('Donn√©es Originales + Composantes Principales', fontsize=14)\n",
    "ax1.set_xlabel('x‚ÇÅ', fontsize=12)\n",
    "ax1.set_ylabel('x‚ÇÇ', fontsize=12)\n",
    "\n",
    "# Donn√©es projet√©es sur les composantes principales\n",
    "X_pca = X_centered @ principal_components\n",
    "ax2.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_title('Donn√©es dans l\\'Espace PCA', fontsize=14)\n",
    "ax2.set_xlabel('PC1', fontsize=12)\n",
    "ax2.set_ylabel('PC2', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©sum√©\n",
    "\n",
    "Dans ce notebook, nous avons vu :\n",
    "\n",
    "1. **Op√©rations vectorielles** : produit scalaire, normes, distances\n",
    "2. **Matrices** : multiplication, transpos√©e, inverse, d√©terminant\n",
    "3. **Valeurs/vecteurs propres** : d√©composition spectrale, transformations lin√©aires\n",
    "4. **SVD** : d√©composition universelle, approximation de rang faible\n",
    "5. **Application ML** : r√©gression lin√©aire par moindres carr√©s\n",
    "6. **PCA** : r√©duction de dimensionnalit√© via SVD\n",
    "\n",
    "Ces concepts sont **fondamentaux** pour comprendre les algorithmes de ML !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}