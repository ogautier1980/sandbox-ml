{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz d'Auto-√âvaluation - Chapitre 01 : Fondamentaux Math√©matiques\n",
    "\n",
    "**Instructions** :\n",
    "- Ce quiz contient 15 questions pour tester votre compr√©hension du chapitre\n",
    "- R√©pondez aux questions par vous-m√™me avant de regarder les r√©ponses\n",
    "- Les r√©ponses sont dans une cellule masqu√©e √† la fin\n",
    "- Comptez 1 point par bonne r√©ponse\n",
    "\n",
    "**Bar√®me** :\n",
    "- 13-15 : Excellent ! Vous ma√Ætrisez le chapitre üí™\n",
    "- 10-12 : Bien, relisez les sections o√π vous avez des lacunes\n",
    "- 7-9 : Moyen, relisez le chapitre attentivement\n",
    "- < 7 : Insuffisant, reprenez le chapitre depuis le d√©but\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Question 1 : Produit Scalaire\n",
    "Soit $\\vec{u} = (2, -1, 3)$ et $\\vec{v} = (1, 4, 2)$. Calculez le produit scalaire $\\vec{u} \\cdot \\vec{v}$.\n",
    "\n",
    "A) 0  \n",
    "B) 4  \n",
    "C) 6  \n",
    "D) 8  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : Normes Vectorielles\n",
    "Pour le vecteur $\\vec{v} = (3, -4)$, quelle est la norme $L^2$ (euclidienne) ?\n",
    "\n",
    "A) 7  \n",
    "B) 5  \n",
    "C) 25  \n",
    "D) 1  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 : Diff√©rence Norme L1 vs L2\n",
    "Quelle affirmation est CORRECTE concernant les normes $L^1$ et $L^2$ ?\n",
    "\n",
    "A) La norme $L^1$ est toujours sup√©rieure √† la norme $L^2$  \n",
    "B) La norme $L^2$ est plus sensible aux outliers que la norme $L^1$  \n",
    "C) Les normes $L^1$ et $L^2$ sont toujours √©gales  \n",
    "D) La norme $L^1$ est utilis√©e pour Ridge, la norme $L^2$ pour Lasso  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 : Multiplication Matricielle\n",
    "Si $A \\in \\mathbb{R}^{3 \\times 4}$ et $B \\in \\mathbb{R}^{4 \\times 2}$, quelle est la dimension de $AB$ ?\n",
    "\n",
    "A) $3 \\times 2$  \n",
    "B) $4 \\times 4$  \n",
    "C) $3 \\times 4$  \n",
    "D) La multiplication n'est pas possible  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 : Valeurs Propres\n",
    "Une matrice $A$ a pour valeurs propres $\\lambda_1 = 5$ et $\\lambda_2 = 2$. Quelle est la trace de $A$ ?\n",
    "\n",
    "A) 3  \n",
    "B) 7  \n",
    "C) 10  \n",
    "D) Impossible √† d√©terminer sans plus d'informations  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : SVD (D√©composition en Valeurs Singuli√®res)\n",
    "Quelle propri√©t√© de la SVD est FAUSSE ?\n",
    "\n",
    "A) La SVD existe pour toute matrice (carr√©e ou rectangulaire)  \n",
    "B) Les valeurs singuli√®res sont toujours positives ou nulles  \n",
    "C) La SVD n√©cessite que la matrice soit sym√©trique  \n",
    "D) La SVD est utilis√©e en PCA  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 : Moindres Carr√©s\n",
    "La solution des moindres carr√©s pour $A\\vec{x} \\approx \\vec{b}$ est donn√©e par :\n",
    "\n",
    "A) $\\vec{x}^* = A^T\\vec{b}$  \n",
    "B) $\\vec{x}^* = (A^TA)^{-1}A^T\\vec{b}$  \n",
    "C) $\\vec{x}^* = A^{-1}\\vec{b}$  \n",
    "D) $\\vec{x}^* = (AA^T)^{-1}\\vec{b}$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 : Th√©or√®me de Bayes\n",
    "La formule du th√©or√®me de Bayes est $P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$. Quel terme repr√©sente la vraisemblance (likelihood) ?\n",
    "\n",
    "A) $P(A)$  \n",
    "B) $P(B|A)$  \n",
    "C) $P(A|B)$  \n",
    "D) $P(B)$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 : Loi Normale\n",
    "Pour une variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, quel pourcentage des valeurs se trouve dans l'intervalle $[\\mu - 2\\sigma, \\mu + 2\\sigma]$ ?\n",
    "\n",
    "A) 68%  \n",
    "B) 95%  \n",
    "C) 99.7%  \n",
    "D) 50%  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 : Esp√©rance et Variance\n",
    "Si $X$ est une variable al√©atoire avec $\\mathbb{E}[X] = 5$ et $\\text{Var}(X) = 4$, quelle est $\\mathbb{E}[X^2]$ ?\n",
    "\n",
    "A) 9  \n",
    "B) 20  \n",
    "C) 25  \n",
    "D) 29  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 : Corr√©lation de Pearson\n",
    "Le coefficient de corr√©lation de Pearson $\\rho$ prend ses valeurs dans quel intervalle ?\n",
    "\n",
    "A) $[0, 1]$  \n",
    "B) $[-1, 1]$  \n",
    "C) $[0, +\\infty)$  \n",
    "D) $(-\\infty, +\\infty)$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12 : Gradient\n",
    "Le gradient $\\nabla f(\\vec{x})$ d'une fonction $f: \\mathbb{R}^n \\to \\mathbb{R}$ indique :\n",
    "\n",
    "A) La direction de plus forte d√©croissance  \n",
    "B) La direction de plus forte croissance  \n",
    "C) Le minimum de la fonction  \n",
    "D) La d√©riv√©e seconde  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13 : Convexit√©\n",
    "Quelle propri√©t√© est VRAIE pour une fonction convexe ?\n",
    "\n",
    "A) Elle peut avoir plusieurs minima locaux  \n",
    "B) Tout minimum local est un minimum global  \n",
    "C) Sa hessienne est toujours n√©gative  \n",
    "D) Elle ne peut pas √™tre diff√©rentiable  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14 : Descente de Gradient\n",
    "Dans l'algorithme de descente de gradient, la formule de mise √† jour est :\n",
    "\n",
    "A) $\\vec{x}_{k+1} = \\vec{x}_k + \\alpha \\nabla f(\\vec{x}_k)$  \n",
    "B) $\\vec{x}_{k+1} = \\vec{x}_k - \\alpha \\nabla f(\\vec{x}_k)$  \n",
    "C) $\\vec{x}_{k+1} = \\alpha \\nabla f(\\vec{x}_k)$  \n",
    "D) $\\vec{x}_{k+1} = \\nabla f(\\vec{x}_k) / \\alpha$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15 : M√©thode de Newton\n",
    "Pourquoi la m√©thode de Newton est-elle rarement utilis√©e en Deep Learning ?\n",
    "\n",
    "A) Elle ne converge jamais  \n",
    "B) Elle n√©cessite le calcul et l'inversion de la hessienne (co√ªt $O(n^3)$)  \n",
    "C) Elle est moins pr√©cise que la descente de gradient  \n",
    "D) Elle ne fonctionne que pour les fonctions lin√©aires  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Auto-Correction\n",
    "\n",
    "Avant de regarder les r√©ponses, comptez combien de r√©ponses vous avez donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrez vos r√©ponses ici (ex: ['B', 'A', 'C', ...])\n",
    "mes_reponses = []  # TODO: remplir avec vos r√©ponses\n",
    "\n",
    "# R√©ponses correctes (masqu√©es)\n",
    "reponses_correctes = ['B', 'B', 'B', 'A', 'B', 'C', 'B', 'B', 'B', 'D', 'B', 'B', 'B', 'B', 'B']\n",
    "\n",
    "if len(mes_reponses) == 15:\n",
    "    score = sum([1 for i, r in enumerate(mes_reponses) if r.upper() == reponses_correctes[i]])\n",
    "    print(f\"Votre score : {score}/15\")\n",
    "    \n",
    "    if score >= 13:\n",
    "        print(\"\\nüéâ Excellent ! Vous ma√Ætrisez le chapitre !\")\n",
    "    elif score >= 10:\n",
    "        print(\"\\n‚úÖ Bien ! Relisez les sections o√π vous avez des lacunes.\")\n",
    "    elif score >= 7:\n",
    "        print(\"\\n‚ö†Ô∏è  Moyen. Relisez le chapitre attentivement.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Insuffisant. Reprenez le chapitre depuis le d√©but.\")\n",
    "    \n",
    "    # Afficher les erreurs\n",
    "    print(\"\\nD√©tail :\")\n",
    "    for i, (ma_rep, bonne_rep) in enumerate(zip(mes_reponses, reponses_correctes), 1):\n",
    "        if ma_rep.upper() == bonne_rep:\n",
    "            print(f\"Q{i}: ‚úì Correct\")\n",
    "        else:\n",
    "            print(f\"Q{i}: ‚úó Votre r√©ponse: {ma_rep}, Correcte: {bonne_rep}\")\n",
    "else:\n",
    "    print(\"Veuillez remplir toutes les r√©ponses (15 lettres A, B, C ou D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Explications des R√©ponses\n",
    "\n",
    "### Q1 : B\n",
    "$\\vec{u} \\cdot \\vec{v} = (2)(1) + (-1)(4) + (3)(2) = 2 - 4 + 6 = 4$\n",
    "\n",
    "### Q2 : B\n",
    "$\\|\\vec{v}\\|_2 = \\sqrt{3^2 + (-4)^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5$\n",
    "\n",
    "### Q3 : B\n",
    "La norme $L^2$ (au carr√©) p√©nalise fortement les grandes valeurs, donc plus sensible aux outliers. Ridge utilise $L^2$, Lasso utilise $L^1$.\n",
    "\n",
    "### Q4 : A\n",
    "Multiplication de matrices : $(m \\times n) \\times (n \\times p) = (m \\times p)$. Donc $(3 \\times 4) \\times (4 \\times 2) = (3 \\times 2)$.\n",
    "\n",
    "### Q5 : B\n",
    "La trace d'une matrice est √©gale √† la somme de ses valeurs propres : $\\text{tr}(A) = \\lambda_1 + \\lambda_2 = 5 + 2 = 7$.\n",
    "\n",
    "### Q6 : C\n",
    "**FAUX** : La SVD existe pour **toute** matrice (carr√©e ou rectangulaire, sym√©trique ou non). C'est la diagonalisation classique qui n√©cessite une matrice carr√©e.\n",
    "\n",
    "### Q7 : B\n",
    "La solution des moindres carr√©s (√©quations normales) est $\\vec{x}^* = (A^TA)^{-1}A^T\\vec{b}$.\n",
    "\n",
    "### Q8 : B\n",
    "Dans le th√©or√®me de Bayes, $P(B|A)$ est la **vraisemblance** (likelihood), $P(A)$ est la probabilit√© a priori, $P(A|B)$ est la probabilit√© a posteriori, $P(B)$ est l'√©vidence.\n",
    "\n",
    "### Q9 : B\n",
    "Pour une loi normale : 68% dans $[\\mu - \\sigma, \\mu + \\sigma]$, **95% dans $[\\mu - 2\\sigma, \\mu + 2\\sigma]$**, 99.7% dans $[\\mu - 3\\sigma, \\mu + 3\\sigma]$.\n",
    "\n",
    "### Q10 : D\n",
    "$\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\Rightarrow \\mathbb{E}[X^2] = \\text{Var}(X) + (\\mathbb{E}[X])^2 = 4 + 5^2 = 4 + 25 = 29$.\n",
    "\n",
    "### Q11 : B\n",
    "Le coefficient de corr√©lation de Pearson $\\rho \\in [-1, 1]$. $\\rho = 1$ (corr√©lation positive parfaite), $\\rho = -1$ (corr√©lation n√©gative parfaite), $\\rho = 0$ (pas de corr√©lation lin√©aire).\n",
    "\n",
    "### Q12 : B\n",
    "Le gradient $\\nabla f$ pointe dans la direction de **plus forte croissance**. $-\\nabla f$ pointe vers la plus forte d√©croissance (utilis√© dans la descente de gradient).\n",
    "\n",
    "### Q13 : B\n",
    "Propri√©t√© fondamentale d'une fonction convexe : **tout minimum local est un minimum global**. Sa hessienne est semi-d√©finie positive (pas n√©gative).\n",
    "\n",
    "### Q14 : B\n",
    "Descente de gradient : $\\vec{x}_{k+1} = \\vec{x}_k - \\alpha \\nabla f(\\vec{x}_k)$ (on se d√©place dans la direction oppos√©e au gradient, avec un pas $\\alpha > 0$).\n",
    "\n",
    "### Q15 : B\n",
    "La m√©thode de Newton n√©cessite le **calcul et l'inversion de la hessienne** (matrice $n \\times n$), ce qui co√ªte $O(n^3)$ - prohibitif en Deep Learning o√π $n$ peut √™tre de l'ordre de millions de param√®tres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaines √âtapes\n",
    "\n",
    "- **Score < 10** : Relisez le chapitre 01 attentivement, en particulier les sections Alg√®bre lin√©aire, Probabilit√©s, et Optimisation\n",
    "- **Score >= 10** : Passez au Chapitre 02 (M√©triques d'√âvaluation)\n",
    "- **R√©vision recommand√©e** : Refaites le quiz dans 2-3 jours pour ancrer les connaissances\n",
    "- **Pratique** : Consultez les notebooks `01_demo_*.ipynb` pour des exercices pratiques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
