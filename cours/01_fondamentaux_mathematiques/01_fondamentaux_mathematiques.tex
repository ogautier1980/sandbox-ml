% Chapitre 01 - Fondamentaux Math√©matiques pour le Machine Learning

\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}

% Math√©matiques
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

% Mise en page
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}

% Graphiques et couleurs
\usepackage{graphicx}
\usepackage{xcolor}

% ===== UNICODE CHARACTERS SUPPORT =====
\usepackage{newunicodechar}

% Emojis et symboles
\newunicodechar{‚úÖ}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚ùå}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚úì}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚úó}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚ö†}{\textcolor{orange!80!black}{\textbf{/!\textbackslash}}}
\newunicodechar{üí°}{\textcolor{blue!70!black}{\textbf{(i)}}}
\newunicodechar{üéØ}{\textcolor{purple!70!black}{\textbf{$\star$}}}
\newunicodechar{üìä}{\textcolor{blue!70!black}{\textbf{[=]}}}

% √âtoiles (pour tableaux)
\newunicodechar{‚òÖ}{\textcolor{orange!80!black}{$\star$}}
\newunicodechar{‚òÜ}{\textcolor{gray!50}{$\star$}}

% Fl√®ches
\newunicodechar{‚Üí}{$\rightarrow$}
\newunicodechar{‚Üê}{$\leftarrow$}
\newunicodechar{‚Üë}{$\uparrow$}
\newunicodechar{‚Üì}{$\downarrow$}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, matrix}

% Tableaux
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}

% Code et algorithmes
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Hyperliens
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 01 - Fondamentaux Math√©matiques},
    pdfauthor={Cours ML},
}

% Boxes color√©es
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}


% ===== TCOLORBOX AVEC EMOJIS =====
\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö† Attention,
    breakable
}

\newtcolorbox{definition}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition,
    breakable
}

\newtcolorbox{astuce}{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

\newtcolorbox{remarque}{
    colback=yellow!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=üí° Remarque,
    breakable
}

\newtcolorbox{important}{
    colback=purple!5!white,
    colframe=purple!75!black,
    fonttitle=\bfseries,
    title=‚ö† Important,
    breakable
}

\newtcolorbox{exemple}{
    colback=gray!5!white,
    colframe=gray!75!black,
    fonttitle=\bfseries,
    title=Exemple,
    breakable
}

% En-t√™tes et pieds de page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 01 - Fondamentaux Math√©matiques}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIGURATION LISTINGS (code Python) =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black},
    inputencoding=utf8,
    extendedchars=true,
    literate=
        {√©}{{\'e}}{1}
        {√®}{{\`e}}{1}
        {√™}{{\^e}}{1}
        {√†}{{\`a}}{1}
        {√¢}{{\^a}}{1}
        {√π}{{\`u}}{1}
        {√ª}{{\^u}}{1}
        {√¥}{{\^o}}{1}
        {√Æ}{{\^i}}{1}
        {√Ø}{{\"i}}{1}
        {√´}{{\"e}}{1}
        {√ß}{{\c{c}}}{1}
}
\lstset{style=pythonstyle}

% ===== CONFIGURATION TCOLORBOX =====
% Box pour d√©finitions


% Box pour th√©or√®mes
\newtcolorbox{theoreme}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Th√©or√®me: #1,
    breakable
}

% Box pour exemples


% Box pour attention/warning


% Box pour astuce/tips


% Box pour intuition
\newtcolorbox{intuition}{
    colback=purple!5!white,
    colframe=purple!75!black,
    fonttitle=\bfseries,
    title=Intuition,
    breakable
}

% Box pour "Pourquoi c'est important"
\newtcolorbox{pourquoi}{
    colback=cyan!5!white,
    colframe=cyan!75!black,
    fonttitle=\bfseries,
    title=Pourquoi c'est important en ML,
    breakable
}

% Box pour mini-projet
\newtcolorbox{miniprojet}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Mini-Projet: #1,
    breakable
}

% ===== COMMANDES PERSONNALIS√âES =====
\newcommand{\vect}[1]{\mathbf{#1}}  % Vecteur
\newcommand{\mat}[1]{\mathbf{#1}}   % Matrice
\newcommand{\R}{\mathbb{R}}         % R√©els
\newcommand{\N}{\mathbb{N}}         % Naturels
\newcommand{\Z}{\mathbb{Z}}         % Entiers
\newcommand{\C}{\mathbb{C}}         % Complexes
\newcommand{\E}{\mathbb{E}}         % Esp√©rance
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\tr}{\operatorname{tr}}  % Trace
\newcommand{\rank}{\operatorname{rank}}  % Rang

% ===== D√âBUT DU DOCUMENT =====
\begin{document}

% ===== PAGE DE TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]

    \vspace{1cm}

    {\LARGE Chapitre 01}\\[0.3cm]
    {\LARGE\bfseries Fondamentaux Math√©matiques}\\[2cm]

    \vfill

    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Ma√Ætriser les concepts d'alg√®bre lin√©aire essentiels (vecteurs, matrices, d√©compositions)
        \item Comprendre les probabilit√©s et statistiques n√©cessaires au ML
        \item Appr√©hender le calcul diff√©rentiel et les techniques d'optimisation
        \item Savoir appliquer ces outils math√©matiques aux algorithmes de ML
    \end{itemize}
    }

    \vfill

    {\large
    \textbf{Pr√©requis :} Math√©matiques niveau Licence (alg√®bre, analyse)\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 5-7 heures\\[0.3cm]
    \textbf{Notebooks :} \texttt{01\_demo\_*.ipynb}
    }

    \vfill

    {\large Cours ML - Sandbox-ML\\
    Version 1.0 - 2026}
\end{titlepage}

% ===== TABLE DES MATI√àRES =====
\tableofcontents
\newpage

% ===== PARTIE I: ALG√àBRE LIN√âAIRE =====
\part{Alg√®bre Lin√©aire}

\begin{pourquoi}
L'alg√®bre lin√©aire est \textbf{partout} en Machine Learning :
\begin{itemize}
    \item \textbf{Vos donn√©es} : Chaque image 28√ó28 pixels = un vecteur de 784 dimensions
    \item \textbf{Les mod√®les} : Un r√©seau de neurones = une s√©rie de multiplications matricielles
    \item \textbf{La r√©duction de dimension} : PCA pour passer de 10,000 features √† 50 = d√©composition SVD
    \item \textbf{L'entra√Ænement} : Trouver les meilleurs poids $\vect{w}$ = r√©soudre un syst√®me lin√©aire
\end{itemize}

Sans alg√®bre lin√©aire, impossible de faire du ML moderne !
\end{pourquoi}

\section{Vecteurs et Espaces Vectoriels}

\subsection{D√©finitions Fondamentales}

\begin{definition}{Vecteur}
Un vecteur $\vect{v} \in \R^n$ est un tuple ordonn√© de $n$ nombres r√©els :
\begin{equation}
    \vect{v} = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}
\end{equation}
On le repr√©sente comme un vecteur colonne par d√©faut. Un vecteur ligne s'√©crit $\vect{v}^T$.
\end{definition}

\textbf{Interpr√©tations :}
\begin{itemize}
    \item \textbf{G√©om√©trique :} Point ou fl√®che dans l'espace $\R^n$
    \item \textbf{ML :} Une instance de donn√©es (sample), un vecteur de features
    \item \textbf{Alg√©brique :} √âl√©ment d'un espace vectoriel
\end{itemize}

\begin{exemple}{Vecteur de features}
Un appartement d√©crit par 3 features :
\begin{equation}
    \vect{x} = \begin{pmatrix} 75 \\ 3 \\ 2010 \end{pmatrix} \quad
    \begin{matrix}
        \text{(surface en m}^2\text{)} \\
        \text{(nombre de pi√®ces)} \\
        \text{(ann√©e de construction)}
    \end{matrix}
\end{equation}
\end{exemple}

\subsection{Op√©rations sur les Vecteurs}

\textbf{Addition vectorielle :}
\begin{equation}
    \vect{u} + \vect{v} = \begin{pmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{pmatrix}
\end{equation}

\textbf{Multiplication par un scalaire :}
\begin{equation}
    \alpha \vect{v} = \begin{pmatrix} \alpha v_1 \\ \alpha v_2 \\ \vdots \\ \alpha v_n \end{pmatrix}
\end{equation}

\subsection{Produit Scalaire (Dot Product)}

\begin{intuition}
Imaginez deux films d√©crits par leurs genres :
\begin{itemize}
    \item Film A : $\vect{u} = \begin{pmatrix} 0.9 \\ 0.1 \\ 0.2 \end{pmatrix}$ (Action, Romance, Com√©die)
    \item Film B : $\vect{v} = \begin{pmatrix} 0.8 \\ 0.3 \\ 0.1 \end{pmatrix}$
\end{itemize}

Le produit scalaire $\vect{u} \cdot \vect{v} = 0.9 \times 0.8 + 0.1 \times 0.3 + 0.2 \times 0.1 = 0.77$ mesure √† quel point ces films sont similaires ! Plus le r√©sultat est grand, plus ils se ressemblent.

\textbf{Application concr√®te :} Syst√®mes de recommandation Netflix, Spotify...
\end{intuition}

\begin{definition}{Produit scalaire}
Le produit scalaire de deux vecteurs $\vect{u}, \vect{v} \in \R^n$ est :
\begin{equation}
    \vect{u} \cdot \vect{v} = \vect{u}^T \vect{v} = \sum_{i=1}^n u_i v_i = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n
\end{equation}
\end{definition}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item Commutativit√© : $\vect{u} \cdot \vect{v} = \vect{v} \cdot \vect{u}$
    \item Lin√©arit√© : $(\alpha\vect{u} + \beta\vect{w}) \cdot \vect{v} = \alpha(\vect{u} \cdot \vect{v}) + \beta(\vect{w} \cdot \vect{v})$
    \item $\vect{v} \cdot \vect{v} \geq 0$, √©galit√© ssi $\vect{v} = \vect{0}$
\end{itemize}

\textbf{Interpr√©tation g√©om√©trique :}
\begin{equation}
    \vect{u} \cdot \vect{v} = \|\vect{u}\| \|\vect{v}\| \cos(\theta)
\end{equation}
o√π $\theta$ est l'angle entre $\vect{u}$ et $\vect{v}$.

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.2]
    % Axes
    \draw[->] (-0.5,0) -- (5,0) node[right] {$x$};
    \draw[->] (0,-0.5) -- (0,4) node[above] {$y$};

    % Vecteur u
    \draw[->, very thick, blue] (0,0) -- (4,1);
    \node[blue, font=\small, below right] at (2,0.5) {$\vect{u}$};
    \node[blue, font=\small, right] at (4.2,1) {$(4, 1)$};

    % Vecteur v
    \draw[->, very thick, red] (0,0) -- (2,3);
    \node[red, font=\small, left] at (1,1.5) {$\vect{v}$};
    \node[red, font=\small, above] at (2,3.2) {$(2, 3)$};

    % Angle theta
    \draw[green!60!black, thick] (0.8,0) arc (0:56:0.8);
    \node[green!60!black, font=\small] at (1.3,0.4) {$\theta$};

    % Projection
    \draw[dashed, purple] (2,3) -- (2.8,0.7);
    \filldraw[purple] (2.8,0.7) circle (1.5pt);

    % Annotations
    \node[purple, font=\small, right, align=left] at (3.0,1.85) {Projection\\[-2pt]de $\vect{v}$ sur $\vect{u}$};

    % Calculs
    \node[align=left, draw, fill=yellow!10, rounded corners] at (7,2) {
        $\vect{u} \cdot \vect{v} = 4 \times 2 + 1 \times 3 = 11$\\[5pt]
        $\|\vect{u}\| = \sqrt{16+1} = \sqrt{17}$\\
        $\|\vect{v}\| = \sqrt{4+9} = \sqrt{13}$\\[5pt]
        $\cos(\theta) = \frac{11}{\sqrt{17}\sqrt{13}} \approx 0.74$\\[3pt]
        $\theta \approx 42¬∞$
    };

\end{tikzpicture}
\caption{Interpr√©tation g√©om√©trique du produit scalaire: $\vect{u} \cdot \vect{v}$ mesure √† quel point les vecteurs pointent dans la m√™me direction. L'angle $\theta$ entre eux d√©termine le cosinus, donc la similarit√©.}
\label{fig:dot_product}
\end{figure}

\begin{astuce}
\textbf{En Machine Learning :}
\begin{itemize}
    \item Le produit scalaire mesure la \textbf{similarit√©} entre vecteurs
    \item Deux vecteurs orthogonaux ($\vect{u} \cdot \vect{v} = 0$) sont \textbf{non corr√©l√©s}
    \item Utilis√© partout : r√©gression lin√©aire ($\vect{w}^T\vect{x}$), r√©seaux de neurones, kernels...
\end{itemize}
\end{astuce}

\subsection{Norme Vectorielle}

\begin{definition}{Norme $L^p$}
La norme $L^p$ d'un vecteur $\vect{v} \in \R^n$ est :
\begin{equation}
    \|\vect{v}\|_p = \left(\sum_{i=1}^n |v_i|^p\right)^{1/p}
\end{equation}
\end{definition}

\textbf{Normes principales :}

\begin{itemize}
    \item \textbf{Norme $L^1$ (Manhattan)} :
    \begin{equation}
        \|\vect{v}\|_1 = \sum_{i=1}^n |v_i|
    \end{equation}
    Utilis√©e pour la r√©gularisation Lasso, robuste aux outliers.

    \item \textbf{Norme $L^2$ (Euclidienne)} :
    \begin{equation}
        \|\vect{v}\|_2 = \sqrt{\sum_{i=1}^n v_i^2} = \sqrt{\vect{v}^T \vect{v}}
    \end{equation}
    La plus courante en ML. Utilis√©e pour mesurer des distances.

    \item \textbf{Norme $L^\infty$ (Maximum)} :
    \begin{equation}
        \|\vect{v}\|_\infty = \max_i |v_i|
    \end{equation}
\end{itemize}

\begin{exemple}{Calcul de normes}
Pour $\vect{v} = \begin{pmatrix} 3 \\ -4 \end{pmatrix}$ :
\begin{align}
    \|\vect{v}\|_1 &= |3| + |-4| = 7 \\
    \|\vect{v}\|_2 &= \sqrt{3^2 + (-4)^2} = \sqrt{25} = 5 \\
    \|\vect{v}\|_\infty &= \max(|3|, |-4|) = 4
\end{align}
\end{exemple}

\subsection{Distance entre Vecteurs}

\begin{definition}{Distance euclidienne}
La distance entre deux points $\vect{u}, \vect{v} \in \R^n$ est :
\begin{equation}
    d(\vect{u}, \vect{v}) = \|\vect{u} - \vect{v}\|_2 = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}
\end{equation}
\end{definition}

\textbf{Applications en ML :}
\begin{itemize}
    \item K-Nearest Neighbors (KNN)
    \item Clustering (K-Means)
    \item Embeddings (mesurer similarit√© s√©mantique)
\end{itemize}

% ===== SECTION 2: MATRICES =====
\section{Matrices}

\subsection{D√©finitions et Notation}

\begin{definition}{Matrice}
Une matrice $\mat{A} \in \R^{m \times n}$ est un tableau rectangulaire de $m$ lignes et $n$ colonnes :
\begin{equation}
    \mat{A} = \begin{pmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix}
\end{equation}
\end{definition}

\textbf{Notation :}
\begin{itemize}
    \item $a_{ij}$ : √©l√©ment ligne $i$, colonne $j$
    \item $\mat{A}_{i:}$ : $i$-√®me ligne (vecteur ligne)
    \item $\mat{A}_{:j}$ : $j$-√®me colonne (vecteur colonne)
\end{itemize}

\textbf{Interpr√©tation ML :}
\begin{itemize}
    \item \textbf{Dataset} : $\mat{X} \in \R^{n \times d}$ o√π $n$ = nombre d'instances, $d$ = nombre de features
    \item Chaque ligne $\mat{X}_{i:} = \vect{x}_i^T$ est une instance
    \item Chaque colonne $\mat{X}_{:j}$ est une feature
\end{itemize}

\subsection{Op√©rations Matricielles}

\textbf{Addition :} $(\mat{A} + \mat{B})_{ij} = a_{ij} + b_{ij}$ (m√™me dimension)

\textbf{Multiplication par scalaire :} $(\alpha \mat{A})_{ij} = \alpha a_{ij}$

\textbf{Transpos√©e :}
\begin{equation}
    (\mat{A}^T)_{ij} = a_{ji}
\end{equation}

\textbf{Propri√©t√©s de la transpos√©e :}
\begin{itemize}
    \item $(\mat{A}^T)^T = \mat{A}$
    \item $(\mat{A} + \mat{B})^T = \mat{A}^T + \mat{B}^T$
    \item $(\mat{A}\mat{B})^T = \mat{B}^T\mat{A}^T$
\end{itemize}

\subsection{Multiplication Matricielle}

\begin{definition}{Produit matriciel}
Si $\mat{A} \in \R^{m \times n}$ et $\mat{B} \in \R^{n \times p}$, alors $\mat{C} = \mat{A}\mat{B} \in \R^{m \times p}$ avec :
\begin{equation}
    c_{ij} = \sum_{k=1}^n a_{ik} b_{kj} = \mat{A}_{i:} \cdot \mat{B}_{:j}
\end{equation}
\end{definition}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.8]
    % Matrice A (2x3)
    \matrix[matrix of math nodes, left delimiter=(, right delimiter=),
            nodes={minimum width=1cm, minimum height=0.8cm}] (A) at (0,0) {
        2 & 1 & 3 \\
        4 & 0 & 1 \\
    };
    \node[above] at (A.north) {$\mat{A}_{2 \times 3}$};

    % Symbole multiplication
    \node at (3,0) {$\times$};

    % Matrice B (3x2)
    \matrix[matrix of math nodes, left delimiter=(, right delimiter=),
            nodes={minimum width=1cm, minimum height=0.8cm}] (B) at (5,0) {
        1 & 2 \\
        0 & 3 \\
        2 & 1 \\
    };
    \node[above] at (B.north) {$\mat{B}_{3 \times 2}$};

    % Symbole √©gal
    \node at (8,0) {$=$};

    % Matrice C (2x2)
    \matrix[matrix of math nodes, left delimiter=(, right delimiter=),
            nodes={minimum width=1.2cm, minimum height=0.8cm}] (C) at (10.5,0) {
        |[fill=green!20]| 8 & 10 \\
        6 & 9 \\
    };
    \node[above] at (C.north) {$\mat{C}_{2 \times 2}$};

    % Mise en √©vidence pour c_11
    \draw[blue, very thick] (A-1-1.north west) rectangle (A-1-3.south east);
    \draw[red, very thick] (B-1-2.north west) rectangle (B-3-2.south east);

    % Fl√®ches et calcul
    \draw[->, blue, thick] (A-1-2.south) -- ++(0,-0.8) -- ++(8,0) -- (C-1-1.west);
    \draw[->, red, thick] (B-2-2.east) -- ++(0.8,0) -- ++(0,-0.5) -- (C-1-1.north);

    % Explication du calcul
    \node[align=left, draw, fill=yellow!10, rounded corners, text width=6cm] at (5,-3.5) {
        \textbf{Calcul de } $c_{11}$ (√©l√©ment vert):\\[5pt]
        Ligne 1 de $\mat{A}$ $\times$ Colonne 1 de $\mat{B}$:\\[3pt]
        $c_{11} = 2 \times 1 + 1 \times 0 + 3 \times 2$\\
        $c_{11} = 2 + 0 + 6 = 8$
    };

\end{tikzpicture}
\caption{Multiplication matricielle: chaque √©l√©ment $c_{ij}$ est le produit scalaire de la ligne $i$ de $\mat{A}$ et de la colonne $j$ de $\mat{B}$. Les dimensions doivent √™tre compatibles: $(m \times n) \times (n \times p) = (m \times p)$.}
\label{fig:matrix_multiplication}
\end{figure}

\begin{attention}
\textbf{Attention :}
\begin{itemize}
    \item La multiplication matricielle n'est \textbf{pas commutative} : $\mat{A}\mat{B} \neq \mat{B}\mat{A}$ en g√©n√©ral
    \item Les dimensions doivent √™tre compatibles : $(m \times n) \times (n \times p) \to (m \times p)$
\end{itemize}
\end{attention}

\textbf{Complexit√© :} Multiplication na√Øve : $O(mnp)$ op√©rations

\begin{exemple}{Pr√©diction en r√©gression lin√©aire}
Pour $n$ instances avec $d$ features, la pr√©diction vectorielle s'√©crit :
\begin{equation}
    \vect{\hat{y}} = \mat{X} \vect{w} + b\mathbf{1}
\end{equation}
o√π $\mat{X} \in \R^{n \times d}$, $\vect{w} \in \R^d$, $\vect{\hat{y}} \in \R^n$. Une seule op√©ration matricielle remplace $n$ produits scalaires !
\end{exemple}

\subsection{Matrices Sp√©ciales}

\textbf{Matrice identit√© :}
\begin{equation}
    \mat{I}_n = \begin{pmatrix}
        1 & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1
    \end{pmatrix}, \quad \mat{A}\mat{I} = \mat{I}\mat{A} = \mat{A}
\end{equation}

\textbf{Matrice diagonale :}
\begin{equation}
    \mat{D} = \begin{pmatrix}
        d_1 & 0 & \cdots & 0 \\
        0 & d_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & d_n
    \end{pmatrix}
\end{equation}

\textbf{Matrice sym√©trique :} $\mat{A} = \mat{A}^T$ (ex: matrices de covariance)

\textbf{Matrice orthogonale :} $\mat{Q}^T\mat{Q} = \mat{Q}\mat{Q}^T = \mat{I}$

\subsection{D√©terminant}

\begin{definition}{D√©terminant}
Pour une matrice carr√©e $\mat{A} \in \R^{n \times n}$, le d√©terminant $\det(\mat{A})$ mesure le ¬´ volume orient√© ¬ª de la transformation lin√©aire associ√©e.
\end{definition}

\textbf{Cas $2 \times 2$ :}
\begin{equation}
    \det\begin{pmatrix} a & b \\ c & d \end{pmatrix} = ad - bc
\end{equation}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item $\det(\mat{A}\mat{B}) = \det(\mat{A}) \det(\mat{B})$
    \item $\det(\mat{A}^T) = \det(\mat{A})$
    \item $\det(\mat{A}^{-1}) = 1/\det(\mat{A})$
    \item Si $\det(\mat{A}) = 0$, alors $\mat{A}$ n'est pas inversible (singuli√®re)
\end{itemize}

\subsection{Inverse de Matrice}

\begin{definition}{Matrice inverse}
Pour une matrice carr√©e $\mat{A} \in \R^{n \times n}$, l'inverse $\mat{A}^{-1}$ (si elle existe) v√©rifie :
\begin{equation}
    \mat{A}\mat{A}^{-1} = \mat{A}^{-1}\mat{A} = \mat{I}
\end{equation}
\end{definition}

\textbf{Conditions d'existence :}
\begin{itemize}
    \item $\mat{A}$ doit √™tre carr√©e
    \item $\det(\mat{A}) \neq 0$ (matrice non-singuli√®re)
\end{itemize}

\textbf{Cas $2 \times 2$ :}
\begin{equation}
    \begin{pmatrix} a & b \\ c & d \end{pmatrix}^{-1} = \frac{1}{ad-bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}
\end{equation}

\begin{attention}
En pratique, on \textbf{n'inverse jamais explicitement} une matrice ! On r√©sout plut√¥t le syst√®me lin√©aire $\mat{A}\vect{x} = \vect{b}$ via des m√©thodes num√©riques (d√©composition LU, Cholesky, etc.).
\end{attention}

\subsection{Trace}

\begin{definition}{Trace}
La trace d'une matrice carr√©e $\mat{A} \in \R^{n \times n}$ est la somme de ses √©l√©ments diagonaux :
\begin{equation}
    \tr(\mat{A}) = \sum_{i=1}^n a_{ii}
\end{equation}
\end{definition}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item $\tr(\mat{A} + \mat{B}) = \tr(\mat{A}) + \tr(\mat{B})$
    \item $\tr(\mat{A}\mat{B}) = \tr(\mat{B}\mat{A})$ (propri√©t√© cyclique)
    \item $\tr(\mat{A}) = \sum_i \lambda_i$ o√π $\lambda_i$ sont les valeurs propres
\end{itemize}

\subsection{Rang}

\begin{definition}{Rang}
Le rang d'une matrice $\mat{A} \in \R^{m \times n}$ est le nombre maximal de colonnes (ou lignes) lin√©airement ind√©pendantes.
\end{definition}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item $\rank(\mat{A}) \leq \min(m, n)$
    \item $\rank(\mat{A}) = \rank(\mat{A}^T)$
    \item Si $\rank(\mat{A}) = n$ (rang plein en colonnes), alors $\mat{A}^T\mat{A}$ est inversible
\end{itemize}

\textbf{En ML :}
\begin{itemize}
    \item Rang faible $\Rightarrow$ redondance dans les features
    \item PCA, SVD exploitent les structures de rang faible
\end{itemize}

% ===== SECTION 3: SYST√àMES LIN√âAIRES =====
\section{Syst√®mes Lin√©aires et R√©solution}

\subsection{Formulation}

Un syst√®me lin√©aire de $m$ √©quations √† $n$ inconnues s'√©crit :
\begin{equation}
    \mat{A}\vect{x} = \vect{b}
\end{equation}
o√π $\mat{A} \in \R^{m \times n}$, $\vect{x} \in \R^n$, $\vect{b} \in \R^m$.

\textbf{Cas en ML :}
\begin{itemize}
    \item \textbf{R√©gression lin√©aire} : Trouver $\vect{w}$ tel que $\mat{X}\vect{w} \approx \vect{y}$
    \item \textbf{Optimisation} : R√©soudre $\nabla f(\vect{x}) = \vect{0}$ (gradient nul)
\end{itemize}

\subsection{Solutions}

\textbf{Cas carr√© ($m = n$) :}
\begin{itemize}
    \item Si $\det(\mat{A}) \neq 0$ : solution unique $\vect{x} = \mat{A}^{-1}\vect{b}$
    \item Si $\det(\mat{A}) = 0$ : aucune solution ou infinit√© de solutions
\end{itemize}

\textbf{Cas rectangulaire ($m \neq n$) :}
\begin{itemize}
    \item $m > n$ (surd√©termin√©) : en g√©n√©ral pas de solution exacte $\Rightarrow$ solution des moindres carr√©s
    \item $m < n$ (sous-d√©termin√©) : infinit√© de solutions $\Rightarrow$ choisir celle de norme minimale
\end{itemize}

\subsection{Solution des Moindres Carr√©s}

\begin{theoreme}{Solution des moindres carr√©s}
Pour le syst√®me surd√©termin√© $\mat{A}\vect{x} \approx \vect{b}$, la solution qui minimise $\|\mat{A}\vect{x} - \vect{b}\|_2^2$ est :
\begin{equation}
    \vect{x}^* = (\mat{A}^T\mat{A})^{-1}\mat{A}^T\vect{b}
\end{equation}
sous r√©serve que $\mat{A}^T\mat{A}$ soit inversible.
\end{theoreme}

\textbf{Application directe :} R√©gression lin√©aire ! Si $\mat{X} \in \R^{n \times d}$ et $\vect{y} \in \R^n$, alors :
\begin{equation}
    \vect{w}^* = (\mat{X}^T\mat{X})^{-1}\mat{X}^T\vect{y}
\end{equation}

\begin{astuce}
La matrice $\mat{A}^\dagger = (\mat{A}^T\mat{A})^{-1}\mat{A}^T$ s'appelle la \textbf{pseudo-inverse de Moore-Penrose}. En Python : \texttt{numpy.linalg.pinv(A)}.
\end{astuce}

% ===== SECTION 4: VALEURS PROPRES =====
\section{Valeurs et Vecteurs Propres}

\subsection{D√©finitions}

\begin{definition}{Valeur propre et vecteur propre}
Pour une matrice carr√©e $\mat{A} \in \R^{n \times n}$, $\lambda \in \C$ est une \textbf{valeur propre} et $\vect{v} \neq \vect{0}$ est un \textbf{vecteur propre} associ√© si :
\begin{equation}
    \mat{A}\vect{v} = \lambda \vect{v}
\end{equation}
\end{definition}

\textbf{Interpr√©tation :} La transformation $\mat{A}$ √©tire le vecteur $\vect{v}$ par un facteur $\lambda$, sans changer sa direction.

\subsection{Calcul des Valeurs Propres}

Les valeurs propres sont les racines du \textbf{polyn√¥me caract√©ristique} :
\begin{equation}
    \det(\mat{A} - \lambda \mat{I}) = 0
\end{equation}

\begin{exemple}{Matrice $2 \times 2$}
Pour $\mat{A} = \begin{pmatrix} 4 & 2 \\ 1 & 3 \end{pmatrix}$ :
\begin{align}
    \det(\mat{A} - \lambda\mat{I}) &= \det\begin{pmatrix} 4-\lambda & 2 \\ 1 & 3-\lambda \end{pmatrix} \\
    &= (4-\lambda)(3-\lambda) - 2 \times 1 \\
    &= \lambda^2 - 7\lambda + 10 = 0
\end{align}
Solutions : $\lambda_1 = 5$, $\lambda_2 = 2$.
\end{exemple}

\subsection{Diagonalisation}

\begin{theoreme}{Diagonalisation}
Si $\mat{A} \in \R^{n \times n}$ poss√®de $n$ vecteurs propres lin√©airement ind√©pendants $\vect{v}_1, \ldots, \vect{v}_n$ avec valeurs propres $\lambda_1, \ldots, \lambda_n$, alors :
\begin{equation}
    \mat{A} = \mat{P}\mat{D}\mat{P}^{-1}
\end{equation}
o√π $\mat{P} = [\vect{v}_1 \, \vect{v}_2 \, \cdots \, \vect{v}_n]$ et $\mat{D} = \text{diag}(\lambda_1, \ldots, \lambda_n)$.
\end{theoreme}

\textbf{Avantages de la diagonalisation :}
\begin{itemize}
    \item Calcul de puissances : $\mat{A}^k = \mat{P}\mat{D}^k\mat{P}^{-1}$
    \item Exponentielle : $e^{\mat{A}} = \mat{P}e^{\mat{D}}\mat{P}^{-1}$
    \item Comprendre la dynamique des syst√®mes lin√©aires
\end{itemize}

\subsection{Matrices Sym√©triques R√©elles}

\begin{theoreme}{Th√©or√®me spectral}
Toute matrice sym√©trique r√©elle $\mat{A} \in \R^{n \times n}$ est diagonalisable avec :
\begin{itemize}
    \item Toutes les valeurs propres sont \textbf{r√©elles}
    \item Les vecteurs propres peuvent √™tre choisis \textbf{orthonormaux}
    \item D√©composition : $\mat{A} = \mat{Q}\mat{\Lambda}\mat{Q}^T$ o√π $\mat{Q}$ est orthogonale
\end{itemize}
\end{theoreme}

\textbf{Applications en ML :}
\begin{itemize}
    \item PCA (Principal Component Analysis)
    \item Matrices de covariance
    \item Kernel PCA
\end{itemize}

% ===== SECTION 5: D√âCOMPOSITIONS =====
\section{D√©compositions Matricielles}

\subsection{D√©composition en Valeurs Singuli√®res (SVD)}

\begin{intuition}
Imaginez une image comme une recette de cuisine. La SVD dit : "cette image de 1000√ó1000 pixels est en fait un m√©lange de" :
\begin{itemize}
    \item 40\% de "pattern horizontal" ($\sigma_1 = 400$)
    \item 30\% de "pattern vertical" ($\sigma_2 = 300$)
    \item 20\% de "pattern diagonal" ($\sigma_3 = 200$)
    \item ...
    \item 0.1\% de bruit ($\sigma_{999}, \sigma_{1000}$ tr√®s petits)
\end{itemize}

\textbf{L'astuce :} On peut jeter le bruit (les petites valeurs) et garder seulement les 50 premiers "patterns" essentiels. R√©sultat : on passe de 1,000,000 de nombres √† stocker √† seulement 50,000 ‚Üí \textbf{compression de 95\% !}

\textbf{C'est exactement ce que fait} : Netflix (recommandations), JPEG (compression d'images), PCA (r√©duction de dimension)
\end{intuition}

\begin{theoreme}{SVD (Singular Value Decomposition)}
Toute matrice $\mat{A} \in \R^{m \times n}$ peut √™tre d√©compos√©e en :
\begin{equation}
    \mat{A} = \mat{U}\mat{\Sigma}\mat{V}^T
\end{equation}
o√π :
\begin{itemize}
    \item $\mat{U} \in \R^{m \times m}$ : matrice orthogonale (vecteurs singuliers gauches)
    \item $\mat{\Sigma} \in \R^{m \times n}$ : matrice diagonale avec valeurs singuli√®res $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$
    \item $\mat{V} \in \R^{n \times n}$ : matrice orthogonale (vecteurs singuliers droits)
\end{itemize}
\end{theoreme}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item Les valeurs singuli√®res sont les racines carr√©es des valeurs propres de $\mat{A}^T\mat{A}$
    \item $\rank(\mat{A}) = $ nombre de valeurs singuli√®res non nulles
    \item Fonctionne pour \textbf{toute} matrice (pas besoin qu'elle soit carr√©e ou sym√©trique)
\end{itemize}

\begin{astuce}
\textbf{SVD en Python :}
\begin{lstlisting}[language=Python]
import numpy as np
U, Sigma, VT = np.linalg.svd(A, full_matrices=False)
# Reconstruction: A_approx = U @ np.diag(Sigma) @ VT
\end{lstlisting}
\end{astuce}

\subsection{Applications de la SVD}

\textbf{1. Approximation de rang faible :}

On peut approximer $\mat{A}$ en gardant seulement les $k$ plus grandes valeurs singuli√®res :
\begin{equation}
    \mat{A}_k = \sum_{i=1}^k \sigma_i \vect{u}_i \vect{v}_i^T
\end{equation}

C'est l'approximation de rang $k$ optimale au sens de la norme de Frobenius.

\textbf{2. PCA (Principal Component Analysis) :}

La PCA est √©quivalente √† une SVD des donn√©es centr√©es :
\begin{itemize}
    \item Centrer : $\mat{X}_c = \mat{X} - \bar{\vect{x}}$
    \item SVD : $\mat{X}_c = \mat{U}\mat{\Sigma}\mat{V}^T$
    \item Composantes principales = colonnes de $\mat{V}$
\end{itemize}

\textbf{3. Compression d'images :}

Une image est une matrice. La SVD permet de compresser en ne gardant que les $k$ premi√®res valeurs singuli√®res.

\textbf{4. Recommandation (Matrix Factorization) :}

Netflix Prize : d√©composer la matrice utilisateurs $\times$ films en produit de matrices de rang faible.

\subsection{Autres D√©compositions}

\textbf{D√©composition LU :} $\mat{A} = \mat{L}\mat{U}$ (Lower-Upper), pour r√©soudre $\mat{A}\vect{x} = \vect{b}$ efficacement.

\textbf{D√©composition de Cholesky :} Si $\mat{A}$ est sym√©trique d√©finie positive, $\mat{A} = \mat{L}\mat{L}^T$.

\textbf{D√©composition QR :} $\mat{A} = \mat{Q}\mat{R}$ o√π $\mat{Q}$ est orthogonale et $\mat{R}$ est triangulaire sup√©rieure.

\begin{miniprojet}{Pr√©dire le prix des maisons}
\textbf{Objectif :} Appliquer l'alg√®bre lin√©aire pour pr√©dire le prix d'une maison.

\textbf{Donn√©es :} Vous avez 3 maisons avec [surface m¬≤, chambres, ann√©e] et leur prix :
\begin{equation*}
    \mat{X} = \begin{pmatrix}
        75 & 3 & 2010 \\
        50 & 2 & 2005 \\
        100 & 4 & 2015
    \end{pmatrix}, \quad
    \vect{y} = \begin{pmatrix} 300k\text{‚Ç¨} \\ 200k\text{‚Ç¨} \\ 400k\text{‚Ç¨} \end{pmatrix}
\end{equation*}

\textbf{√Ä faire :}
\begin{enumerate}
    \item Normaliser les features avec $\vect{x}_{\text{norm}} = \frac{\vect{x} - \bar{\vect{x}}}{\|\vect{x}\|_2}$
    \item Calculer les poids optimaux : $\vect{w}^* = (\mat{X}^T\mat{X})^{-1}\mat{X}^T\vect{y}$
    \item Pr√©dire le prix d'une nouvelle maison [80 m¬≤, 3 chambres, 2012]
    \item Calculer l'erreur : $\|\mat{X}\vect{w}^* - \vect{y}\|_2$
\end{enumerate}

\textbf{Code complet dans :} \texttt{01\_demo\_algebre\_lineaire.ipynb}
\end{miniprojet}

% ===== PARTIE II: PROBABILIT√âS ET STATISTIQUES =====
\part{Probabilit√©s et Statistiques}

\begin{pourquoi}
Les probabilit√©s sont le langage de l'incertitude en ML :
\begin{itemize}
    \item \textbf{Classification} : "Il y a 87\% de chances que cet email soit un spam" ‚Üí Probabilit√© conditionnelle
    \item \textbf{Mod√®les g√©n√©ratifs} : ChatGPT pr√©dit le prochain mot en calculant $P(\text{mot} \mid \text{contexte})$ ‚Üí Loi normale multivari√©e
    \item \textbf{Diagnostic m√©dical} : Th√©or√®me de Bayes pour calculer $P(\text{malade} \mid \text{test positif})$
    \item \textbf{A/B Testing} : Tester si une nouvelle feature am√©liore r√©ellement votre app ‚Üí Tests statistiques
\end{itemize}

\textbf{Le Machine Learning = apprendre des distributions de probabilit√© √† partir des donn√©es !}
\end{pourquoi}

\section{Probabilit√©s Fondamentales}

\subsection{Concepts de Base}

\begin{definition}{Probabilit√©}
Une probabilit√© $P$ est une mesure sur un espace d'√©v√©nements $\Omega$ telle que :
\begin{itemize}
    \item $0 \leq P(A) \leq 1$ pour tout √©v√©nement $A$
    \item $P(\Omega) = 1$
    \item Si $A_1, A_2, \ldots$ sont disjoints, $P(\cup_i A_i) = \sum_i P(A_i)$
\end{itemize}
\end{definition}

\textbf{R√®gles de base :}
\begin{align}
    P(A \cup B) &= P(A) + P(B) - P(A \cap B) \\
    P(A^c) &= 1 - P(A) \\
    P(A \mid B) &= \frac{P(A \cap B)}{P(B)} \quad \text{(probabilit√© conditionnelle)}
\end{align}

\subsection{Ind√©pendance}

\begin{definition}{Ind√©pendance}
Deux √©v√©nements $A$ et $B$ sont ind√©pendants si :
\begin{equation}
    P(A \cap B) = P(A) \cdot P(B)
\end{equation}
√âquivalent √† : $P(A \mid B) = P(A)$.
\end{definition}

\subsection{Th√©or√®me de Bayes}

\begin{intuition}
\textbf{Le paradoxe du test m√©dical :}

Vous faites un test pour une maladie rare (0.1\% de la population). Le test est pr√©cis √† 99\%.

\textbf{Question :} Si votre test est positif, quelle est la probabilit√© que vous soyez vraiment malade ?

\textbf{Intuition na√Øve :} "Le test est pr√©cis √† 99\%, donc j'ai 99\% de chances d'√™tre malade !" [FAUX]

\textbf{R√©alit√© avec Bayes :} Seulement \textbf{9\%} de chances d'√™tre malade !

\textbf{Pourquoi ?} Parmi 10,000 personnes :
\begin{itemize}
    \item 10 sont vraiment malades ‚Üí 9.9 tests positifs (99\% de pr√©cision)
    \item 9,990 sont saines ‚Üí 99.9 faux positifs (1\% d'erreur)
    \item Total tests positifs : $\approx$ 110
    \item Vraiment malades : seulement 9.9 sur 110 = 9\%
\end{itemize}

\textbf{Morale :} Les tests sur maladies rares donnent beaucoup de faux positifs, m√™me s'ils sont pr√©cis !
\end{intuition}

\begin{theoreme}{Th√©or√®me de Bayes}
\begin{equation}
    P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
\end{equation}
\end{theoreme}

\textbf{Terminologie :}
\begin{itemize}
    \item $P(A)$ : probabilit√© a priori
    \item $P(A \mid B)$ : probabilit√© a posteriori
    \item $P(B \mid A)$ : vraisemblance (likelihood)
    \item $P(B)$ : √©vidence
\end{itemize}

\textbf{Forme √©tendue :}
\begin{equation}
    P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{\sum_{i} P(B \mid A_i) \cdot P(A_i)}
\end{equation}

\begin{astuce}
\textbf{Applications en ML :}
\begin{itemize}
    \item Naive Bayes Classifier
    \item Inf√©rence bay√©sienne
    \item Filtres bay√©siens (spam, Kalman)
\end{itemize}
\end{astuce}

\section{Variables Al√©atoires}

\subsection{Variable Al√©atoire Discr√®te}

\begin{definition}{Variable al√©atoire discr√®te}
Une variable al√©atoire $X$ prend des valeurs dans un ensemble discret (fini ou d√©nombrable). Sa distribution est caract√©ris√©e par la \textbf{fonction de masse} :
\begin{equation}
    p_X(x) = P(X = x)
\end{equation}
avec $\sum_x p_X(x) = 1$.
\end{definition}

\textbf{Distributions classiques :}

\begin{itemize}
    \item \textbf{Bernoulli} : $X \in \{0, 1\}$, $P(X=1) = p$
    \begin{equation}
        p_X(x) = p^x (1-p)^{1-x}
    \end{equation}

    \item \textbf{Binomiale} : $X \sim \text{Bin}(n, p)$ (nombre de succ√®s en $n$ essais)
    \begin{equation}
        p_X(k) = \binom{n}{k} p^k (1-p)^{n-k}
    \end{equation}

    \item \textbf{Poisson} : $X \sim \text{Poisson}(\lambda)$ (√©v√©nements rares)
    \begin{equation}
        p_X(k) = \frac{\lambda^k e^{-\lambda}}{k!}
    \end{equation}
\end{itemize}

\subsection{Variable Al√©atoire Continue}

\begin{definition}{Variable al√©atoire continue}
Une variable continue $X$ est caract√©ris√©e par une \textbf{fonction de densit√©} $f_X(x)$ telle que :
\begin{equation}
    P(a \leq X \leq b) = \int_a^b f_X(x) \, dx
\end{equation}
avec $\int_{-\infty}^{\infty} f_X(x) \, dx = 1$.
\end{definition}

\textbf{Distributions classiques :}

\begin{itemize}
    \item \textbf{Uniforme} : $X \sim \text{Uniform}(a, b)$
    \begin{equation}
        f_X(x) = \begin{cases}
            \frac{1}{b-a} & \text{si } a \leq x \leq b \\
            0 & \text{sinon}
        \end{cases}
    \end{equation}

    \item \textbf{Normale (Gaussienne)} : $X \sim \mathcal{N}(\mu, \sigma^2)$
    \begin{equation}
        f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
    \end{equation}

    \item \textbf{Exponentielle} : $X \sim \text{Exp}(\lambda)$ (temps entre √©v√©nements)
    \begin{equation}
        f_X(x) = \lambda e^{-\lambda x} \quad (x \geq 0)
    \end{equation}
\end{itemize}

\subsection{Esp√©rance et Variance}

\begin{definition}{Esp√©rance}
L'esp√©rance (moyenne) d'une variable al√©atoire $X$ est :
\begin{equation}
    \E[X] = \begin{cases}
        \sum_x x \cdot p_X(x) & \text{(discret)} \\
        \int_{-\infty}^{\infty} x \cdot f_X(x) \, dx & \text{(continu)}
    \end{cases}
\end{equation}
\end{definition}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item Lin√©arit√© : $\E[aX + bY] = a\E[X] + b\E[Y]$
    \item $\E[X + c] = \E[X] + c$
\end{itemize}

\begin{definition}{Variance}
La variance mesure la dispersion autour de la moyenne :
\begin{equation}
    \Var(X) = \E[(X - \E[X])^2] = \E[X^2] - (\E[X])^2
\end{equation}
L'√©cart-type est $\sigma_X = \sqrt{\Var(X)}$.
\end{definition}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item $\Var(aX + b) = a^2 \Var(X)$
    \item Si $X, Y$ ind√©pendants : $\Var(X + Y) = \Var(X) + \Var(Y)$
\end{itemize}

\textbf{Exemples :}
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Loi} & \textbf{Esp√©rance} & \textbf{Variance} \\
\midrule
Bernoulli$(p)$ & $p$ & $p(1-p)$ \\
Binomiale$(n, p)$ & $np$ & $np(1-p)$ \\
Poisson$(\lambda)$ & $\lambda$ & $\lambda$ \\
Uniforme$(a, b)$ & $\frac{a+b}{2}$ & $\frac{(b-a)^2}{12}$ \\
Normale$(\mu, \sigma^2)$ & $\mu$ & $\sigma^2$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Loi Normale (Gaussienne)}

La loi normale est \textbf{fondamentale} en ML et statistiques.

\begin{theoreme}{Th√©or√®me Central Limite (TCL)}
Soit $X_1, \ldots, X_n$ des variables i.i.d. d'esp√©rance $\mu$ et variance $\sigma^2$. Alors :
\begin{equation}
    \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \xrightarrow{n \to \infty} \mathcal{N}(0, 1)
\end{equation}
o√π $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$.
\end{theoreme}

\textbf{Cons√©quence :} Beaucoup de ph√©nom√®nes naturels suivent approximativement une loi normale gr√¢ce au TCL.

\textbf{Propri√©t√©s de la loi normale :}
\begin{itemize}
    \item Somme de gaussiennes = gaussienne
    \item 68\% des valeurs dans $[\mu - \sigma, \mu + \sigma]$
    \item 95\% dans $[\mu - 2\sigma, \mu + 2\sigma]$
    \item 99.7\% dans $[\mu - 3\sigma, \mu + 3\sigma]$
\end{itemize}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.1]
    % Axes
    \draw[->] (-4.5,0) -- (4.5,0) node[right] {$x$};
    \draw[->] (0,0) -- (0,4.2) node[above] {Densit√©};

    % Courbe gaussienne (approximation)
    \draw[blue, very thick, smooth, domain=-4:4, samples=100]
        plot (\x, {3.5*exp(-0.5*\x*\x)});

    % Marques sur l'axe x
    \foreach \x/\label in {-3/$\mu-3\sigma$, -2/$\mu-2\sigma$, -1/$\mu-\sigma$, 0/$\mu$, 1/$\mu+\sigma$, 2/$\mu+2\sigma$, 3/$\mu+3\sigma$}
        \draw (\x,0.05) -- (\x,-0.05) node[below, font=\small] {\label};

    % Zone 68% (1 sigma)
    \fill[green!20, opacity=0.6, domain=-1:1, samples=50]
        plot (\x, {3.5*exp(-0.5*\x*\x)}) -- (1,0) -- (-1,0) -- cycle;
    \draw[green!60!black, very thick] (-1,0) -- (-1,2.12);
    \draw[green!60!black, very thick] (1,0) -- (1,2.12);
    \node[green!60!black, font=\bfseries] at (0, 1) {68\%};

    % Zone 95% (2 sigma)
    \fill[yellow!30, opacity=0.5, domain=-2:-1, samples=30]
        plot (\x, {3.5*exp(-0.5*\x*\x)}) -- (-1,0) -- (-2,0) -- cycle;
    \fill[yellow!30, opacity=0.5, domain=1:2, samples=30]
        plot (\x, {3.5*exp(-0.5*\x*\x)}) -- (2,0) -- (1,0) -- cycle;
    \draw[orange, very thick] (-2,0) -- (-2,0.47);
    \draw[orange, very thick] (2,0) -- (2,0.47);
    \node[orange, font=\small] at (-1.5, 0.7) {13.5\%};
    \node[orange, font=\small] at (1.5, 0.7) {13.5\%};

    % Zone 99.7% (3 sigma)
    \fill[red!20, opacity=0.4, domain=-3:-2, samples=30]
        plot (\x, {3.5*exp(-0.5*\x*\x)}) -- (-2,0) -- (-3,0) -- cycle;
    \fill[red!20, opacity=0.4, domain=2:3, samples=30]
        plot (\x, {3.5*exp(-0.5*\x*\x)}) -- (3,0) -- (2,0) -- cycle;
    \node[red!60!black, font=\tiny] at (-2.5, 0.3) {2.35\%};
    \node[red!60!black, font=\tiny] at (2.5, 0.3) {2.35\%};

    % Annotations
    \draw[<->, thick] (-1, 3.6) -- (1, 3.6);
    \node[above, font=\scriptsize] at (0, 3.6) {68\% des donn√©es};

    \draw[<->, thick] (-2, 4.4) -- (2, 4.4);
    \node[above, font=\scriptsize] at (0, 4.4) {95\% des donn√©es};

    % L√©gende
    \node[draw, fill=white, align=left, rounded corners] at (6, 2.5) {
        \textbf{R√®gle 68-95-99.7}\\[3pt]
        \textcolor{green!60!black}{$\blacksquare$} 68\% dans $[\mu-\sigma, \mu+\sigma]$\\
        \textcolor{orange}{$\blacksquare$} 95\% dans $[\mu-2\sigma, \mu+2\sigma]$\\
        \textcolor{red!60!black}{$\blacksquare$} 99.7\% dans $[\mu-3\sigma, \mu+3\sigma]$
    };

\end{tikzpicture}
\caption{Loi normale (Gaussienne): la r√®gle 68-95-99.7 permet de quantifier rapidement les probabilit√©s. Environ 68\% des donn√©es se trouvent √† moins d'un √©cart-type de la moyenne, 95\% √† moins de deux, et 99.7\% √† moins de trois.}
\label{fig:gaussian}
\end{figure}

\subsection{Loi Normale Multivari√©e}

\begin{definition}{Loi normale multivari√©e}
Un vecteur al√©atoire $\vect{X} \in \R^d$ suit une loi normale multivari√©e $\mathcal{N}(\vect{\mu}, \mat{\Sigma})$ si sa densit√© est :
\begin{equation}
    f_{\vect{X}}(\vect{x}) = \frac{1}{(2\pi)^{d/2} |\mat{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\vect{x} - \vect{\mu})^T \mat{\Sigma}^{-1} (\vect{x} - \vect{\mu})\right)
\end{equation}
o√π $\vect{\mu} \in \R^d$ est le vecteur moyenne et $\mat{\Sigma} \in \R^{d \times d}$ est la matrice de covariance.
\end{definition}

\textbf{Utilisations en ML :}
\begin{itemize}
    \item Gaussian Mixture Models (GMM)
    \item Analyse discriminante lin√©aire (LDA)
    \item Processus gaussiens
\end{itemize}

\section{Statistiques Descriptives}

\subsection{Mesures de Tendance Centrale}

\textbf{Moyenne empirique :}
\begin{equation}
    \bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
\end{equation}

\textbf{M√©diane :} Valeur centrale quand les donn√©es sont tri√©es. Robuste aux outliers.

\textbf{Mode :} Valeur la plus fr√©quente.

\subsection{Mesures de Dispersion}

\textbf{Variance empirique :}
\begin{equation}
    s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
\end{equation}

\textbf{√âcart-type :} $s = \sqrt{s^2}$

\textbf{Intervalle interquartile (IQR) :} $IQR = Q_3 - Q_1$ (robuste)

\subsection{Covariance et Corr√©lation}

\begin{definition}{Covariance}
La covariance entre deux variables $X$ et $Y$ mesure leur variation conjointe :
\begin{equation}
    \Cov(X, Y) = \E[(X - \E[X])(Y - \E[Y])] = \E[XY] - \E[X]\E[Y]
\end{equation}
Version empirique :
\begin{equation}
    \text{cov}(x, y) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
\end{equation}
\end{definition}

\begin{definition}{Coefficient de corr√©lation de Pearson}
\begin{equation}
    \rho_{XY} = \frac{\Cov(X, Y)}{\sigma_X \sigma_Y} \in [-1, 1]
\end{equation}
\end{definition}

\textbf{Interpr√©tation :}
\begin{itemize}
    \item $\rho = 1$ : corr√©lation lin√©aire positive parfaite
    \item $\rho = -1$ : corr√©lation lin√©aire n√©gative parfaite
    \item $\rho = 0$ : pas de corr√©lation lin√©aire
\end{itemize}

\textbf{Matrice de covariance :}
Pour un vecteur al√©atoire $\vect{X} = (X_1, \ldots, X_d)^T$ :
\begin{equation}
    \mat{\Sigma} = \E[(\vect{X} - \E[\vect{X}])(\vect{X} - \E[\vect{X}])^T]
\end{equation}
avec $\Sigma_{ij} = \Cov(X_i, X_j)$.

\begin{astuce}
En Python (NumPy/Pandas) :
\begin{lstlisting}[language=Python]
# Matrice de covariance
cov_matrix = np.cov(X, rowvar=False)  # colonnes = variables

# Matrice de corr√©lation
corr_matrix = np.corrcoef(X, rowvar=False)
# ou avec pandas:
corr_matrix = df.corr()
\end{lstlisting}
\end{astuce}

\begin{miniprojet}{Classifier des emails (spam ou non)}
\textbf{Objectif :} Utiliser le th√©or√®me de Bayes pour d√©tecter les spams.

\textbf{Donn√©es :} Sur 1000 emails :
\begin{itemize}
    \item 300 sont des spams (30\%)
    \item Le mot "gratuit" appara√Æt dans 80\% des spams
    \item Le mot "gratuit" appara√Æt dans 10\% des emails normaux
\end{itemize}

\textbf{Questions :}
\begin{enumerate}
    \item Calculer $P(\text{spam} \mid \text{"gratuit"})$ avec Bayes
    \item Si un email contient "gratuit" ET "urgent", quelle est la probabilit√© que ce soit un spam ?
    \item Simuler 1000 emails avec une loi de Bernoulli et calculer la pr√©cision du classifieur
    \item Tracer la courbe ROC (taux de vrais positifs vs faux positifs)
\end{enumerate}

\textbf{Indice :}
\begin{equation*}
    P(\text{spam} \mid \text{mot}) = \frac{P(\text{mot} \mid \text{spam}) \cdot P(\text{spam})}{P(\text{mot})}
\end{equation*}

\textbf{Code complet dans :} \texttt{01\_demo\_probabilites.ipynb}
\end{miniprojet}

% ===== PARTIE III: CALCUL DIFF√âRENTIEL =====
\part{Calcul Diff√©rentiel et Optimisation}

\begin{pourquoi}
L'optimisation est \textbf{LE C≈íUR} du Machine Learning :
\begin{itemize}
    \item \textbf{Entra√Æner un mod√®le} = Minimiser une fonction de perte (loss function)
    \item \textbf{Gradient descent} = L'algorithme qui fait tout fonctionner (r√©seaux de neurones, r√©gression, etc.)
    \item \textbf{Backpropagation} = Calcul automatique des gradients via la r√®gle de la cha√Æne
\end{itemize}

\textbf{Exemple concret :}
\begin{itemize}
    \item Vous avez un mod√®le qui fait 30\% d'erreur sur vos donn√©es
    \item Le gradient vous dit : "Change ce poids de +0.5, cet autre de -0.2..."
    \item Apr√®s des millions d'it√©rations ‚Üí Erreur tombe √† 2\% !
\end{itemize}

\textbf{Sans gradient descent, pas de deep learning moderne !}
\end{pourquoi}

\section{D√©riv√©es}

\subsection{D√©riv√©e d'une Fonction Scalaire}

\begin{definition}{D√©riv√©e}
La d√©riv√©e de $f: \R \to \R$ en $x$ est :
\begin{equation}
    f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\end{equation}
\end{definition}

\textbf{Interpr√©tation :} Pente de la tangente √† la courbe en $x$.

\textbf{R√®gles de d√©rivation :}
\begin{align}
    (cf)' &= cf' \\
    (f + g)' &= f' + g' \\
    (fg)' &= f'g + fg' \quad \text{(r√®gle du produit)} \\
    \left(\frac{f}{g}\right)' &= \frac{f'g - fg'}{g^2} \quad \text{(r√®gle du quotient)} \\
    (f \circ g)' &= (f' \circ g) \cdot g' \quad \text{(r√®gle de la cha√Æne)}
\end{align}

\textbf{D√©riv√©es usuelles :}
\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
$f(x)$ & $f'(x)$ \\
\midrule
$x^n$ & $nx^{n-1}$ \\
$e^x$ & $e^x$ \\
$\ln(x)$ & $1/x$ \\
$\sin(x)$ & $\cos(x)$ \\
$\cos(x)$ & $-\sin(x)$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Gradient}

\begin{intuition}
Imaginez que vous √™tes perdu en montagne dans le brouillard et voulez descendre :
\begin{itemize}
    \item La \textbf{fonction $f(\vect{x})$} = votre altitude √† la position $\vect{x}$
    \item Le \textbf{gradient $\nabla f$} = une fl√®che qui pointe vers la mont√©e la plus rapide
    \item \textbf{Descente de gradient} = Suivre la direction oppos√©e ($-\nabla f$) pour descendre
\end{itemize}

\textbf{En ML :}
\begin{itemize}
    \item $f(\vect{w})$ = l'erreur de votre mod√®le avec les poids $\vect{w}$
    \item Vous voulez minimiser l'erreur (descendre)
    \item Le gradient vous dit comment ajuster chaque poids pour r√©duire l'erreur !
\end{itemize}
\end{intuition}

\begin{definition}{Gradient}
Pour une fonction $f: \R^n \to \R$, le gradient est le vecteur des d√©riv√©es partielles :
\begin{equation}
    \nabla f(\vect{x}) = \begin{pmatrix}
        \frac{\partial f}{\partial x_1} \\
        \frac{\partial f}{\partial x_2} \\
        \vdots \\
        \frac{\partial f}{\partial x_n}
    \end{pmatrix}
\end{equation}
\end{definition}

\textbf{Interpr√©tation g√©om√©trique :}
\begin{itemize}
    \item Le gradient pointe dans la direction de \textbf{plus forte croissance}
    \item Sa norme $\|\nabla f\|$ mesure le taux de croissance
    \item $-\nabla f$ pointe vers la plus forte d√©croissance
\end{itemize}

\begin{exemple}{Gradient de fonctions courantes}
\begin{align}
    f(\vect{x}) &= \vect{a}^T \vect{x} = \sum_i a_i x_i \quad &\Rightarrow \quad \nabla f &= \vect{a} \\
    f(\vect{x}) &= \vect{x}^T \mat{A} \vect{x} \quad &\Rightarrow \quad \nabla f &= (\mat{A} + \mat{A}^T)\vect{x} \\
    f(\vect{x}) &= \|\vect{x}\|_2^2 = \vect{x}^T\vect{x} \quad &\Rightarrow \quad \nabla f &= 2\vect{x}
\end{align}
\end{exemple}

\subsection{Matrice Hessienne}

\begin{definition}{Hessienne}
Pour $f: \R^n \to \R$ deux fois diff√©rentiable, la matrice hessienne est la matrice des d√©riv√©es secondes :
\begin{equation}
    \mat{H}_f(\vect{x}) = \begin{pmatrix}
        \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \\
        \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \\
        \vdots & \vdots & \ddots
    \end{pmatrix}
\end{equation}
\end{definition}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item Si $f$ est $C^2$, alors $\mat{H}$ est sym√©trique (th√©or√®me de Schwarz)
    \item La hessienne mesure la \textbf{courbure} de $f$
    \item Utilis√©e dans les m√©thodes d'optimisation du second ordre (Newton)
\end{itemize}

\subsection{Jacobienne}

\begin{definition}{Jacobienne}
Pour une fonction vectorielle $\vect{f}: \R^n \to \R^m$, la jacobienne est la matrice des d√©riv√©es partielles :
\begin{equation}
    \mat{J}_{\vect{f}}(\vect{x}) = \begin{pmatrix}
        \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
    \end{pmatrix} \in \R^{m \times n}
\end{equation}
\end{definition}

\textbf{Application en Deep Learning :} La backpropagation utilise la r√®gle de la cha√Æne avec des jacobiennes pour calculer les gradients.

\section{Optimisation}

\subsection{Conditions d'Optimalit√©}

\begin{theoreme}{Condition n√©cessaire du premier ordre}
Si $\vect{x}^*$ est un minimum local de $f: \R^n \to \R$ diff√©rentiable, alors :
\begin{equation}
    \nabla f(\vect{x}^*) = \vect{0}
\end{equation}
\end{theoreme}

\begin{theoreme}{Condition suffisante du second ordre}
Si $\nabla f(\vect{x}^*) = \vect{0}$ et la hessienne $\mat{H}_f(\vect{x}^*)$ est \textbf{d√©finie positive} (toutes les valeurs propres $> 0$), alors $\vect{x}^*$ est un minimum local strict.
\end{theoreme}

\textbf{Cas de la hessienne :}
\begin{itemize}
    \item D√©finie positive $\Rightarrow$ minimum local
    \item D√©finie n√©gative $\Rightarrow$ maximum local
    \item Ind√©finie $\Rightarrow$ point-selle
\end{itemize}

\subsection{Convexit√©}

\begin{definition}{Fonction convexe}
Une fonction $f: \R^n \to \R$ est convexe si pour tous $\vect{x}, \vect{y}$ et $\lambda \in [0, 1]$ :
\begin{equation}
    f(\lambda \vect{x} + (1-\lambda) \vect{y}) \leq \lambda f(\vect{x}) + (1-\lambda) f(\vect{y})
\end{equation}
\end{definition}

\textbf{Crit√®re diff√©rentiel :}
\begin{itemize}
    \item Si $\mat{H}_f(\vect{x})$ est semi-d√©finie positive partout, alors $f$ est convexe
    \item Si $\mat{H}_f(\vect{x})$ est d√©finie positive partout, alors $f$ est strictement convexe
\end{itemize}

\textbf{Propri√©t√© fondamentale :}
Pour une fonction convexe, \textbf{tout minimum local est global}.

\begin{exemple}{Fonctions convexes courantes}
\begin{itemize}
    \item Fonctions lin√©aires/affines : $f(\vect{x}) = \vect{a}^T\vect{x} + b$
    \item Norme $L^2$ au carr√© : $f(\vect{x}) = \|\vect{x}\|_2^2$
    \item Exponentielle : $f(x) = e^x$
    \item Logarithme n√©gatif : $f(x) = -\ln(x)$ pour $x > 0$
\end{itemize}
\end{exemple}

\subsection{Descente de Gradient}

\begin{algorithm}[H]
\caption{Descente de Gradient}
\label{alg:gd}
\begin{algorithmic}[1]
\REQUIRE Fonction $f$, point initial $\vect{x}_0$, learning rate $\alpha > 0$, tol√©rance $\epsilon$
\ENSURE Minimum approximatif $\vect{x}^*$
\STATE $k \leftarrow 0$
\REPEAT
    \STATE Calculer le gradient : $\vect{g}_k = \nabla f(\vect{x}_k)$
    \STATE Mettre √† jour : $\vect{x}_{k+1} = \vect{x}_k - \alpha \vect{g}_k$
    \STATE $k \leftarrow k + 1$
\UNTIL{$\|\vect{g}_k\| < \epsilon$}
\RETURN $\vect{x}_k$
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0]
    % Axes
    \draw[->] (0,0) -- (8,0) node[right] {$x_1$};
    \draw[->] (0,0) -- (0,6) node[above] {$x_2$};

    % Courbes de niveau (ellipses concentriques)
    \draw[gray, thick] (4,3) ellipse (3 and 2);
    \draw[gray, thick] (4,3) ellipse (2.2 and 1.5);
    \draw[gray, thick] (4,3) ellipse (1.5 and 1);
    \draw[gray, thick] (4,3) ellipse (0.8 and 0.5);
    \draw[gray, thick] (4,3) ellipse (0.3 and 0.2);

    % Minimum global
    \filldraw[red] (4,3) circle (3pt);
    \node[red, below] at (4,2.7) {$\vect{x}^*$ (minimum)};

    % Trajectoire de descente de gradient
    \coordinate (x0) at (1.5,5);
    \coordinate (x1) at (2.2,4.5);
    \coordinate (x2) at (2.7,4.1);
    \coordinate (x3) at (3.1,3.8);
    \coordinate (x4) at (3.4,3.5);
    \coordinate (x5) at (3.7,3.25);
    \coordinate (x6) at (3.9,3.1);

    % Points de la trajectoire
    \filldraw[blue] (x0) circle (2pt) node[above left] {$\vect{x}_0$};
    \filldraw[blue] (x1) circle (2pt);
    \filldraw[blue] (x2) circle (2pt);
    \filldraw[blue] (x3) circle (2pt);
    \filldraw[blue] (x4) circle (2pt);
    \filldraw[blue] (x5) circle (2pt);
    \filldraw[blue] (x6) circle (2pt);

    % Fl√®ches entre les points
    \draw[->, very thick, blue] (x0) -- (x1);
    \draw[->, very thick, blue] (x1) -- (x2);
    \draw[->, very thick, blue] (x2) -- (x3);
    \draw[->, very thick, blue] (x3) -- (x4);
    \draw[->, very thick, blue] (x4) -- (x5);
    \draw[->, very thick, blue] (x5) -- (x6);
    \draw[->, very thick, blue] (x6) -- (4,3);

    % Gradient √† x0
    \draw[->, very thick, green!60!black] (x0) -- (2.3,5.8);
    \node[green!60!black, right] at (2.3,5.8) {$\nabla f(\vect{x}_0)$};

    % Direction de descente √† x0
    \draw[->, very thick, purple, dashed] (x0) -- (1.2,4.5);
    \node[purple, left, align=center, font=\small] at (1.2,4.5) {$-\nabla f$\\(descente)};

    % L√©gende
    \node[draw, fill=yellow!10, align=left, rounded corners] at (6.5, 5) {
        Courbes de niveau de $f$\\
        $\vect{x}_{k+1} = \vect{x}_k - \alpha \nabla f(\vect{x}_k)$\\[3pt]
        Learning rate $\alpha$ contr√¥le\\
        la taille des pas
    };

\end{tikzpicture}
\caption{Descente de gradient: la trajectoire suit la direction oppos√©e au gradient ($-\nabla f$) pour descendre vers le minimum. Les courbes de niveau montrent les valeurs constantes de $f$. √Ä chaque it√©ration, on se rapproche du minimum $\vect{x}^*$.}
\label{fig:gradient_descent}
\end{figure}

\textbf{Intuition :} On se d√©place it√©rativement dans la direction oppos√©e au gradient (plus forte descente).

\textbf{Param√®tres :}
\begin{itemize}
    \item \textbf{Learning rate $\alpha$} : Trop grand $\Rightarrow$ divergence ; trop petit $\Rightarrow$ convergence lente
    \item \textbf{Crit√®re d'arr√™t} : $\|\nabla f\| < \epsilon$ ou nombre max d'it√©rations
\end{itemize}

\begin{astuce}
\textbf{Variantes modernes (Deep Learning) :}
\begin{itemize}
    \item SGD (Stochastic Gradient Descent)
    \item Momentum
    \item Adam, RMSprop, AdaGrad
\end{itemize}
Voir Chapitre 06 pour les d√©tails.
\end{astuce}

\subsection{M√©thode de Newton}

L'id√©e est d'utiliser l'information du second ordre (hessienne) :
\begin{equation}
    \vect{x}_{k+1} = \vect{x}_k - \mat{H}_f(\vect{x}_k)^{-1} \nabla f(\vect{x}_k)
\end{equation}

\textbf{Avantages :}
\begin{itemize}
    \item Convergence quadratique (tr√®s rapide pr√®s du minimum)
    \item Pas besoin de tuner le learning rate
\end{itemize}

\textbf{Inconv√©nients :}
\begin{itemize}
    \item Co√ªt : calcul et inversion de la hessienne ($O(n^3)$)
    \item Ne fonctionne que si $\mat{H}$ est d√©finie positive
    \item Rarement utilis√© en Deep Learning (trop co√ªteux)
\end{itemize}

\textbf{Compromis :} M√©thodes quasi-Newton (L-BFGS) qui approximent la hessienne.

\begin{miniprojet}{Entra√Æner une r√©gression lin√©aire}
\textbf{Objectif :} Impl√©menter la descente de gradient pour entra√Æner un mod√®le.

\textbf{Probl√®me :} Pr√©dire le salaire en fonction des ann√©es d'exp√©rience.

\textbf{Donn√©es :} 5 personnes :
\begin{equation*}
    \text{Exp√©rience (ans) : } [1, 2, 3, 4, 5] \quad \text{Salaire (k‚Ç¨) : } [30, 35, 45, 50, 60]
\end{equation*}

\textbf{Mod√®le :} $\hat{y} = w_1 x + w_0$ (r√©gression lin√©aire)

\textbf{Loss function :} $L(\vect{w}) = \frac{1}{n}\sum_{i=1}^n (\hat{y}_i - y_i)^2$ (MSE)

\textbf{√Ä faire :}
\begin{enumerate}
    \item Calculer le gradient : $\nabla L = \begin{pmatrix} \frac{\partial L}{\partial w_0} \\ \frac{\partial L}{\partial w_1} \end{pmatrix}$
    \item Initialiser $\vect{w} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$
    \item Appliquer la descente de gradient pendant 100 it√©rations avec $\alpha = 0.01$
    \item Tracer l'√©volution de la loss et de la pr√©diction
    \item Comparer avec la solution analytique : $\vect{w}^* = (\mat{X}^T\mat{X})^{-1}\mat{X}^T\vect{y}$
\end{enumerate}

\textbf{R√©sultat attendu :} $w_1 \approx 7.5$ (chaque ann√©e d'exp√©rience = +7.5k‚Ç¨), $w_0 \approx 22.5$ (salaire de base)

\textbf{Code complet dans :} \texttt{01\_demo\_optimisation.ipynb}
\end{miniprojet}

% ===== SECTION R√âSUM√â =====
\section{R√©sum√© du Chapitre}

\subsection{Points Cl√©s}

\textbf{Alg√®bre Lin√©aire :}
\begin{itemize}
    \item Vecteurs et matrices sont omnipr√©sents en ML (donn√©es, poids, transformations)
    \item Produit scalaire $\Rightarrow$ similarit√© ; norme $\Rightarrow$ distance
    \item SVD = d√©composition universelle (PCA, compression, recommandation)
    \item Syst√®mes lin√©aires $\Rightarrow$ r√©gression lin√©aire (moindres carr√©s)
\end{itemize}

\textbf{Probabilit√©s et Statistiques :}
\begin{itemize}
    \item Loi normale = fondamentale (TCL, mod√®les g√©n√©ratifs)
    \item Th√©or√®me de Bayes = base de l'inf√©rence bay√©sienne
    \item Covariance/corr√©lation $\Rightarrow$ d√©pendance entre variables
    \item Esp√©rance, variance = r√©sument une distribution
\end{itemize}

\textbf{Calcul Diff√©rentiel et Optimisation :}
\begin{itemize}
    \item Gradient = direction de plus forte croissance
    \item Descente de gradient = algorithme d'optimisation de base en ML
    \item Convexit√© $\Rightarrow$ minimum local = global
    \item Hessienne = courbure (m√©thodes du second ordre)
\end{itemize}

\subsection{Formules Essentielles}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Formules √† retenir]
\textbf{Alg√®bre lin√©aire :}
\begin{align}
    \text{Produit scalaire : } & \vect{u} \cdot \vect{v} = \sum_i u_i v_i \\
    \text{Norme $L^2$ : } & \|\vect{v}\|_2 = \sqrt{\sum_i v_i^2} \\
    \text{Moindres carr√©s : } & \vect{w}^* = (\mat{X}^T\mat{X})^{-1}\mat{X}^T\vect{y} \\
    \text{SVD : } & \mat{A} = \mat{U}\mat{\Sigma}\mat{V}^T
\end{align}

\textbf{Probabilit√©s :}
\begin{align}
    \text{Bayes : } & P(A|B) = \frac{P(B|A)P(A)}{P(B)} \\
    \text{Loi normale : } & \mathcal{N}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu)^2/(2\sigma^2)} \\
    \text{Variance : } & \Var(X) = \E[X^2] - (\E[X])^2
\end{align}

\textbf{Optimisation :}
\begin{align}
    \text{Gradient : } & \nabla f = \left(\frac{\partial f}{\partial x_1}, \ldots, \frac{\partial f}{\partial x_n}\right)^T \\
    \text{Descente de gradient : } & \vect{x}_{k+1} = \vect{x}_k - \alpha \nabla f(\vect{x}_k)
\end{align}
\end{tcolorbox}

% ===== EXERCICES =====
\section{Exercices}

\subsection{Alg√®bre Lin√©aire}

\begin{enumerate}
    \item Calculer le produit scalaire de $\vect{u} = (1, 2, 3)$ et $\vect{v} = (4, -1, 2)$. Les vecteurs sont-ils orthogonaux ?

    \item Pour la matrice $\mat{A} = \begin{pmatrix} 2 & 1 \\ 1 & 3 \end{pmatrix}$, calculer :
    \begin{itemize}
        \item Les valeurs propres et vecteurs propres
        \item La d√©composition spectrale
    \end{itemize}

    \item Appliquer la SVD √† la matrice $\mat{A} = \begin{pmatrix} 3 & 1 \\ 1 & 3 \\ 1 & 1 \end{pmatrix}$ (en Python). Reconstruire $\mat{A}$ √† partir de la SVD.

    \item R√©soudre le syst√®me lin√©aire au sens des moindres carr√©s :
    \begin{equation}
        \begin{pmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \end{pmatrix} \begin{pmatrix} a \\ b \end{pmatrix} \approx \begin{pmatrix} 2 \\ 3 \\ 5 \end{pmatrix}
    \end{equation}
\end{enumerate}

\subsection{Probabilit√©s et Statistiques}

\begin{enumerate}
    \item Un test m√©dical d√©tecte une maladie avec 99\% de pr√©cision (vrai positif). La pr√©valence de la maladie est 0.1\%. Si le test est positif, quelle est la probabilit√© d'√™tre r√©ellement malade ? (Bayes)

    \item Pour $X \sim \mathcal{N}(10, 4)$, calculer $P(8 \leq X \leq 12)$.

    \item G√©n√©rer 1000 √©chantillons d'une loi normale $\mathcal{N}(5, 2)$ en Python. Calculer la moyenne et variance empiriques. Tracer l'histogramme.

    \item Calculer la matrice de covariance pour les donn√©es :
    \begin{equation}
        \mat{X} = \begin{pmatrix} 1 & 2 \\ 2 & 4 \\ 3 & 5 \end{pmatrix}
    \end{equation}
    Interpr√©ter la corr√©lation entre les deux variables.
\end{enumerate}

\subsection{Calcul Diff√©rentiel et Optimisation}

\begin{enumerate}
    \item Calculer le gradient de $f(\vect{x}) = x_1^2 + 2x_1x_2 + 3x_2^2$.

    \item Montrer que $f(\vect{x}) = \|\mat{A}\vect{x} - \vect{b}\|_2^2$ est convexe. Calculer son gradient.

    \item Impl√©menter la descente de gradient pour minimiser $f(x) = (x-3)^2 + 5$ en partant de $x_0 = 0$. Tester diff√©rents learning rates.

    \item Minimiser $f(x_1, x_2) = x_1^2 + 4x_2^2$ par descente de gradient. Visualiser la trajectoire.
\end{enumerate}

\textit{Solutions d√©taill√©es dans} \texttt{01\_exercices.ipynb} \textit{(solutions int√©gr√©es dans le notebook)}

% ===== POUR ALLER PLUS LOIN =====
\section{Pour Aller Plus Loin}

\subsection{Lectures Recommand√©es}

\textbf{Livres :}
\begin{itemize}
    \item \textit{Linear Algebra and Its Applications} (4e √©d., 2006) - Gilbert Strang
    \item \textit{Probability and Statistics for Engineers} (9e √©d., 2016) - Montgomery \& Runger
    \item \textit{Convex Optimization} (2004) - Boyd \& Vandenberghe
    \item \textit{Mathematics for Machine Learning} (2020) - Deisenroth, Faisal, Ong (gratuit en ligne)
\end{itemize}

\textbf{Ressources en ligne :}
\begin{itemize}
    \item MIT OCW : Linear Algebra (18.06) - Gilbert Strang
    \item 3Blue1Brown : Essence of Linear Algebra (YouTube)
    \item Khan Academy : Probabilit√©s et statistiques
\end{itemize}

\subsection{Outils Pratiques}

\textbf{NumPy (calcul num√©rique) :}
\begin{lstlisting}[language=Python]
import numpy as np

# Alg√®bre lin√©aire
A = np.array([[1, 2], [3, 4]])
eigenvalues, eigenvectors = np.linalg.eig(A)
U, Sigma, VT = np.linalg.svd(A)

# Statistiques
mean = np.mean(data)
std = np.std(data)
cov_matrix = np.cov(X, rowvar=False)
\end{lstlisting}

\textbf{SciPy (fonctions avanc√©es) :}
\begin{lstlisting}[language=Python]
from scipy import stats, optimize

# Distributions
rv = stats.norm(loc=0, scale=1)  # N(0,1)
pdf_values = rv.pdf(x)

# Optimisation
result = optimize.minimize(f, x0, method='BFGS')
\end{lstlisting}

\subsection{Prochaines √âtapes}

Chapitre suivant : \textbf{Chapitre 02 - M√©triques d'√âvaluation}

Ces fondamentaux seront utilis√©s tout au long du cours :
\begin{itemize}
    \item R√©gression lin√©aire (Ch. 03) : moindres carr√©s, gradient
    \item PCA (Ch. 05) : SVD, valeurs propres
    \item R√©seaux de neurones (Ch. 06) : backpropagation, descente de gradient
    \item Probabilit√©s : mod√®les g√©n√©ratifs, bay√©siens
\end{itemize}

% ===== BIBLIOGRAPHIE =====
\section*{R√©f√©rences}

\begin{enumerate}
    \item Strang, G. (2006). \textit{Linear Algebra and Its Applications} (4e √©d.). Cengage Learning.

    \item Deisenroth, M. P., Faisal, A. A., \& Ong, C. S. (2020). \textit{Mathematics for Machine Learning}. Cambridge University Press.

    \item Boyd, S., \& Vandenberghe, L. (2004). \textit{Convex Optimization}. Cambridge University Press.

    \item Bishop, C. M. (2006). \textit{Pattern Recognition and Machine Learning}. Springer.

    \item Murphy, K. P. (2022). \textit{Probabilistic Machine Learning: An Introduction}. MIT Press.

    \item Goodfellow, I., Bengio, Y., \& Courville, A. (2016). \textit{Deep Learning}. MIT Press.
\end{enumerate}

\end{document}
