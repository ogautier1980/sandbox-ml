{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/01_fondamentaux_mathematiques/01_exercices.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '01_exercices.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 01 - Exercices : Fondamentaux Math√©matiques\n",
    "\n",
    "Ces exercices couvrent l'alg√®bre lin√©aire, les probabilit√©s et l'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Alg√®bre Lin√©aire\n",
    "\n",
    "### Exercice 1.1 : Produit Scalaire et Orthogonalit√©\n",
    "\n",
    "Calculer le produit scalaire de u = (1, 2, 3) et v = (4, -1, 2). Les vecteurs sont-ils orthogonaux ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "u = np.array([1, 2, 3])\n",
    "v = np.array([4, -1, 2])\n",
    "\n",
    "# Calculer le produit scalaire\n",
    "dot_product = # TODO\n",
    "\n",
    "print(f\"Produit scalaire: {dot_product}\")\n",
    "print(f\"Orthogonaux? {dot_product == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.2 : Valeurs Propres et Diagonalisation\n",
    "\n",
    "Pour la matrice A = [[2, 1], [1, 3]], calculer :\n",
    "1. Les valeurs propres et vecteurs propres\n",
    "2. La d√©composition spectrale A = P @ D @ P^(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "A = np.array([[2, 1],\n",
    "              [1, 3]])\n",
    "\n",
    "# Calculer valeurs/vecteurs propres\n",
    "eigenvalues, eigenvectors = # TODO\n",
    "\n",
    "print(\"Valeurs propres:\", eigenvalues)\n",
    "print(\"Vecteurs propres:\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# V√©rifier la d√©composition\n",
    "# A = P @ D @ P^(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.3 : SVD\n",
    "\n",
    "Appliquer la SVD √† la matrice A = [[3, 1], [1, 3], [1, 1]] et reconstruire A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "A = np.array([[3, 1],\n",
    "              [1, 3],\n",
    "              [1, 1]])\n",
    "\n",
    "# SVD\n",
    "U, Sigma, VT = # TODO\n",
    "\n",
    "print(\"U shape:\", U.shape)\n",
    "print(\"Sigma:\", Sigma)\n",
    "print(\"VT shape:\", VT.shape)\n",
    "\n",
    "# Reconstruire A\n",
    "A_reconstructed = # TODO\n",
    "\n",
    "print(\"\\nErreur de reconstruction:\", np.linalg.norm(A - A_reconstructed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.4 : Moindres Carr√©s\n",
    "\n",
    "R√©soudre le syst√®me lin√©aire surd√©termin√© au sens des moindres carr√©s :\n",
    "```\n",
    "[[1, 1],     [[a],      [[2],\n",
    " [1, 2],  @   [b]]  ‚âà    [3],\n",
    " [1, 3]]                 [5]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "X = np.array([[1, 1],\n",
    "              [1, 2],\n",
    "              [1, 3]])\n",
    "y = np.array([2, 3, 5])\n",
    "\n",
    "# Solution par √©quation normale : w = (X^T X)^(-1) X^T y\n",
    "w = # TODO\n",
    "\n",
    "print(\"Solution [a, b]:\", w)\n",
    "\n",
    "# V√©rifier\n",
    "y_pred = X @ w\n",
    "print(\"Pr√©dictions:\", y_pred)\n",
    "print(\"Erreur MSE:\", np.mean((y_pred - y)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Probabilit√©s et Statistiques\n",
    "\n",
    "### Exercice 2.1 : Th√©or√®me de Bayes\n",
    "\n",
    "Un test m√©dical d√©tecte une maladie avec 99% de pr√©cision (vrai positif).  \n",
    "La pr√©valence de la maladie est 0.1%.  \n",
    "Si le test est positif, quelle est la probabilit√© d'√™tre r√©ellement malade ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "P_disease = 0.001  # Pr√©valence\n",
    "P_pos_given_disease = 0.99  # Sensibilit√©\n",
    "P_pos_given_healthy = 0.01  # Faux positif\n",
    "\n",
    "# Calculer P(Test+)\n",
    "P_pos = # TODO\n",
    "\n",
    "# Bayes : P(Maladie | Test+)\n",
    "P_disease_given_pos = # TODO\n",
    "\n",
    "print(f\"P(Maladie | Test+) = {P_disease_given_pos*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.2 : Loi Normale\n",
    "\n",
    "Pour X ~ N(10, 4), calculer P(8 ‚â§ X ‚â§ 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "mu, sigma = 10, 2  # sigma = sqrt(4) = 2\n",
    "rv = stats.norm(mu, sigma)\n",
    "\n",
    "# Calculer P(8 <= X <= 12)\n",
    "prob = # TODO\n",
    "\n",
    "print(f\"P(8 ‚â§ X ‚â§ 12) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.3 : G√©n√©ration et Statistiques\n",
    "\n",
    "G√©n√©rer 1000 √©chantillons d'une loi normale N(5, 2).  \n",
    "Calculer la moyenne et variance empiriques.  \n",
    "Tracer l'histogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "mu_true, sigma_true = 5, np.sqrt(2)\n",
    "\n",
    "# G√©n√©rer √©chantillons\n",
    "samples = # TODO\n",
    "\n",
    "# Statistiques empiriques\n",
    "mean_empirical = # TODO\n",
    "var_empirical = # TODO\n",
    "\n",
    "print(f\"Moyenne empirique: {mean_empirical:.3f} (th√©orique: {mu_true})\")\n",
    "print(f\"Variance empirique: {var_empirical:.3f} (th√©orique: {sigma_true**2})\")\n",
    "\n",
    "# Histogramme\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(samples, bins=40, density=True, alpha=0.7, edgecolor='black')\n",
    "# Superposer la PDF th√©orique\n",
    "x = np.linspace(samples.min(), samples.max(), 100)\n",
    "plt.plot(x, stats.norm(mu_true, sigma_true).pdf(x), 'r-', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Densit√©')\n",
    "plt.title('Histogramme vs PDF Th√©orique')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.4 : Matrice de Covariance\n",
    "\n",
    "Calculer la matrice de covariance pour les donn√©es :\n",
    "```\n",
    "X = [[1, 2],\n",
    "     [2, 4],\n",
    "     [3, 5]]\n",
    "```\n",
    "Interpr√©ter la corr√©lation entre les deux variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "X = np.array([[1, 2],\n",
    "              [2, 4],\n",
    "              [3, 5]])\n",
    "\n",
    "# Matrice de covariance\n",
    "cov_matrix = # TODO (rowvar=False car colonnes = variables)\n",
    "\n",
    "print(\"Matrice de covariance:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Matrice de corr√©lation\n",
    "corr_matrix = np.corrcoef(X, rowvar=False)\n",
    "print(\"\\nMatrice de corr√©lation:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "print(f\"\\nCorr√©lation entre var1 et var2: {corr_matrix[0, 1]:.3f}\")\n",
    "print(\"Interpr√©tation: ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : Calcul Diff√©rentiel et Optimisation\n",
    "\n",
    "### Exercice 3.1 : Calcul de Gradient\n",
    "\n",
    "Calculer le gradient de f(x‚ÇÅ, x‚ÇÇ) = x‚ÇÅ¬≤ + 2x‚ÇÅx‚ÇÇ + 3x‚ÇÇ¬≤  \n",
    "√âvaluer en (1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "def f(x1, x2):\n",
    "    return x1**2 + 2*x1*x2 + 3*x2**2\n",
    "\n",
    "def grad_f(x1, x2):\n",
    "    # ‚àÇf/‚àÇx1 = 2x1 + 2x2\n",
    "    # ‚àÇf/‚àÇx2 = 2x1 + 6x2\n",
    "    grad_x1 = # TODO\n",
    "    grad_x2 = # TODO\n",
    "    return np.array([grad_x1, grad_x2])\n",
    "\n",
    "# √âvaluer en (1, 1)\n",
    "grad_at_1_1 = grad_f(1, 1)\n",
    "print(f\"‚àáf(1, 1) = {grad_at_1_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.2 : Convexit√©\n",
    "\n",
    "Montrer que f(x) = ||Ax - b||‚ÇÇ¬≤ est convexe.  \n",
    "Calculer son gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th√©orique : √©crire les √©tapes sur papier\n",
    "# f(x) = (Ax - b)^T (Ax - b) = x^T A^T A x - 2b^T A x + b^T b\n",
    "# Hessienne : H = 2 A^T A (semi-d√©finie positive si A de rang plein)\n",
    "# Donc f est convexe\n",
    "\n",
    "# Gradient : ‚àáf = 2 A^T (Ax - b)\n",
    "\n",
    "# Impl√©menter\n",
    "def f_least_squares(x, A, b):\n",
    "    residual = A @ x - b\n",
    "    return np.dot(residual, residual)\n",
    "\n",
    "def grad_f_least_squares(x, A, b):\n",
    "    return # TODO : 2 * A^T @ (A @ x - b)\n",
    "\n",
    "# Tester\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "b = np.array([1, 2, 3])\n",
    "x = np.array([0.5, 0.5])\n",
    "\n",
    "print(\"f(x):\", f_least_squares(x, A, b))\n",
    "print(\"‚àáf(x):\", grad_f_least_squares(x, A, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.3 : Descente de Gradient Simple\n",
    "\n",
    "Minimiser f(x) = (x - 3)¬≤ + 5 en partant de x‚ÇÄ = 0 par descente de gradient.  \n",
    "Tester diff√©rents learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "def f(x):\n",
    "    return (x - 3)**2 + 5\n",
    "\n",
    "def grad_f(x):\n",
    "    return # TODO : 2(x - 3)\n",
    "\n",
    "def gradient_descent(x0, lr, n_iter):\n",
    "    x = x0\n",
    "    history = [x]\n",
    "    for _ in range(n_iter):\n",
    "        x = # TODO : x - lr * grad\n",
    "        history.append(x)\n",
    "    return np.array(history)\n",
    "\n",
    "# Tester diff√©rents learning rates\n",
    "x0 = 0.0\n",
    "learning_rates = [0.1, 0.5, 1.0]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for lr in learning_rates:\n",
    "    history = gradient_descent(x0, lr, 20)\n",
    "    plt.plot(history, label=f'lr={lr}')\n",
    "\n",
    "plt.axhline(3, color='r', linestyle='--', label='Minimum x=3')\n",
    "plt.xlabel('It√©ration')\n",
    "plt.ylabel('x')\n",
    "plt.title('Convergence de la Descente de Gradient')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.4 : Descente de Gradient 2D\n",
    "\n",
    "Minimiser f(x‚ÇÅ, x‚ÇÇ) = x‚ÇÅ¬≤ + 4x‚ÇÇ¬≤ par descente de gradient.  \n",
    "Visualiser la trajectoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "def f(x):\n",
    "    return x[0]**2 + 4*x[1]**2\n",
    "\n",
    "def grad_f(x):\n",
    "    return np.array([# TODO : 2*x[0], 8*x[1]\n",
    "                    ])\n",
    "\n",
    "# Descente de gradient\n",
    "x0 = np.array([2.0, 2.0])\n",
    "lr = 0.1\n",
    "n_iter = 50\n",
    "\n",
    "x = x0.copy()\n",
    "history = [x.copy()]\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    x = # TODO\n",
    "    history.append(x.copy())\n",
    "\n",
    "history = np.array(history)\n",
    "\n",
    "# Visualisation\n",
    "x1_range = np.linspace(-3, 3, 100)\n",
    "x2_range = np.linspace(-3, 3, 100)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "Z = X1**2 + 4*X2**2\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X1, X2, Z, levels=30, cmap='viridis')\n",
    "plt.plot(history[:, 0], history[:, 1], 'r-o', markersize=5, label='Trajectoire')\n",
    "plt.plot(x0[0], x0[1], 'g*', markersize=15, label='D√©part')\n",
    "plt.plot(history[-1, 0], history[-1, 1], 'r*', markersize=15, label='Arriv√©e')\n",
    "plt.xlabel('x‚ÇÅ')\n",
    "plt.ylabel('x‚ÇÇ')\n",
    "plt.title('Descente de Gradient 2D')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Solution finale: {history[-1]}\")\n",
    "print(f\"Valeur f: {f(history[-1]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : Projet Int√©gratif\n",
    "\n",
    "### R√©gression Lin√©aire Compl√®te\n",
    "\n",
    "1. G√©n√©rer des donn√©es synth√©tiques y = 3x + 2 + bruit\n",
    "2. R√©soudre par moindres carr√©s (solution analytique)\n",
    "3. R√©soudre par descente de gradient\n",
    "4. Comparer les deux solutions\n",
    "5. Visualiser la surface de co√ªt et la trajectoire GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Ä vous de jouer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Solutions d√©taill√©es dans** `01_exercices_solutions.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}