{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/12_vision_avancee/12_demo_vision_transformers.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '12_demo_vision_transformers.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 13 - Vision Transformers (ViT) et CLIP\n",
    "\n",
    "Ce notebook explore les **Vision Transformers** et les mod√®les **vision-langage** (CLIP).\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre l'architecture Vision Transformer (ViT)\n",
    "- Utiliser ViT avec timm (PyTorch Image Models)\n",
    "- Fine-tuner ViT sur CIFAR-10\n",
    "- Comparer ViT vs CNN (ResNet)\n",
    "- Visualiser attention maps\n",
    "- D√©couvrir CLIP pour zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# timm (PyTorch Image Models)\n",
    "try:\n",
    "    import timm\n",
    "    TIMM_AVAILABLE = True\n",
    "    print(f\"timm version: {timm.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è timm not installed. Install with: pip install timm\")\n",
    "    TIMM_AVAILABLE = False\n",
    "\n",
    "# CLIP (optionnel)\n",
    "try:\n",
    "    import clip\n",
    "    CLIP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è CLIP not installed. Install with: pip install git+https://github.com/openai/CLIP.git\")\n",
    "    CLIP_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture Vision Transformer (ViT)\n",
    "\n",
    "Impl√©mentation simplifi√©e de ViT pour comprendre les m√©canismes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"D√©coupe l'image en patches et les projette en embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Convolution pour d√©couper en patches\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        x = self.projection(x)  # [B, embed_dim, H/P, W/P]\n",
    "        x = x.flatten(2)  # [B, embed_dim, n_patches]\n",
    "        x = x.transpose(1, 2)  # [B, n_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, num_heads, N, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, num_heads, N, N]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_dropout(x)\n",
    "        \n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP avec GELU activation.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, hidden_dim=3072, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Attention with residual\n",
    "        attn_output, attn_weights = self.attn(self.norm1(x))\n",
    "        x = x + attn_output\n",
    "        \n",
    "        # MLP with residual\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer (ViT) Architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=1000,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # [CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Positional embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
    "        self.pos_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # [B, n_patches, embed_dim]\n",
    "        \n",
    "        # Add [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # [B, n_patches+1, embed_dim]\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        attn_weights = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x)\n",
    "            attn_weights.append(attn)\n",
    "        \n",
    "        # Classification head (use [CLS] token)\n",
    "        x = self.norm(x[:, 0])  # [B, embed_dim]\n",
    "        x = self.head(x)  # [B, num_classes]\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "# Tester architecture\n",
    "model = VisionTransformer(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=10,\n",
    "    embed_dim=384,\n",
    "    depth=6,\n",
    "    num_heads=6\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "y, attn = model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Number of attention layers: {len(attn)}\")\n",
    "print(f\"Attention shape: {attn[0].shape}\")\n",
    "print(f\"\\nNombre de param√®tres: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vision Transformers avec timm\n",
    "\n",
    "Utiliser timm pour des mod√®les ViT pr√©-entra√Æn√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIMM_AVAILABLE:\n",
    "    # Lister tous les mod√®les ViT disponibles\n",
    "    vit_models = timm.list_models('vit*', pretrained=True)\n",
    "    print(f\"Nombre de mod√®les ViT pr√©-entra√Æn√©s: {len(vit_models)}\")\n",
    "    print(f\"\\nExemples:\")\n",
    "    for model_name in vit_models[:10]:\n",
    "        print(f\"  - {model_name}\")\n",
    "else:\n",
    "    print(\"timm not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIMM_AVAILABLE:\n",
    "    # Charger ViT-Base/16 pr√©-entra√Æn√© sur ImageNet\n",
    "    model_vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=10)\n",
    "    model_vit.to(device)\n",
    "    \n",
    "    print(f\"Mod√®le charg√©: vit_base_patch16_224\")\n",
    "    print(f\"Param√®tres: {sum(p.numel() for p in model_vit.parameters()) / 1e6:.2f}M\")\n",
    "    \n",
    "    # Comparer avec ResNet-50\n",
    "    model_resnet = timm.create_model('resnet50', pretrained=True, num_classes=10)\n",
    "    model_resnet.to(device)\n",
    "    \n",
    "    print(f\"\\nMod√®le charg√©: resnet50\")\n",
    "    print(f\"Param√®tres: {sum(p.numel() for p in model_resnet.parameters()) / 1e6:.2f}M\")\n",
    "    \n",
    "    # Info mod√®le\n",
    "    print(f\"\\nViT-Base configuration:\")\n",
    "    print(f\"  Patch size: 16x16\")\n",
    "    print(f\"  Embed dim: 768\")\n",
    "    print(f\"  Depth: 12 layers\")\n",
    "    print(f\"  Num heads: 12\")\n",
    "    print(f\"  Number of patches: {(224//16)**2} = 196\")\n",
    "else:\n",
    "    print(\"timm not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset CIFAR-10\n",
    "\n",
    "Pr√©parer CIFAR-10 pour l'entra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(224),  # ViT attend 224x224\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(224, padding=16),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Charger CIFAR-10\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} images\")\n",
    "print(f\"Test dataset: {len(test_dataset)} images\")\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser √©chantillons\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    img, label = train_dataset[i]\n",
    "    \n",
    "    # D√©normaliser\n",
    "    img_display = img.permute(1, 2, 0).numpy()\n",
    "    img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "    \n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(img_display)\n",
    "    ax.set_title(classes[label])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning ViT sur CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIMM_AVAILABLE:\n",
    "    def train_epoch(model, loader, criterion, optimizer, device):\n",
    "        \"\"\"Entra√Æne le mod√®le sur une epoch.\"\"\"\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(loader, desc='Training')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', \n",
    "                             'acc': f'{100.*correct/total:.2f}%'})\n",
    "        \n",
    "        return running_loss / len(loader), 100. * correct / total\n",
    "    \n",
    "    \n",
    "    def validate(model, loader, criterion, device):\n",
    "        \"\"\"Valide le mod√®le.\"\"\"\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(loader, desc='Validation'):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        return running_loss / len(loader), 100. * correct / total\n",
    "    \n",
    "    \n",
    "    # Entra√Æner ViT (fine-tuning)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Fine-tuning ViT-Base sur CIFAR-10\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Geler l'encoder, entra√Æner seulement la t√™te\n",
    "    for param in model_vit.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model_vit.head.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model_vit.head.parameters(), lr=1e-3)\n",
    "    \n",
    "    num_epochs = 3  # Rapide pour d√©mo\n",
    "    \n",
    "    history_vit = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model_vit, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model_vit, test_loader, criterion, device)\n",
    "        \n",
    "        history_vit['train_loss'].append(train_loss)\n",
    "        history_vit['train_acc'].append(train_acc)\n",
    "        history_vit['val_loss'].append(val_loss)\n",
    "        history_vit['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Fine-tuning termin√© ! Accuracy finale: {history_vit['val_acc'][-1]:.2f}%\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"timm not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison ViT vs ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIMM_AVAILABLE:\n",
    "    # Entra√Æner ResNet pour comparaison\n",
    "    print(\"\\nFine-tuning ResNet-50 sur CIFAR-10\")\n",
    "    \n",
    "    for param in model_resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model_resnet.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer_resnet = torch.optim.AdamW(model_resnet.fc.parameters(), lr=1e-3)\n",
    "    \n",
    "    history_resnet = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model_resnet, train_loader, criterion, optimizer_resnet, device)\n",
    "        val_loss, val_acc = validate(model_resnet, test_loader, criterion, device)\n",
    "        \n",
    "        history_resnet['train_loss'].append(train_loss)\n",
    "        history_resnet['train_acc'].append(train_acc)\n",
    "        history_resnet['val_loss'].append(val_loss)\n",
    "        history_resnet['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nResNet accuracy finale: {history_resnet['val_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIMM_AVAILABLE:\n",
    "    # Comparaison graphique\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(epochs, history_vit['train_loss'], 'b-', marker='o', label='ViT Train')\n",
    "    axes[0].plot(epochs, history_vit['val_loss'], 'b--', marker='o', label='ViT Val')\n",
    "    axes[0].plot(epochs, history_resnet['train_loss'], 'r-', marker='s', label='ResNet Train')\n",
    "    axes[0].plot(epochs, history_resnet['val_loss'], 'r--', marker='s', label='ResNet Val')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Loss Comparison')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(epochs, history_vit['train_acc'], 'b-', marker='o', label='ViT Train')\n",
    "    axes[1].plot(epochs, history_vit['val_acc'], 'b--', marker='o', label='ViT Val')\n",
    "    axes[1].plot(epochs, history_resnet['train_acc'], 'r-', marker='s', label='ResNet Train')\n",
    "    axes[1].plot(epochs, history_resnet['val_acc'], 'r--', marker='s', label='ResNet Val')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Accuracy Comparison')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tableau comparatif\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"{'Mod√®le':<20} {'Params (M)':<15} {'Val Accuracy':<20} {'Val Loss'}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'ViT-Base':<20} {sum(p.numel() for p in model_vit.parameters())/1e6:<15.2f} \"\n",
    "          f\"{history_vit['val_acc'][-1]:<20.2f} {history_vit['val_loss'][-1]:.4f}\")\n",
    "    print(f\"{'ResNet-50':<20} {sum(p.numel() for p in model_resnet.parameters())/1e6:<15.2f} \"\n",
    "          f\"{history_resnet['val_acc'][-1]:<20.2f} {history_resnet['val_loss'][-1]:.4f}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation Attention Maps\n",
    "\n",
    "Visualiser ce que ViT \"regarde\" dans l'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, image, device, patch_size=16):\n",
    "    \"\"\"Visualise l'attention du dernier layer.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Hook pour capturer attention\n",
    "    attentions = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # output[1] contient les attention weights pour timm\n",
    "        attentions.append(output[1])\n",
    "    \n",
    "    # Registrer hook sur le dernier block d'attention\n",
    "    # Note: Cette partie d√©pend de l'impl√©mentation exacte de timm\n",
    "    # Pour une d√©mo, on utilise notre impl√©mentation custom\n",
    "    \n",
    "    # Alternative: Utiliser notre VisionTransformer custom\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, 'blocks'):  # Notre impl√©mentation\n",
    "            logits, attn_weights = model(image.unsqueeze(0).to(device))\n",
    "            return attn_weights[-1][0]  # Dernier layer, premier batch\n",
    "        else:\n",
    "            print(\"Attention visualization non support√©e pour ce mod√®le timm\")\n",
    "            return None\n",
    "\n",
    "# Exemple avec notre VisionTransformer custom\n",
    "model_custom = VisionTransformer(\n",
    "    img_size=224, patch_size=16, num_classes=10,\n",
    "    embed_dim=384, depth=6, num_heads=6\n",
    ").to(device)\n",
    "\n",
    "# Charger une image\n",
    "img, label = test_dataset[0]\n",
    "\n",
    "# Obtenir attention\n",
    "attn = visualize_attention(model_custom, img, device)\n",
    "\n",
    "if attn is not None:\n",
    "    print(f\"Attention shape: {attn.shape}\")\n",
    "    print(f\"[num_heads, num_patches+1, num_patches+1]\")\n",
    "    \n",
    "    # Visualiser attention du [CLS] token\n",
    "    n_heads = attn.shape[0]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Image originale\n",
    "    img_display = img.permute(1, 2, 0).cpu().numpy()\n",
    "    img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "    \n",
    "    for i in range(6):\n",
    "        if i < n_heads:\n",
    "            # Attention du [CLS] token (premi√®re row, skip [CLS] lui-m√™me)\n",
    "            attn_map = attn[i, 0, 1:].cpu().numpy()\n",
    "            \n",
    "            # Reshape en grille de patches\n",
    "            n_patches = int(np.sqrt(len(attn_map)))\n",
    "            attn_map = attn_map.reshape(n_patches, n_patches)\n",
    "            \n",
    "            # Interpoler √† la taille de l'image\n",
    "            attn_map_resized = cv2.resize(attn_map, (224, 224))\n",
    "            \n",
    "            # Overlay\n",
    "            axes[i].imshow(img_display)\n",
    "            axes[i].imshow(attn_map_resized, cmap='hot', alpha=0.6)\n",
    "            axes[i].set_title(f'Head {i+1}')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Attention Maps - Class: {classes[label]}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CLIP - Zero-Shot Classification\n",
    "\n",
    "Utiliser CLIP pour classifier sans entra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLIP_AVAILABLE:\n",
    "    # Charger CLIP\n",
    "    model_clip, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "    \n",
    "    print(\"CLIP ViT-B/32 charg√©\")\n",
    "    print(f\"Param√®tres: {sum(p.numel() for p in model_clip.parameters()) / 1e6:.2f}M\")\n",
    "else:\n",
    "    print(\"CLIP non disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLIP_AVAILABLE:\n",
    "    # Zero-shot sur CIFAR-10\n",
    "    text_prompts = [f\"a photo of a {c}\" for c in classes]\n",
    "    text_tokens = clip.tokenize(text_prompts).to(device)\n",
    "    \n",
    "    # Tester sur √©chantillons\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encoder les prompts texte une seule fois\n",
    "        text_features = model_clip.encode_text(text_tokens)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        for i in range(10):\n",
    "            # Image CIFAR-10 (32x32)\n",
    "            img_pil, label = test_dataset.dataset[i]  # Image PIL originale\n",
    "            \n",
    "            # Pr√©processer pour CLIP\n",
    "            img_clip = preprocess_clip(img_pil).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Encoder image\n",
    "            image_features = model_clip.encode_image(img_clip)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Calculer similarit√©s\n",
    "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            values, indices = similarity[0].topk(3)\n",
    "            \n",
    "            # Pr√©diction\n",
    "            pred_idx = indices[0].item()\n",
    "            pred_class = classes[pred_idx]\n",
    "            true_class = classes[label]\n",
    "            \n",
    "            if pred_idx == label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "            # Afficher\n",
    "            axes[i].imshow(np.array(img_pil))\n",
    "            axes[i].set_title(f'True: {true_class}\\nPred: {pred_class} ({values[0].item():.1f}%)',\n",
    "                             color='green' if pred_idx == label else 'red')\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "            # Afficher top-3\n",
    "            print(f\"\\nImage {i+1} - True: {true_class}\")\n",
    "            for rank, (value, index) in enumerate(zip(values, indices)):\n",
    "                print(f\"  {rank+1}. {classes[index]:12s} {value.item():5.1f}%\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLIP Zero-Shot Accuracy: {100.*correct/total:.2f}% ({correct}/{total})\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"CLIP non disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©sum√©\n",
    "\n",
    "Dans ce notebook, nous avons explor√© :\n",
    "\n",
    "1. **Architecture ViT** :\n",
    "   - Patch embedding (d√©coupage en patches)\n",
    "   - Positional encoding\n",
    "   - Multi-head self-attention\n",
    "   - Transformer encoder\n",
    "   - Classification via [CLS] token\n",
    "\n",
    "2. **timm (PyTorch Image Models)** :\n",
    "   - ViT-Base pr√©-entra√Æn√© sur ImageNet\n",
    "   - Fine-tuning sur CIFAR-10\n",
    "   - Comparaison avec ResNet-50\n",
    "\n",
    "3. **Attention Maps** :\n",
    "   - Visualisation de ce que ViT \"regarde\"\n",
    "   - Interpr√©tabilit√© des multi-heads\n",
    "\n",
    "4. **CLIP** :\n",
    "   - Vision-language model\n",
    "   - Zero-shot classification\n",
    "   - Pas besoin d'entra√Ænement pour nouvelles classes\n",
    "\n",
    "### Points Cl√©s\n",
    "- **ViT** : Transformers adapt√©s √† la vision via patches\n",
    "- **Self-attention** : capture d√©pendances globales (vs locales CNN)\n",
    "- **Donn√©es** : ViT n√©cessite beaucoup de donn√©es (ImageNet-21k, JFT-300M)\n",
    "- **CLIP** : aligne images et textes pour zero-shot\n",
    "- **Attention maps** : visualisation plus interpr√©table que CNN\n",
    "\n",
    "### Trade-offs ViT vs CNN\n",
    "- **ViT** : Meilleure pr√©cision avec beaucoup de donn√©es, d√©pendances globales\n",
    "- **CNN** : Meilleur avec peu de donn√©es, inductive bias (localit√©, translation)\n",
    "\n",
    "### Prochaines √âtapes\n",
    "- Explorer Swin Transformer (attention locale)\n",
    "- Tester DeiT (Data-efficient ViT)\n",
    "- Appliquer CLIP √† vos propres t√¢ches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}