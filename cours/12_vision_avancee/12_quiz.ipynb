{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz d'Auto-√âvaluation - Chapitre 12 : Vision par Ordinateur Avanc√©e\n",
    "\n",
    "**Instructions** :\n",
    "- Ce quiz contient 15 questions pour tester votre compr√©hension du chapitre\n",
    "- R√©pondez aux questions par vous-m√™me avant de regarder les r√©ponses\n",
    "- Les r√©ponses sont dans une cellule masqu√©e √† la fin\n",
    "- Comptez 1 point par bonne r√©ponse\n",
    "\n",
    "**Bar√®me** :\n",
    "- 13-15 : Excellent ! Vous ma√Ætrisez le chapitre üí™\n",
    "- 10-12 : Bien, relisez les sections o√π vous avez des lacunes\n",
    "- 7-9 : Moyen, relisez le chapitre attentivement\n",
    "- < 7 : Insuffisant, reprenez le chapitre depuis le d√©but\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Question 1 : Object Detection\n",
    "Quelle information N'EST PAS pr√©dite par un mod√®le de d√©tection d'objets ?\n",
    "\n",
    "A) Bounding box (coordonn√©es)  \n",
    "B) Classe de l'objet  \n",
    "C) Score de confiance  \n",
    "D) Masque de segmentation pixel par pixel  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : IoU (Intersection over Union)\n",
    "Une d√©tection avec IoU = 0.7 signifie :\n",
    "\n",
    "A) 70% de l'objet est d√©tect√©  \n",
    "B) 70% de chevauchement entre box pr√©dite et ground truth  \n",
    "C) 70% de confiance dans la pr√©diction  \n",
    "D) 70% des objets sont correctement d√©tect√©s  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 : R-CNN vs Fast R-CNN\n",
    "Quelle est l'am√©lioration principale de Fast R-CNN par rapport √† R-CNN ?\n",
    "\n",
    "A) Utilise un CNN plus profond  \n",
    "B) Calcule les features CNN une seule fois pour toute l'image (RoI Pooling)  \n",
    "C) D√©tecte plus d'objets  \n",
    "D) N'utilise pas de r√©gion proposals  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 : RPN (Region Proposal Network)\n",
    "Le RPN dans Faster R-CNN remplace :\n",
    "\n",
    "A) Le backbone CNN  \n",
    "B) Selective Search (g√©n√©ration de r√©gion proposals)  \n",
    "C) La classification finale  \n",
    "D) Le Non-Maximum Suppression  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 : YOLO - Philosophie\n",
    "Quelle est la diff√©rence principale entre YOLO et Faster R-CNN ?\n",
    "\n",
    "A) YOLO utilise un CNN plus profond  \n",
    "B) YOLO fait tout en une seule passe (one-stage), Faster R-CNN en deux (two-stage)  \n",
    "C) YOLO ne peut d√©tecter qu'un seul objet par image  \n",
    "D) YOLO n'utilise pas de bounding boxes  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : NMS (Non-Maximum Suppression)\n",
    "√Ä quoi sert le Non-Maximum Suppression ?\n",
    "\n",
    "A) √Ä augmenter la vitesse d'inf√©rence  \n",
    "B) √Ä supprimer les d√©tections multiples du m√™me objet  \n",
    "C) √Ä augmenter le nombre de d√©tections  \n",
    "D) √Ä normaliser les scores de confiance  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 : Segmentation S√©mantique vs D√©tection\n",
    "Quelle est la diff√©rence entre segmentation s√©mantique et d√©tection d'objets ?\n",
    "\n",
    "A) La segmentation est plus rapide  \n",
    "B) La segmentation assigne une classe √† chaque pixel (pas de boxes)  \n",
    "C) La d√©tection est plus pr√©cise  \n",
    "D) Il n'y a pas de diff√©rence  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 : U-Net - Architecture\n",
    "Dans U-Net, les skip connections connectent :\n",
    "\n",
    "A) L'entr√©e directement √† la sortie  \n",
    "B) Les features de l'encoder aux features du decoder (par concat√©nation)  \n",
    "C) Uniquement les couches de m√™me profondeur  \n",
    "D) Le bottleneck √† la sortie  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 : DeepLab - Atrous Convolution\n",
    "L'atrous convolution (dilated convolution) permet de :\n",
    "\n",
    "A) R√©duire le nombre de param√®tres  \n",
    "B) Augmenter le champ r√©ceptif sans r√©duire la r√©solution  \n",
    "C) Acc√©l√©rer le calcul  \n",
    "D) Am√©liorer la normalisation  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 : Mask R-CNN\n",
    "Mask R-CNN ajoute √† Faster R-CNN :\n",
    "\n",
    "A) Une branche de segmentation (masque) pour chaque RoI  \n",
    "B) Un meilleur backbone CNN  \n",
    "C) Plus de r√©gion proposals  \n",
    "D) Une loss diff√©rente pour la classification  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 : Vision Transformer (ViT)\n",
    "Comment ViT traite-t-il une image ?\n",
    "\n",
    "A) Convolutions classiques comme un CNN  \n",
    "B) D√©coupe en patches, les traite comme des tokens avec self-attention  \n",
    "C) Pooling global uniquement  \n",
    "D) RNN sur les pixels  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12 : ViT - Patch Size\n",
    "Pour une image 224√ó224 avec patch size 16√ó16, combien de patches sont cr√©√©s ?\n",
    "\n",
    "A) 14 patches  \n",
    "B) 49 patches  \n",
    "C) 196 patches  \n",
    "D) 784 patches  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13 : Swin Transformer\n",
    "Quelle am√©lioration Swin Transformer apporte-t-il par rapport √† ViT ?\n",
    "\n",
    "A) Plus de param√®tres  \n",
    "B) Attention locale dans des fen√™tres d√©cal√©es (complexit√© lin√©aire)  \n",
    "C) Utilise des convolutions  \n",
    "D) Plus de couches Transformer  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14 : CLIP - Fonction\n",
    "CLIP permet principalement de :\n",
    "\n",
    "A) D√©tecter des objets plus rapidement  \n",
    "B) Aligner images et textes, permettant la classification zero-shot  \n",
    "C) Segmenter des images  \n",
    "D) G√©n√©rer des images √† partir de texte  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15 : Trade-off Vitesse vs Pr√©cision\n",
    "Pour une application temps r√©el (vid√©o surveillance), quel mod√®le choisir ?\n",
    "\n",
    "A) Faster R-CNN (pr√©cis mais lent, ~5 FPS)  \n",
    "B) YOLOv8 (rapide, ~80 FPS)  \n",
    "C) Mask R-CNN (tr√®s pr√©cis mais tr√®s lent)  \n",
    "D) U-Net (segmentation s√©mantique)  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Auto-Correction\n",
    "\n",
    "Avant de regarder les r√©ponses, comptez combien de r√©ponses vous avez donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrez vos r√©ponses ici (ex: ['D', 'B', 'B', ...])\n",
    "mes_reponses = []  # TODO: remplir avec vos r√©ponses\n",
    "\n",
    "# R√©ponses correctes (masqu√©es)\n",
    "reponses_correctes = ['D', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'C', 'B', 'B', 'B']\n",
    "\n",
    "if len(mes_reponses) == 15:\n",
    "    score = sum([1 for i, r in enumerate(mes_reponses) if r.upper() == reponses_correctes[i]])\n",
    "    print(f\"Votre score : {score}/15\")\n",
    "    \n",
    "    if score >= 13:\n",
    "        print(\"\\nüéâ Excellent ! Vous ma√Ætrisez le chapitre !\")\n",
    "    elif score >= 10:\n",
    "        print(\"\\n‚úÖ Bien ! Relisez les sections o√π vous avez des lacunes.\")\n",
    "    elif score >= 7:\n",
    "        print(\"\\n‚ö†Ô∏è  Moyen. Relisez le chapitre attentivement.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Insuffisant. Reprenez le chapitre depuis le d√©but.\")\n",
    "    \n",
    "    # Afficher les erreurs\n",
    "    print(\"\\nD√©tail :\")\n",
    "    for i, (ma_rep, bonne_rep) in enumerate(zip(mes_reponses, reponses_correctes), 1):\n",
    "        if ma_rep.upper() == bonne_rep:\n",
    "            print(f\"Q{i}: ‚úì Correct\")\n",
    "        else:\n",
    "            print(f\"Q{i}: ‚úó Votre r√©ponse: {ma_rep}, Correcte: {bonne_rep}\")\n",
    "else:\n",
    "    print(\"Veuillez remplir toutes les r√©ponses (15 lettres A, B, C ou D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Explications des R√©ponses\n",
    "\n",
    "### Q1 : D\n",
    "La d√©tection d'objets pr√©dit : bounding box, classe, score. Le **masque pixel par pixel** est fourni par la segmentation (Mask R-CNN), pas la d√©tection classique.\n",
    "\n",
    "### Q2 : B\n",
    "IoU (Intersection over Union) = $\\frac{\\text{Aire}(A \\cap B)}{\\text{Aire}(A \\cup B)}$ mesure le **pourcentage de chevauchement** entre box pr√©dite et ground truth.\n",
    "\n",
    "### Q3 : B\n",
    "Fast R-CNN am√©liore R-CNN en calculant les **features CNN une seule fois** pour toute l'image, puis utilise RoI Pooling pour extraire les features de chaque r√©gion (vs 2000 forward passes dans R-CNN).\n",
    "\n",
    "### Q4 : B\n",
    "Le RPN (Region Proposal Network) remplace **Selective Search** : il g√©n√®re des r√©gion proposals directement depuis la feature map par apprentissage.\n",
    "\n",
    "### Q5 : B\n",
    "YOLO est **one-stage** (une seule passe forward : d√©tection directe), Faster R-CNN est **two-stage** (1. RPN ‚Üí proposals, 2. Classification + r√©gression).\n",
    "\n",
    "### Q6 : B\n",
    "NMS (Non-Maximum Suppression) **supprime les d√©tections multiples du m√™me objet** : garde la box avec le score max, supprime les boxes chevauchantes (IoU > seuil).\n",
    "\n",
    "### Q7 : B\n",
    "Segmentation s√©mantique : **assigne une classe √† chaque pixel** (pas de bounding boxes). D√©tection : pr√©dit des boxes rectangulaires.\n",
    "\n",
    "### Q8 : B\n",
    "Dans U-Net, les skip connections **concat√®nent** les features de l'encoder avec celles du decoder (pr√©serve les d√©tails haute r√©solution).\n",
    "\n",
    "### Q9 : B\n",
    "Atrous convolution (dilated) : ins√®re des z√©ros entre les poids du filtre ‚Üí **augmente le champ r√©ceptif sans r√©duire la r√©solution** ni augmenter les param√®tres.\n",
    "\n",
    "### Q10 : A\n",
    "Mask R-CNN = Faster R-CNN + **branche de masque** qui pr√©dit un masque binaire de segmentation pour chaque RoI d√©tect√©.\n",
    "\n",
    "### Q11 : B\n",
    "ViT d√©coupe l'image en **patches** (ex: 16√ó16), les aplatit en vecteurs, les traite comme des **tokens** avec self-attention (comme en NLP).\n",
    "\n",
    "### Q12 : C\n",
    "Nombre de patches = $\\frac{224 \\times 224}{16 \\times 16} = \\frac{50176}{256} = 196$ patches (14√ó14 grille).\n",
    "\n",
    "### Q13 : B\n",
    "Swin Transformer utilise **attention locale dans des fen√™tres** d√©cal√©es (shifted windows) ‚Üí **complexit√© lin√©aire** $O(H \\cdot W)$ au lieu de quadratique $O((H \\cdot W)^2)$.\n",
    "\n",
    "### Q14 : B\n",
    "CLIP **aligne images et textes** dans un espace latent commun via contrastive learning ‚Üí permet la **classification zero-shot** (classifier sans entra√Ænement sp√©cifique).\n",
    "\n",
    "### Q15 : B\n",
    "Pour le temps r√©el, **YOLOv8** (~80 FPS) est le meilleur choix. Faster R-CNN (~5 FPS) et Mask R-CNN (encore plus lent) sont trop lents pour la vid√©o en direct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaines √âtapes\n",
    "\n",
    "- **Score < 10** : Relisez le chapitre 12 attentivement\n",
    "- **Score >= 10** : Passez au Chapitre 13 (Syst√®mes de Recommandation)\n",
    "- **R√©vision recommand√©e** : Refaites le quiz dans 2-3 jours pour ancrer les connaissances\n",
    "- **Pratique** : Impl√©mentez YOLOv8, U-Net et ViT sur des datasets r√©els (voir notebooks)"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
