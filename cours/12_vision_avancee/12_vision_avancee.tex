% Chapitre 12 - Vision par Ordinateur Avanc√©e
\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}

% Math√©matiques
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

% Mise en page
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}

% Graphiques et couleurs
\usepackage{graphicx}
\usepackage{xcolor}

% ===== UNICODE CHARACTERS SUPPORT =====
\usepackage{newunicodechar}

% Emojis et symboles
\newunicodechar{‚úÖ}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚ùå}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚úì}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚úó}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚ö†}{\textcolor{orange!80!black}{\textbf{/!\textbackslash}}}
\newunicodechar{üí°}{\textcolor{blue!70!black}{\textbf{(i)}}}
\newunicodechar{üéØ}{\textcolor{purple!70!black}{\textbf{$\star$}}}
\newunicodechar{üìä}{\textcolor{blue!70!black}{\textbf{[=]}}}

% √âtoiles (pour tableaux)
\newunicodechar{‚òÖ}{\textcolor{orange!80!black}{$\star$}}
\newunicodechar{‚òÜ}{\textcolor{gray!50}{$\star$}}

% Fl√®ches
\newunicodechar{‚Üí}{$\rightarrow$}
\newunicodechar{‚Üê}{$\leftarrow$}
\newunicodechar{‚Üë}{$\uparrow$}
\newunicodechar{‚Üì}{$\downarrow$}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric}

% Tableaux
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}

% Code et algorithmes
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Hyperliens
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 12 - Vision par Ordinateur Avanc√©e},
    pdfauthor={Cours ML},
}

% Boxes color√©es
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}


% ===== TCOLORBOX AVEC EMOJIS =====
\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö† Attention,
    breakable
}

\newtcolorbox{definition}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition,
    breakable
}

\newtcolorbox{astuce}{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

\newtcolorbox{remarque}{
    colback=yellow!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=üí° Remarque,
    breakable
}

\newtcolorbox{important}{
    colback=purple!5!white,
    colframe=purple!75!black,
    fonttitle=\bfseries,
    title=‚ö† Important,
    breakable
}

\newtcolorbox{exemple}{
    colback=gray!5!white,
    colframe=gray!75!black,
    fonttitle=\bfseries,
    title=Exemple,
    breakable
}

% En-t√™tes et pieds de page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 13 - Vision par Ordinateur Avanc√©e}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIGURATION LISTINGS (code Python) =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}
\lstset{style=pythonstyle}

% ===== CONFIGURATION TCOLORBOX =====


\newtcolorbox{theoreme}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Th√©or√®me: #1,
    breakable
}







% ===== COMMANDES PERSONNALIS√âES =====
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}

% ===== D√âBUT DU DOCUMENT =====
\begin{document}

% ===== PAGE DE TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]

    \vspace{1cm}

    {\LARGE Chapitre 13}\\[0.3cm]
    {\LARGE\bfseries Vision par Ordinateur Avanc√©e}\\[2cm]

    \vfill

    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Ma√Ætriser les architectures de d√©tection d'objets (R-CNN, YOLO, Faster R-CNN)
        \item Comprendre la segmentation s√©mantique et d'instances (U-Net, Mask R-CNN)
        \item D√©couvrir les Vision Transformers (ViT) et leur application
        \item Comprendre les mod√®les vision-langage (CLIP)
        \item Impl√©menter des pipelines de d√©tection et segmentation
    \end{itemize}
    }

    \vfill

    {\large
    \textbf{Pr√©requis :} Chapitres 06 (MLP), 07 (CNN), 08 (Transformers)\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 8-10 heures\\[0.3cm]
    \textbf{Notebooks :} \texttt{12\_demo\_*.ipynb}
    }

    \vfill

    {\large Cours ML - Sandbox-ML\\
    Version 1.0 - 2026}
\end{titlepage}

% ===== TABLE DES MATI√àRES =====
\tableofcontents
\newpage

% ===== SECTION 1: MOTIVATION =====
\section{Motivation}

La vision par ordinateur classique (chapitre 07) nous a permis de classifier des images avec des CNN. Cependant, de nombreuses applications r√©elles n√©cessitent bien plus qu'une simple classification :

\begin{exemple}{Applications n√©cessitant la vision avanc√©e}
\begin{itemize}
    \item \textbf{Conduite autonome} : D√©tecter et localiser les voitures, pi√©tons, panneaux
    \item \textbf{Imagerie m√©dicale} : Segmenter pr√©cis√©ment les tumeurs, organes
    \item \textbf{Surveillance} : Suivre et identifier les personnes dans une vid√©o
    \item \textbf{Commerce √©lectronique} : Rechercher des produits par image
    \item \textbf{Robotique} : Manipuler des objets d√©tect√©s dans une sc√®ne
\end{itemize}
\end{exemple}

Ces probl√®mes requi√®rent trois capacit√©s avanc√©es :

\begin{enumerate}
    \item \textbf{Object Detection} : O√π sont les objets ? (bounding boxes)
    \item \textbf{Semantic Segmentation} : Quel est le label de chaque pixel ?
    \item \textbf{Instance Segmentation} : Identifier chaque instance individuelle d'un objet
\end{enumerate}

Ce chapitre explore les architectures deep learning qui r√©solvent ces t√¢ches complexes.

% ===== SECTION 2: OBJECT DETECTION =====
\section{D√©tection d'Objets (Object Detection)}

\subsection{D√©finition du Probl√®me}

\begin{definition}{Object Detection}
La d√©tection d'objets consiste √† localiser et classifier simultan√©ment plusieurs objets dans une image. Pour chaque objet, on pr√©dit :
\begin{itemize}
    \item Une \textbf{bounding box} : $(x, y, w, h)$ o√π $(x,y)$ est le coin sup√©rieur gauche, $w$ la largeur, $h$ la hauteur
    \item Une \textbf{classe} : probabilit√©s $P(c_i | \text{box})$ pour chaque classe $c_i$
    \item Un \textbf{score de confiance} : probabilit√© qu'il y ait un objet dans la box
\end{itemize}
\end{definition}

\subsection{M√©triques d'√âvaluation}

\subsubsection{Intersection over Union (IoU)}

L'IoU mesure le chevauchement entre la box pr√©dite et la ground truth :

\begin{equation}
\text{IoU} = \frac{\text{Aire}(\text{Box}_{\text{pred}} \cap \text{Box}_{\text{gt}})}{\text{Aire}(\text{Box}_{\text{pred}} \cup \text{Box}_{\text{gt}})}
\end{equation}

\begin{itemize}
    \item $\text{IoU} = 1.0$ : chevauchement parfait
    \item $\text{IoU} = 0.0$ : aucun chevauchement
    \item $\text{IoU} \geq 0.5$ : g√©n√©ralement consid√©r√© comme une d√©tection correcte
\end{itemize}

\subsubsection{Mean Average Precision (mAP)}

La m√©trique standard pour √©valuer les d√©tecteurs :

\begin{enumerate}
    \item Pour chaque classe $c$, calculer la courbe Precision-Recall
    \item Calculer l'aire sous la courbe (Average Precision, AP)
    \item Faire la moyenne sur toutes les classes : $\text{mAP} = \frac{1}{C} \sum_{c=1}^{C} AP_c$
\end{enumerate}

\begin{itemize}
    \item \textbf{mAP@0.5} : seuil IoU = 0.5
    \item \textbf{mAP@0.5:0.95} : moyenne sur seuils IoU de 0.5 √† 0.95 (par pas de 0.05)
\end{itemize}

\subsection{R-CNN (2014) - R√©gion-based CNN}

\begin{definition}{R-CNN}
R-CNN propose de combiner :
\begin{enumerate}
    \item \textbf{Selective Search} : algorithme traditionnel proposant ~2000 r√©gions candidates
    \item \textbf{CNN} : extraction de features pour chaque r√©gion
    \item \textbf{SVM} : classification de chaque r√©gion
    \item \textbf{R√©gression} : ajustement des bounding boxes
\end{enumerate}
\end{definition}

\subsubsection{Pipeline R-CNN}

\begin{algorithm}[H]
\caption{R-CNN}
\label{alg:rcnn}
\begin{algorithmic}[1]
\REQUIRE Image $I$
\ENSURE Liste de d√©tections (boxes, classes, scores)
\STATE G√©n√©rer ~2000 r√©gion proposals avec Selective Search
\FOR{chaque r√©gion $R$}
    \STATE Redimensionner $R$ en $227 \times 227$
    \STATE Extraire features : $f_R = \text{CNN}(R)$
    \STATE Classifier : $c = \text{SVM}(f_R)$
    \STATE Ajuster box : $b' = \text{Regressor}(f_R)$
\ENDFOR
\STATE Appliquer Non-Maximum Suppression (NMS)
\RETURN D√©tections filtr√©es
\end{algorithmic}
\end{algorithm}

\subsubsection{Limites de R-CNN}

\begin{itemize}
    \item ‚ùå \textbf{Tr√®s lent} : 47 secondes par image (2000 forward passes CNN)
    \item ‚ùå \textbf{Entra√Ænement en 3 √©tapes} : CNN, SVM, r√©gression (s√©par√©ment)
    \item ‚ùå \textbf{Stockage important} : features extraites pour toutes les r√©gions
\end{itemize}

\subsection{Fast R-CNN (2015)}

\textbf{Id√©e cl√©} : Ne calculer les features CNN qu'une seule fois pour toute l'image.

\begin{definition}{RoI Pooling}
Le \textbf{Region of Interest (RoI) Pooling} permet d'extraire des features de taille fixe depuis n'importe quelle r√©gion de la feature map :
\begin{enumerate}
    \item Projeter la r√©gion proposal sur la feature map
    \item Diviser la r√©gion en $H \times W$ sous-r√©gions
    \item Appliquer max pooling sur chaque sous-r√©gion
\end{enumerate}
R√©sultat : un vecteur de features de taille fixe $H \times W \times C$ pour chaque RoI.
\end{definition}

\subsubsection{Architecture Fast R-CNN}

\begin{algorithm}[H]
\caption{Fast R-CNN}
\label{alg:fast-rcnn}
\begin{algorithmic}[1]
\REQUIRE Image $I$, r√©gions proposals $\{R_i\}$
\ENSURE D√©tections
\STATE Calculer feature map : $F = \text{CNN}(I)$ \quad (une seule fois !)
\FOR{chaque r√©gion $R_i$}
    \STATE Extraire features : $f_i = \text{RoIPool}(F, R_i)$
    \STATE Pr√©dire classe : $P(c | R_i) = \text{FC}(f_i)$
    \STATE Ajuster box : $\Delta b_i = \text{FC}(f_i)$
\ENDFOR
\STATE Appliquer NMS
\RETURN D√©tections
\end{algorithmic}
\end{algorithm}

\textbf{Am√©liorations} :
\begin{itemize}
    \item ‚úÖ \textbf{25x plus rapide} que R-CNN (0.32s par image)
    \item ‚úÖ \textbf{Entra√Ænement end-to-end} : une seule loss combin√©e
    \item ‚úÖ \textbf{Multi-task loss} : classification + r√©gression de box
\end{itemize}

\subsection{Faster R-CNN (2015)}

\textbf{Id√©e cl√©} : Remplacer Selective Search par un r√©seau de neurones.

\begin{definition}{Region Proposal Network (RPN)}
Le RPN est un petit r√©seau fully convolutional qui pr√©dit des r√©gion proposals directement depuis la feature map :
\begin{enumerate}
    \item Faire glisser une fen√™tre $3 \times 3$ sur la feature map
    \item Pour chaque position, pr√©dire $k$ anchors boxes (diff√©rentes tailles/ratios)
    \item Pour chaque anchor : pr√©dire score objectness + ajustement de box
\end{enumerate}
\end{definition}

\subsubsection{Architecture Faster R-CNN}

\begin{enumerate}
    \item \textbf{Backbone CNN} : ResNet-50, VGG, etc. $\rightarrow$ feature map
    \item \textbf{RPN} : propose des r√©gions candidates
    \item \textbf{RoI Pooling} : extrait features pour chaque proposition
    \item \textbf{T√™tes de classification/r√©gression} : pr√©dictions finales
\end{enumerate}

\subsubsection{Loss Function}

Loss multi-task combinant RPN et d√©tection :

\begin{equation}
L = L_{\text{RPN}}(\{p_i\}, \{t_i\}) + L_{\text{det}}(\{p_i'\}, \{t_i'\})
\end{equation}

o√π :
\begin{itemize}
    \item $L_{\text{RPN}} = L_{\text{cls}}(p_i, p_i^*) + \lambda L_{\text{reg}}(t_i, t_i^*)$ : loss du RPN
    \item $L_{\text{det}}$ : loss de d√©tection (similaire)
    \item $p_i$ : probabilit√© objectness, $t_i$ : coordonn√©es box
\end{itemize}

\textbf{Am√©liorations} :
\begin{itemize}
    \item ‚úÖ \textbf{10x plus rapide} que Fast R-CNN (0.2s par image, 5 FPS)
    \item ‚úÖ \textbf{Enti√®rement appris} : plus besoin de Selective Search
    \item ‚úÖ \textbf{√âtat de l'art} en pr√©cision (mAP ~70\% sur COCO)
\end{itemize}

\subsection{YOLO (You Only Look Once)}

\textbf{Philosophie diff√©rente} : Faster R-CNN fait deux passes (RPN puis d√©tection). YOLO fait tout en une seule passe forward !

\begin{definition}{YOLO}
YOLO divise l'image en une grille $S \times S$ et pr√©dit directement, pour chaque cellule :
\begin{itemize}
    \item $B$ bounding boxes avec leurs coordonn√©es $(x, y, w, h)$
    \item Un score de confiance par box : $P(\text{object}) \times \text{IoU}$
    \item Des probabilit√©s de classe : $P(c_i | \text{object})$
\end{itemize}
Le r√©seau produit un tenseur de taille $S \times S \times (B \cdot 5 + C)$.
\end{definition}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.8, every node/.style={font=\small}]
    % Input image with grid overlay
    \draw[thick] (0,0) rectangle (6,6);
    \node[above] at (3, 6) {Image d'entr√©e ($448 \times 448$)};

    % Draw 7x7 grid
    \foreach \x in {0,0.857,...,6} {
        \draw[gray, thin] (\x, 0) -- (\x, 6);
    }
    \foreach \y in {0,0.857,...,6} {
        \draw[gray, thin] (0, \y) -- (6, \y);
    }

    % Highlight one cell
    \draw[red, very thick] (2.571, 3.429) rectangle (3.428, 4.286);
    \node[red, font=\scriptsize] at (3, 3.85) {Cell $(i,j)$};

    % Draw sample objects
    \draw[blue, ultra thick, rounded corners] (1.2, 4.2) rectangle (2.8, 5.5);
    \node[blue, font=\scriptsize, above] at (2, 5.5) {Chien};
    \fill[blue] (2, 4.85) circle (0.08);

    \draw[orange, ultra thick, rounded corners] (3.5, 1.5) rectangle (5.2, 3.2);
    \node[orange, font=\scriptsize, above] at (4.35, 3.2) {Chat};
    \fill[orange] (4.35, 2.35) circle (0.08);

    % Arrow to output tensor
    \draw[->, very thick] (7, 3) -- (9, 3);

    % Output tensor representation
    \begin{scope}[xshift=9.5cm, yshift=1cm]
        % 7x7 grid representation
        \foreach \i in {0,...,6} {
            \foreach \j in {0,...,6} {
                \draw[fill=gray!20] (\i*0.5, \j*0.5) rectangle ++ (0.5, 0.5);
            }
        }
        \node[below, font=\footnotesize, align=center] at (1.75, -0.3) {$7 \times 7$ grille};

        % Depth dimension
        \foreach \k in {1,...,4} {
            \draw[fill=blue!15, opacity=0.7] (0.15*\k, 0.15*\k) rectangle ++ (3.5, 3.5);
        }
        \node[right, font=\footnotesize, align=left] at (4.2, 2) {$B \cdot 5 + C = 30$\\(2 boxes,\\20 classes)};
    \end{scope}

    % Detailed cell output (zoom)
    \begin{scope}[yshift=-4cm]
        \node[align=center, font=\small] at (3, 0.5) {
            \textbf{Pour chaque cellule $(i,j)$ :}
        };

        % Box 1
        \draw[draw=purple!70!black, fill=purple!10, thick, rounded corners] (0, -0.5) rectangle (2.8, -3.5);
        \node[purple!70!black, font=\scriptsize, align=left] at (1.4, -1.5) {
            \textbf{Box 1:}\\
            $(x_1, y_1, w_1, h_1)$\\
            $\text{conf}_1$
        };

        % Box 2
        \draw[draw=purple!70!black, fill=purple!10, thick, rounded corners] (3.2, -0.5) rectangle (6, -3.5);
        \node[purple!70!black, font=\scriptsize, align=left] at (4.6, -1.5) {
            \textbf{Box 2:}\\
            $(x_2, y_2, w_2, h_2)$\\
            $\text{conf}_2$
        };

        % Class probabilities
        \draw[draw=green!60!black, fill=green!10, thick, rounded corners] (6.5, -0.5) rectangle (10.5, -3.5);
        \node[green!60!black, font=\scriptsize, align=left] at (8.5, -2) {
            \textbf{Classes (20):}\\
            $P(\text{chat} | \text{obj})$\\
            $P(\text{chien} | \text{obj})$\\
            $\vdots$
        };
    \end{scope}

    % Prediction visualization
    \begin{scope}[xshift=12cm]
        \draw[thick] (0,0) rectangle (6,6);
        \node[above] at (3, 6) {Pr√©dictions finales};

        % Draw grid lightly
        \foreach \x in {0,0.857,...,6} {
            \draw[gray!30, thin] (\x, 0) -- (\x, 6);
        }
        \foreach \y in {0,0.857,...,6} {
            \draw[gray!30, thin] (0, \y) -- (6, \y);
        }

        % Predicted boxes with NMS
        \draw[blue, ultra thick, rounded corners] (1.15, 4.15) rectangle (2.85, 5.55);
        \node[blue, font=\scriptsize, fill=white, inner sep=1pt] at (1.5, 5.8) {Chien 0.92};

        \draw[orange, ultra thick, rounded corners] (3.45, 1.45) rectangle (5.25, 3.25);
        \node[orange, font=\scriptsize, fill=white, inner sep=1pt] at (4, 3.5) {Chat 0.87};

        % Show responsible cells
        \fill[blue!30, opacity=0.4] (0.857, 3.429) rectangle (1.714, 4.286);
        \fill[orange!30, opacity=0.4] (3.428, 1.714) rectangle (4.285, 2.571);

        \node[font=\tiny, align=center] at (3, -0.5) {apr√®s NMS\\(suppression\\non-max)};
    \end{scope}
\end{tikzpicture}
\caption{Principe de d√©tection YOLO. \textbf{(Gauche)} L'image est divis√©e en grille $S \times S$ (ici $S=7$). Chaque cellule est responsable de d√©tecter les objets dont le centre tombe dans cette cellule. \textbf{(Centre)} Chaque cellule pr√©dit $B=2$ bounding boxes (coordonn√©es + confiance) et $C=20$ probabilit√©s de classe, produisant un tenseur $7 \times 7 \times 30$. \textbf{(Droite)} Apr√®s NMS, on garde les d√©tections avec haute confiance. Une seule passe forward permet de d√©tecter tous les objets simultan√©ment.}
\label{fig:yolo_detection_grid}
\end{figure}

\subsubsection{Architecture YOLO (v1, 2016)}

\begin{enumerate}
    \item 24 couches convolutionnelles (inspir√©es de GoogLeNet)
    \item 2 couches fully connected
    \item Sortie : $7 \times 7 \times 30$ (pour $S=7$, $B=2$, $C=20$)
\end{enumerate}

\subsubsection{Loss Function YOLO}

Loss complexe combinant 3 termes :

\begin{align}
L = &\lambda_{\text{coord}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} [(x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2] \\
    &+ \lambda_{\text{coord}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} [(\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2] \\
    &+ \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} (C_i - \hat{C}_i)^2 \\
    &+ \lambda_{\text{noobj}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{noobj}} (C_i - \hat{C}_i)^2 \\
    &+ \sum_{i=0}^{S^2} \mathbb{1}_{i}^{\text{obj}} \sum_{c \in \text{classes}} (p_i(c) - \hat{p}_i(c))^2
\end{align}

o√π $\mathbb{1}_{ij}^{\text{obj}}$ indique si un objet est pr√©sent dans la cellule $i$, box $j$.

\subsubsection{YOLOv2 et YOLOv3 (2017-2018)}

\textbf{YOLOv2 (YOLO9000)} :
\begin{itemize}
    \item Batch Normalization dans toutes les couches
    \item Anchor boxes (comme Faster R-CNN)
    \item Haute r√©solution : $416 \times 416$ au lieu de $448 \times 448$
    \item Multi-scale training
    \item Darknet-19 backbone (19 couches)
\end{itemize}

\textbf{YOLOv3} :
\begin{itemize}
    \item Darknet-53 backbone (53 couches + residual connections)
    \item Pr√©dictions multi-√©chelles (3 √©chelles : $13 \times 13$, $26 \times 26$, $52 \times 52$)
    \item Meilleure d√©tection des petits objets
    \item Logistic regression pour objectness
\end{itemize}

\subsubsection{YOLOv5, YOLOv8 (2020-2023)}

\textbf{YOLOv5} (Ultralytics) :
\begin{itemize}
    \item Impl√©mentation PyTorch moderne
    \item CSPDarknet backbone
    \item Auto-anchor, auto-learning bounding box anchors
    \item Mosaic augmentation
    \item \textbf{Tr√®s rapide} : 140 FPS sur GPU
\end{itemize}

\textbf{YOLOv8} (2023) :
\begin{itemize}
    \item Architecture am√©lior√©e (anchor-free)
    \item Meilleure pr√©cision (mAP 53\% sur COCO)
    \item API simplifi√©e : \texttt{from ultralytics import YOLO}
    \item Support natif de la segmentation d'instances
\end{itemize}

\subsection{Comparaison R-CNN vs YOLO}

\begin{table}[h]
\centering
\caption{Comparaison des architectures de d√©tection}
\label{tab:detection-comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Mod√®le} & \textbf{mAP (\%)} & \textbf{FPS} & \textbf{Approche} & \textbf{Temps r√©el} \\
\midrule
R-CNN & 66.0 & 0.02 & Two-stage & ‚ùå \\
Fast R-CNN & 70.0 & 3.1 & Two-stage & ‚ùå \\
Faster R-CNN & 73.2 & 5 & Two-stage & ‚ùå \\
YOLOv1 & 63.4 & 45 & One-stage & ‚úÖ \\
YOLOv3 & 57.9 & 65 & One-stage & ‚úÖ \\
YOLOv5 & 50.7 & 140 & One-stage & ‚úÖ \\
YOLOv8 & 53.9 & 80 & One-stage & ‚úÖ \\
\bottomrule
\end{tabular}
\end{table}

\begin{astuce}
\textbf{Quand utiliser quoi ?}
\begin{itemize}
    \item \textbf{Faster R-CNN} : Pr√©cision maximale, applications non temps r√©el (imagerie m√©dicale)
    \item \textbf{YOLO} : Temps r√©el, vid√©o, applications embarqu√©es (conduite autonome, surveillance)
\end{itemize}
\end{astuce}

\subsection{Non-Maximum Suppression (NMS)}

Probl√®me : Les d√©tecteurs produisent souvent plusieurs boxes pour le m√™me objet.

\begin{algorithm}[H]
\caption{Non-Maximum Suppression}
\label{alg:nms}
\begin{algorithmic}[1]
\REQUIRE Boxes $B = \{b_1, \dots, b_n\}$, scores $S = \{s_1, \dots, s_n\}$, seuil IoU $\tau$
\ENSURE Boxes filtr√©es $D$
\STATE $D \leftarrow \emptyset$
\WHILE{$B \neq \emptyset$}
    \STATE $b^* \leftarrow \argmax_{b \in B} S(b)$ \quad (box avec le score max)
    \STATE $D \leftarrow D \cup \{b^*\}$
    \STATE $B \leftarrow B \setminus \{b^*\}$
    \FOR{chaque box $b_i \in B$}
        \IF{$\text{IoU}(b^*, b_i) > \tau$}
            \STATE $B \leftarrow B \setminus \{b_i\}$ \quad (supprimer box chevauchante)
        \ENDIF
    \ENDFOR
\ENDWHILE
\RETURN $D$
\end{algorithmic}
\end{algorithm}

\textbf{Variantes} :
\begin{itemize}
    \item \textbf{Soft NMS} : Au lieu de supprimer, r√©duire le score proportionnellement √† l'IoU
    \item \textbf{DIoU-NMS} : Utiliser la Distance-IoU au lieu de l'IoU standard
\end{itemize}

% ===== SECTION 3: SEGMENTATION S√âMANTIQUE =====
\section{Segmentation S√©mantique}

\subsection{D√©finition du Probl√®me}

\begin{definition}{Segmentation S√©mantique}
La segmentation s√©mantique consiste √† assigner une √©tiquette de classe √† chaque pixel de l'image. Pour une image $I \in \R^{H \times W \times 3}$, on pr√©dit une carte de segmentation $S \in \{1, \dots, C\}^{H \times W}$ o√π $S_{ij}$ est la classe du pixel $(i,j)$.
\end{definition}

\textbf{Diff√©rence avec la d√©tection} :
\begin{itemize}
    \item D√©tection : boxes rectangulaires
    \item Segmentation s√©mantique : contours pr√©cis au pixel pr√®s
    \item Segmentation d'instances : distingue les instances individuelles d'une m√™me classe
\end{itemize}

\subsection{M√©triques d'√âvaluation}

\subsubsection{Intersection over Union (IoU) par classe}

\begin{equation}
\text{IoU}_c = \frac{TP_c}{TP_c + FP_c + FN_c}
\end{equation}

o√π $TP_c$ = pixels correctement pr√©dits de classe $c$, $FP_c$ = pixels faussement pr√©dits, $FN_c$ = pixels manqu√©s.

\subsubsection{Mean IoU (mIoU)}

Moyenne de l'IoU sur toutes les classes :

\begin{equation}
\text{mIoU} = \frac{1}{C} \sum_{c=1}^{C} \text{IoU}_c
\end{equation}

\subsubsection{Dice Coefficient}

Particuli√®rement utilis√© en imagerie m√©dicale :

\begin{equation}
\text{Dice} = \frac{2 \cdot |A \cap B|}{|A| + |B|} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
\end{equation}

Le Dice est √©quivalent √† la F1-score pour la segmentation.

\subsection{Fully Convolutional Networks (FCN, 2015)}

\textbf{Id√©e cl√©} : Remplacer les couches fully connected par des convolutions pour produire une carte de segmentation.

\begin{definition}{FCN}
Un FCN transforme un r√©seau de classification (VGG, ResNet) en r√©seau de segmentation :
\begin{enumerate}
    \item Encoder : extraire features avec convolutions + pooling
    \item Decoder : upsampling progressif pour retrouver la r√©solution originale
    \item Skip connections : combiner features haute et basse r√©solution
\end{enumerate}
\end{definition}

\subsubsection{Architecture FCN}

\begin{enumerate}
    \item \textbf{Convolutionalization} : Remplacer FC layers par convolutions $1 \times 1$
    \item \textbf{Upsampling} : Transposed convolutions (deconvolutions) pour augmenter la r√©solution
    \item \textbf{Skip connections} : Additionner les features du decoder avec celles de l'encoder
\end{enumerate}

\textbf{Variantes} :
\begin{itemize}
    \item \textbf{FCN-32s} : upsampling x32 en une seule √©tape
    \item \textbf{FCN-16s} : skip connection de pool4
    \item \textbf{FCN-8s} : skip connections de pool3 et pool4 (meilleur)
\end{itemize}

\subsection{U-Net (2015)}

\textbf{Architecture embl√©matique} pour la segmentation m√©dicale.

\begin{definition}{U-Net}
U-Net a une architecture en U sym√©trique :
\begin{itemize}
    \item \textbf{Contracting path} (encoder) : Convolutions + max pooling $\downarrow$
    \item \textbf{Expansive path} (decoder) : Transposed convolutions $\uparrow$
    \item \textbf{Skip connections} : Concat√©nation (pas addition) des features
\end{itemize}
\end{definition}

\subsubsection{Architecture D√©taill√©e}

\begin{verbatim}
Encoder (Contracting Path):
  Input (572x572x1)
    ‚Üí Conv 3x3 ReLU (570x570x64)
    ‚Üí Conv 3x3 ReLU (568x568x64)
    ‚Üí MaxPool 2x2 (284x284x64)
    ‚Üí Conv 3x3 ReLU (282x282x128)
    ‚Üí Conv 3x3 ReLU (280x280x128)
    ‚Üí MaxPool 2x2 (140x140x128)
    ‚Üí ... (4 niveaux au total)

Bottleneck:
    ‚Üí Conv 3x3 ReLU (28x28x1024)
    ‚Üí Conv 3x3 ReLU (28x28x1024)

Decoder (Expansive Path):
    ‚Üí UpConv 2x2 (56x56x512)
    ‚Üí Concatenate avec skip connection de l'encoder
    ‚Üí Conv 3x3 ReLU
    ‚Üí Conv 3x3 ReLU
    ‚Üí ... (4 niveaux au total)

Output:
    ‚Üí Conv 1x1 (388x388xC) pour C classes
\end{verbatim}

\textbf{Points cl√©s} :
\begin{itemize}
    \item \textbf{Skip connections par concat√©nation} : pr√©serve mieux les d√©tails que l'addition
    \item \textbf{Data augmentation intensive} : rotations, d√©formations √©lastiques
    \item \textbf{Weighted loss} : pond√©rer la loss aux fronti√®res entre cellules
\end{itemize}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.7, every node/.style={font=\scriptsize}]
    % Define styles
    \tikzstyle{conv}=[rectangle, draw, fill=blue!20, minimum width=1.2cm, minimum height=0.6cm]
    \tikzstyle{pool}=[rectangle, draw, fill=red!20, minimum width=1.2cm, minimum height=0.4cm]
    \tikzstyle{upconv}=[rectangle, draw, fill=green!20, minimum width=1.2cm, minimum height=0.6cm]
    \tikzstyle{concat}=[circle, draw, fill=orange!30, minimum size=0.5cm]

    % Encoder (left side, downward)
    % Level 0 (input)
    \node[conv] (e0a) at (0, 0) {Conv 3√ó3};
    \node[conv] (e0b) at (0, -0.8) {Conv 3√ó3};
    \node[above=0.1cm of e0a, font=\tiny] {572√ó572√ó1};
    \node[right=0.1cm of e0b, font=\tiny] {568√ó568√ó64};
    \node[pool] (p0) at (0, -1.5) {MaxPool};

    % Level 1
    \node[conv] (e1a) at (0, -2.5) {Conv 3√ó3};
    \node[conv] (e1b) at (0, -3.3) {Conv 3√ó3};
    \node[right=0.1cm of e1b, font=\tiny] {280√ó280√ó128};
    \node[pool] (p1) at (0, -4.0) {MaxPool};

    % Level 2
    \node[conv] (e2a) at (0, -5.0) {Conv 3√ó3};
    \node[conv] (e2b) at (0, -5.8) {Conv 3√ó3};
    \node[right=0.1cm of e2b, font=\tiny] {136√ó136√ó256};
    \node[pool] (p2) at (0, -6.5) {MaxPool};

    % Level 3
    \node[conv] (e3a) at (0, -7.5) {Conv 3√ó3};
    \node[conv] (e3b) at (0, -8.3) {Conv 3√ó3};
    \node[right=0.1cm of e3b, font=\tiny] {64√ó64√ó512};
    \node[pool] (p3) at (0, -9.0) {MaxPool};

    % Bottleneck (bottom)
    \node[conv, fill=purple!20] (b0) at (4, -10.0) {Conv 3√ó3};
    \node[conv, fill=purple!20] (b1) at (4, -10.8) {Conv 3√ó3};
    \node[below=0.1cm of b1, font=\tiny] {28√ó28√ó1024};

    % Decoder (right side, upward)
    % Level 3
    \node[upconv] (u3) at (8, -9.0) {UpConv 2√ó2};
    \node[concat] (c3) at (8, -8.3) {+};
    \node[conv] (d3a) at (8, -7.5) {Conv 3√ó3};
    \node[conv] (d3b) at (8, -6.8) {Conv 3√ó3};
    \node[left=0.1cm of d3b, font=\tiny] {64√ó64√ó512};

    % Level 2
    \node[upconv] (u2) at (8, -6.0) {UpConv 2√ó2};
    \node[concat] (c2) at (8, -5.3) {+};
    \node[conv] (d2a) at (8, -4.5) {Conv 3√ó3};
    \node[conv] (d2b) at (8, -3.8) {Conv 3√ó3};
    \node[left=0.1cm of d2b, font=\tiny] {136√ó136√ó256};

    % Level 1
    \node[upconv] (u1) at (8, -3.0) {UpConv 2√ó2};
    \node[concat] (c1) at (8, -2.3) {+};
    \node[conv] (d1a) at (8, -1.5) {Conv 3√ó3};
    \node[conv] (d1b) at (8, -0.8) {Conv 3√ó3};
    \node[left=0.1cm of d1b, font=\tiny] {280√ó280√ó128};

    % Level 0 (output)
    \node[upconv] (u0) at (8, 0) {UpConv 2√ó2};
    \node[concat] (c0) at (8, 0.7) {+};
    \node[conv] (d0a) at (8, 1.5) {Conv 3√ó3};
    \node[conv] (d0b) at (8, 2.3) {Conv 3√ó3};
    \node[conv, fill=yellow!30] (out) at (8, 3.1) {Conv 1√ó1};
    \node[above=0.1cm of out, font=\tiny] {388√ó388√óC};

    % Arrows - Encoder path
    \draw[->, thick] (e0a) -- (e0b);
    \draw[->, thick] (e0b) -- (p0);
    \draw[->, thick] (p0) -- (e1a);
    \draw[->, thick] (e1a) -- (e1b);
    \draw[->, thick] (e1b) -- (p1);
    \draw[->, thick] (p1) -- (e2a);
    \draw[->, thick] (e2a) -- (e2b);
    \draw[->, thick] (e2b) -- (p2);
    \draw[->, thick] (p2) -- (e3a);
    \draw[->, thick] (e3a) -- (e3b);
    \draw[->, thick] (e3b) -- (p3);
    \draw[->, thick] (p3) -- (b0);
    \draw[->, thick] (b0) -- (b1);

    % Arrows - Decoder path
    \draw[->, thick] (b1) -- (u3);
    \draw[->, thick] (u3) -- (c3);
    \draw[->, thick] (c3) -- (d3a);
    \draw[->, thick] (d3a) -- (d3b);
    \draw[->, thick] (d3b) -- (u2);
    \draw[->, thick] (u2) -- (c2);
    \draw[->, thick] (c2) -- (d2a);
    \draw[->, thick] (d2a) -- (d2b);
    \draw[->, thick] (d2b) -- (u1);
    \draw[->, thick] (u1) -- (c1);
    \draw[->, thick] (c1) -- (d1a);
    \draw[->, thick] (d1a) -- (d1b);
    \draw[->, thick] (d1b) -- (u0);
    \draw[->, thick] (u0) -- (c0);
    \draw[->, thick] (c0) -- (d0a);
    \draw[->, thick] (d0a) -- (d0b);
    \draw[->, thick] (d0b) -- (out);

    % Skip connections (horizontal arrows with concatenation)
    \draw[->, thick, dashed, orange!70!black] (e3b.east) -- (c3.west) node[midway, above, font=\tiny] {concat};
    \draw[->, thick, dashed, orange!70!black] (e2b.east) -- (c2.west) node[midway, above, font=\tiny] {concat};
    \draw[->, thick, dashed, orange!70!black] (e1b.east) -- (c1.west) node[midway, above, font=\tiny] {concat};
    \draw[->, thick, dashed, orange!70!black] (e0b.east) -- (c0.west) node[midway, above, font=\tiny] {concat};

    % Labels
    \node[font=\small, blue!70!black] at (-1.5, -4.5) {\textbf{Encoder}};
    \node[font=\small, blue!70!black] at (-1.5, -5.0) {(Contracting)};
    \node[font=\small, purple!70!black] at (4, -11.5) {\textbf{Bottleneck}};
    \node[font=\small, green!60!black] at (9.5, -4.5) {\textbf{Decoder}};
    \node[font=\small, green!60!black] at (9.5, -5.0) {(Expansive)};

    % Dimension annotations
    \draw[<->, gray, thick] (-2, 0) -- (-2, -1.5) node[midway, left, font=\tiny] {‚Üì /2};
    \draw[<->, gray, thick] (-2, -2.5) -- (-2, -4.0) node[midway, left, font=\tiny] {‚Üì /2};
    \draw[<->, gray, thick] (-2, -5.0) -- (-2, -6.5) node[midway, left, font=\tiny] {‚Üì /2};
    \draw[<->, gray, thick] (-2, -7.5) -- (-2, -9.0) node[midway, left, font=\tiny] {‚Üì /2};

    \draw[<->, gray, thick] (10, -9.0) -- (10, -6.8) node[midway, right, font=\tiny] {‚Üë √ó2};
    \draw[<->, gray, thick] (10, -6.0) -- (10, -3.8) node[midway, right, font=\tiny] {‚Üë √ó2};
    \draw[<->, gray, thick] (10, -3.0) -- (10, -0.8) node[midway, right, font=\tiny] {‚Üë √ó2};
    \draw[<->, gray, thick] (10, 0) -- (10, 2.3) node[midway, right, font=\tiny] {‚Üë √ó2};
\end{tikzpicture}
\caption{Architecture U-Net. L'\textbf{encoder} (gauche) extrait des features √† diff√©rentes √©chelles via convolutions et max pooling. Le \textbf{bottleneck} (bas) capture les features de plus haut niveau. Le \textbf{decoder} (droite) reconstruit la segmentation via upsampling et convolutions. Les \textbf{skip connections} (fl√®ches orange) concat√®nent les features de l'encoder au decoder pour pr√©server les d√©tails spatiaux fins. La forme en U caract√©ristique permet de combiner informations contextuelles (encoder) et locales (skip connections).}
\label{fig:unet_architecture}
\end{figure}

\subsubsection{Loss Function U-Net}

Weighted cross-entropy pour g√©rer le d√©s√©quilibre de classes :

\begin{equation}
L = \sum_{x \in \Omega} w(x) \log(p_{l(x)}(x))
\end{equation}

o√π :
\begin{itemize}
    \item $w(x)$ : poids du pixel $x$ (plus √©lev√© aux fronti√®res)
    \item $p_{l(x)}(x)$ : probabilit√© softmax de la classe $l(x)$ au pixel $x$
\end{itemize}

Poids calcul√© pour s√©parer les instances proches :

\begin{equation}
w(x) = w_c(x) + w_0 \cdot \exp\left(-\frac{(d_1(x) + d_2(x))^2}{2\sigma^2}\right)
\end{equation}

o√π $d_1(x)$, $d_2(x)$ sont les distances aux deux instances les plus proches.

\subsection{DeepLab (v1-v3, 2015-2018)}

\textbf{Id√©e cl√©} : Utiliser des \textbf{atrous convolutions} (dilated convolutions) pour augmenter le champ r√©ceptif sans r√©duire la r√©solution.

\begin{definition}{Atrous Convolution}
Une convolution atrous avec taux de dilatation $r$ ins√®re $r-1$ z√©ros entre chaque poids du filtre :
\begin{equation}
y[i] = \sum_{k} x[i + r \cdot k] \cdot w[k]
\end{equation}
Cela permet d'augmenter le champ r√©ceptif de mani√®re exponentielle sans augmenter le nombre de param√®tres.
\end{definition}

\subsubsection{Atrous Spatial Pyramid Pooling (ASPP)}

Module cl√© de DeepLab v2/v3 :

\begin{enumerate}
    \item Appliquer des convolutions atrous avec diff√©rents taux : $r = \{6, 12, 18, 24\}$
    \item Appliquer global average pooling
    \item Concat√©ner toutes les features
    \item Convolution $1 \times 1$ pour fusion
\end{enumerate}

Cela capture le contexte √† plusieurs √©chelles.

\subsubsection{DeepLab v3+}

Am√©lioration avec un decoder :

\begin{itemize}
    \item Encoder : ResNet + ASPP
    \item Decoder : Upsampling progressif avec skip connections
    \item √âtat de l'art sur PASCAL VOC (mIoU 89\%)
\end{itemize}

\subsection{Mask R-CNN (2017)}

Extension de Faster R-CNN pour la segmentation d'instances.

\begin{definition}{Mask R-CNN}
Mask R-CNN ajoute une branche de segmentation parall√®le √† Faster R-CNN :
\begin{enumerate}
    \item Backbone + RPN + RoI Align (am√©lioration de RoI Pooling)
    \item Branche de classification + r√©gression de box (comme Faster R-CNN)
    \item \textbf{Branche de masque} : FCN qui pr√©dit un masque binaire pour chaque RoI
\end{enumerate}
\end{definition}

\subsubsection{RoI Align}

\textbf{Probl√®me de RoI Pooling} : quantification qui cause un misalignment pixel-level.

\textbf{Solution RoI Align} :
\begin{itemize}
    \item Ne pas quantifier les coordonn√©es de la RoI
    \item Utiliser une interpolation bilin√©aire pour √©chantillonner les features
    \item Pr√©cision au sub-pixel level
\end{itemize}

\subsubsection{Loss Function}

Multi-task loss :

\begin{equation}
L = L_{\text{cls}} + L_{\text{box}} + L_{\text{mask}}
\end{equation}

o√π :
\begin{itemize}
    \item $L_{\text{cls}}$ : classification loss
    \item $L_{\text{box}}$ : bounding box regression loss
    \item $L_{\text{mask}}$ : binary cross-entropy par pixel pour le masque
\end{itemize}

\textbf{Astuce} : La loss du masque est calcul√©e uniquement pour la classe pr√©dite, ce qui d√©couple classification et segmentation.

\subsection{Comparaison des Architectures de Segmentation}

\begin{table}[h]
\centering
\caption{Comparaison des architectures de segmentation}
\label{tab:segmentation-comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Mod√®le} & \textbf{Type} & \textbf{mIoU (\%)} & \textbf{Application} \\
\midrule
FCN-8s & S√©mantique & 62.2 & Sc√®nes g√©n√©rales \\
U-Net & S√©mantique & 92.0 & Imagerie m√©dicale \\
DeepLab v3+ & S√©mantique & 89.0 & Sc√®nes g√©n√©rales \\
Mask R-CNN & Instance & 37.1 & D√©tection + seg. \\
\bottomrule
\end{tabular}
\end{table}

% ===== SECTION 4: VISION TRANSFORMERS =====
\section{Vision Transformers (ViT)}

\subsection{Motivation}

Les CNN ont domin√© la vision par ordinateur depuis 2012. Cependant, les Transformers (chapitre 08) ont r√©volutionn√© le NLP. Peut-on appliquer les Transformers √† la vision ?

\textbf{D√©fis} :
\begin{itemize}
    \item Une image $224 \times 224$ a 50,176 pixels (vs ~100 tokens en NLP)
    \item Attention sur tous les pixels : complexit√© $O(n^2)$ prohibitive
\end{itemize}

\subsection{Architecture Vision Transformer (ViT, 2020)}

\begin{definition}{Vision Transformer}
ViT d√©coupe l'image en patches et les traite comme des tokens :
\begin{enumerate}
    \item Diviser l'image en patches $16 \times 16$ (ou $32 \times 32$)
    \item Aplatir chaque patch en vecteur
    \item Embedding lin√©aire + positional encoding
    \item Transformer encoder standard
    \item Classification via un token [CLS]
\end{enumerate}
\end{definition}

\subsubsection{Patchification}

Pour une image $I \in \R^{H \times W \times C}$ et taille de patch $P$ :

\begin{enumerate}
    \item Nombre de patches : $N = \frac{H \cdot W}{P^2}$
    \item Chaque patch : $x_p \in \R^{P^2 \cdot C}$ (vecteur aplati)
    \item Embedding lin√©aire : $z_p = E \cdot x_p$ o√π $E \in \R^{D \times (P^2 \cdot C)}$
\end{enumerate}

\subsubsection{Architecture Compl√®te}

\begin{algorithm}[H]
\caption{Vision Transformer (ViT)}
\label{alg:vit}
\begin{algorithmic}[1]
\REQUIRE Image $I \in \R^{H \times W \times 3}$
\ENSURE Pr√©diction de classe $y$
\STATE D√©couper $I$ en $N$ patches de taille $P \times P$
\STATE Aplatir chaque patch : $\{x_p^1, \dots, x_p^N\}$
\STATE Embedding lin√©aire : $z_p^i = E \cdot x_p^i$ pour $i=1, \dots, N$
\STATE Ajouter token [CLS] : $z_0 = z_{\text{cls}}$
\STATE Ajouter positional encoding : $z_i \leftarrow z_i + E_{pos}^i$
\STATE Passer dans $L$ couches de Transformer Encoder
\STATE Extraire $z_0^L$ (√©tat final du token [CLS])
\STATE Classification : $y = \text{MLP}(z_0^L)$
\RETURN $y$
\end{algorithmic}
\end{algorithm}

\subsubsection{Positional Encoding}

Contrairement au NLP, on utilise des \textbf{positional embeddings appris} :

\begin{equation}
E_{pos} \in \R^{(N+1) \times D}
\end{equation}

Chaque position (patch) a son embedding de position appris durant l'entra√Ænement.

\textbf{Alternative} : 2D positional encodings qui encodent s√©par√©ment les coordonn√©es $x$ et $y$ du patch.

\subsection{Variantes de ViT}

\subsubsection{ViT-Base, ViT-Large, ViT-Huge}

\begin{table}[h]
\centering
\caption{Variantes de ViT}
\label{tab:vit-variants}
\begin{tabular}{lccc}
\toprule
\textbf{Mod√®le} & \textbf{Layers} & \textbf{Hidden Size} & \textbf{Heads} \\
\midrule
ViT-Base & 12 & 768 & 12 \\
ViT-Large & 24 & 1024 & 16 \\
ViT-Huge & 32 & 1280 & 16 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{DeiT (Data-efficient image Transformers)}

Am√©liore l'entra√Ænement de ViT :
\begin{itemize}
    \item \textbf{Distillation token} : apprendre d'un CNN teacher
    \item Augmentations agressives
    \item Entra√Ænement plus efficace (moins de donn√©es)
\end{itemize}

\subsubsection{Swin Transformer (2021)}

\textbf{Id√©e cl√©} : Attention locale dans des fen√™tres d√©cal√©es (shifted windows).

\begin{enumerate}
    \item Diviser l'image en fen√™tres non-chevauchantes
    \item Appliquer self-attention uniquement dans chaque fen√™tre
    \item D√©caler les fen√™tres entre couches pour capturer interactions cross-window
    \item Hierarchical architecture (comme CNN) avec downsampling progressif
\end{enumerate}

\textbf{Avantages} :
\begin{itemize}
    \item ‚úÖ Complexit√© lin√©aire : $O(H \cdot W)$ au lieu de $O((H \cdot W)^2)$
    \item ‚úÖ Pyramidal features (multi-scale)
    \item ‚úÖ √âtat de l'art sur d√©tection et segmentation
\end{itemize}

\subsection{CNN vs ViT}

\begin{table}[h]
\centering
\caption{Comparaison CNN vs ViT}
\label{tab:cnn-vs-vit}
\begin{tabular}{lcc}
\toprule
\textbf{Propri√©t√©} & \textbf{CNN} & \textbf{ViT} \\
\midrule
Inductive bias & Fort (localit√©, translation) & Faible \\
Donn√©es requises & Moins (ImageNet) & Plus (JFT-300M) \\
Pr√©cision (ImageNet) & 88.5\% (EfficientNet) & 90.4\% (ViT-Huge) \\
Complexit√© & $O(H \cdot W)$ & $O((H \cdot W)^2)$ \\
Interpr√©tabilit√© & Filtres, feature maps & Attention maps \\
\bottomrule
\end{tabular}
\end{table}

\begin{astuce}
\textbf{Quand utiliser ViT ?}
\begin{itemize}
    \item Beaucoup de donn√©es disponibles (pr√©-entra√Ænement sur large dataset)
    \item Besoin de capturer des d√©pendances globales (pas seulement locales)
    \item Ressources GPU importantes
\end{itemize}
\textbf{Quand utiliser CNN ?}
\begin{itemize}
    \item Dataset modeste (< 100k images)
    \item Contraintes computationnelles (edge devices)
    \item T√¢ches o√π la localit√© est importante
\end{itemize}
\end{astuce}

% ===== SECTION 5: VISION-LANGAGE (CLIP) =====
\section{Mod√®les Vision-Langage : CLIP}

\subsection{Motivation}

Les mod√®les de vision classiques apprennent √† classifier des images en classes fixes. CLIP (Contrastive Language-Image Pre-training) apprend √† associer images et textes libres.

\textbf{Applications} :
\begin{itemize}
    \item Zero-shot classification : classifier sans entra√Ænement sp√©cifique
    \item Image retrieval : chercher des images par description textuelle
    \item Vision-language reasoning
\end{itemize}

\subsection{Architecture CLIP}

\begin{definition}{CLIP}
CLIP entra√Æne conjointement un encodeur d'images et un encodeur de texte pour aligner leurs repr√©sentations dans un espace latent commun.
\end{definition}

\subsubsection{Composants}

\begin{enumerate}
    \item \textbf{Image Encoder} : ViT ou ResNet $\rightarrow$ vecteur $v_I \in \R^D$
    \item \textbf{Text Encoder} : Transformer $\rightarrow$ vecteur $v_T \in \R^D$
    \item \textbf{Contrastive Learning} : maximiser similarit√© cosinus pour paires correctes
\end{enumerate}

\subsubsection{Loss Contrastive}

Pour un batch de $N$ paires (image, texte) :

\begin{equation}
L = -\frac{1}{N} \sum_{i=1}^{N} \left[ \log \frac{\exp(v_I^i \cdot v_T^i / \tau)}{\sum_{j=1}^{N} \exp(v_I^i \cdot v_T^j / \tau)} + \log \frac{\exp(v_I^i \cdot v_T^i / \tau)}{\sum_{j=1}^{N} \exp(v_I^j \cdot v_T^i / \tau)} \right]
\end{equation}

o√π $\tau$ est une temp√©rature apprise.

\textbf{Intuition} : Maximiser la similarit√© entre image $i$ et texte $i$, minimiser avec les autres.

\subsection{Zero-Shot Classification}

\begin{algorithm}[H]
\caption{CLIP Zero-Shot Classification}
\label{alg:clip-zeroshot}
\begin{algorithmic}[1]
\REQUIRE Image $I$, classes $\{c_1, \dots, c_K\}$
\ENSURE Classe pr√©dite
\STATE Encoder l'image : $v_I = \text{ImageEncoder}(I)$
\FOR{chaque classe $c_k$}
    \STATE Cr√©er prompt : $t_k = $ "A photo of a \{$c_k$\}"
    \STATE Encoder le texte : $v_T^k = \text{TextEncoder}(t_k)$
    \STATE Calculer similarit√© : $s_k = v_I \cdot v_T^k$
\ENDFOR
\STATE Softmax : $P(c_k) = \frac{\exp(s_k / \tau)}{\sum_j \exp(s_j / \tau)}$
\RETURN $\argmax_k P(c_k)$
\end{algorithmic}
\end{algorithm}

\textbf{Avantage} : Pas besoin de r√©entra√Æner pour de nouvelles classes, il suffit de changer les prompts !

\subsection{R√©sultats CLIP}

\begin{itemize}
    \item Entra√Æn√© sur 400M paires (image, texte) du web
    \item Zero-shot sur ImageNet : 76.2\% (comparable √† ResNet-50 entra√Æn√© supervis√©)
    \item Tr√®s robuste aux distribution shifts (ImageNet-A, ImageNet-R)
    \item G√©n√©ralisation impressionnante √† de nouveaux domaines
\end{itemize}

% ===== SECTION 6: IMPL√âMENTATION =====
\section{Impl√©mentation}

\subsection{D√©tection d'Objets avec YOLOv8}

\begin{lstlisting}[language=Python, caption=YOLOv8 avec Ultralytics]
from ultralytics import YOLO
import cv2

# Charger mod√®le pr√©-entra√Æn√©
model = YOLO('yolov8n.pt')  # n, s, m, l, x

# Inf√©rence sur une image
results = model('image.jpg')

# Afficher r√©sultats
for r in results:
    boxes = r.boxes  # Bounding boxes
    for box in boxes:
        x1, y1, x2, y2 = box.xyxy[0]
        conf = box.conf[0]
        cls = int(box.cls[0])
        label = model.names[cls]
        print(f"{label} ({conf:.2f}): [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]")

# Entra√Æner sur custom dataset (COCO format)
model.train(
    data='custom_dataset.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    name='yolov8_custom'
)

# √âvaluation
metrics = model.val()
print(f"mAP50: {metrics.box.map50}")
print(f"mAP50-95: {metrics.box.map}")
\end{lstlisting}

\subsection{Segmentation avec U-Net (PyTorch)}

\begin{lstlisting}[language=Python, caption=U-Net impl√©mentation]
import torch
import torch.nn as nn

class UNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=1):
        super().__init__()

        # Encoder
        self.enc1 = self.conv_block(in_channels, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)

        # Bottleneck
        self.bottleneck = self.conv_block(512, 1024)

        # Decoder
        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = self.conv_block(1024, 512)

        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = self.conv_block(512, 256)

        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = self.conv_block(256, 128)

        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = self.conv_block(128, 64)

        # Output
        self.out = nn.Conv2d(64, out_channels, 1)

        self.pool = nn.MaxPool2d(2)

    def conv_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))

        # Bottleneck
        bottleneck = self.bottleneck(self.pool(enc4))

        # Decoder with skip connections
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat([dec4, enc4], dim=1)
        dec4 = self.dec4(dec4)

        dec3 = self.upconv3(dec4)
        dec3 = torch.cat([dec3, enc3], dim=1)
        dec3 = self.dec3(dec3)

        dec2 = self.upconv2(dec3)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)

        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)

        return self.out(dec1)

# Entra√Ænement
model = UNet(in_channels=3, out_channels=1).cuda()
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

for epoch in range(epochs):
    for images, masks in train_loader:
        images, masks = images.cuda(), masks.cuda()

        outputs = model(images)
        loss = criterion(outputs, masks)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
\end{lstlisting}

\subsection{Vision Transformer avec timm}

\begin{lstlisting}[language=Python, caption=ViT avec timm]
import timm
import torch

# Charger mod√®le pr√©-entra√Æn√©
model = timm.create_model('vit_base_patch16_224', pretrained=True)

# Voir tous les mod√®les ViT disponibles
vit_models = timm.list_models('vit*')
print(f"Mod√®les ViT disponibles: {len(vit_models)}")

# Fine-tuning sur dataset custom
model = timm.create_model('vit_base_patch16_224',
                          pretrained=True,
                          num_classes=10)

# Geler l'encoder, entra√Æner seulement la t√™te
for param in model.parameters():
    param.requires_grad = False
for param in model.head.parameters():
    param.requires_grad = True

# Entra√Ænement
optimizer = torch.optim.AdamW(model.head.parameters(), lr=1e-3)
criterion = torch.nn.CrossEntropyLoss()

model.train()
for images, labels in train_loader:
    outputs = model(images)
    loss = criterion(outputs, labels)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Visualiser attention maps
attention_weights = model.blocks[-1].attn.get_attention_map()
\end{lstlisting}

\subsection{CLIP Zero-Shot}

\begin{lstlisting}[language=Python, caption=CLIP avec OpenAI]
import torch
import clip
from PIL import Image

# Charger mod√®le CLIP
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Pr√©parer image
image = preprocess(Image.open("cat.jpg")).unsqueeze(0).to(device)

# D√©finir classes possibles
text_prompts = [
    "a photo of a cat",
    "a photo of a dog",
    "a photo of a bird",
    "a photo of a car"
]
text = clip.tokenize(text_prompts).to(device)

# Inf√©rence
with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)

    # Normaliser
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)

    # Calculer similarit√©s
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
    values, indices = similarity[0].topk(4)

# R√©sultats
for value, index in zip(values, indices):
    print(f"{text_prompts[index]:20s}: {100 * value.item():.2f}%")
\end{lstlisting}

% ===== SECTION 7: AVANTAGES ET LIMITES =====
\section{Avantages et Limites}

\subsection{Object Detection}

\subsubsection{Avantages}
\begin{itemize}
    \item ‚úÖ Localisation pr√©cise des objets dans l'image
    \item ‚úÖ Performance temps r√©el avec YOLO (vid√©o, applications embarqu√©es)
    \item ‚úÖ Datasets annot√©s disponibles (COCO, Pascal VOC)
    \item ‚úÖ Transfert d'apprentissage efficace
\end{itemize}

\subsubsection{Limites}
\begin{itemize}
    \item ‚ùå Annotation co√ªteuse (bounding boxes pour chaque objet)
    \item ‚ùå Difficult√© avec objets tr√®s petits ou occult√©s
    \item ‚ùå Trade-off pr√©cision vs vitesse
    \item ‚ùå Sensible aux variations d'√©chelle
\end{itemize}

\subsection{Segmentation}

\subsubsection{Avantages}
\begin{itemize}
    \item ‚úÖ Pr√©cision au pixel pr√®s (vs boxes rectangulaires)
    \item ‚úÖ U-Net tr√®s efficace en imagerie m√©dicale
    \item ‚úÖ Mask R-CNN : d√©tection + segmentation en un mod√®le
    \item ‚úÖ Interpr√©tabilit√© : visualiser exactement les r√©gions d'int√©r√™t
\end{itemize}

\subsubsection{Limites}
\begin{itemize}
    \item ‚ùå Annotation pixel-level extr√™mement co√ªteuse
    \item ‚ùå Plus lent que la d√©tection (surtout segmentation d'instances)
    \item ‚ùå Difficult√© avec les fronti√®res ambigu√´s
    \item ‚ùå M√©moire GPU importante
\end{itemize}

\subsection{Vision Transformers}

\subsubsection{Avantages}
\begin{itemize}
    \item ‚úÖ Capture des d√©pendances globales (pas seulement locales comme CNN)
    \item ‚úÖ Meilleure pr√©cision avec beaucoup de donn√©es
    \item ‚úÖ Attention maps interpr√©tables
    \item ‚úÖ Architecture unifi√©e pour vision et NLP
\end{itemize}

\subsubsection{Limites}
\begin{itemize}
    \item ‚ùå Requiert √©norm√©ment de donn√©es (JFT-300M, ImageNet-21k)
    \item ‚ùå Complexit√© quadratique en la taille de l'image
    \item ‚ùå Moins bon que CNN avec peu de donn√©es
    \item ‚ùå Co√ªt computationnel √©lev√©
\end{itemize}

\subsection{CLIP}

\subsubsection{Avantages}
\begin{itemize}
    \item ‚úÖ Zero-shot learning : pas besoin de r√©entra√Æner pour nouvelles classes
    \item ‚úÖ Robuste aux distribution shifts
    \item ‚úÖ Multimodal : image + texte
    \item ‚úÖ Flexibilit√© : prompts textuels adaptables
\end{itemize}

\subsubsection{Limites}
\begin{itemize}
    \item ‚ùå Performances inf√©rieures au fine-tuning sur t√¢ches sp√©cifiques
    \item ‚ùå Sensible √† la formulation des prompts
    \item ‚ùå Biais des donn√©es web (400M paires non filtr√©es)
    \item ‚ùå Difficult√© avec t√¢ches n√©cessitant localisation pr√©cise
\end{itemize}

% ===== SECTION 8: HYPERPARAM√àTRES =====
\section{Hyperparam√®tres et Tuning}

\subsection{D√©tection d'Objets}

\begin{table}[h]
\centering
\caption{Hyperparam√®tres cl√©s pour la d√©tection}
\label{tab:hyperparams-detection}
\begin{tabular}{lll}
\toprule
\textbf{Param√®tre} & \textbf{Valeurs typiques} & \textbf{Impact} \\
\midrule
learning\_rate & $10^{-4}$ √† $10^{-2}$ & Convergence \\
image\_size & $416$, $640$, $1024$ & Pr√©cision vs vitesse \\
batch\_size & $8$ √† $32$ & Stabilit√© gradient \\
IoU\_threshold & $0.5$ √† $0.7$ & NMS agressivit√© \\
conf\_threshold & $0.25$ √† $0.5$ & Rappel vs pr√©cision \\
anchor\_scales & Auto ou manuel & D√©tection multi-√©chelle \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Segmentation}

\begin{table}[h]
\centering
\caption{Hyperparam√®tres cl√©s pour la segmentation}
\label{tab:hyperparams-segmentation}
\begin{tabular}{lll}
\toprule
\textbf{Param√®tre} & \textbf{Valeurs typiques} & \textbf{Impact} \\
\midrule
learning\_rate & $10^{-4}$ √† $10^{-3}$ & Convergence \\
batch\_size & $2$ √† $16$ & M√©moire GPU \\
image\_size & $256$, $512$, $1024$ & D√©tails vs vitesse \\
encoder\_depth & $4$ √† $5$ & Complexit√© mod√®le \\
dropout & $0.1$ √† $0.5$ & R√©gularisation \\
class\_weights & Auto ou manuel & D√©s√©quilibre classes \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Vision Transformers}

\begin{table}[h]
\centering
\caption{Hyperparam√®tres cl√©s pour ViT}
\label{tab:hyperparams-vit}
\begin{tabular}{lll}
\toprule
\textbf{Param√®tre} & \textbf{Valeurs typiques} & \textbf{Impact} \\
\midrule
patch\_size & $16$, $32$ & Nb tokens vs d√©tails \\
num\_layers & $12$ √† $32$ & Capacit√© mod√®le \\
hidden\_dim & $768$ √† $1280$ & Capacit√© repr√©sentation \\
num\_heads & $12$ √† $16$ & Attention multi-√©chelle \\
learning\_rate & $10^{-4}$ √† $10^{-3}$ & Convergence \\
warmup\_steps & $500$ √† $10000$ & Stabilit√© initiale \\
\bottomrule
\end{tabular}
\end{table}

\begin{astuce}
\textbf{Strat√©gies d'entra√Ænement efficaces} :
\begin{itemize}
    \item \textbf{Transfer learning} : Toujours partir d'un mod√®le pr√©-entra√Æn√© (ImageNet, COCO)
    \item \textbf{Progressive resizing} : Commencer avec petites images, augmenter progressivement
    \item \textbf{Mixed precision} : FP16 pour acc√©l√©rer et √©conomiser m√©moire
    \item \textbf{Data augmentation} : Mosaic, MixUp, CutMix pour la d√©tection
    \item \textbf{Learning rate schedule} : Cosine annealing ou OneCycleLR
\end{itemize}
\end{astuce}

% ===== SECTION 9: APPLICATIONS =====
\section{Applications Pratiques}

\subsection{Conduite Autonome}

\begin{itemize}
    \item \textbf{D√©tection} : Voitures, pi√©tons, v√©los, panneaux de signalisation
    \item \textbf{Segmentation s√©mantique} : Route, trottoir, v√©g√©tation, b√¢timents
    \item \textbf{Segmentation d'instances} : Suivi de chaque voiture/pi√©ton individuellement
    \item \textbf{Mod√®les} : YOLOv8, Mask R-CNN, DeepLab v3+
    \item \textbf{Datasets} : KITTI, Cityscapes, nuScenes
\end{itemize}

\subsection{Imagerie M√©dicale}

\begin{itemize}
    \item \textbf{Segmentation d'organes} : U-Net pour foie, reins, cerveau
    \item \textbf{D√©tection de tumeurs} : Faster R-CNN, RetinaNet
    \item \textbf{Segmentation cellulaire} : U-Net, Mask R-CNN
    \item \textbf{Mod√®les} : U-Net (r√©f√©rence), nnU-Net (auto-configuration)
    \item \textbf{Datasets} : Medical Segmentation Decathlon, LIDC-IDRI
\end{itemize}

\subsection{Reconnaissance Faciale}

\begin{itemize}
    \item \textbf{D√©tection de visages} : MTCNN, RetinaFace
    \item \textbf{Landmarks faciaux} : 68 points de rep√®re (yeux, nez, bouche)
    \item \textbf{Segmentation} : Cheveux, peau, arri√®re-plan
    \item \textbf{Applications} : S√©curit√©, filtres AR, analyse d'√©motions
\end{itemize}

\subsection{Commerce √âlectronique}

\begin{itemize}
    \item \textbf{Recherche visuelle} : CLIP pour "trouver des produits similaires"
    \item \textbf{D√©tection de produits} : Compter articles en rayon (retail)
    \item \textbf{Segmentation produits} : Extraction de produit pour montage
    \item \textbf{Mod√®les} : CLIP, YOLOv8, Mask R-CNN
\end{itemize}

\subsection{Surveillance et S√©curit√©}

\begin{itemize}
    \item \textbf{D√©tection d'intrusion} : YOLO temps r√©el
    \item \textbf{Suivi multi-objets} : DeepSORT avec d√©tecteur
    \item \textbf{D√©tection d'anomalies} : Comportements inhabituels
    \item \textbf{Comptage de personnes} : Segmentation + tracking
\end{itemize}

% ===== SECTION 10: R√âSUM√â =====
\section{R√©sum√© du Chapitre}

\subsection{Points Cl√©s}

\begin{itemize}
    \item \textbf{Object Detection} : Localiser et classifier des objets
    \begin{itemize}
        \item Two-stage : Faster R-CNN (pr√©cis, ~5 FPS)
        \item One-stage : YOLO (rapide, ~80 FPS)
        \item M√©triques : IoU, mAP@0.5, mAP@0.5:0.95
    \end{itemize}

    \item \textbf{Segmentation S√©mantique} : Classifier chaque pixel
    \begin{itemize}
        \item FCN : premi√®re architecture fully convolutional
        \item U-Net : architecture embl√©matique (m√©dical)
        \item DeepLab : atrous convolutions + ASPP
        \item M√©triques : mIoU, Dice coefficient
    \end{itemize}

    \item \textbf{Segmentation d'Instances} : D√©tecter et segmenter chaque instance
    \begin{itemize}
        \item Mask R-CNN = Faster R-CNN + branche de masque
        \item RoI Align pour pr√©cision pixel-level
    \end{itemize}

    \item \textbf{Vision Transformers} : Appliquer les Transformers √† la vision
    \begin{itemize}
        \item ViT : d√©couper en patches, self-attention
        \item Swin Transformer : attention locale + shifted windows
        \item N√©cessite beaucoup de donn√©es
    \end{itemize}

    \item \textbf{Vision-Langage (CLIP)} : Aligner images et textes
    \begin{itemize}
        \item Contrastive learning sur 400M paires
        \item Zero-shot classification par prompts
        \item Applications : recherche, multimodal
    \end{itemize}
\end{itemize}

\subsection{Formules Essentielles}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Formules √† retenir]

\textbf{IoU (Intersection over Union)} :
\begin{equation*}
\text{IoU} = \frac{\text{Aire}(A \cap B)}{\text{Aire}(A \cup B)}
\end{equation*}

\textbf{Mean Average Precision} :
\begin{equation*}
\text{mAP} = \frac{1}{C} \sum_{c=1}^{C} AP_c
\end{equation*}

\textbf{Dice Coefficient} :
\begin{equation*}
\text{Dice} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
\end{equation*}

\textbf{ViT Patch Embedding} :
\begin{equation*}
z_p = E \cdot x_p + E_{pos}, \quad x_p \in \R^{P^2 \cdot C}
\end{equation*}

\textbf{CLIP Contrastive Loss} :
\begin{equation*}
L = -\log \frac{\exp(v_I \cdot v_T / \tau)}{\sum_j \exp(v_I \cdot v_T^j / \tau)}
\end{equation*}

\end{tcolorbox}

\subsection{Tableau R√©capitulatif}

\begin{table}[h]
\centering
\caption{Comparaison des approches de vision avanc√©e}
\label{tab:recap}
\begin{tabular}{lccc}
\toprule
\textbf{T√¢che} & \textbf{Mod√®le Recommand√©} & \textbf{Pr√©cision} & \textbf{Vitesse} \\
\midrule
D√©tection (pr√©cision) & Faster R-CNN & ‚òÖ‚òÖ‚òÖ & ‚òÖ‚òÜ‚òÜ \\
D√©tection (temps r√©el) & YOLOv8 & ‚òÖ‚òÖ‚òÜ & ‚òÖ‚òÖ‚òÖ \\
Seg. s√©mantique & DeepLab v3+ & ‚òÖ‚òÖ‚òÖ & ‚òÖ‚òÖ‚òÜ \\
Seg. m√©dicale & U-Net & ‚òÖ‚òÖ‚òÖ & ‚òÖ‚òÖ‚òÜ \\
Seg. instances & Mask R-CNN & ‚òÖ‚òÖ‚òÖ & ‚òÖ‚òÜ‚òÜ \\
Classification & ViT-Large & ‚òÖ‚òÖ‚òÖ & ‚òÖ‚òÜ‚òÜ \\
Zero-shot & CLIP & ‚òÖ‚òÖ‚òÜ & ‚òÖ‚òÖ‚òÖ \\
\bottomrule
\end{tabular}
\end{table}

% ===== SECTION 11: EXERCICES =====
\section{Exercices}

\subsection{Questions de Compr√©hension}

\begin{enumerate}
    \item Expliquer la diff√©rence entre R-CNN, Fast R-CNN et Faster R-CNN. Pourquoi chaque version est-elle plus rapide que la pr√©c√©dente ?

    \item Pourquoi YOLO est-il appel√© "You Only Look Once" ? Quelle est sa principale diff√©rence philosophique avec Faster R-CNN ?

    \item Qu'est-ce que le RoI Pooling ? Pourquoi RoI Align est-il meilleur pour la segmentation ?

    \item Expliquer le r√¥le des skip connections dans U-Net. Pourquoi utiliser la concat√©nation plut√¥t que l'addition ?

    \item Comment fonctionne l'atrous convolution dans DeepLab ? Quel est son avantage ?

    \item Pourquoi ViT n√©cessite-t-il beaucoup plus de donn√©es d'entra√Ænement que les CNN ?

    \item Comment CLIP permet-il la classification zero-shot ? Donner un exemple d'application.

    \item Quelle est la diff√©rence entre segmentation s√©mantique et segmentation d'instances ?
\end{enumerate}

\subsection{Exercices Pratiques}

\begin{enumerate}
    \item \textbf{D√©tection avec YOLOv8}
    \begin{itemize}
        \item Entra√Æner YOLOv8 sur un dataset custom (par ex. d√©tecter des visages)
        \item Tester sur vid√©o et mesurer le FPS
        \item Comparer YOLOv8n (nano) vs YOLOv8x (extra-large) : pr√©cision vs vitesse
        \item Notebook : \texttt{12\_demo\_object\_detection.ipynb}
    \end{itemize}

    \item \textbf{Segmentation m√©dicale avec U-Net}
    \begin{itemize}
        \item Impl√©menter U-Net from scratch en PyTorch
        \item Entra√Æner sur un dataset de segmentation (par ex. cellules, tumeurs)
        \item Calculer Dice coefficient et mIoU
        \item Visualiser les pr√©dictions
        \item Notebook : \texttt{12\_demo\_segmentation.ipynb}
    \end{itemize}

    \item \textbf{Vision Transformers}
    \begin{itemize}
        \item Fine-tuner ViT-Base sur CIFAR-10
        \item Comparer avec ResNet-50
        \item Visualiser les attention maps
        \item Tester DeiT avec distillation
        \item Notebook : \texttt{12\_demo\_vision\_transformers.ipynb}
    \end{itemize}

    \item \textbf{CLIP Zero-Shot}
    \begin{itemize}
        \item Utiliser CLIP pour classifier des images sans fine-tuning
        \item Tester diff√©rents prompts et mesurer l'impact
        \item Impl√©menter un moteur de recherche d'images par description textuelle
    \end{itemize}
\end{enumerate}

% ===== SECTION 12: POUR ALLER PLUS LOIN =====
\section{Pour Aller Plus Loin}

\subsection{Lectures Recommand√©es}

\subsubsection{Papers Fondateurs}

\begin{itemize}
    \item \textbf{R-CNN} : Girshick et al. (2014). "Rich feature hierarchies for accurate object detection and semantic segmentation"
    \item \textbf{Faster R-CNN} : Ren et al. (2015). "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
    \item \textbf{YOLO} : Redmon et al. (2016). "You Only Look Once: Unified, Real-Time Object Detection"
    \item \textbf{U-Net} : Ronneberger et al. (2015). "U-Net: Convolutional Networks for Biomedical Image Segmentation"
    \item \textbf{Mask R-CNN} : He et al. (2017). "Mask R-CNN"
    \item \textbf{ViT} : Dosovitskiy et al. (2020). "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    \item \textbf{CLIP} : Radford et al. (2021). "Learning Transferable Visual Models From Natural Language Supervision"
\end{itemize}

\subsubsection{Papers R√©cents}

\begin{itemize}
    \item \textbf{YOLOv8} : Ultralytics (2023). Documentation officielle
    \item \textbf{Swin Transformer} : Liu et al. (2021). "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
    \item \textbf{Segment Anything (SAM)} : Kirillov et al. (2023). "Segment Anything"
    \item \textbf{DINOv2} : Oquab et al. (2023). "DINOv2: Learning Robust Visual Features without Supervision"
\end{itemize}

\subsection{Ressources en Ligne}

\begin{itemize}
    \item \textbf{Ultralytics YOLOv8} : \url{https://docs.ultralytics.com/}
    \item \textbf{Detectron2} (Facebook) : \url{https://github.com/facebookresearch/detectron2}
    \item \textbf{MMDetection} : \url{https://github.com/open-mmlab/mmdetection}
    \item \textbf{Timm (PyTorch Image Models)} : \url{https://github.com/huggingface/pytorch-image-models}
    \item \textbf{OpenAI CLIP} : \url{https://github.com/openai/CLIP}
    \item \textbf{Hugging Face Transformers} : \url{https://huggingface.co/docs/transformers/}
\end{itemize}

\subsection{Datasets}

\begin{itemize}
    \item \textbf{COCO} (detection, segmentation) : \url{https://cocodataset.org/}
    \item \textbf{Pascal VOC} : \url{http://host.robots.ox.ac.uk/pascal/VOC/}
    \item \textbf{Cityscapes} (conduite) : \url{https://www.cityscapes-dataset.com/}
    \item \textbf{Medical Segmentation Decathlon} : \url{http://medicaldecathlon.com/}
    \item \textbf{Open Images} : \url{https://storage.googleapis.com/openimages/web/index.html}
\end{itemize}

\subsection{Outils et Biblioth√®ques}

\begin{itemize}
    \item \textbf{Ultralytics} : YOLO v5/v8 (PyTorch)
    \item \textbf{torchvision} : Mod√®les pr√©-entra√Æn√©s (Faster R-CNN, Mask R-CNN)
    \item \textbf{segmentation\_models.pytorch} : U-Net, DeepLab, etc.
    \item \textbf{timm} : Vision Transformers
    \item \textbf{transformers} : CLIP, ViT (Hugging Face)
    \item \textbf{albumentations} : Augmentations avanc√©es
\end{itemize}

\subsection{Prochaines √âtapes}

\begin{itemize}
    \item \textbf{Chapitre 14 - GANs et Mod√®les G√©n√©ratifs} : StyleGAN, Diffusion Models
    \item \textbf{Chapitre 15 - AutoML et NAS} : Architecture search automatique
    \item \textbf{Projets pratiques} : Appliquer ces techniques √† vos propres donn√©es
\end{itemize}

% ===== BIBLIOGRAPHIE =====
\section*{R√©f√©rences}

\begin{enumerate}
    \item Girshick, R., Donahue, J., Darrell, T., \& Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. \textit{CVPR}.

    \item Ren, S., He, K., Girshick, R., \& Sun, J. (2015). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. \textit{NeurIPS}.

    \item Redmon, J., Divvala, S., Girshick, R., \& Farhadi, A. (2016). You Only Look Once: Unified, Real-Time Object Detection. \textit{CVPR}.

    \item Ronneberger, O., Fischer, P., \& Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. \textit{MICCAI}.

    \item He, K., Gkioxari, G., Doll√°r, P., \& Girshick, R. (2017). Mask R-CNN. \textit{ICCV}.

    \item Chen, L. C., Papandreou, G., Schroff, F., \& Adam, H. (2017). Rethinking Atrous Convolution for Semantic Image Segmentation. \textit{arXiv}.

    \item Dosovitskiy, A., et al. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. \textit{ICLR 2021}.

    \item Liu, Z., et al. (2021). Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. \textit{ICCV}.

    \item Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. \textit{ICML}.

    \item Kirillov, A., et al. (2023). Segment Anything. \textit{ICCV}.
\end{enumerate}

\end{document}
