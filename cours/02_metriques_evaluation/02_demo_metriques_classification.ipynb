{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/02_metriques_evaluation/02_demo_metriques_classification.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '02_demo_metriques_classification.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 02 - M√©triques de Classification\n",
    "\n",
    "**Objectifs :**\n",
    "- Calculer manuellement les m√©triques de classification (Precision, Recall, F1)\n",
    "- Utiliser scikit-learn pour les m√©triques\n",
    "- Visualiser la matrice de confusion\n",
    "- Tracer et interpr√©ter les courbes ROC et Precision-Recall\n",
    "- Comprendre l'impact du seuil de classification\n",
    "\n",
    "**Dataset :** Breast Cancer Wisconsin (classification binaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.data  # type: ignoresets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    classification_report,\n    roc_curve,\n    roc_auc_score,\n    precision_recall_curve,\n    average_precision_score,\n    ConfusionMatrixDisplay\n)\n\n# Configuration des graphiques\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Exploration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Chargement du dataset Breast Cancer\ndata = load_breast_cancer()\nX = data.data  # type: ignore\ny = data.target  # type: ignore  # 0 = malignant (cancer), 1 = benign (b√©nin)\n\nprint(\"Dataset Breast Cancer Wisconsin\")\nprint(f\"Nombre d'exemples : {X.shape[0]}\")\nprint(f\"Nombre de features : {X.shape[1]}\")\nprint(f\"\\nClasses : {data.target  # type: ignore_names}\")  # type: ignore\nprint(f\"Distribution des classes :\")\nprint(f\"  - Malignant (0) : {(y == 0).sum()} ({(y == 0).sum() / len(y) * 100:.1f}%)\")\nprint(f\"  - Benign (1) : {(y == 1).sum()} ({(y == 1).sum() / len(y) * 100:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution des classes\n",
    "plt.figure(figsize=(8, 5))\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "plt.bar(['Malignant', 'Benign'], counts, color=['#FF6B6B', '#4ECDC4'])\n",
    "plt.ylabel('Nombre d\\'exemples')\n",
    "plt.title('Distribution des Classes - Breast Cancer Dataset')\n",
    "for i, (label, count) in enumerate(zip(['Malignant', 'Benign'], counts)):\n",
    "    plt.text(i, count + 10, f'{count}\\n({count/len(y)*100:.1f}%)', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Constat : Les classes sont relativement √©quilibr√©es (63% / 37%)\")\n",
    "print(\"L'accuracy sera donc une m√©trique raisonnable (mais pas suffisante).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entra√Ænement d'un Mod√®le Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Pr√©serve la distribution des classes\n",
    ")\n",
    "\n",
    "print(f\"Train set : {X_train.shape[0]} exemples\")\n",
    "print(f\"Test set : {X_test.shape[0]} exemples\")\n",
    "print(f\"\\nDistribution train : {np.bincount(y_train)}\")\n",
    "print(f\"Distribution test : {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement d'un mod√®le Logistic Regression\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilit√©s pour la classe 1 (benign)\n",
    "\n",
    "print(\"Mod√®le entra√Æn√© avec succ√®s !\")\n",
    "print(f\"Nombre de pr√©dictions : {len(y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Matrice de Confusion\n",
    "\n",
    "La matrice de confusion est la base de toutes les m√©triques de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Matrice de Confusion :\")\n",
    "print(cm)\n",
    "print(\"\\nInterpr√©tation :\")\n",
    "print(f\"  TN (True Negative) = {cm[0, 0]} : Malignant correctement classifi√©s\")\n",
    "print(f\"  FP (False Positive) = {cm[0, 1]} : Malignant class√©s comme Benign (dangereux !)\")\n",
    "print(f\"  FN (False Negative) = {cm[1, 0]} : Benign class√©s comme Malignant\")\n",
    "print(f\"  TP (True Positive) = {cm[1, 1]} : Benign correctement classifi√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la matrice de confusion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Matrice avec valeurs absolues\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Malignant', 'Benign'])\n",
    "disp1.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title('Matrice de Confusion (valeurs absolues)', fontsize=12)\n",
    "\n",
    "# Matrice normalis√©e (pourcentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=['Malignant', 'Benign'])\n",
    "disp2.plot(ax=axes[1], cmap='Greens', values_format='.2%')\n",
    "axes[1].set_title('Matrice de Confusion (normalis√©e)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnalyse :\")\n",
    "print(f\"  - Le mod√®le a correctement classifi√© {cm[0,0] + cm[1,1]} / {cm.sum()} exemples\")\n",
    "print(f\"  - {cm[0, 1]} cas malignant ont √©t√© class√©s comme benign (DANGEREUX en m√©decine !)\")\n",
    "print(f\"  - {cm[1, 0]} cas benign ont √©t√© class√©s comme malignant (faux positif, stress inutile)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calcul Manuel des M√©triques\n",
    "\n",
    "Avant d'utiliser scikit-learn, calculons les m√©triques √† la main pour bien comprendre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction de TP, TN, FP, FN\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "TP = cm[1, 1]\n",
    "\n",
    "# Calcul manuel des m√©triques\n",
    "accuracy_manual = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision_manual = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall_manual = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual) if (precision_manual + recall_manual) > 0 else 0\n",
    "specificity_manual = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"M√âTRIQUES CALCUL√âES MANUELLEMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy    = (TP + TN) / Total = ({TP} + {TN}) / {TP+TN+FP+FN} = {accuracy_manual:.4f}\")\n",
    "print(f\"Precision   = TP / (TP + FP) = {TP} / ({TP} + {FP}) = {precision_manual:.4f}\")\n",
    "print(f\"Recall      = TP / (TP + FN) = {TP} / ({TP} + {FN}) = {recall_manual:.4f}\")\n",
    "print(f\"F1-Score    = 2 * (P * R) / (P + R) = {f1_manual:.4f}\")\n",
    "print(f\"Specificity = TN / (TN + FP) = {TN} / ({TN} + {FP}) = {specificity_manual:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification avec scikit-learn\n",
    "print(\"=\" * 60)\n",
    "print(\"V√âRIFICATION AVEC SCIKIT-LEARN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy    (sklearn) = {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision   (sklearn) = {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall      (sklearn) = {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score    (sklearn) = {f1_score(y_test, y_pred):.4f}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ Les calculs manuels correspondent parfaitement √† scikit-learn !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rapport de Classification Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport complet avec classification_report\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RAPPORT DE CLASSIFICATION COMPLET\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))\n",
    "\n",
    "print(\"\\nInterpr√©tation :\")\n",
    "print(\"  - Support : nombre d'exemples r√©els pour chaque classe\")\n",
    "print(\"  - Macro avg : moyenne simple des m√©triques (classes √©quivalentes)\")\n",
    "print(\"  - Weighted avg : moyenne pond√©r√©e par le nombre d'exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Courbe ROC et AUC\n",
    "\n",
    "La courbe ROC montre le compromis entre TPR (Recall) et FPR pour diff√©rents seuils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Courbe ROC\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Ligne de r√©f√©rence (classifieur al√©atoire)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Classifieur al√©atoire (AUC = 0.5)')\n",
    "\n",
    "# Point correspondant au seuil par d√©faut (0.5)\n",
    "default_threshold_idx = np.argmin(np.abs(thresholds - 0.5))\n",
    "plt.scatter(fpr[default_threshold_idx], tpr[default_threshold_idx], \n",
    "            color='red', s=100, zorder=5, label=f'Seuil 0.5 (FPR={fpr[default_threshold_idx]:.3f}, TPR={tpr[default_threshold_idx]:.3f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR) = 1 - Specificity', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (TPR) = Recall', fontsize=12)\n",
    "plt.title('Courbe ROC - Breast Cancer Classification', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAUC-ROC = {roc_auc:.4f}\")\n",
    "print(\"\\nInterpr√©tation de l'AUC :\")\n",
    "print(\"  - AUC = 0.50 : Classifieur al√©atoire (inutile)\")\n",
    "print(\"  - AUC = 0.70 : Acceptable\")\n",
    "print(\"  - AUC = 0.80 : Bon\")\n",
    "print(\"  - AUC = 0.90 : Excellent\")\n",
    "print(\"  - AUC = 1.00 : Parfait (rare, souvent signe d'overfitting)\")\n",
    "print(f\"\\n‚û°Ô∏è Notre mod√®le : AUC = {roc_auc:.3f} (Excellent !)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Courbe Precision-Recall\n",
    "\n",
    "Plus informative que ROC pour les classes d√©s√©quilibr√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la courbe Precision-Recall\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Courbe PR\n",
    "plt.plot(recall_curve, precision_curve, color='blue', lw=2, \n",
    "         label=f'PR curve (AP = {avg_precision:.3f})')\n",
    "\n",
    "# Ligne de base (proportion de positifs)\n",
    "baseline = (y_test == 1).sum() / len(y_test)\n",
    "plt.plot([0, 1], [baseline, baseline], linestyle='--', color='red', lw=2,\n",
    "         label=f'Baseline (proportion positifs = {baseline:.3f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall (TPR)', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Courbe Precision-Recall - Breast Cancer Classification', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower left\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage Precision (AP) = {avg_precision:.4f}\")\n",
    "print(\"\\nL'Average Precision est l'aire sous la courbe PR.\")\n",
    "print(\"Plus elle est proche de 1, meilleur est le mod√®le.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Impact du Seuil de Classification\n",
    "\n",
    "Le seuil par d√©faut est 0.5, mais on peut l'ajuster selon le contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester diff√©rents seuils\n",
    "thresholds_to_test = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "results = []\n",
    "for threshold in thresholds_to_test:\n",
    "    y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred_threshold)\n",
    "    prec = precision_score(y_test, y_pred_threshold, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_threshold, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_threshold, zero_division=0)\n",
    "    \n",
    "    results.append({\n",
    "        'Threshold': threshold,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMPACT DU SEUIL DE CLASSIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"\\nObservations :\")\n",
    "print(\"  - Seuil bas (0.3) : Recall √©lev√© (peu de FN) mais Precision faible (beaucoup de FP)\")\n",
    "print(\"  - Seuil haut (0.9) : Precision √©lev√©e (peu de FP) mais Recall faible (beaucoup de FN)\")\n",
    "print(\"  - Seuil m√©dian (0.5) : Compromis √©quilibr√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'impact du seuil\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    ax.plot(df_results['Threshold'], df_results[metric], marker='o', \n",
    "            linewidth=2, markersize=8, color=color, label=metric)\n",
    "    ax.set_xlabel('Seuil de classification', fontsize=11)\n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_title(f'Impact du seuil sur {metric}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    \n",
    "    # Marquer le seuil par d√©faut (0.5)\n",
    "    default_value = df_results[df_results['Threshold'] == 0.5][metric].values[0]\n",
    "    ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Seuil par d√©faut')\n",
    "    ax.scatter([0.5], [default_value], color='red', s=100, zorder=5)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConclusion :\")\n",
    "print(\"Le choix du seuil d√©pend du contexte m√©tier :\")\n",
    "print(\"  - D√©tection de cancer : Privil√©gier Recall (ne pas manquer de malades) ‚Üí seuil bas\")\n",
    "print(\"  - Filtrage spam : Privil√©gier Precision (ne pas perdre d'emails l√©gitimes) ‚Üí seuil haut\")\n",
    "print(\"  - Usage g√©n√©ral : F1-Score maximal ‚Üí seuil optimal autour de 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparaison ROC vs Precision-Recall\n",
    "\n",
    "Affichons les deux courbes c√¥te √† c√¥te pour mieux comprendre leurs diff√©rences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Courbe ROC\n",
    "axes[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Al√©atoire')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "axes[0].set_title('Courbe ROC', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Courbe PR\n",
    "axes[1].plot(recall_curve, precision_curve, color='blue', lw=2, label=f'PR (AP = {avg_precision:.3f})')\n",
    "axes[1].axhline(y=baseline, linestyle='--', color='red', lw=2, label=f'Baseline ({baseline:.3f})')\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Courbe Precision-Recall', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc=\"lower left\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQuand utiliser ROC vs PR ?\")\n",
    "print(\"\\nüìä Courbe ROC :\")\n",
    "print(\"  ‚úÖ Classes relativement √©quilibr√©es\")\n",
    "print(\"  ‚úÖ Les vrais n√©gatifs (TN) sont importants\")\n",
    "print(\"  ‚úÖ Vue d'ensemble de la performance\")\n",
    "print(\"\\nüìä Courbe Precision-Recall :\")\n",
    "print(\"  ‚úÖ Classes tr√®s d√©s√©quilibr√©es (ex: 1% de positifs)\")\n",
    "print(\"  ‚úÖ On se concentre sur les positifs (ignore les TN)\")\n",
    "print(\"  ‚úÖ Plus sensible aux changements pour la classe minoritaire\")\n",
    "print(\"  ‚úÖ Exemples : d√©tection de fraude, anomalies, maladies rares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. R√©sum√© et Conclusions\n",
    "\n",
    "### M√©triques calcul√©es pour notre mod√®le :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"R√âSUM√â DES M√âTRIQUES - BREAST CANCER CLASSIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüéØ Accuracy       : {accuracy_score(y_test, y_pred):.4f} ({accuracy_score(y_test, y_pred)*100:.2f}%)\")\n",
    "print(f\"üéØ Precision      : {precision_score(y_test, y_pred):.4f} ({precision_score(y_test, y_pred)*100:.2f}%)\")\n",
    "print(f\"üéØ Recall         : {recall_score(y_test, y_pred):.4f} ({recall_score(y_test, y_pred)*100:.2f}%)\")\n",
    "print(f\"üéØ F1-Score       : {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"üéØ AUC-ROC        : {roc_auc:.4f}\")\n",
    "print(f\"üéØ Avg Precision  : {avg_precision:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Points cl√©s √† retenir :\")\n",
    "print(\"\\n1. La matrice de confusion est la base de toutes les m√©triques\")\n",
    "print(\"   TP, TN, FP, FN permettent de calculer toutes les autres m√©triques.\")\n",
    "print(\"\\n2. L'accuracy peut √™tre trompeuse avec des classes d√©s√©quilibr√©es\")\n",
    "print(\"   Toujours regarder Precision, Recall, F1 en compl√©ment.\")\n",
    "print(\"\\n3. Le choix de la m√©trique d√©pend du contexte m√©tier\")\n",
    "print(\"   - M√©decine : privil√©gier Recall (ne pas manquer de malades)\")\n",
    "print(\"   - Spam : privil√©gier Precision (ne pas perdre d'emails importants)\")\n",
    "print(\"   - G√©n√©ral : F1-Score (compromis √©quilibr√©)\")\n",
    "print(\"\\n4. AUC-ROC mesure la performance ind√©pendamment du seuil\")\n",
    "print(\"   Utile pour comparer des mod√®les sans choisir de seuil.\")\n",
    "print(\"\\n5. Courbe PR > ROC pour les classes d√©s√©quilibr√©es\")\n",
    "print(\"   Elle ignore les TN et se concentre sur les positifs.\")\n",
    "print(\"\\n6. Le seuil de classification (par d√©faut 0.5) peut √™tre ajust√©\")\n",
    "print(\"   Selon que l'on privil√©gie Precision ou Recall.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "1. Modifiez le seuil de classification pour maximiser le F1-score. Quel seuil obtenez-vous ?\n",
    "\n",
    "2. Cr√©ez un dataset d√©s√©quilibr√© (95% / 5%) et comparez les courbes ROC et PR.\n",
    "\n",
    "3. Impl√©mentez vous-m√™me les fonctions `precision_score()`, `recall_score()` et `f1_score()` en NumPy pur.\n",
    "\n",
    "4. Tracez la courbe du F1-score en fonction du seuil et trouvez le seuil optimal.\n",
    "\n",
    "5. Calculez la matrice de confusion pour un probl√®me multi-classes (ex: Iris dataset)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}