{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/02_metriques_evaluation/02_demo_metriques_regression.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '02_demo_metriques_regression.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 02 - M√©triques de R√©gression\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre et calculer les m√©triques de r√©gression (MSE, RMSE, MAE, R¬≤, MAPE)\n",
    "- Visualiser les erreurs de pr√©diction\n",
    "- Comparer diff√©rents mod√®les avec les bonnes m√©triques\n",
    "- Identifier l'impact des outliers sur les m√©triques\n",
    "- Savoir choisir la bonne m√©trique selon le contexte\n",
    "\n",
    "**Dataset :** Diabetes Dataset (r√©gression continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Seed pour reproductibilit√©\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Exploration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset Diabetes\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target  # Mesure quantitative de progression du diab√®te apr√®s 1 an\n",
    "\n",
    "print(\"Dataset Diabetes\")\n",
    "print(f\"Nombre d'exemples : {X.shape[0]}\")\n",
    "print(f\"Nombre de features : {X.shape[1]}\")\n",
    "print(f\"\\nFeatures : {data.feature_names}\")\n",
    "print(f\"\\nCible (y) : Progression du diab√®te\")\n",
    "print(f\"  - Min : {y.min():.2f}\")\n",
    "print(f\"  - Max : {y.max():.2f}\")\n",
    "print(f\"  - Moyenne : {y.mean():.2f}\")\n",
    "print(f\"  - √âcart-type : {y.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution de la cible\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogramme\n",
    "axes[0].hist(y, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(y.mean(), color='red', linestyle='--', linewidth=2, label=f'Moyenne = {y.mean():.2f}')\n",
    "axes[0].set_xlabel('Progression du diab√®te', fontsize=12)\n",
    "axes[0].set_ylabel('Fr√©quence', fontsize=12)\n",
    "axes[0].set_title('Distribution de la Variable Cible', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot(y, vert=True, patch_artist=True, \n",
    "                boxprops=dict(facecolor='lightgreen', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[1].set_ylabel('Progression du diab√®te', fontsize=12)\n",
    "axes[1].set_title('Boxplot de la Variable Cible', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation : La distribution est relativement normale, sans outliers extr√™mes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entra√Ænement de Mod√®les de R√©gression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set : {X_train.shape[0]} exemples\")\n",
    "print(f\"Test set : {X_test.shape[0]} exemples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement de plusieurs mod√®les\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (alpha=1)': Ridge(alpha=1.0),\n",
    "    'Lasso (alpha=1)': Lasso(alpha=1.0)\n",
    "}\n",
    "\n",
    "# Dictionnaire pour stocker les pr√©dictions\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions[name] = y_pred\n",
    "    print(f\"‚úÖ {name} entra√Æn√©\")\n",
    "\n",
    "print(\"\\nTous les mod√®les sont entra√Æn√©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calcul Manuel des M√©triques\n",
    "\n",
    "Calculons les m√©triques √† la main pour bien comprendre les formules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisons les pr√©dictions du mod√®le Linear Regression\n",
    "y_pred = predictions['Linear Regression']\n",
    "\n",
    "# Calcul manuel des m√©triques\n",
    "n = len(y_test)\n",
    "\n",
    "# Erreurs\n",
    "errors = y_test - y_pred\n",
    "\n",
    "# MSE : Mean Squared Error\n",
    "mse_manual = np.sum(errors ** 2) / n\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse_manual = np.sqrt(mse_manual)\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae_manual = np.sum(np.abs(errors)) / n\n",
    "\n",
    "# R¬≤ : Coefficient de D√©termination\n",
    "ss_res = np.sum(errors ** 2)  # Somme des carr√©s des r√©sidus\n",
    "ss_tot = np.sum((y_test - y_test.mean()) ** 2)  # Variance totale\n",
    "r2_manual = 1 - (ss_res / ss_tot)\n",
    "\n",
    "# MAPE : Mean Absolute Percentage Error (en %)\n",
    "mape_manual = np.mean(np.abs(errors / y_test)) * 100\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"M√âTRIQUES CALCUL√âES MANUELLEMENT (Linear Regression)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nMSE  = (1/n) * Œ£(y - ≈∑)¬≤\")\n",
    "print(f\"     = (1/{n}) * {ss_res:.2f}\")\n",
    "print(f\"     = {mse_manual:.4f}\")\n",
    "print(f\"\\nRMSE = ‚àö(MSE)\")\n",
    "print(f\"     = ‚àö({mse_manual:.4f})\")\n",
    "print(f\"     = {rmse_manual:.4f}\")\n",
    "print(f\"\\nMAE  = (1/n) * Œ£|y - ≈∑|\")\n",
    "print(f\"     = {mae_manual:.4f}\")\n",
    "print(f\"\\nR¬≤   = 1 - (SS_res / SS_tot)\")\n",
    "print(f\"     = 1 - ({ss_res:.2f} / {ss_tot:.2f})\")\n",
    "print(f\"     = {r2_manual:.4f}\")\n",
    "print(f\"\\nMAPE = (100/n) * Œ£|y - ≈∑| / y\")\n",
    "print(f\"     = {mape_manual:.4f} %\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification avec scikit-learn\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"V√âRIFICATION AVEC SCIKIT-LEARN\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"MSE  (sklearn) = {mean_squared_error(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE (sklearn) = {mean_squared_error(y_test, y_pred, squared=False):.4f}\")\n",
    "print(f\"MAE  (sklearn) = {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "print(f\"R¬≤   (sklearn) = {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"MAPE (sklearn) = {mean_absolute_percentage_error(y_test, y_pred) * 100:.4f} %\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úÖ Les calculs manuels correspondent parfaitement √† scikit-learn !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpr√©tation des M√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPR√âTATION DES M√âTRIQUES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä MSE = {mse_manual:.2f}\")\n",
    "print(\"   ‚Üí Unit√© : (progression)¬≤ (difficile √† interpr√©ter)\")\n",
    "print(\"   ‚Üí P√©nalise fortement les grandes erreurs (√† cause du carr√©)\")\n",
    "print(\"   ‚Üí Sensible aux outliers\")\n",
    "\n",
    "print(f\"\\nüìä RMSE = {rmse_manual:.2f}\")\n",
    "print(\"   ‚Üí Unit√© : progression (m√™me unit√© que y)\")\n",
    "print(f\"   ‚Üí Interpr√©tation : En moyenne, les pr√©dictions s'√©cartent de {rmse_manual:.2f} unit√©s\")\n",
    "print(\"   ‚Üí Plus facile √† interpr√©ter que MSE\")\n",
    "\n",
    "print(f\"\\nüìä MAE = {mae_manual:.2f}\")\n",
    "print(\"   ‚Üí Unit√© : progression (m√™me unit√© que y)\")\n",
    "print(f\"   ‚Üí Interpr√©tation : Erreur absolue moyenne de {mae_manual:.2f} unit√©s\")\n",
    "print(\"   ‚Üí Moins sensible aux outliers que RMSE\")\n",
    "print(f\"   ‚Üí √âcart RMSE - MAE = {rmse_manual - mae_manual:.2f} (faible = peu d'outliers)\")\n",
    "\n",
    "print(f\"\\nüìä R¬≤ = {r2_manual:.4f}\")\n",
    "print(f\"   ‚Üí Interpr√©tation : Le mod√®le explique {r2_manual * 100:.2f}% de la variance\")\n",
    "print(f\"   ‚Üí Les {(1 - r2_manual) * 100:.2f}% restants sont dus au bruit ou variables non captur√©es\")\n",
    "print(\"   ‚Üí R¬≤ = 0 : mod√®le √©quivalent √† pr√©dire la moyenne\")\n",
    "print(\"   ‚Üí R¬≤ = 1 : mod√®le parfait (rare en pratique)\")\n",
    "\n",
    "print(f\"\\nüìä MAPE = {mape_manual:.2f}%\")\n",
    "print(f\"   ‚Üí Interpr√©tation : Erreur moyenne de {mape_manual:.2f}% par rapport aux vraies valeurs\")\n",
    "print(\"   ‚Üí Ind√©pendant de l'√©chelle (utile pour comparer diff√©rents datasets)\")\n",
    "print(\"   ‚Üí Attention : non d√©fini si y contient des 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des m√©triques pour tous les mod√®les\n",
    "results = []\n",
    "\n",
    "for name, y_pred in predictions.items():\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    \n",
    "    results.append({\n",
    "        'Mod√®le': name,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'MAPE (%)': mape\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"COMPARAISON DES MOD√àLES\")\n",
    "print(\"=\" * 90)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"\\nüí° Plus petit = meilleur pour MSE, RMSE, MAE, MAPE\")\n",
    "print(\"üí° Plus grand = meilleur pour R¬≤ (max = 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative des m√©triques\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "metrics_to_plot = ['MSE', 'RMSE', 'MAE', 'R¬≤', 'MAPE (%)']\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics_to_plot, colors)):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    \n",
    "    bars = ax.bar(df_results['Mod√®le'], df_results[metric], color=color, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Mettre en √©vidence le meilleur mod√®le\n",
    "    if metric == 'R¬≤':\n",
    "        best_idx = df_results[metric].idxmax()\n",
    "    else:\n",
    "        best_idx = df_results[metric].idxmin()\n",
    "    \n",
    "    bars[best_idx].set_color('gold')\n",
    "    bars[best_idx].set_edgecolor('red')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "    \n",
    "    ax.set_ylabel(metric, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'Comparaison - {metric}', fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Supprimer le subplot vide\n",
    "fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚≠ê Mod√®le en OR = meilleur pour cette m√©trique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation des Pr√©dictions vs Vraies Valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot : pr√©dictions vs vraies valeurs\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (name, y_pred) in enumerate(predictions.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(y_test, y_pred, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Ligne de r√©f√©rence (pr√©diction parfaite : y = ≈∑)\n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Pr√©diction parfaite')\n",
    "    \n",
    "    # M√©triques\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    ax.set_xlabel('Vraies Valeurs', fontsize=12)\n",
    "    ax.set_ylabel('Pr√©dictions', fontsize=12)\n",
    "    ax.set_title(f'{name}\\nR¬≤ = {r2:.3f}, RMSE = {rmse:.2f}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpr√©tation :\")\n",
    "print(\"  - Points proches de la ligne rouge : bonnes pr√©dictions\")\n",
    "print(\"  - Points au-dessus de la ligne : surestimation\")\n",
    "print(\"  - Points en-dessous de la ligne : sous-estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distribution des Erreurs (Residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des r√©sidus (erreurs) pour chaque mod√®le\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for i, (name, y_pred) in enumerate(predictions.items()):\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    # Histogramme des r√©sidus\n",
    "    ax1 = axes[0, i]\n",
    "    ax1.hist(residuals, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    ax1.axvline(0, color='red', linestyle='--', linewidth=2, label='Erreur nulle')\n",
    "    ax1.axvline(residuals.mean(), color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Moyenne = {residuals.mean():.2f}')\n",
    "    ax1.set_xlabel('R√©sidus (y - ≈∑)', fontsize=11)\n",
    "    ax1.set_ylabel('Fr√©quence', fontsize=11)\n",
    "    ax1.set_title(f'{name} - Distribution des R√©sidus', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(fontsize=9)\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # R√©sidus vs Pr√©dictions (d√©tection de patterns)\n",
    "    ax2 = axes[1, i]\n",
    "    ax2.scatter(y_pred, residuals, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "    ax2.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel('Pr√©dictions', fontsize=11)\n",
    "    ax2.set_ylabel('R√©sidus', fontsize=11)\n",
    "    ax2.set_title(f'{name} - R√©sidus vs Pr√©dictions', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnalyse des r√©sidus :\")\n",
    "print(\"\\nüìà Histogramme :\")\n",
    "print(\"  - Centr√© sur 0 : le mod√®le n'est pas biais√©\")\n",
    "print(\"  - Forme en cloche (normale) : hypoth√®se de normalit√© v√©rifi√©e\")\n",
    "print(\"\\nüìà R√©sidus vs Pr√©dictions :\")\n",
    "print(\"  - Points al√©atoires autour de 0 : bon mod√®le (pas de pattern)\")\n",
    "print(\"  - Pattern visible (courbe, tendance) : mod√®le ne capture pas toute la complexit√©\")\n",
    "print(\"  - Variance croissante : h√©t√©rosc√©dasticit√© (probl√®me potentiel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Impact des Outliers sur les M√©triques\n",
    "\n",
    "Cr√©ons un dataset avec outliers pour comprendre la diff√©rence entre RMSE et MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des donn√©es synth√©tiques\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Vraies valeurs\n",
    "y_true = np.linspace(0, 100, n_samples)\n",
    "\n",
    "# Pr√©dictions avec erreur normale\n",
    "y_pred_normal = y_true + np.random.normal(0, 5, n_samples)\n",
    "\n",
    "# Pr√©dictions avec quelques outliers (erreurs tr√®s importantes)\n",
    "y_pred_outliers = y_pred_normal.copy()\n",
    "outlier_indices = [10, 25, 50, 75, 90]\n",
    "y_pred_outliers[outlier_indices] += np.random.choice([-40, -30, 30, 40], size=len(outlier_indices))\n",
    "\n",
    "print(\"Dataset synth√©tique cr√©√© :\")\n",
    "print(f\"  - {n_samples} exemples\")\n",
    "print(f\"  - Sans outliers : erreur normale (std=5)\")\n",
    "print(f\"  - Avec outliers : {len(outlier_indices)} erreurs extr√™mes ajout√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des m√©triques avec et sans outliers\n",
    "metrics_comparison = []\n",
    "\n",
    "for label, y_pred in [('Sans outliers', y_pred_normal), ('Avec outliers', y_pred_outliers)]:\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    metrics_comparison.append({\n",
    "        'Cas': label,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'Ratio RMSE/MAE': rmse / mae\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(metrics_comparison)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMPACT DES OUTLIERS SUR LES M√âTRIQUES\")\n",
    "print(\"=\" * 80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Calcul des variations\n",
    "mse_increase = (df_comparison.loc[1, 'MSE'] - df_comparison.loc[0, 'MSE']) / df_comparison.loc[0, 'MSE'] * 100\n",
    "rmse_increase = (df_comparison.loc[1, 'RMSE'] - df_comparison.loc[0, 'RMSE']) / df_comparison.loc[0, 'RMSE'] * 100\n",
    "mae_increase = (df_comparison.loc[1, 'MAE'] - df_comparison.loc[0, 'MAE']) / df_comparison.loc[0, 'MAE'] * 100\n",
    "\n",
    "print(\"\\nüìä ANALYSE :\")\n",
    "print(f\"  - MSE augmente de {mse_increase:.1f}% avec les outliers (tr√®s sensible !)\")\n",
    "print(f\"  - RMSE augmente de {rmse_increase:.1f}% avec les outliers (sensible)\")\n",
    "print(f\"  - MAE augmente de {mae_increase:.1f}% avec les outliers (robuste)\")\n",
    "print(\"\\nüí° CONCLUSION :\")\n",
    "print(\"  - MAE est beaucoup plus robuste aux outliers que RMSE/MSE\")\n",
    "print(\"  - Le ratio RMSE/MAE indique la pr√©sence d'outliers :\")\n",
    "print(f\"    ‚Ä¢ Sans outliers : {df_comparison.loc[0, 'Ratio RMSE/MAE']:.2f} (proche de 1)\")\n",
    "print(f\"    ‚Ä¢ Avec outliers : {df_comparison.loc[1, 'Ratio RMSE/MAE']:.2f} (> 1.5 = outliers d√©tect√©s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'impact des outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Sans outliers\n",
    "ax = axes[0]\n",
    "ax.scatter(y_true, y_pred_normal, alpha=0.6, s=50, label='Pr√©dictions', color='blue')\n",
    "ax.plot([0, 100], [0, 100], 'r--', linewidth=2, label='Pr√©diction parfaite')\n",
    "ax.set_xlabel('Vraies Valeurs', fontsize=12)\n",
    "ax.set_ylabel('Pr√©dictions', fontsize=12)\n",
    "ax.set_title(f'Sans Outliers\\nRMSE={df_comparison.loc[0, \"RMSE\"]:.2f}, MAE={df_comparison.loc[0, \"MAE\"]:.2f}', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Avec outliers\n",
    "ax = axes[1]\n",
    "ax.scatter(y_true, y_pred_outliers, alpha=0.6, s=50, label='Pr√©dictions', color='green')\n",
    "ax.scatter(y_true[outlier_indices], y_pred_outliers[outlier_indices], \n",
    "           color='red', s=150, marker='X', edgecolors='black', linewidth=2, \n",
    "           label='Outliers', zorder=5)\n",
    "ax.plot([0, 100], [0, 100], 'r--', linewidth=2, label='Pr√©diction parfaite')\n",
    "ax.set_xlabel('Vraies Valeurs', fontsize=12)\n",
    "ax.set_ylabel('Pr√©dictions', fontsize=12)\n",
    "ax.set_title(f'Avec Outliers\\nRMSE={df_comparison.loc[1, \"RMSE\"]:.2f}, MAE={df_comparison.loc[1, \"MAE\"]:.2f}', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisuellement, on voit que :\")\n",
    "print(\"  - Les outliers (croix rouges) s'√©cartent beaucoup de la ligne de pr√©diction parfaite\")\n",
    "print(\"  - RMSE augmente plus que MAE car il met au carr√© les erreurs (p√©nalise les grandes erreurs)\")\n",
    "print(\"  - MAE traite toutes les erreurs de mani√®re proportionnelle (plus robuste)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Guide de S√©lection des M√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GUIDE DE S√âLECTION DES M√âTRIQUES DE R√âGRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéØ MSE (Mean Squared Error)\")\n",
    "print(\"   ‚úÖ Quand utiliser :\")\n",
    "print(\"      - Optimisation math√©matique (d√©rivable, convexe)\")\n",
    "print(\"      - Grandes erreurs tr√®s p√©nalisantes (finance, s√©curit√©)\")\n",
    "print(\"   ‚ùå √âviter si :\")\n",
    "print(\"      - Dataset contient des outliers\")\n",
    "print(\"      - Besoin d'interpr√©tabilit√© (unit√© au carr√©)\")\n",
    "\n",
    "print(\"\\nüéØ RMSE (Root Mean Squared Error)\")\n",
    "print(\"   ‚úÖ Quand utiliser :\")\n",
    "print(\"      - M√™me unit√© que la cible (facile √† interpr√©ter)\")\n",
    "print(\"      - Grandes erreurs plus p√©nalisantes\")\n",
    "print(\"      - Standard pour la r√©gression (d√©faut)\")\n",
    "print(\"   ‚ùå √âviter si :\")\n",
    "print(\"      - Dataset contient beaucoup d'outliers\")\n",
    "\n",
    "print(\"\\nüéØ MAE (Mean Absolute Error)\")\n",
    "print(\"   ‚úÖ Quand utiliser :\")\n",
    "print(\"      - Dataset avec outliers\")\n",
    "print(\"      - Toutes les erreurs ont un co√ªt similaire\")\n",
    "print(\"      - Besoin de robustesse\")\n",
    "print(\"   ‚ùå √âviter si :\")\n",
    "print(\"      - Grandes erreurs doivent √™tre fortement p√©nalis√©es\")\n",
    "\n",
    "print(\"\\nüéØ R¬≤ (Coefficient de D√©termination)\")\n",
    "print(\"   ‚úÖ Quand utiliser :\")\n",
    "print(\"      - Comparer des mod√®les (ind√©pendant de l'√©chelle)\")\n",
    "print(\"      - Mesurer la proportion de variance expliqu√©e\")\n",
    "print(\"      - Communication (facile √† expliquer : 0-100%)\")\n",
    "print(\"   ‚ùå √âviter si :\")\n",
    "print(\"      - Seul crit√®re (peut masquer des probl√®mes)\")\n",
    "print(\"      - Beaucoup de features (pr√©f√©rer R¬≤ ajust√©)\")\n",
    "\n",
    "print(\"\\nüéØ MAPE (Mean Absolute Percentage Error)\")\n",
    "print(\"   ‚úÖ Quand utiliser :\")\n",
    "print(\"      - Comparer diff√©rents datasets (ind√©pendant de l'√©chelle)\")\n",
    "print(\"      - Interpr√©tation en pourcentage (business)\")\n",
    "print(\"   ‚ùå √âviter si :\")\n",
    "print(\"      - Cible contient des 0 (division par 0)\")\n",
    "print(\"      - Asym√©trie probl√©matique (p√©nalise plus surestimation)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° RECOMMANDATION G√âN√âRALE :\")\n",
    "print(\"   Toujours reporter PLUSIEURS m√©triques (ex: RMSE + MAE + R¬≤)\")\n",
    "print(\"   Cela donne une vision compl√®te de la performance du mod√®le.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. R√©sum√© et Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"R√âSUM√â DU NOTEBOOK - M√âTRIQUES DE R√âGRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Ce que nous avons appris :\")\n",
    "print(\"\\n1. MSE vs RMSE vs MAE :\")\n",
    "print(\"   - MSE p√©nalise fortement les grandes erreurs (au carr√©)\")\n",
    "print(\"   - RMSE est plus interpr√©table (m√™me unit√© que y)\")\n",
    "print(\"   - MAE est robuste aux outliers\")\n",
    "\n",
    "print(\"\\n2. R¬≤ mesure la variance expliqu√©e :\")\n",
    "print(\"   - R¬≤ = 1 : mod√®le parfait\")\n",
    "print(\"   - R¬≤ = 0 : mod√®le √©quivalent √† pr√©dire la moyenne\")\n",
    "print(\"   - R¬≤ < 0 : mod√®le tr√®s mauvais\")\n",
    "\n",
    "print(\"\\n3. MAPE donne une erreur en pourcentage :\")\n",
    "print(\"   - Ind√©pendant de l'√©chelle\")\n",
    "print(\"   - Attention aux divisions par 0\")\n",
    "\n",
    "print(\"\\n4. Les r√©sidus donnent des informations pr√©cieuses :\")\n",
    "print(\"   - Distribution normale centr√©e sur 0 : bon mod√®le\")\n",
    "print(\"   - Pattern dans les r√©sidus : mod√®le incomplet\")\n",
    "\n",
    "print(\"\\n5. Le ratio RMSE/MAE d√©tecte les outliers :\")\n",
    "print(\"   - Proche de 1 : peu d'outliers\")\n",
    "print(\"   - > 1.5 : pr√©sence d'outliers significatifs\")\n",
    "\n",
    "print(\"\\n6. Toujours reporter PLUSIEURS m√©triques :\")\n",
    "print(\"   - RMSE + MAE + R¬≤ est un bon trio par d√©faut\")\n",
    "print(\"   - Chaque m√©trique apporte une information diff√©rente\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìö Prochaine √©tape : Validation crois√©e pour une √©valuation robuste\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "1. Calculez le R¬≤ ajust√© pour chaque mod√®le. Lequel reste le meilleur ?\n",
    "\n",
    "2. Cr√©ez un dataset o√π RMSE >> MAE et expliquez pourquoi.\n",
    "\n",
    "3. Impl√©mentez vous-m√™me les fonctions `mse()`, `mae()`, `r2_score()` en NumPy pur.\n",
    "\n",
    "4. Tracez la courbe de learning (train vs test scores) pour d√©tecter l'overfitting.\n",
    "\n",
    "5. Comparez MAPE avec sMAPE (symmetric MAPE) qui corrige l'asym√©trie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}