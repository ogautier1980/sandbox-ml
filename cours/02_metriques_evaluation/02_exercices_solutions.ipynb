{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/02_metriques_evaluation/02_exercices_solutions.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('üì¶ Installation des packages...')\n",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn\n",
    "    print('‚úÖ Installation termin√©e !')\n",
    "else:\n",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 02 - Solutions : M√©triques d'√âvaluation\n",
    "\n",
    "Ce notebook contient les solutions des exercices sur les m√©triques d'√©valuation en Machine Learning.\n",
    "\n",
    "**Objectifs** :\n",
    "- Calculer et interpr√©ter les m√©triques de classification\n",
    "- Utiliser les m√©triques de r√©gression\n",
    "- Appliquer la validation crois√©e\n",
    "- Choisir les bonnes m√©triques selon le contexte\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 1 : Matrice de Confusion et M√©triques de Base\n",
    "\n",
    "**Objectif** : Calculer manuellement les m√©triques de classification √† partir d'une matrice de confusion.\n",
    "\n",
    "**Contexte** : Un syst√®me de d√©tection de fraude bancaire a les r√©sultats suivants :\n",
    "\n",
    "```\n",
    "Matrice de Confusion :\n",
    "                Pr√©dit Non-Fraude  Pr√©dit Fraude\n",
    "R√©el Non-Fraude       950              50\n",
    "R√©el Fraude            20              80\n",
    "```\n",
    "\n",
    "**Questions** :\n",
    "\n",
    "1. Identifiez les valeurs de TP, TN, FP, FN\n",
    "2. Calculez manuellement :\n",
    "   - Accuracy\n",
    "   - Precision\n",
    "   - Recall (Sensibilit√©)\n",
    "   - Sp√©cificit√©\n",
    "   - F1-Score\n",
    "3. Dans ce contexte de d√©tection de fraude, quelle m√©trique est la plus importante ? Pourquoi ?\n",
    "4. Le syst√®me est-il performant ? Justifiez votre r√©ponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# D√©finissez la matrice de confusion\n",
    "# Format scikit-learn : cm[i, j] o√π i = classe r√©elle, j = classe pr√©dite\n",
    "cm = np.array([\n",
    "    [950, 50],   # R√©el Non-Fraude (classe 0) : 950 correctement class√©s, 50 mal class√©s\n",
    "    [20, 80]     # R√©el Fraude (classe 1) : 20 mal class√©s, 80 correctement class√©s\n",
    "])\n",
    "\n",
    "print(\"Matrice de Confusion :\")\n",
    "print(cm)\n",
    "print(\"\")\n",
    "\n",
    "# Question 1 : Identifiez TP, TN, FP, FN\n",
    "# Pour la classe \"Fraude\" (classe positive) :\n",
    "TN = cm[0, 0]  # Vrai N√©gatif : R√©el Non-Fraude, Pr√©dit Non-Fraude\n",
    "FP = cm[0, 1]  # Faux Positif : R√©el Non-Fraude, Pr√©dit Fraude\n",
    "FN = cm[1, 0]  # Faux N√©gatif : R√©el Fraude, Pr√©dit Non-Fraude\n",
    "TP = cm[1, 1]  # Vrai Positif : R√©el Fraude, Pr√©dit Fraude\n",
    "\n",
    "print(\"1. Identification des valeurs :\")\n",
    "print(f\"   TP (Vrai Positif - fraude d√©tect√©e) : {TP}\")\n",
    "print(f\"   TN (Vrai N√©gatif - non-fraude d√©tect√©e) : {TN}\")\n",
    "print(f\"   FP (Faux Positif - fausse alerte) : {FP}\")\n",
    "print(f\"   FN (Faux N√©gatif - fraude manqu√©e) : {FN}\")\n",
    "print(\"\")\n",
    "\n",
    "# Question 2 : Calculez les m√©triques\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)  # Aussi appel√©e Sensibilit√© ou TPR (True Positive Rate)\n",
    "specificity = TN / (TN + FP)  # TNR (True Negative Rate)\n",
    "f1_score_manual = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"2. M√©triques calcul√©es manuellement :\")\n",
    "print(f\"   Accuracy    : {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision   : {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"   Recall      : {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"   Specificity : {specificity:.4f} ({specificity*100:.2f}%)\")\n",
    "print(f\"   F1-Score    : {f1_score_manual:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Interpr√©tation de chaque m√©trique\n",
    "print(\"   Interpr√©tations :\")\n",
    "print(f\"   - Accuracy : {accuracy*100:.1f}% des transactions sont correctement classifi√©es\")\n",
    "print(f\"   - Precision : {precision*100:.1f}% des alertes de fraude sont r√©elles\")\n",
    "print(f\"   - Recall : {recall*100:.1f}% des fraudes r√©elles sont d√©tect√©es\")\n",
    "print(f\"   - Specificity : {specificity*100:.1f}% des transactions l√©gitimes sont correctement identifi√©es\")\n",
    "print(\"\")\n",
    "\n",
    "# Question 3 : M√©trique la plus importante pour la d√©tection de fraude\n",
    "print(\"3. M√©trique la plus importante pour la d√©tection de fraude :\")\n",
    "print(\"\")\n",
    "print(\"   ‚û§ RECALL (Sensibilit√©) est la m√©trique la plus critique !\")\n",
    "print(\"\")\n",
    "print(\"   Justification :\")\n",
    "print(\"   - Il est CRUCIAL de d√©tecter le maximum de fraudes (minimiser FN)\")\n",
    "print(\"   - Manquer une fraude (FN) peut co√ªter tr√®s cher √† la banque\")\n",
    "print(\"   - Un faux positif (FP) est moins grave : on v√©rifie manuellement\")\n",
    "print(\"   - Mieux vaut trop d'alertes que pas assez en contexte de fraude\")\n",
    "print(\"\")\n",
    "print(\"   Dans ce cas : Recall = 80% signifie que 20% des fraudes ne sont PAS d√©tect√©es\")\n",
    "print(\"   ‚Üí C'est un probl√®me majeur pour un syst√®me de d√©tection de fraude !\")\n",
    "print(\"\")\n",
    "\n",
    "# Question 4 : Le syst√®me est-il performant ?\n",
    "print(\"4. √âvaluation globale du syst√®me :\")\n",
    "print(\"\")\n",
    "print(\"   ‚ùå Le syst√®me n'est PAS suffisamment performant\")\n",
    "print(\"\")\n",
    "print(\"   Probl√®mes identifi√©s :\")\n",
    "print(f\"   - Recall de {recall*100:.1f}% est INSUFFISANT : 20 fraudes sur 100 passent inaper√ßues\")\n",
    "print(f\"   - 20 fraudes manqu√©es (FN) repr√©sentent un risque financier important\")\n",
    "print(\"\")\n",
    "print(\"   Points positifs :\")\n",
    "print(f\"   - Precision de {precision*100:.1f}% est correcte : peu de fausses alertes\")\n",
    "print(f\"   - Specificity de {specificity*100:.1f}% est excellente : peu de clients l√©gitimes bloqu√©s\")\n",
    "print(\"\")\n",
    "print(\"   Recommandations :\")\n",
    "print(\"   1. Ajuster le seuil de d√©cision pour augmenter le Recall (accepter plus de FP)\")\n",
    "print(\"   2. Enrichir les features du mod√®le\")\n",
    "print(\"   3. Utiliser des techniques de r√©√©quilibrage (SMOTE, class_weight)\")\n",
    "print(\"   4. Tester des mod√®les plus complexes (Random Forest, XGBoost)\")\n",
    "print(\"   5. Objectif : Atteindre au minimum 95% de Recall\")\n",
    "\n",
    "# Visualisation de la matrice de confusion\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Fraude', 'Fraude'],\n",
    "            yticklabels=['Non-Fraude', 'Fraude'],\n",
    "            cbar_kws={'label': 'Nombre de transactions'})\n",
    "plt.xlabel('Classe Pr√©dite')\n",
    "plt.ylabel('Classe R√©elle')\n",
    "plt.title('Matrice de Confusion - D√©tection de Fraude Bancaire')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualisation comparative des m√©triques\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score']\n",
    "metrics_values = [accuracy, precision, recall, specificity, f1_score_manual]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['green' if m >= 0.9 else 'orange' if m >= 0.8 else 'red' for m in metrics_values]\n",
    "bars = ax.bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{value:.3f}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax.axhline(y=0.95, color='green', linestyle='--', linewidth=2, label='Seuil excellent (95%)', alpha=0.5)\n",
    "ax.axhline(y=0.80, color='orange', linestyle='--', linewidth=2, label='Seuil acceptable (80%)', alpha=0.5)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparaison des M√©triques de Performance')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 2 : Courbe ROC et Choix du Seuil\n",
    "\n",
    "**Objectif** : Comprendre l'impact du seuil de classification sur les performances.\n",
    "\n",
    "**Consignes** :\n",
    "\n",
    "1. Chargez le dataset Breast Cancer\n",
    "2. Entra√Ænez un mod√®le Logistic Regression\n",
    "3. Tracez la courbe ROC et calculez l'AUC\n",
    "4. Testez 3 seuils diff√©rents (0.3, 0.5, 0.7) et pour chacun :\n",
    "   - Calculez Precision, Recall, F1-Score\n",
    "   - Affichez la matrice de confusion\n",
    "5. Quel seuil choisiriez-vous pour :\n",
    "   - Minimiser les faux n√©gatifs (ne pas manquer de malades) ?\n",
    "   - Maximiser la pr√©cision globale ?\n",
    "   - Obtenir un bon compromis ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# 1. Chargez les donn√©es\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(f\"Dataset Breast Cancer :\")\n",
    "print(f\"  - Nombre d'√©chantillons : {X.shape[0]}\")\n",
    "print(f\"  - Nombre de features : {X.shape[1]}\")\n",
    "print(f\"  - Classes : {data.target_names}\")\n",
    "print(f\"  - Distribution : {np.bincount(y)} (0=malin, 1=b√©nin)\")\n",
    "print(\"\")\n",
    "\n",
    "# 2. Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Split train/test :\")\n",
    "print(f\"  - Train : {X_train.shape[0]} √©chantillons\")\n",
    "print(f\"  - Test : {X_test.shape[0]} √©chantillons\")\n",
    "print(\"\")\n",
    "\n",
    "# 3. Entra√Ænez le mod√®le\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les probabilit√©s pr√©dites\n",
    "y_proba = model.predict_proba(X_test)[:, 1]  # Probabilit√© de la classe positive (b√©nin)\n",
    "\n",
    "print(\"‚úÖ Mod√®le entra√Æn√©\")\n",
    "print(\"\")\n",
    "\n",
    "# 4. Tracez la courbe ROC\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_proba)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Chance (AUC = 0.5)')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('Courbe ROC - Breast Cancer Classification', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score : {auc:.4f}\")\n",
    "print(f\"Interpr√©tation : AUC proche de 1 indique un excellent mod√®le discriminant\")\n",
    "print(\"\")\n",
    "\n",
    "# 5. Testez diff√©rents seuils\n",
    "thresholds_to_test = [0.3, 0.5, 0.7]\n",
    "\n",
    "results = []\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, threshold in enumerate(thresholds_to_test):\n",
    "    # Pr√©dictions avec le seuil personnalis√©\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculer les m√©triques\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Stocker les r√©sultats\n",
    "    results.append({\n",
    "        'Threshold': threshold,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'FN': cm[1, 0] if cm.shape[0] > 1 else 0  # Faux n√©gatifs\n",
    "    })\n",
    "    \n",
    "    # Visualiser la matrice de confusion\n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Malin', 'B√©nin'],\n",
    "                yticklabels=['Malin', 'B√©nin'])\n",
    "    ax.set_xlabel('Pr√©diction')\n",
    "    ax.set_ylabel('R√©alit√©')\n",
    "    ax.set_title(f'Seuil = {threshold}\\nF1={f1:.3f}, Recall={recall:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Afficher le tableau de comparaison\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparaison des seuils :\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\")\n",
    "\n",
    "# Visualisation comparative\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(thresholds_to_test))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - width*1.5, results_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x - width*0.5, results_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width*0.5, results_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "ax.bar(x + width*1.5, results_df['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Seuil')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Impact du Seuil sur les M√©triques de Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{t:.1f}' for t in thresholds_to_test])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# R√©pondre aux questions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMANDATIONS SELON L'OBJECTIF\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "print(\"1Ô∏è‚É£  Pour MINIMISER les faux n√©gatifs (ne pas manquer de malades) :\")\n",
    "print(f\"   ‚Üí Choisir seuil = 0.3\")\n",
    "print(f\"   ‚Üí Recall maximal = {results_df.loc[0, 'Recall']:.3f}\")\n",
    "print(f\"   ‚Üí Seulement {results_df.loc[0, 'FN']:.0f} faux n√©gatifs\")\n",
    "print(f\"   ‚Üí Justification : En m√©dical, il vaut mieux sur-diagnostiquer que manquer un cancer\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"2Ô∏è‚É£  Pour MAXIMISER la pr√©cision globale :\")\n",
    "print(f\"   ‚Üí Choisir seuil = 0.5 (d√©faut)\")\n",
    "print(f\"   ‚Üí Accuracy = {results_df.loc[1, 'Accuracy']:.3f}\")\n",
    "print(f\"   ‚Üí Bon √©quilibre entre toutes les m√©triques\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"3Ô∏è‚É£  Pour un BON COMPROMIS (Precision-Recall) :\")\n",
    "print(f\"   ‚Üí Choisir seuil = 0.5\")\n",
    "print(f\"   ‚Üí F1-Score maximal = {results_df.loc[1, 'F1-Score']:.3f}\")\n",
    "print(f\"   ‚Üí Meilleur √©quilibre entre Precision ({results_df.loc[1, 'Precision']:.3f}) et Recall ({results_df.loc[1, 'Recall']:.3f})\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"üí° CONSEIL G√âN√âRAL :\")\n",
    "print(\"   En contexte m√©dical, PRIVIL√âGIER LE RECALL pour ne manquer aucun cas grave.\")\n",
    "print(\"   ‚Üí Recommandation finale : Seuil = 0.3 ou 0.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 3 : M√©triques de R√©gression\n",
    "\n",
    "**Objectif** : √âvaluer un mod√®le de r√©gression avec diff√©rentes m√©triques.\n",
    "\n",
    "**Consignes** :\n",
    "\n",
    "1. Chargez le dataset Diabetes\n",
    "2. Entra√Ænez deux mod√®les :\n",
    "   - LinearRegression\n",
    "   - RandomForestRegressor\n",
    "3. Pour chaque mod√®le, calculez :\n",
    "   - MSE (Mean Squared Error)\n",
    "   - RMSE (Root Mean Squared Error)\n",
    "   - MAE (Mean Absolute Error)\n",
    "   - R¬≤ (Coefficient de d√©termination)\n",
    "4. Comparez les deux mod√®les\n",
    "5. Visualisez les pr√©dictions vs valeurs r√©elles pour le meilleur mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# 1. Chargez le dataset Diabetes\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "print(\"Dataset Diabetes :\")\n",
    "print(f\"  - √âchantillons : {X.shape[0]}\")\n",
    "print(f\"  - Features : {X.shape[1]}\")\n",
    "print(f\"  - Target : Progression du diab√®te (valeur continue)\")\n",
    "print(f\"  - Range target : [{y.min():.1f}, {y.max():.1f}]\")\n",
    "print(\"\")\n",
    "\n",
    "# 2. Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Entra√Ænez les deux mod√®les\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Entra√Ænement de {name}...\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calcul des m√©triques sur le test set\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # M√©triques sur train (pour d√©tecter l'overfitting)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤ (test)': r2,\n",
    "        'R¬≤ (train)': r2_train,\n",
    "        'Predictions': y_pred_test\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úì {name} entra√Æn√©\")\n",
    "    print(\"\")\n",
    "\n",
    "# 4. Comparer les mod√®les\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARAISON DES MOD√àLES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "# Cr√©er un DataFrame pour la comparaison\n",
    "comparison_df = pd.DataFrame(results)[['Model', 'MSE', 'RMSE', 'MAE', 'R¬≤ (test)', 'R¬≤ (train)']]\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\")\n",
    "\n",
    "# Interpr√©tation des m√©triques\n",
    "print(\"INTERPR√âTATION DES M√âTRIQUES :\")\n",
    "print(\"\")\n",
    "print(\"1. MSE (Mean Squared Error) :\")\n",
    "print(\"   - P√©nalise fortement les grandes erreurs (au carr√©)\")\n",
    "print(\"   - Plus le MSE est bas, meilleur est le mod√®le\")\n",
    "print(\"   - Unit√© : carr√© de l'unit√© de y (difficile √† interpr√©ter directement)\")\n",
    "print(\"\")\n",
    "print(\"2. RMSE (Root Mean Squared Error) :\")\n",
    "print(\"   - Racine carr√©e du MSE\")\n",
    "print(\"   - M√™me unit√© que y (plus interpr√©table)\")\n",
    "print(\"   - Repr√©sente l'erreur moyenne en unit√©s originales\")\n",
    "print(\"\")\n",
    "print(\"3. MAE (Mean Absolute Error) :\")\n",
    "print(\"   - Moyenne des valeurs absolues des erreurs\")\n",
    "print(\"   - Moins sensible aux outliers que MSE/RMSE\")\n",
    "print(\"   - M√™me unit√© que y\")\n",
    "print(\"\")\n",
    "print(\"4. R¬≤ (Coefficient de d√©termination) :\")\n",
    "print(\"   - Mesure la proportion de variance expliqu√©e par le mod√®le\")\n",
    "print(\"   - Varie entre -‚àû et 1 (1 = parfait, 0 = mod√®le constant)\")\n",
    "print(\"   - R¬≤ train vs test permet de d√©tecter l'overfitting\")\n",
    "print(\"\")\n",
    "\n",
    "# D√©terminer le meilleur mod√®le\n",
    "best_idx = comparison_df['R¬≤ (test)'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_idx, 'Model']\n",
    "best_r2 = comparison_df.loc[best_idx, 'R¬≤ (test)']\n",
    "\n",
    "print(f\"üèÜ MEILLEUR MOD√àLE : {best_model_name}\")\n",
    "print(f\"   R¬≤ = {best_r2:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Analyse de l'overfitting\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    diff = row['R¬≤ (train)'] - row['R¬≤ (test)']\n",
    "    print(f\"{row['Model']} :\")\n",
    "    print(f\"  R¬≤ train = {row['R¬≤ (train)']:.4f}\")\n",
    "    print(f\"  R¬≤ test  = {row['R¬≤ (test)']:.4f}\")\n",
    "    print(f\"  √âcart    = {diff:.4f}\")\n",
    "    if diff > 0.1:\n",
    "        print(f\"  ‚ö†Ô∏è  Overfitting d√©tect√© !\")\n",
    "    else:\n",
    "        print(f\"  ‚úì Pas d'overfitting majeur\")\n",
    "    print(\"\")\n",
    "\n",
    "# 5. Visualisations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# a) Comparaison visuelle des m√©triques\n",
    "ax1 = axes[0, 0]\n",
    "metrics = ['MSE', 'RMSE', 'MAE']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    values = [result[m] for m in metrics]\n",
    "    ax1.bar(x + i*width, values, width, label=result['Model'], alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('M√©triques')\n",
    "ax1.set_ylabel('Erreur')\n",
    "ax1.set_title('Comparaison des Erreurs')\n",
    "ax1.set_xticks(x + width / 2)\n",
    "ax1.set_xticklabels(metrics)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# b) Comparaison R¬≤\n",
    "ax2 = axes[0, 1]\n",
    "model_names = [r['Model'] for r in results]\n",
    "r2_train_values = [r['R¬≤ (train)'] for r in results]\n",
    "r2_test_values = [r['R¬≤ (test)'] for r in results]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "ax2.bar(x - width/2, r2_train_values, width, label='R¬≤ Train', alpha=0.8)\n",
    "ax2.bar(x + width/2, r2_test_values, width, label='R¬≤ Test', alpha=0.8)\n",
    "ax2.set_xlabel('Mod√®les')\n",
    "ax2.set_ylabel('R¬≤ Score')\n",
    "ax2.set_title('Comparaison R¬≤ (Train vs Test)')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# c) Pr√©dictions vs R√©alit√© pour Linear Regression\n",
    "ax3 = axes[1, 0]\n",
    "y_pred_lr = results[0]['Predictions']\n",
    "ax3.scatter(y_test, y_pred_lr, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "ax3.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Pr√©diction parfaite')\n",
    "ax3.set_xlabel('Valeurs R√©elles')\n",
    "ax3.set_ylabel('Pr√©dictions')\n",
    "ax3.set_title(f'Linear Regression (R¬≤={results[0][\"R¬≤ (test)\"]:.3f})')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# d) Pr√©dictions vs R√©alit√© pour Random Forest\n",
    "ax4 = axes[1, 1]\n",
    "y_pred_rf = results[1]['Predictions']\n",
    "ax4.scatter(y_test, y_pred_rf, alpha=0.6, edgecolors='k', linewidth=0.5, color='green')\n",
    "ax4.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Pr√©diction parfaite')\n",
    "ax4.set_xlabel('Valeurs R√©elles')\n",
    "ax4.set_ylabel('Pr√©dictions')\n",
    "ax4.set_title(f'Random Forest (R¬≤={results[1][\"R¬≤ (test)\"]:.3f})')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution des r√©sidus pour le meilleur mod√®le\n",
    "best_predictions = results[best_idx]['Predictions']\n",
    "residuals = y_test - best_predictions\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogramme des r√©sidus\n",
    "axes[0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='R√©sidu nul')\n",
    "axes[0].set_xlabel('R√©sidus (R√©el - Pr√©dit)')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "axes[0].set_title(f'Distribution des R√©sidus - {best_model_name}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# R√©sidus vs Pr√©dictions\n",
    "axes[1].scatter(best_predictions, residuals, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=2, label='R√©sidu nul')\n",
    "axes[1].set_xlabel('Pr√©dictions')\n",
    "axes[1].set_ylabel('R√©sidus')\n",
    "axes[1].set_title(f'R√©sidus vs Pr√©dictions - {best_model_name}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "print(f\"Le mod√®le {best_model_name} performe mieux avec un R¬≤ de {best_r2:.4f}.\")\n",
    "print(\"\")\n",
    "print(\"Observations :\")\n",
    "print(\"- Random Forest capture mieux les relations non-lin√©aires\")\n",
    "print(\"- Les r√©sidus devraient id√©alement √™tre centr√©s sur 0 et distribu√©s normalement\")\n",
    "print(\"- Un R¬≤ autour de 0.4-0.5 indique que le mod√®le explique environ 40-50% de la variance\")\n",
    "print(\"- Il reste de la marge d'am√©lioration (feature engineering, hyperparameter tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 4 : Validation Crois√©e\n",
    "\n",
    "**Objectif** : Comparer la validation simple (train/test split) avec la validation crois√©e.\n",
    "\n",
    "**Consignes** :\n",
    "\n",
    "1. Utilisez le dataset Breast Cancer\n",
    "2. **M√©thode 1** : Entra√Ænez un RandomForestClassifier avec un simple split 70/30\n",
    "   - Calculez l'accuracy sur le test set\n",
    "3. **M√©thode 2** : Utilisez la validation crois√©e 5-fold (`cross_val_score`)\n",
    "   - Calculez la moyenne et l'√©cart-type des scores\n",
    "4. **M√©thode 3** : Testez diff√©rents nombres de folds (3, 5, 10, 20)\n",
    "   - Visualisez l'impact sur la variance des r√©sultats\n",
    "5. Quelle m√©thode vous semble la plus fiable ? Pourquoi ?\n",
    "6. Quel nombre de folds recommanderiez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# 1. Chargez les donn√©es\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(\"Dataset Breast Cancer pour validation crois√©e\")\n",
    "print(f\"  - Total √©chantillons : {len(X)}\")\n",
    "print(\"\")\n",
    "\n",
    "# 2. M√©thode 1 : Simple train/test split\n",
    "print(\"=\"*70)\n",
    "print(\"M√âTHODE 1 : SIMPLE TRAIN/TEST SPLIT (70/30)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "model_simple = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_simple.fit(X_train, y_train)\n",
    "accuracy_simple = model_simple.score(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy sur le test set : {accuracy_simple:.4f}\")\n",
    "print(\"\")\n",
    "print(\"Limites de cette m√©thode :\")\n",
    "print(\"  ‚ùå D√©pend fortement du split al√©atoire\")\n",
    "print(\"  ‚ùå Pas d'estimation de la variance\")\n",
    "print(\"  ‚ùå Une partie des donn√©es (30%) n'est jamais utilis√©e pour l'entra√Ænement\")\n",
    "print(\"\")\n",
    "\n",
    "# 3. M√©thode 2 : Validation crois√©e 5-fold\n",
    "print(\"=\"*70)\n",
    "print(\"M√âTHODE 2 : VALIDATION CROIS√âE 5-FOLD\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "model_cv = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "cv_scores_5 = cross_val_score(model_cv, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Scores des 5 folds : {cv_scores_5}\")\n",
    "print(f\"Moyenne : {cv_scores_5.mean():.4f}\")\n",
    "print(f\"√âcart-type : {cv_scores_5.std():.4f}\")\n",
    "print(f\"Intervalle de confiance (¬±2œÉ) : [{cv_scores_5.mean() - 2*cv_scores_5.std():.4f}, {cv_scores_5.mean() + 2*cv_scores_5.std():.4f}]\")\n",
    "print(\"\")\n",
    "print(\"Avantages :\")\n",
    "print(\"  ‚úÖ Tous les exemples sont utilis√©s pour train ET test\")\n",
    "print(\"  ‚úÖ Estimation de la variance du mod√®le\")\n",
    "print(\"  ‚úÖ R√©sultat plus fiable et robuste\")\n",
    "print(\"\")\n",
    "\n",
    "# 4. M√©thode 3 : Tester diff√©rents nombres de folds\n",
    "print(\"=\"*70)\n",
    "print(\"M√âTHODE 3 : IMPACT DU NOMBRE DE FOLDS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "n_folds_list = [3, 5, 10, 20]\n",
    "cv_results = []\n",
    "\n",
    "for n_folds in n_folds_list:\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=n_folds, scoring='accuracy')\n",
    "    \n",
    "    cv_results.append({\n",
    "        'n_folds': n_folds,\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'min': scores.min(),\n",
    "        'max': scores.max(),\n",
    "        'scores': scores\n",
    "    })\n",
    "    \n",
    "    print(f\"{n_folds}-fold CV :\")\n",
    "    print(f\"  Moyenne : {scores.mean():.4f}\")\n",
    "    print(f\"  √âcart-type : {scores.std():.4f}\")\n",
    "    print(f\"  Range : [{scores.min():.4f}, {scores.max():.4f}]\")\n",
    "    print(\"\")\n",
    "\n",
    "# Visualisations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# a) Boxplot des scores selon le nombre de folds\n",
    "ax1 = axes[0, 0]\n",
    "data_for_boxplot = [result['scores'] for result in cv_results]\n",
    "bp = ax1.boxplot(data_for_boxplot, labels=[f\"{nf}-fold\" for nf in n_folds_list],\n",
    "                  patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "ax1.set_xlabel('Nombre de Folds')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Distribution des Scores selon le Nombre de Folds')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# b) Moyenne et √©cart-type\n",
    "ax2 = axes[0, 1]\n",
    "means = [r['mean'] for r in cv_results]\n",
    "stds = [r['std'] for r in cv_results]\n",
    "ax2.errorbar(n_folds_list, means, yerr=stds, marker='o', markersize=8,\n",
    "             capsize=5, capthick=2, linewidth=2, label='Moyenne ¬± √âcart-type')\n",
    "ax2.axhline(accuracy_simple, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Simple split ({accuracy_simple:.4f})')\n",
    "ax2.set_xlabel('Nombre de Folds')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Moyenne et Variance selon le Nombre de Folds')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# c) √âcart-type uniquement\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(n_folds_list, stds, marker='o', markersize=8, linewidth=2, color='orange')\n",
    "ax3.set_xlabel('Nombre de Folds')\n",
    "ax3.set_ylabel('√âcart-type')\n",
    "ax3.set_title('Stabilit√© du Mod√®le (√âcart-type) vs Nombre de Folds')\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# d) Scores individuels pour 5-fold CV\n",
    "ax4 = axes[1, 1]\n",
    "for i, result in enumerate(cv_results):\n",
    "    n_folds = result['n_folds']\n",
    "    scores = result['scores']\n",
    "    ax4.scatter([n_folds] * len(scores), scores, alpha=0.6, s=50, label=f\"{n_folds}-fold\")\n",
    "ax4.set_xlabel('Nombre de Folds')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title('Scores Individuels de Chaque Fold')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5 & 6. Recommandations\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMANDATIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "print(\"5Ô∏è‚É£  Quelle m√©thode est la plus fiable ?\")\n",
    "print(\"\")\n",
    "print(\"   ‚Üí LA VALIDATION CROIS√âE est NETTEMENT plus fiable !\")\n",
    "print(\"\")\n",
    "print(\"   Raisons :\")\n",
    "print(\"   ‚úÖ Utilise toutes les donn√©es (train + test)\")\n",
    "print(\"   ‚úÖ Fournit une estimation de la variance\")\n",
    "print(\"   ‚úÖ R√©duit le biais li√© √† un split particulier\")\n",
    "print(\"   ‚úÖ Plus robuste pour √©valuer la g√©n√©ralisation\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"6Ô∏è‚É£  Nombre de folds recommand√© ?\")\n",
    "print(\"\")\n",
    "print(\"   ‚Üí RECOMMANDATION : 5 ou 10 folds\")\n",
    "print(\"\")\n",
    "print(\"   Compromis :\")\n",
    "print(\"   ‚Ä¢ 3 folds : Rapide, mais variance √©lev√©e\")\n",
    "print(\"   ‚Ä¢ 5 folds : BON COMPROMIS - Standard en pratique\")\n",
    "print(\"   ‚Ä¢ 10 folds : Estimation plus pr√©cise, temps de calcul acceptable\")\n",
    "print(\"   ‚Ä¢ 20 folds : Tr√®s pr√©cis, mais co√ªteux en temps (et variance peut augmenter)\")\n",
    "print(\"\")\n",
    "print(\"   R√®gle g√©n√©rale :\")\n",
    "print(\"   - Dataset petit (<1000 exemples) : 10-fold ou LOOCV\")\n",
    "print(\"   - Dataset moyen : 5-fold (standard)\")\n",
    "print(\"   - Dataset large (>100k exemples) : 3-fold ou simple split suffit\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"üí° CONSEIL FINAL :\")\n",
    "print(\"   Utilisez TOUJOURS la validation crois√©e pour √©valuer vos mod√®les.\")\n",
    "print(\"   Le simple train/test split ne devrait √™tre utilis√© que pour :\")\n",
    "print(\"   - Des datasets tr√®s larges (>1M d'exemples)\")\n",
    "print(\"   - Des prototypes rapides\")\n",
    "print(\"   - Le set de test FINAL (apr√®s s√©lection du mod√®le)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 5 : Cas Pratique - Syst√®me de Recommandation de Films\n",
    "\n",
    "**Objectif** : Choisir les bonnes m√©triques selon le contexte m√©tier.\n",
    "\n",
    "**Contexte** : Vous d√©veloppez un syst√®me de recommandation de films. Vous avez deux versions du mod√®le avec les r√©sultats suivants sur 1000 utilisateurs :\n",
    "\n",
    "**Mod√®le A** :\n",
    "- Films recommand√©s et regard√©s (TP) : 400\n",
    "- Films recommand√©s mais non regard√©s (FP) : 100\n",
    "- Films non recommand√©s mais regard√©s (FN) : 200\n",
    "- Films non recommand√©s et non regard√©s (TN) : 300\n",
    "\n",
    "**Mod√®le B** :\n",
    "- Films recommand√©s et regard√©s (TP) : 500\n",
    "- Films recommand√©s mais non regard√©s (FP) : 300\n",
    "- Films non recommand√©s mais regard√©s (FN) : 100\n",
    "- Films non recommand√©s et non regard√©s (TN) : 100\n",
    "\n",
    "**Questions** :\n",
    "\n",
    "1. Calculez pour chaque mod√®le : Accuracy, Precision, Recall, F1-Score\n",
    "2. Quel mod√®le choisiriez-vous si :\n",
    "   - Vous voulez √©viter de recommander de mauvais films (utilisateur satisfait) ?\n",
    "   - Vous voulez maximiser la d√©couverte de films (ne pas manquer de bons films) ?\n",
    "   - Vous voulez un compromis √©quilibr√© ?\n",
    "3. Quelle m√©trique est la plus pertinente pour un syst√®me de recommandation ? Justifiez.\n",
    "4. Cr√©ez une visualisation comparative des deux mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# 1. D√©finissez les matrices de confusion\n",
    "models_data = {\n",
    "    'Mod√®le A': {'TP': 400, 'FP': 100, 'FN': 200, 'TN': 300},\n",
    "    'Mod√®le B': {'TP': 500, 'FP': 300, 'FN': 100, 'TN': 100}\n",
    "}\n",
    "\n",
    "# Calculer les m√©triques pour chaque mod√®le\n",
    "results = []\n",
    "\n",
    "for model_name, data in models_data.items():\n",
    "    TP, FP, FN, TN = data['TP'], data['FP'], data['FN'], data['TN']\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = np.array([[TN, FP], [FN, TP]])\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'TN': TN,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'CM': cm\n",
    "    })\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"=\"*70)\n",
    "print(\"1. M√âTRIQUES CALCUL√âES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "comparison_df = pd.DataFrame(results)[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'TP', 'FP', 'FN', 'TN']]\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\")\n",
    "\n",
    "# 2. Analyse selon les objectifs m√©tier\n",
    "print(\"=\"*70)\n",
    "print(\"2. CHOIX DU MOD√àLE SELON L'OBJECTIF M√âTIER\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "print(\"‚ù∂ √âviter de recommander de MAUVAIS films (utilisateur satisfait) :\")\n",
    "print(\"\")\n",
    "print(\"   ‚Üí M√©trique cl√© : PRECISION\")\n",
    "print(\"   ‚Üí Objectif : Minimiser les FP (recommandations non pertinentes)\")\n",
    "print(\"\")\n",
    "best_precision_idx = comparison_df['Precision'].idxmax()\n",
    "best_precision_model = comparison_df.loc[best_precision_idx, 'Model']\n",
    "best_precision_value = comparison_df.loc[best_precision_idx, 'Precision']\n",
    "print(f\"   ‚úÖ CHOIX : {best_precision_model}\")\n",
    "print(f\"   Precision = {best_precision_value:.1%}\")\n",
    "print(f\"   ‚Üí {best_precision_value*100:.0f}% des films recommand√©s sont effectivement regard√©s\")\n",
    "print(\"   ‚Üí Moins de frustration utilisateur\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"‚ù∑ Maximiser la D√âCOUVERTE de films (ne pas manquer de bons films) :\")\n",
    "print(\"\")\n",
    "print(\"   ‚Üí M√©trique cl√© : RECALL\")\n",
    "print(\"   ‚Üí Objectif : Minimiser les FN (opportunit√©s manqu√©es)\")\n",
    "print(\"\")\n",
    "best_recall_idx = comparison_df['Recall'].idxmax()\n",
    "best_recall_model = comparison_df.loc[best_recall_idx, 'Model']\n",
    "best_recall_value = comparison_df.loc[best_recall_idx, 'Recall']\n",
    "print(f\"   ‚úÖ CHOIX : {best_recall_model}\")\n",
    "print(f\"   Recall = {best_recall_value:.1%}\")\n",
    "print(f\"   ‚Üí {best_recall_value*100:.0f}% des films potentiellement int√©ressants sont recommand√©s\")\n",
    "print(\"   ‚Üí Maximise l'engagement utilisateur\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"‚ù∏ Compromis √âQUILIBR√â :\")\n",
    "print(\"\")\n",
    "print(\"   ‚Üí M√©trique cl√© : F1-SCORE\")\n",
    "print(\"   ‚Üí Objectif : Balance entre Precision et Recall\")\n",
    "print(\"\")\n",
    "best_f1_idx = comparison_df['F1-Score'].idxmax()\n",
    "best_f1_model = comparison_df.loc[best_f1_idx, 'Model']\n",
    "best_f1_value = comparison_df.loc[best_f1_idx, 'F1-Score']\n",
    "print(f\"   ‚úÖ CHOIX : {best_f1_model}\")\n",
    "print(f\"   F1-Score = {best_f1_value:.3f}\")\n",
    "print(\"   ‚Üí Meilleur √©quilibre qualit√©/quantit√© des recommandations\")\n",
    "print(\"\")\n",
    "\n",
    "# 3. M√©trique la plus pertinente\n",
    "print(\"=\"*70)\n",
    "print(\"3. M√âTRIQUE LA PLUS PERTINENTE POUR LA RECOMMANDATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "print(\"   ‚Üí RECALL est g√©n√©ralement PLUS IMPORTANT en recommandation !\")\n",
    "print(\"\")\n",
    "print(\"   Justification :\")\n",
    "print(\"   üìå Objectif principal : Maximiser l'engagement et la satisfaction\")\n",
    "print(\"   üìå Il vaut mieux recommander plus (quitte √† avoir quelques rat√©s)\")\n",
    "print(\"      que de manquer des films que l'utilisateur aurait ador√©\")\n",
    "print(\"   üìå Un FP (recommandation non regard√©e) est moins grave qu'un FN (opportunit√© manqu√©e)\")\n",
    "print(\"   üìå L'utilisateur peut facilement ignorer une recommandation\")\n",
    "print(\"   üìå Mais il ne saura jamais ce qu'il a manqu√© (FN)\")\n",
    "print(\"\")\n",
    "print(\"   ‚ö†Ô∏è  MAIS : Trop de FP peut nuire √† la confiance dans le syst√®me\")\n",
    "print(\"   ‚Üí Solution : Trouver le bon compromis via F1-Score ou F2-Score\")\n",
    "print(\"   ‚Üí F2-Score = met plus de poids sur le Recall que la Precision\")\n",
    "print(\"\")\n",
    "\n",
    "# Calculer F2-Score\n",
    "for result in results:\n",
    "    precision = result['Precision']\n",
    "    recall = result['Recall']\n",
    "    beta = 2\n",
    "    f2 = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall) if (precision + recall) > 0 else 0\n",
    "    result['F2-Score'] = f2\n",
    "\n",
    "print(\"   Comparaison F1 vs F2 (F2 favorise le Recall) :\")\n",
    "for result in results:\n",
    "    print(f\"   {result['Model']} : F1={result['F1-Score']:.3f}, F2={result['F2-Score']:.3f}\")\n",
    "print(\"\")\n",
    "\n",
    "# 4. Visualisations\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# a) Matrices de confusion c√¥te √† c√¥te\n",
    "for idx, result in enumerate(results):\n",
    "    ax = fig.add_subplot(gs[0, idx])\n",
    "    cm = result['CM']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Non Regard√©', 'Regard√©'],\n",
    "                yticklabels=['Non Recommand√©', 'Recommand√©'])\n",
    "    ax.set_xlabel('R√©alit√©')\n",
    "    ax.set_ylabel('Recommandation')\n",
    "    ax.set_title(f'{result[\"Model\"]} - Matrice de Confusion')\n",
    "\n",
    "# b) Comparaison des m√©triques principales\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    values = [result[m] for m in metrics_to_plot]\n",
    "    ax3.bar(x + i*width, values, width, label=result['Model'], alpha=0.8)\n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for j, v in enumerate(values):\n",
    "        ax3.text(x[j] + i*width, v + 0.02, f'{v:.2f}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax3.set_xlabel('M√©triques', fontsize=12)\n",
    "ax3.set_ylabel('Score', fontsize=12)\n",
    "ax3.set_title('Comparaison des M√©triques de Performance', fontsize=14)\n",
    "ax3.set_xticks(x + width / 2)\n",
    "ax3.set_xticklabels(metrics_to_plot)\n",
    "ax3.legend(fontsize=11)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "ax3.set_ylim(0, 1.1)\n",
    "\n",
    "# c) Radar chart pour visualisation multidimensionnelle\n",
    "ax4 = fig.add_subplot(gs[2, :], projection='polar')\n",
    "\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "N = len(categories)\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "for result in results:\n",
    "    values = [result[m] for m in categories]\n",
    "    values += values[:1]\n",
    "    ax4.plot(angles, values, 'o-', linewidth=2, label=result['Model'])\n",
    "    ax4.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax4.set_xticks(angles[:-1])\n",
    "ax4.set_xticklabels(categories)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.set_title('Comparaison Multi-M√©triques (Radar Chart)', fontsize=14, pad=20)\n",
    "ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Tableau r√©capitulatif des recommandations\n",
    "print(\"=\"*70)\n",
    "print(\"RECOMMANDATION FINALE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "print(f\"Pour un syst√®me de recommandation de films, nous recommandons : {best_recall_model}\")\n",
    "print(\"\")\n",
    "print(\"Raisons :\")\n",
    "print(f\"  ‚úì Recall √©lev√© ({best_recall_value:.1%}) : Capture la majorit√© des films pertinents\")\n",
    "print(\"  ‚úì Maximise l'engagement utilisateur\")\n",
    "print(\"  ‚úì R√©duit les opportunit√©s manqu√©es\")\n",
    "print(\"\")\n",
    "print(\"Trade-offs accept√©s :\")\n",
    "modelB_precision = comparison_df.loc[comparison_df['Model'] == best_recall_model, 'Precision'].values[0]\n",
    "print(f\"  ‚Ä¢ Precision l√©g√®rement plus basse ({modelB_precision:.1%})\")\n",
    "print(\"  ‚Ä¢ Mais acceptable car les utilisateurs peuvent ignorer les recommandations non pertinentes\")\n",
    "print(\"\")\n",
    "print(\"üí° Pour am√©liorer encore :\")\n",
    "print(\"  1. Impl√©menter un syst√®me de ranking (ne pas montrer toutes les recommandations)\")\n",
    "print(\"  2. Utiliser un feedback utilisateur pour ajuster le seuil\")\n",
    "print(\"  3. Segmenter les utilisateurs (certains pr√©f√®rent qualit√©, d'autres quantit√©)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 6 : Dataset D√©s√©quilibr√©\n",
    "\n",
    "**Objectif** : Comprendre l'impact du d√©s√©quilibre des classes sur les m√©triques.\n",
    "\n",
    "**Consignes** :\n",
    "\n",
    "1. Cr√©ez un dataset d√©s√©quilibr√© synth√©tique :\n",
    "   - Classe 0 (majoritaire) : 950 exemples\n",
    "   - Classe 1 (minoritaire) : 50 exemples\n",
    "2. Entra√Ænez un mod√®le simple (LogisticRegression)\n",
    "3. Calculez et comparez :\n",
    "   - Accuracy\n",
    "   - Precision, Recall, F1 pour chaque classe\n",
    "   - Courbe ROC et AUC\n",
    "   - Courbe Precision-Recall\n",
    "4. Que se passe-t-il si le mod√®le pr√©dit toujours la classe majoritaire ?\n",
    "5. Quelle m√©trique est la plus r√©v√©latrice du probl√®me ?\n",
    "6. **Bonus** : Testez une technique de r√©√©quilibrage (SMOTE ou class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Cr√©ez un dataset d√©s√©quilibr√©\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    weights=[0.95, 0.05],  # 95% classe 0, 5% classe 1\n",
    "    random_state=42,\n",
    "    flip_y=0.01  # L√©g√®re noise\n",
    ")\n",
    "\n",
    "print(\"Dataset d√©s√©quilibr√© cr√©√© :\")\n",
    "print(f\"  Total √©chantillons : {len(y)}\")\n",
    "print(f\"  Distribution : {np.bincount(y)}\")\n",
    "print(f\"  Ratio : {np.bincount(y)[0]/len(y):.1%} classe 0, {np.bincount(y)[1]/len(y):.1%} classe 1\")\n",
    "print(\"\")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Distribution train : {np.bincount(y_train)}\")\n",
    "print(f\"Distribution test : {np.bincount(y_test)}\")\n",
    "print(\"\")\n",
    "\n",
    "# 2. Entra√Ænez un mod√®le simple sans traitement du d√©s√©quilibre\n",
    "print(\"=\"*70)\n",
    "print(\"MOD√àLE 1 : SANS TRAITEMENT DU D√âS√âQUILIBRE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "model_imbalanced = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_imbalanced.fit(X_train, y_train)\n",
    "y_pred_imbalanced = model_imbalanced.predict(X_test)\n",
    "y_proba_imbalanced = model_imbalanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 3. Calculez les m√©triques\n",
    "accuracy_imb = accuracy_score(y_test, y_pred_imbalanced)\n",
    "cm_imb = confusion_matrix(y_test, y_pred_imbalanced)\n",
    "\n",
    "print(\"Matrice de confusion :\")\n",
    "print(cm_imb)\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Accuracy : {accuracy_imb:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Classification report d√©taill√©\n",
    "print(\"Classification Report :\")\n",
    "print(classification_report(y_test, y_pred_imbalanced, target_names=['Classe 0 (maj)', 'Classe 1 (min)']))\n",
    "\n",
    "# 4. Mod√®le na√Øf : toujours pr√©dire la classe majoritaire\n",
    "print(\"=\"*70)\n",
    "print(\"MOD√àLE NA√èF : TOUJOURS PR√âDIRE CLASSE 0 (MAJORITAIRE)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "y_pred_naive = np.zeros_like(y_test)  # Toujours pr√©dire 0\n",
    "accuracy_naive = accuracy_score(y_test, y_pred_naive)\n",
    "cm_naive = confusion_matrix(y_test, y_pred_naive)\n",
    "\n",
    "print(\"Matrice de confusion :\")\n",
    "print(cm_naive)\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Accuracy : {accuracy_naive:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"‚ö†Ô∏è  OBSERVATION CRITIQUE :\")\n",
    "print(f\"   Un mod√®le qui pr√©dit TOUJOURS la classe majoritaire obtient {accuracy_naive:.1%} d'accuracy !\")\n",
    "print(\"   ‚Üí L'accuracy est une m√©trique TROMPEUSE pour les datasets d√©s√©quilibr√©s\")\n",
    "print(\"   ‚Üí Le mod√®le n'a RIEN appris, mais semble performant\")\n",
    "print(\"\")\n",
    "\n",
    "# 5. Courbes ROC et Precision-Recall\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_imbalanced)\n",
    "roc_auc = roc_auc_score(y_test, y_proba_imbalanced)\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC={roc_auc:.3f})')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Chance')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Courbe ROC')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_proba_imbalanced)\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(recall_vals, precision_vals, linewidth=2, color='orange')\n",
    "baseline = np.sum(y_test) / len(y_test)  # Proportion de la classe minoritaire\n",
    "ax2.axhline(baseline, color='k', linestyle='--', linewidth=1, label=f'Baseline ({baseline:.3f})')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Courbe Precision-Recall')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Matrice de confusion\n",
    "ax3 = axes[0, 2]\n",
    "sns.heatmap(cm_imb, annot=True, fmt='d', cmap='Blues', ax=ax3,\n",
    "            xticklabels=['Classe 0', 'Classe 1'],\n",
    "            yticklabels=['Classe 0', 'Classe 1'])\n",
    "ax3.set_xlabel('Pr√©diction')\n",
    "ax3.set_ylabel('R√©alit√©')\n",
    "ax3.set_title('Matrice de Confusion - Mod√®le d√©s√©quilibr√©')\n",
    "\n",
    "# 6. BONUS : Technique de r√©√©quilibrage avec class_weight\n",
    "print(\"=\"*70)\n",
    "print(\"MOD√àLE 2 : AVEC R√â√âQUILIBRAGE (class_weight='balanced')\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "model_balanced = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "model_balanced.fit(X_train, y_train)\n",
    "y_pred_balanced = model_balanced.predict(X_test)\n",
    "y_proba_balanced = model_balanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy_bal = accuracy_score(y_test, y_pred_balanced)\n",
    "cm_bal = confusion_matrix(y_test, y_pred_balanced)\n",
    "\n",
    "print(\"Matrice de confusion :\")\n",
    "print(cm_bal)\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Accuracy : {accuracy_bal:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report :\")\n",
    "print(classification_report(y_test, y_pred_balanced, target_names=['Classe 0 (maj)', 'Classe 1 (min)']))\n",
    "\n",
    "# Courbes pour le mod√®le balanced\n",
    "fpr_bal, tpr_bal, _ = roc_curve(y_test, y_proba_balanced)\n",
    "roc_auc_bal = roc_auc_score(y_test, y_proba_balanced)\n",
    "\n",
    "ax4 = axes[1, 0]\n",
    "ax4.plot(fpr, tpr, linewidth=2, label=f'Sans √©quilibrage (AUC={roc_auc:.3f})', alpha=0.7)\n",
    "ax4.plot(fpr_bal, tpr_bal, linewidth=2, label=f'Avec √©quilibrage (AUC={roc_auc_bal:.3f})')\n",
    "ax4.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax4.set_xlabel('False Positive Rate')\n",
    "ax4.set_ylabel('True Positive Rate')\n",
    "ax4.set_title('Comparaison ROC')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "precision_vals_bal, recall_vals_bal, _ = precision_recall_curve(y_test, y_proba_balanced)\n",
    "\n",
    "ax5 = axes[1, 1]\n",
    "ax5.plot(recall_vals, precision_vals, linewidth=2, label='Sans √©quilibrage', alpha=0.7, color='orange')\n",
    "ax5.plot(recall_vals_bal, precision_vals_bal, linewidth=2, label='Avec √©quilibrage', color='green')\n",
    "ax5.axhline(baseline, color='k', linestyle='--', linewidth=1, label=f'Baseline ({baseline:.3f})')\n",
    "ax5.set_xlabel('Recall')\n",
    "ax5.set_ylabel('Precision')\n",
    "ax5.set_title('Comparaison Precision-Recall')\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "ax6 = axes[1, 2]\n",
    "sns.heatmap(cm_bal, annot=True, fmt='d', cmap='Greens', ax=ax6,\n",
    "            xticklabels=['Classe 0', 'Classe 1'],\n",
    "            yticklabels=['Classe 0', 'Classe 1'])\n",
    "ax6.set_xlabel('Pr√©diction')\n",
    "ax6.set_ylabel('R√©alit√©')\n",
    "ax6.set_title('Matrice de Confusion - Mod√®le √©quilibr√©')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparaison finale\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSE COMPARATIVE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "# Extraire les m√©triques pour la classe minoritaire\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "prec_imb, rec_imb, f1_imb, _ = precision_recall_fscore_support(y_test, y_pred_imbalanced, average=None)\n",
    "prec_bal, rec_bal, f1_bal, _ = precision_recall_fscore_support(y_test, y_pred_balanced, average=None)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'M√©trique': ['Accuracy', 'Precision (classe 1)', 'Recall (classe 1)', 'F1-Score (classe 1)', 'ROC AUC'],\n",
    "    'Sans √©quilibrage': [accuracy_imb, prec_imb[1], rec_imb[1], f1_imb[1], roc_auc],\n",
    "    'Avec √©quilibrage': [accuracy_bal, prec_bal[1], rec_bal[1], f1_bal[1], roc_auc_bal],\n",
    "    'Mod√®le na√Øf': [accuracy_naive, 0, 0, 0, 0.5]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\")\n",
    "\n",
    "print(\"OBSERVATIONS CL√âS :\")\n",
    "print(\"\")\n",
    "print(\"1Ô∏è‚É£  L'Accuracy est TROMPEUSE :\")\n",
    "print(f\"   - Mod√®le na√Øf : {accuracy_naive:.1%} (pr√©dit toujours classe 0)\")\n",
    "print(f\"   - Sans √©quilibrage : {accuracy_imb:.1%}\")\n",
    "print(f\"   - Avec √©quilibrage : {accuracy_bal:.1%} (semble pire, mais plus informatif !)\")\n",
    "print(\"\")\n",
    "print(\"2Ô∏è‚É£  Le Recall de la classe minoritaire est CRITIQUE :\")\n",
    "print(f\"   - Sans √©quilibrage : {rec_imb[1]:.1%} (manque beaucoup d'exemples)\")\n",
    "print(f\"   - Avec √©quilibrage : {rec_bal[1]:.1%} (bien meilleur !)\")\n",
    "print(\"\")\n",
    "print(\"3Ô∏è‚É£  La courbe Precision-Recall est PLUS INFORMATIVE que ROC :\")\n",
    "print(\"   - ROC peut √™tre optimiste sur datasets d√©s√©quilibr√©s\")\n",
    "print(\"   - Precision-Recall montre mieux les performances sur la classe minoritaire\")\n",
    "print(\"\")\n",
    "print(\"4Ô∏è‚É£  Le F1-Score de la classe minoritaire est le meilleur indicateur :\")\n",
    "print(f\"   - Sans √©quilibrage : {f1_imb[1]:.3f}\")\n",
    "print(f\"   - Avec √©quilibrage : {f1_bal[1]:.3f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"üí° RECOMMANDATIONS pour datasets d√©s√©quilibr√©s :\")\n",
    "print(\"\")\n",
    "print(\"1. NE PAS utiliser l'Accuracy seule\")\n",
    "print(\"2. Privil√©gier : Precision, Recall, F1-Score de la classe minoritaire\")\n",
    "print(\"3. Utiliser la courbe Precision-Recall plut√¥t que ROC\")\n",
    "print(\"4. Techniques de r√©√©quilibrage :\")\n",
    "print(\"   - class_weight='balanced' (simple et efficace)\")\n",
    "print(\"   - SMOTE (sur-√©chantillonnage synth√©tique)\")\n",
    "print(\"   - Sous-√©chantillonnage de la classe majoritaire\")\n",
    "print(\"   - Ensembles avec r√©√©quilibrage (BalancedRandomForest)\")\n",
    "print(\"5. Ajuster le seuil de d√©cision selon le co√ªt m√©tier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 7 : Bonus - M√©triques Personnalis√©es\n",
    "\n",
    "**Objectif** : Cr√©er une m√©trique personnalis√©e adapt√©e √† un contexte sp√©cifique.\n",
    "\n",
    "**Contexte** : Une entreprise de e-commerce veut minimiser les retours produits. Un retour produit co√ªte 20‚Ç¨ √† l'entreprise, tandis qu'une vente r√©ussie rapporte 10‚Ç¨.\n",
    "\n",
    "**Consignes** :\n",
    "\n",
    "1. Cr√©ez une fonction de co√ªt personnalis√©e qui calcule le b√©n√©fice/perte total\n",
    "2. Utilisez cette fonction pour √©valuer plusieurs mod√®les\n",
    "3. Comparez avec les m√©triques classiques (accuracy, F1)\n",
    "4. Quel mod√®le choisiriez-vous selon la m√©trique de co√ªt ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "def custom_cost_metric(y_true, y_pred, profit_per_sale=10, cost_per_return=20):\n",
    "    \"\"\"\n",
    "    Calcule le b√©n√©fice total selon un mod√®le m√©tier.\n",
    "    \n",
    "    Matrice de co√ªt :\n",
    "    - TP (vente r√©ussie, bien pr√©dite) : +10‚Ç¨\n",
    "    - TN (pas de vente, bien pr√©dite) : 0‚Ç¨\n",
    "    - FP (vente avec retour non anticip√©) : -20‚Ç¨ (le pire cas !)\n",
    "    - FN (vente manqu√©e, opportunit√© perdue) : 0‚Ç¨ (pas de perte directe)\n",
    "    \n",
    "    Args:\n",
    "        y_true : Labels r√©els (1 = vente r√©ussie, 0 = retour)\n",
    "        y_pred : Pr√©dictions (1 = recommander, 0 = ne pas recommander)\n",
    "        profit_per_sale : B√©n√©fice par vente r√©ussie\n",
    "        cost_per_return : Co√ªt d'un retour produit\n",
    "    \n",
    "    Returns:\n",
    "        total_profit : B√©n√©fice total en ‚Ç¨\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    # Calcul du profit total\n",
    "    profit_from_tp = TP * profit_per_sale  # Ventes r√©ussies bien pr√©dites\n",
    "    cost_from_fp = FP * cost_per_return    # Retours non anticip√©s (co√ªt √©lev√©)\n",
    "    \n",
    "    total_profit = profit_from_tp - cost_from_fp\n",
    "    \n",
    "    return total_profit, {'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN}\n",
    "\n",
    "# Cr√©er un dataset synth√©tique pour le e-commerce\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=7,\n",
    "    n_redundant=2,\n",
    "    weights=[0.3, 0.7],  # 70% de ventes r√©ussies, 30% de retours\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Dataset E-Commerce :\")\n",
    "print(f\"  Total : {len(y)} transactions\")\n",
    "print(f\"  Distribution : {np.bincount(y)[0]} retours, {np.bincount(y)[1]} ventes r√©ussies\")\n",
    "print(f\"  Taux de retour : {np.bincount(y)[0]/len(y):.1%}\")\n",
    "print(\"\")\n",
    "\n",
    "# Tester diff√©rents mod√®les avec diff√©rents hyperparam√®tres\n",
    "models_configs = [\n",
    "    ('Logistic (Conservative)', LogisticRegression(C=0.1, random_state=42)),\n",
    "    ('Logistic (Standard)', LogisticRegression(C=1.0, random_state=42)),\n",
    "    ('Logistic (Aggressive)', LogisticRegression(C=10.0, random_state=42)),\n",
    "    ('RF (Conservative)', RandomForestClassifier(n_estimators=50, max_depth=3, random_state=42)),\n",
    "    ('RF (Standard)', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"√âVALUATION DES MOD√àLES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "for name, model in models_configs:\n",
    "    # Entra√Æner\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # M√©triques classiques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # M√©trique personnalis√©e\n",
    "    profit, components = custom_cost_metric(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Profit (‚Ç¨)': profit,\n",
    "        'TP': components['TP'],\n",
    "        'FP': components['FP'],\n",
    "        'FN': components['FN'],\n",
    "        'TN': components['TN']\n",
    "    })\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Profit (‚Ç¨)']].to_string(index=False))\n",
    "print(\"\")\n",
    "\n",
    "# Identifier le meilleur selon diff√©rents crit√®res\n",
    "best_accuracy_idx = results_df['Accuracy'].idxmax()\n",
    "best_f1_idx = results_df['F1-Score'].idxmax()\n",
    "best_profit_idx = results_df['Profit (‚Ç¨)'].idxmax()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARAISON DES CRIT√àRES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "\n",
    "print(f\"üéØ Meilleur Accuracy : {results_df.loc[best_accuracy_idx, 'Model']}\")\n",
    "print(f\"   Accuracy = {results_df.loc[best_accuracy_idx, 'Accuracy']:.3f}\")\n",
    "print(f\"   Profit = {results_df.loc[best_accuracy_idx, 'Profit (‚Ç¨)']:.0f}‚Ç¨\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"üéØ Meilleur F1-Score : {results_df.loc[best_f1_idx, 'Model']}\")\n",
    "print(f\"   F1-Score = {results_df.loc[best_f1_idx, 'F1-Score']:.3f}\")\n",
    "print(f\"   Profit = {results_df.loc[best_f1_idx, 'Profit (‚Ç¨)']:.0f}‚Ç¨\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"üí∞ MEILLEUR PROFIT (M√âTRIQUE M√âTIER) : {results_df.loc[best_profit_idx, 'Model']}\")\n",
    "print(f\"   Profit = {results_df.loc[best_profit_idx, 'Profit (‚Ç¨)']:.0f}‚Ç¨\")\n",
    "print(f\"   Accuracy = {results_df.loc[best_profit_idx, 'Accuracy']:.3f}\")\n",
    "print(f\"   F1-Score = {results_df.loc[best_profit_idx, 'F1-Score']:.3f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Analyse d√©taill√©e du meilleur mod√®le selon le profit\n",
    "best_model = results_df.loc[best_profit_idx]\n",
    "print(f\"D√©tails du mod√®le optimal ({best_model['Model']}) :\")\n",
    "print(f\"   TP (ventes r√©ussies d√©tect√©es) : {best_model['TP']:.0f} ‚Üí +{best_model['TP']*10:.0f}‚Ç¨\")\n",
    "print(f\"   FP (retours non anticip√©s) : {best_model['FP']:.0f} ‚Üí -{best_model['FP']*20:.0f}‚Ç¨\")\n",
    "print(f\"   FN (opportunit√©s manqu√©es) : {best_model['FN']:.0f} ‚Üí 0‚Ç¨\")\n",
    "print(f\"   TN (retours bien √©vit√©s) : {best_model['TN']:.0f} ‚Üí 0‚Ç¨\")\n",
    "print(f\"   PROFIT TOTAL : {best_model['Profit (‚Ç¨)']:.0f}‚Ç¨\")\n",
    "print(\"\")\n",
    "\n",
    "# Visualisations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# a) Comparaison Profit vs M√©triques classiques\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(results_df))\n",
    "ax1.bar(x, results_df['Profit (‚Ç¨)'], alpha=0.7, edgecolor='black')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax1.set_ylabel('Profit (‚Ç¨)')\n",
    "ax1.set_title('Profit Total par Mod√®le')\n",
    "ax1.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for i, v in enumerate(results_df['Profit (‚Ç¨)']):\n",
    "    ax1.text(i, v + 50, f'{v:.0f}‚Ç¨', ha='center', fontweight='bold')\n",
    "\n",
    "# b) Profit vs F1-Score (scatter)\n",
    "ax2 = axes[0, 1]\n",
    "scatter = ax2.scatter(results_df['F1-Score'], results_df['Profit (‚Ç¨)'],\n",
    "                      s=100, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "ax2.set_xlabel('F1-Score')\n",
    "ax2.set_ylabel('Profit (‚Ç¨)')\n",
    "ax2.set_title('Profit vs F1-Score')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Annoter les points\n",
    "for idx, row in results_df.iterrows():\n",
    "    ax2.annotate(row['Model'], (row['F1-Score'], row['Profit (‚Ç¨)']),\n",
    "                fontsize=8, alpha=0.7)\n",
    "\n",
    "# c) Comparaison multi-m√©triques\n",
    "ax3 = axes[1, 0]\n",
    "metrics_to_compare = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x_pos = np.arange(len(results_df))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics_to_compare):\n",
    "    ax3.bar(x_pos + i*width, results_df[metric], width, label=metric, alpha=0.8)\n",
    "\n",
    "ax3.set_xticks(x_pos + width * 1.5)\n",
    "ax3.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_title('M√©triques Classiques par Mod√®le')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# d) Heatmap de corr√©lation entre m√©triques et profit\n",
    "ax4 = axes[1, 1]\n",
    "corr_data = results_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Profit (‚Ç¨)']].corr()\n",
    "sns.heatmap(corr_data, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax4,\n",
    "            vmin=-1, vmax=1, square=True, linewidths=1)\n",
    "ax4.set_title('Corr√©lation entre M√©triques et Profit')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "print(\"üîç OBSERVATIONS IMPORTANTES :\")\n",
    "print(\"\")\n",
    "print(\"1. Le mod√®le avec le meilleur F1-Score N'EST PAS forc√©ment celui avec le meilleur profit\")\n",
    "print(\"\")\n",
    "print(\"2. La m√©trique m√©tier (profit) refl√®te mieux les objectifs business :\")\n",
    "print(\"   - Minimiser les FP (retours non d√©tect√©s) est CRITIQUE (co√ªt -20‚Ç¨)\")\n",
    "print(\"   - Les FN (opportunit√©s manqu√©es) ont moins d'impact (co√ªt 0‚Ç¨)\")\n",
    "print(\"   ‚Üí Un mod√®le plus conservateur (haute Precision) peut √™tre pr√©f√©rable\")\n",
    "print(\"\")\n",
    "print(\"3. Les m√©triques classiques supposent des co√ªts √©gaux pour FP et FN\")\n",
    "print(\"   ‚Üí Inadapt√© quand les erreurs ont des co√ªts asym√©triques\")\n",
    "print(\"\")\n",
    "print(\"üí° RECOMMANDATIONS :\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ TOUJOURS d√©finir une m√©trique align√©e avec les objectifs m√©tier\")\n",
    "print(\"‚úÖ Quantifier les co√ªts r√©els de chaque type d'erreur (FP vs FN)\")\n",
    "print(\"‚úÖ Optimiser directement cette m√©trique m√©tier\")\n",
    "print(\"‚úÖ Utiliser les m√©triques classiques pour le diagnostic, pas la d√©cision finale\")\n",
    "print(\"\")\n",
    "print(f\"üèÜ D√âCISION FINALE : Choisir {results_df.loc[best_profit_idx, 'Model']}\")\n",
    "print(f\"   ‚Üí Maximise le profit : {results_df.loc[best_profit_idx, 'Profit (‚Ç¨)']:.0f}‚Ç¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## R√©sum√© et Points Cl√©s\n",
    "\n",
    "√Ä la fin de ces exercices, vous devriez √™tre capable de :\n",
    "\n",
    "‚úÖ Calculer et interpr√©ter toutes les m√©triques de classification\n",
    "\n",
    "‚úÖ Comprendre le compromis Precision vs Recall\n",
    "\n",
    "‚úÖ Utiliser la courbe ROC pour choisir un seuil optimal\n",
    "\n",
    "‚úÖ √âvaluer un mod√®le de r√©gression avec MSE, MAE, R¬≤\n",
    "\n",
    "‚úÖ Appliquer la validation crois√©e pour une √©valuation robuste\n",
    "\n",
    "‚úÖ Adapter votre choix de m√©triques au contexte m√©tier\n",
    "\n",
    "‚úÖ G√©rer les datasets d√©s√©quilibr√©s\n",
    "\n",
    "---\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "- Chapitre 03 : R√©gression Lin√©aire et R√©gularisation\n",
    "- Chapitre 04 : Classification Supervis√©e Avanc√©e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
