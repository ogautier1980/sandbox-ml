% Chapitre 02 - M√©triques d'√âvaluation
% Cours Machine Learning - Sandbox-ML

\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}

% Math√©matiques
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

% Mise en page
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}

% Graphiques et couleurs
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, matrix, fit}

% Tableaux
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}

% Code et algorithmes
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Hyperliens
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 02 - M√©triques d'√âvaluation},
    pdfauthor={Cours ML},
}

% Boxes color√©es
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}

% En-t√™tes et pieds de page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 02 - M√©triques d'√âvaluation}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIGURATION LISTINGS (code Python) =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}
\lstset{style=pythonstyle}

% ===== CONFIGURATION TCOLORBOX =====
% Box pour d√©finitions
\newtcolorbox{definition}[1]{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition: #1,
    breakable
}

% Box pour th√©or√®mes
\newtcolorbox{theoreme}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Th√©or√®me: #1,
    breakable
}

% Box pour exemples
\newtcolorbox{exemple}[1]{
    colback=orange!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=Exemple: #1,
    breakable
}

% Box pour attention/warning
\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö†Ô∏è Attention,
    breakable
}

% Box pour astuce/tips
\newtcolorbox{astuce}{
    colback=yellow!10!white,
    colframe=yellow!75!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

% ===== COMMANDES PERSONNALIS√âES =====
\newcommand{\vect}[1]{\mathbf{#1}}  % Vecteur
\newcommand{\mat}[1]{\mathbf{#1}}   % Matrice
\newcommand{\R}{\mathbb{R}}         % R√©els
\newcommand{\N}{\mathbb{N}}         % Naturels
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}

% ===== D√âBUT DU DOCUMENT =====
\begin{document}

% ===== PAGE DE TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]

    \vspace{1cm}

    {\LARGE Chapitre 02}\\[0.3cm]
    {\LARGE\bfseries M√©triques d'√âvaluation}\\[2cm]

    \vfill

    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Ma√Ætriser les m√©triques de classification (matrice de confusion, accuracy, precision, recall, F1-score, ROC, AUC)
        \item Comprendre les m√©triques de r√©gression (MSE, RMSE, MAE, R¬≤, MAPE)
        \item Savoir choisir la bonne m√©trique selon le probl√®me
        \item Ma√Ætriser les techniques de validation (train/test split, K-fold, stratified K-fold)
        \item Identifier les pi√®ges courants et √©viter l'overfitting
    \end{itemize}
    }

    \vfill

    {\large
    \textbf{Pr√©requis :} Chapitre 01 - Fondamentaux Math√©matiques\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 6-8 heures\\[0.3cm]
    \textbf{Notebooks :} \texttt{02_demo_*.ipynb}
    }

    \vfill

    {\large Cours ML - Sandbox-ML\\
    Version 1.0 - 2026}
\end{titlepage}

% ===== TABLE DES MATI√àRES =====
\tableofcontents
\newpage

% ===== SECTION 1: MOTIVATION =====
\section{Motivation}

L'√©valuation des mod√®les de Machine Learning est une √©tape cruciale qui d√©termine si un mod√®le est pr√™t pour la production. Contrairement √† la programmation traditionnelle o√π on peut v√©rifier la correction d'un programme par des tests unitaires, en ML, un mod√®le n'est jamais parfait √† 100\%.

\begin{exemple}{Le dilemme du diagnostic m√©dical}
Imaginez un syst√®me de d√©tection automatique du cancer √† partir de radiographies :

\textbf{Sc√©nario A :} Le mod√®le d√©tecte 95\% des cancers (excellent !) mais donne 40\% de faux positifs (mauvais !).
\begin{itemize}
    \item \textbf{Cons√©quence :} 40\% des patients sains passent des examens invasifs inutiles
    \item \textbf{Co√ªt :} Anxi√©t√© des patients + surcharge du syst√®me de sant√©
\end{itemize}

\textbf{Sc√©nario B :} Le mod√®le ne donne que 2\% de faux positifs (excellent !) mais ne d√©tecte que 60\% des cancers (catastrophique !).
\begin{itemize}
    \item \textbf{Cons√©quence :} 40\% des malades ne sont pas diagnostiqu√©s
    \item \textbf{Co√ªt :} Vies humaines
\end{itemize}

\textbf{Question cl√© :} Quelle m√©trique unique utiliser pour comparer ces deux mod√®les ? L'accuracy ? La pr√©cision ? Le recall ? Le F1-score ?

\textbf{R√©ponse :} Cela d√©pend du contexte m√©dical et des co√ªts associ√©s. Ce chapitre vous apprendra √† faire ce choix de mani√®re √©clair√©e.
\end{exemple}

\begin{attention}
Une m√©trique unique ne suffit jamais ! Un mod√®le avec 99\% d'accuracy peut √™tre compl√®tement inutile si :
\begin{itemize}
    \item Les classes sont d√©s√©quilibr√©es (ex: 1\% de fraudes bancaires)
    \item Les co√ªts d'erreur sont asym√©triques (faux n√©gatif en m√©decine >> faux positif)
    \item Le mod√®le ne g√©n√©ralise pas (overfitting)
\end{itemize}
\end{attention}

% ===== SECTION 2: M√âTRIQUES DE CLASSIFICATION =====
\section{M√©triques de Classification}

\subsection{Matrice de Confusion}

\begin{definition}{Matrice de Confusion}
La \textbf{matrice de confusion} est un tableau qui d√©crit les performances d'un mod√®le de classification en comparant les pr√©dictions aux vraies valeurs. Pour un probl√®me de classification binaire, elle contient 4 valeurs :
\end{definition}

\begin{table}[h]
\centering
\caption{Matrice de Confusion (Classification Binaire)}
\label{tab:confusion_matrix}
\begin{tabular}{cc|cc}
\toprule
\multicolumn{2}{c|}{} & \multicolumn{2}{c}{\textbf{Pr√©diction}} \\
\multicolumn{2}{c|}{} & Positive & Negative \\
\midrule
\multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{R√©alit√©}}}
& Positive & \cellcolor{green!20}TP (Vrai Positif) & \cellcolor{red!20}FN (Faux N√©gatif) \\
& Negative & \cellcolor{red!20}FP (Faux Positif) & \cellcolor{green!20}TN (Vrai N√©gatif) \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{TP (True Positive)} : Cas positifs correctement class√©s comme positifs
    \item \textbf{TN (True Negative)} : Cas n√©gatifs correctement class√©s comme n√©gatifs
    \item \textbf{FP (False Positive)} : Cas n√©gatifs incorrectement class√©s comme positifs (Erreur de Type I)
    \item \textbf{FN (False Negative)} : Cas positifs incorrectement class√©s comme n√©gatifs (Erreur de Type II)
\end{itemize}

\begin{exemple}{D√©tection de spam}
Sur 1000 emails analys√©s :
\begin{itemize}
    \item 150 vrais spams, 850 emails l√©gitimes
    \item TP = 130 : spams d√©tect√©s correctement
    \item FN = 20 : spams non d√©tect√©s (passent en bo√Æte de r√©ception)
    \item FP = 40 : emails l√©gitimes class√©s comme spam (perdus !)
    \item TN = 810 : emails l√©gitimes correctement class√©s
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{cc|cc|c}
\toprule
\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{\textbf{Pr√©diction}} & \\
\multicolumn{2}{c|}{} & Spam & L√©gitime & Total \\
\midrule
\multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{R√©alit√©}}}
& Spam & \cellcolor{green!20}130 & \cellcolor{red!20}20 & 150 \\
& L√©gitime & \cellcolor{red!20}40 & \cellcolor{green!20}810 & 850 \\
\midrule
& Total & 170 & 830 & 1000 \\
\bottomrule
\end{tabular}
\end{table}
\end{exemple}

\subsection{Accuracy (Exactitude)}

\begin{definition}{Accuracy}
L'\textbf{accuracy} (exactitude) est la proportion de pr√©dictions correctes sur l'ensemble total :
\begin{equation}
    \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}} = \frac{\text{Correct}}{\text{Total}}
\end{equation}
\end{definition}

Valeur : entre 0 et 1 (ou 0\% et 100\%).

\textbf{Pour l'exemple spam :}
\begin{equation}
    \text{Accuracy} = \frac{130 + 810}{1000} = \frac{940}{1000} = 0.94 = 94\%
\end{equation}

\begin{attention}
L'accuracy peut √™tre trompeuse avec des classes d√©s√©quilibr√©es !

\textbf{Exemple :} D√©tection de fraude bancaire (0.1\% de fraudes)
\begin{itemize}
    \item Sur 10 000 transactions : 10 fraudes, 9 990 l√©gitimes
    \item Mod√®le stupide : pr√©dire toujours "l√©gitime"
    \item Accuracy = $\frac{9990}{10000} = 99.9\%$ (excellent en apparence !)
    \item Mais 0\% de fraudes d√©tect√©es (catastrophique !)
\end{itemize}

\textbf{Conclusion :} L'accuracy seule est insuffisante pour des classes d√©s√©quilibr√©es.
\end{attention}

\subsection{Precision (Pr√©cision)}

\begin{definition}{Precision}
La \textbf{precision} (pr√©cision) mesure la proportion de pr√©dictions positives qui sont correctes :
\begin{equation}
    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{\text{TP}}{\text{Total Pr√©dictions Positives}}
\end{equation}
\end{definition}

\textbf{Interpr√©tation :} "Quand le mod√®le pr√©dit 'positif', √† quelle fr√©quence a-t-il raison ?"

\textbf{Pour l'exemple spam :}
\begin{equation}
    \text{Precision} = \frac{130}{130 + 40} = \frac{130}{170} \approx 0.765 = 76.5\%
\end{equation}

Cela signifie que 76.5\% des emails class√©s comme spam sont r√©ellement des spams. Les 23.5\% restants sont des faux positifs (emails l√©gitimes perdus).

\begin{astuce}
Utilisez la \textbf{precision} quand les \textbf{faux positifs sont co√ªteux} :
\begin{itemize}
    \item Filtrage spam : √©viter de perdre des emails importants
    \item Recommandation de produits : √©viter de recommander des produits non pertinents
    \item Publicit√© cibl√©e : √©viter de d√©penser du budget sur de mauvaises cibles
\end{itemize}
\end{astuce}

\subsection{Recall (Rappel / Sensibilit√©)}

\begin{definition}{Recall (Sensibilit√©)}
Le \textbf{recall} (rappel ou sensibilit√©) mesure la proportion de cas positifs r√©els qui sont d√©tect√©s :
\begin{equation}
    \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{\text{TP}}{\text{Total Cas Positifs R√©els}}
\end{equation}
\end{definition}

\textbf{Interpr√©tation :} "Parmi tous les cas positifs r√©els, combien le mod√®le en d√©tecte-t-il ?"

\textbf{Pour l'exemple spam :}
\begin{equation}
    \text{Recall} = \frac{130}{130 + 20} = \frac{130}{150} \approx 0.867 = 86.7\%
\end{equation}

Cela signifie que 86.7\% des spams sont d√©tect√©s. Les 13.3\% restants passent en bo√Æte de r√©ception (faux n√©gatifs).

\begin{astuce}
Utilisez le \textbf{recall} quand les \textbf{faux n√©gatifs sont co√ªteux} :
\begin{itemize}
    \item D√©tection de cancer : manquer un cancer peut √™tre fatal
    \item D√©tection de fraude : manquer une fraude peut co√ªter cher
    \item D√©tection d'intrusion r√©seau : manquer une attaque peut compromettre le syst√®me
\end{itemize}
\end{astuce}

\subsection{Sp√©cificit√©}

\begin{definition}{Sp√©cificit√©}
La \textbf{sp√©cificit√©} mesure la proportion de cas n√©gatifs r√©els qui sont correctement class√©s :
\begin{equation}
    \text{Sp√©cificit√©} = \frac{\text{TN}}{\text{TN} + \text{FP}} = \frac{\text{TN}}{\text{Total Cas N√©gatifs R√©els}}
\end{equation}
\end{definition}

\textbf{Pour l'exemple spam :}
\begin{equation}
    \text{Sp√©cificit√©} = \frac{810}{810 + 40} = \frac{810}{850} \approx 0.953 = 95.3\%
\end{equation}

La sp√©cificit√© est le compl√©ment du taux de faux positifs (FPR) :
\begin{equation}
    \text{FPR} = 1 - \text{Sp√©cificit√©} = \frac{\text{FP}}{\text{TN} + \text{FP}}
\end{equation}

\subsection{F1-Score (Moyenne Harmonique)}

\begin{definition}{F1-Score}
Le \textbf{F1-score} est la moyenne harmonique entre la precision et le recall :
\begin{equation}
    F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}
\end{equation}
\end{definition}

\textbf{Pour l'exemple spam :}
\begin{equation}
    F_1 = 2 \times \frac{0.765 \times 0.867}{0.765 + 0.867} = 2 \times \frac{0.663}{1.632} \approx 0.813 = 81.3\%
\end{equation}

\textbf{Pourquoi la moyenne harmonique ?}

La moyenne harmonique p√©nalise les d√©s√©quilibres :
\begin{itemize}
    \item Si Precision = 100\% et Recall = 10\% : $F_1 \approx 18\%$ (faible !)
    \item Si Precision = 90\% et Recall = 90\% : $F_1 = 90\%$ (bon √©quilibre)
\end{itemize}

En comparaison, la moyenne arithm√©tique donnerait :
\begin{itemize}
    \item $(100\% + 10\%) / 2 = 55\%$ (trop optimiste !)
    \item $(90\% + 90\%) / 2 = 90\%$ (identique)
\end{itemize}

\begin{astuce}
Le F1-score est la m√©trique par d√©faut quand :
\begin{itemize}
    \item Les classes sont d√©s√©quilibr√©es
    \item Precision et Recall sont tous deux importants
    \item Vous cherchez un bon compromis entre FP et FN
\end{itemize}
\end{astuce}

\subsubsection{Variante : F-Beta Score}

Pour donner plus de poids au recall ou √† la precision, on utilise le \textbf{F-beta score} :
\begin{equation}
    F_\beta = (1 + \beta^2) \times \frac{\text{Precision} \times \text{Recall}}{\beta^2 \times \text{Precision} + \text{Recall}}
\end{equation}

\begin{itemize}
    \item $\beta = 1$ : F1-score (poids √©gal)
    \item $\beta = 0.5$ : F0.5-score (favorise la precision)
    \item $\beta = 2$ : F2-score (favorise le recall)
\end{itemize}

\subsection{Courbe ROC et AUC}

\subsubsection{Courbe ROC}

\begin{definition}{Courbe ROC}
La \textbf{courbe ROC} (Receiver Operating Characteristic) repr√©sente le compromis entre le taux de vrais positifs (TPR = Recall) et le taux de faux positifs (FPR) pour diff√©rents seuils de classification.

\textbf{Axes :}
\begin{itemize}
    \item Axe X : FPR (False Positive Rate) = $\frac{\text{FP}}{\text{FP} + \text{TN}} = 1 - \text{Sp√©cificit√©}$
    \item Axe Y : TPR (True Positive Rate) = $\frac{\text{TP}}{\text{TP} + \text{FN}} = \text{Recall}$
\end{itemize}
\end{definition}

La plupart des classifieurs produisent des probabilit√©s (ex: \texttt{predict\_proba()} en scikit-learn). La classification finale d√©pend d'un seuil (g√©n√©ralement 0.5) :
\begin{itemize}
    \item Si $P(\text{classe positive}) \geq 0.5$ : pr√©dire "positif"
    \item Si $P(\text{classe positive}) < 0.5$ : pr√©dire "n√©gatif"
\end{itemize}

La courbe ROC trace TPR vs FPR pour tous les seuils possibles (0 √† 1).

\textbf{Interpr√©tation visuelle :}
\begin{itemize}
    \item \textbf{Classifieur al√©atoire} : ligne diagonale (TPR = FPR)
    \item \textbf{Classifieur parfait} : passe par le point (0, 1) (TPR = 100\%, FPR = 0\%)
    \item \textbf{Meilleur mod√®le} : courbe la plus proche du coin sup√©rieur gauche
\end{itemize}

\subsubsection{AUC (Area Under Curve)}

\begin{definition}{AUC}
L'\textbf{AUC} (Area Under the ROC Curve) mesure l'aire sous la courbe ROC. Elle repr√©sente la probabilit√© qu'un classifieur classe un exemple positif al√©atoire plus haut qu'un exemple n√©gatif al√©atoire.
\begin{equation}
    \text{AUC} \in [0, 1]
\end{equation}
\end{definition}

\textbf{Interpr√©tation :}
\begin{itemize}
    \item AUC = 0.5 : Classifieur al√©atoire (inutile)
    \item AUC = 0.7 : Acceptable
    \item AUC = 0.8 : Bon
    \item AUC = 0.9 : Excellent
    \item AUC = 1.0 : Parfait (rare en pratique, souvent signe d'overfitting)
\end{itemize}

\begin{astuce}
L'AUC est une excellente m√©trique car :
\begin{itemize}
    \item Insensible au seuil de classification
    \item Robuste aux classes d√©s√©quilibr√©es (dans une certaine mesure)
    \item Permet de comparer diff√©rents mod√®les ind√©pendamment du seuil choisi
\end{itemize}
\end{astuce}

\subsection{Courbe Precision-Recall}

\begin{definition}{Courbe Precision-Recall}
La \textbf{courbe Precision-Recall} trace la precision en fonction du recall pour diff√©rents seuils.

\textbf{Axes :}
\begin{itemize}
    \item Axe X : Recall (TPR)
    \item Axe Y : Precision
\end{itemize}
\end{definition}

\textbf{Diff√©rence avec ROC :}
\begin{itemize}
    \item La courbe ROC utilise FPR (sensible aux TN)
    \item La courbe PR utilise Precision (ignore les TN)
\end{itemize}

\begin{astuce}
Pr√©f√©rez la courbe \textbf{Precision-Recall} plut√¥t que ROC quand :
\begin{itemize}
    \item Les classes sont tr√®s d√©s√©quilibr√©es (ex: 1\% de positifs)
    \item Vous vous souciez plus des positifs que des n√©gatifs
    \item Exemples : d√©tection de fraude, d√©tection d'anomalies, recherche d'information
\end{itemize}

\textbf{Raison :} Avec 99\% de n√©gatifs, FPR reste proche de 0 m√™me avec beaucoup de FP, ce qui rend la courbe ROC trop optimiste.
\end{astuce}

\subsection{M√©triques Multi-Classes}

Pour les probl√®mes de classification √† plus de 2 classes, on peut calculer les m√©triques de trois fa√ßons :

\begin{definition}{Moyennes pour Multi-Classes}
\begin{itemize}
    \item \textbf{Macro Average} : moyenne simple des m√©triques de chaque classe
    \begin{equation}
        \text{Precision}_{\text{macro}} = \frac{1}{C} \sum_{i=1}^{C} \text{Precision}_i
    \end{equation}
    Traite toutes les classes de mani√®re √©gale (utile si toutes les classes sont aussi importantes).

    \item \textbf{Micro Average} : calcule les m√©triques globalement (agr√®ge TP, FP, FN)
    \begin{equation}
        \text{Precision}_{\text{micro}} = \frac{\sum_{i=1}^{C} \text{TP}_i}{\sum_{i=1}^{C} (\text{TP}_i + \text{FP}_i)}
    \end{equation}
    Favorise les classes majoritaires (utile si les classes d√©s√©quilibr√©es refl√®tent la r√©alit√©).

    \item \textbf{Weighted Average} : moyenne pond√©r√©e par le nombre d'exemples de chaque classe
    \begin{equation}
        \text{Precision}_{\text{weighted}} = \frac{1}{N} \sum_{i=1}^{C} n_i \times \text{Precision}_i
    \end{equation}
    o√π $n_i$ est le nombre d'exemples de la classe $i$ et $N = \sum n_i$.
\end{itemize}
\end{definition}

\begin{exemple}{Classification d'images (3 classes)}
\begin{table}[h]
\centering
\caption{R√©sultats par classe}
\begin{tabular}{lcccc}
\toprule
\textbf{Classe} & \textbf{Support} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
Chat & 100 & 0.90 & 0.85 & 0.87 \\
Chien & 100 & 0.88 & 0.90 & 0.89 \\
Oiseau & 20 & 0.70 & 0.60 & 0.65 \\
\midrule
\textbf{Macro avg} & 220 & 0.83 & 0.78 & 0.80 \\
\textbf{Weighted avg} & 220 & 0.87 & 0.85 & 0.86 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analyse :}
\begin{itemize}
    \item Macro average = $(0.90 + 0.88 + 0.70) / 3 = 0.83$ (classe "Oiseau" p√®se autant que les autres)
    \item Weighted average plus √©lev√©e car elle donne plus de poids aux classes majoritaires (Chat, Chien)
\end{itemize}
\end{exemple}

% ===== SECTION 3: M√âTRIQUES DE R√âGRESSION =====
\section{M√©triques de R√©gression}

Pour les probl√®mes de r√©gression, on cherche √† mesurer l'√©cart entre les pr√©dictions continues et les vraies valeurs.

\textbf{Notations :}
\begin{itemize}
    \item $y_i$ : vraie valeur pour l'exemple $i$
    \item $\hat{y}_i$ : pr√©diction pour l'exemple $i$
    \item $n$ : nombre d'exemples
    \item $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ : moyenne des vraies valeurs
\end{itemize}

\subsection{MSE (Mean Squared Error)}

\begin{definition}{MSE}
Le \textbf{MSE} (Mean Squared Error) est la moyenne des carr√©s des erreurs :
\begin{equation}
    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}
\end{definition}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item Toujours $\geq 0$ (0 = pr√©dictions parfaites)
    \item P√©nalise fortement les grandes erreurs (√† cause du carr√©)
    \item Unit√© : carr√© de l'unit√© de $y$ (difficile √† interpr√©ter)
    \item Tr√®s sensible aux outliers
\end{itemize}

\begin{exemple}{Pr√©diction de prix immobiliers}
Prix r√©els : $[200, 250, 300, 350, 400]$ (en k‚Ç¨)\\
Pr√©dictions : $[210, 240, 310, 340, 500]$ (en k‚Ç¨)\\
Erreurs : $[10, -10, 10, -10, 100]$

\begin{equation}
    \text{MSE} = \frac{10^2 + 10^2 + 10^2 + 10^2 + 100^2}{5} = \frac{10300}{5} = 2060 \text{ (k‚Ç¨)}^2
\end{equation}

L'erreur de 100 k‚Ç¨ domine compl√®tement le MSE.
\end{exemple}

\subsection{RMSE (Root Mean Squared Error)}

\begin{definition}{RMSE}
Le \textbf{RMSE} est la racine carr√©e du MSE :
\begin{equation}
    \text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\end{equation}
\end{definition}

\textbf{Avantage :} M√™me unit√© que $y$ (plus facile √† interpr√©ter).

\textbf{Pour l'exemple pr√©c√©dent :}
\begin{equation}
    \text{RMSE} = \sqrt{2060} \approx 45.4 \text{ k‚Ç¨}
\end{equation}

Interpr√©tation : en moyenne, les pr√©dictions s'√©cartent de 45.4 k‚Ç¨ de la r√©alit√©.

\subsection{MAE (Mean Absolute Error)}

\begin{definition}{MAE}
Le \textbf{MAE} (Mean Absolute Error) est la moyenne des valeurs absolues des erreurs :
\begin{equation}
    \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}
\end{definition}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item Toujours $\geq 0$
    \item Moins sensible aux outliers que MSE/RMSE
    \item M√™me unit√© que $y$
    \item P√©nalise toutes les erreurs de mani√®re proportionnelle
\end{itemize}

\textbf{Pour l'exemple pr√©c√©dent :}
\begin{equation}
    \text{MAE} = \frac{|10| + |10| + |10| + |10| + |100|}{5} = \frac{140}{5} = 28 \text{ k‚Ç¨}
\end{equation}

\textbf{Comparaison MSE vs MAE :}
\begin{itemize}
    \item RMSE = 45.4 k‚Ç¨ (fortement influenc√© par l'outlier de 100 k‚Ç¨)
    \item MAE = 28 k‚Ç¨ (moins influenc√© par l'outlier)
\end{itemize}

\begin{astuce}
Choisir entre RMSE et MAE :
\begin{itemize}
    \item Utilisez \textbf{RMSE} si les grandes erreurs sont tr√®s p√©nalisantes (finance, s√©curit√©)
    \item Utilisez \textbf{MAE} si toutes les erreurs ont un co√ªt similaire (m√©t√©o, estimation g√©n√©rale)
    \item En cas de doute, reportez les deux !
\end{itemize}
\end{astuce}

\subsection{R¬≤ (Coefficient de D√©termination)}

\begin{definition}{R¬≤}
Le \textbf{R¬≤} (coefficient de d√©termination) mesure la proportion de variance expliqu√©e par le mod√®le :
\begin{equation}
    R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation}

o√π :
\begin{itemize}
    \item $\text{SS}_{\text{res}} = \sum (y_i - \hat{y}_i)^2$ : somme des carr√©s des r√©sidus (variance non expliqu√©e)
    \item $\text{SS}_{\text{tot}} = \sum (y_i - \bar{y})^2$ : variance totale
\end{itemize}
\end{definition}

\textbf{Interpr√©tation :}
\begin{itemize}
    \item $R^2 = 1$ : mod√®le parfait (pr√©dictions exactes)
    \item $R^2 = 0$ : mod√®le √©quivalent √† pr√©dire la moyenne $\bar{y}$
    \item $R^2 < 0$ : mod√®le pire que pr√©dire la moyenne (tr√®s mauvais !)
\end{itemize}

\textbf{Exemple :}
\begin{itemize}
    \item Si $R^2 = 0.85$ : le mod√®le explique 85\% de la variance des donn√©es
    \item Les 15\% restants sont dus au bruit ou √† des variables non prises en compte
\end{itemize}

\begin{attention}
Limites du R¬≤ :
\begin{itemize}
    \item Augmente automatiquement quand on ajoute des features (m√™me inutiles)
    \item Ne d√©tecte pas l'overfitting
    \item Solution : utiliser le \textbf{R¬≤ ajust√©} qui p√©nalise le nombre de features
\end{itemize}

\begin{equation}
    R^2_{\text{ajust√©}} = 1 - \frac{(1 - R^2)(n - 1)}{n - p - 1}
\end{equation}
o√π $p$ est le nombre de features.
\end{attention}

\subsection{MAPE (Mean Absolute Percentage Error)}

\begin{definition}{MAPE}
Le \textbf{MAPE} mesure l'erreur moyenne en pourcentage :
\begin{equation}
    \text{MAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
\end{equation}
\end{definition}

\textbf{Avantage :} Ind√©pendant de l'√©chelle (permet de comparer des mod√®les sur diff√©rents datasets).

\textbf{Interpr√©tation :} MAPE = 10\% signifie que les pr√©dictions s'√©cartent en moyenne de 10\% des vraies valeurs.

\begin{attention}
Probl√®me du MAPE :
\begin{itemize}
    \item Non d√©fini si $y_i = 0$
    \item Asym√©trique : p√©nalise plus les surestimations que les sous-estimations
    \item Exemple : erreur de +50\% vs -50\% ne donnent pas le m√™me MAPE
\end{itemize}
\end{attention}

\subsection{Comparaison des M√©triques de R√©gression}

\begin{table}[h]
\centering
\caption{Comparaison des m√©triques de r√©gression}
\label{tab:regression_metrics}
\begin{tabular}{lccc}
\toprule
\textbf{M√©trique} & \textbf{Sensibilit√© outliers} & \textbf{Interpr√©tation} & \textbf{Unit√©} \\
\midrule
MSE & Tr√®s √©lev√©e & Difficile & $(y)^2$ \\
RMSE & √âlev√©e & Facile & $y$ \\
MAE & Mod√©r√©e & Facile & $y$ \\
R¬≤ & N/A & Facile & Sans unit√© \\
MAPE & Mod√©r√©e & Facile & \% \\
\bottomrule
\end{tabular}
\end{table}

% ===== SECTION 4: VALIDATION =====
\section{Techniques de Validation}

L'√©valuation d'un mod√®le ne peut pas se faire sur les donn√©es d'entra√Ænement ! Un mod√®le peut m√©moriser les donn√©es (overfitting) et obtenir 100\% d'accuracy en entra√Ænement mais √©chouer en production.

\begin{attention}
\textbf{R√®gle d'or :} Ne jamais √©valuer un mod√®le sur les donn√©es utilis√©es pour l'entra√Æner !
\end{attention}

\subsection{Train/Test Split}

\begin{definition}{Train/Test Split}
La technique la plus simple : diviser al√©atoirement le dataset en deux ensembles :
\begin{itemize}
    \item \textbf{Training set} (70-80\%) : pour entra√Æner le mod√®le
    \item \textbf{Test set} (20-30\%) : pour √©valuer le mod√®le
\end{itemize}
\end{definition}

\begin{lstlisting}[language=Python, caption=Train/Test Split avec scikit-learn]
from sklearn.model_selection import train_test_split

# Split 80/20
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 20% pour le test
    random_state=42,    # reproductibilit√©
    stratify=y          # pr√©serve distribution des classes
)

# Entra√Ænement
model.fit(X_train, y_train)

# √âvaluation sur donn√©es JAMAIS vues
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
\end{lstlisting}

\textbf{Avantages :}
\begin{itemize}
    \item Simple et rapide
    \item Suffisant pour les grands datasets
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item R√©sultat d√©pend du split al√©atoire (variance √©lev√©e)
    \item Perte de donn√©es (20-30\% non utilis√©s pour l'entra√Ænement)
    \item Risque de split non repr√©sentatif sur petits datasets
\end{itemize}

\subsection{Validation Set (Train/Validation/Test)}

Pour optimiser les hyperparam√®tres, il faut un 3√®me ensemble :

\begin{definition}{Train/Validation/Test Split}
\begin{itemize}
    \item \textbf{Training set} (60\%) : entra√Ænement du mod√®le
    \item \textbf{Validation set} (20\%) : tuning des hyperparam√®tres
    \item \textbf{Test set} (20\%) : √©valuation finale (jamais touch√© avant)
\end{itemize}
\end{definition}

\textbf{Workflow :}
\begin{enumerate}
    \item Entra√Æner plusieurs mod√®les avec diff√©rents hyperparam√®tres sur le training set
    \item √âvaluer chaque mod√®le sur le validation set
    \item Choisir le meilleur mod√®le
    \item √âvaluation finale sur le test set (une seule fois !)
\end{enumerate}

\begin{attention}
Si vous √©valuez plusieurs fois sur le test set et ajustez le mod√®le en cons√©quence, le test set devient un validation set d√©guis√© ! Vous risquez l'overfitting sur le test set.
\end{attention}

\subsection{K-Fold Cross-Validation}

\begin{definition}{K-Fold Cross-Validation}
La \textbf{validation crois√©e K-fold} divise le dataset en $K$ sous-ensembles (folds) de taille √©gale. Le mod√®le est entra√Æn√© et √©valu√© $K$ fois, chaque fold servant une fois de test set.
\end{definition}

\textbf{Algorithme :}
\begin{algorithm}[H]
\caption{K-Fold Cross-Validation}
\label{alg:kfold}
\begin{algorithmic}[1]
\REQUIRE Dataset $D$, Mod√®le $M$, Nombre de folds $K$
\ENSURE Score moyen et variance
\STATE Diviser $D$ en $K$ folds : $D_1, D_2, \ldots, D_K$
\STATE Initialiser liste de scores : $\text{scores} = []$
\FOR{$i = 1$ \TO $K$}
    \STATE $\text{test\_set} = D_i$
    \STATE $\text{train\_set} = D \setminus D_i$ (tous les folds sauf $D_i$)
    \STATE Entra√Æner $M$ sur $\text{train\_set}$
    \STATE $\text{score}_i = $ √©valuer $M$ sur $\text{test\_set}$
    \STATE Ajouter $\text{score}_i$ √† $\text{scores}$
\ENDFOR
\RETURN $\text{mean}(\text{scores})$, $\text{std}(\text{scores})$
\end{algorithmic}
\end{algorithm}

\begin{lstlisting}[language=Python, caption=K-Fold Cross-Validation avec scikit-learn]
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()

# 5-fold cross-validation
scores = cross_val_score(
    model, X, y,
    cv=5,                    # 5 folds
    scoring='accuracy'       # m√©trique
)

print(f"Scores: {scores}")
print(f"Moyenne: {scores.mean():.3f} (+/- {scores.std():.3f})")

# Exemple de sortie :
# Scores: [0.82, 0.85, 0.81, 0.84, 0.83]
# Moyenne: 0.830 (+/- 0.015)
\end{lstlisting}

\textbf{Avantages :}
\begin{itemize}
    \item Utilise toutes les donn√©es pour l'entra√Ænement et le test
    \item R√©duit la variance de l'estimation (score moyen sur $K$ runs)
    \item Fournit un intervalle de confiance (√©cart-type)
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item Co√ªt computationnel : $K$ fois plus long qu'un simple train/test
    \item Valeur typique : $K = 5$ ou $K = 10$
\end{itemize}

\subsection{Stratified K-Fold}

\begin{definition}{Stratified K-Fold}
Le \textbf{Stratified K-Fold} est une variante de K-Fold qui pr√©serve la proportion de chaque classe dans chaque fold.
\end{definition}

\textbf{Pourquoi c'est important :}

Exemple : Dataset de 100 exemples (90 classe A, 10 classe B)

\textbf{K-Fold classique :}
\begin{itemize}
    \item Un fold pourrait contenir 0 exemple de classe B (probl√®me !)
    \item Entra√Ænement sur donn√©es non repr√©sentatives
\end{itemize}

\textbf{Stratified K-Fold :}
\begin{itemize}
    \item Chaque fold contient environ 90\% classe A et 10\% classe B
    \item Distribution pr√©serv√©e
\end{itemize}

\begin{lstlisting}[language=Python, caption=Stratified K-Fold]
from sklearn.model_selection import StratifiedKFold, cross_val_score

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores = cross_val_score(
    model, X, y,
    cv=skf,                  # utiliser stratified K-fold
    scoring='f1_weighted'
)
\end{lstlisting}

\begin{astuce}
Utilisez \textbf{Stratified K-Fold} par d√©faut pour la classification, surtout si :
\begin{itemize}
    \item Les classes sont d√©s√©quilibr√©es
    \item Le dataset est petit
\end{itemize}

Pour la r√©gression, utilisez K-Fold classique.
\end{astuce}

\subsection{Leave-One-Out Cross-Validation (LOOCV)}

\begin{definition}{LOOCV}
Le \textbf{LOOCV} est un cas extr√™me de K-Fold o√π $K = n$ (nombre d'exemples). Chaque exemple sert une fois de test set.
\end{definition}

\textbf{Avantages :}
\begin{itemize}
    \item Estimation quasi non biais√©e
    \item Utile pour tr√®s petits datasets (< 100 exemples)
\end{itemize}

\textbf{Limites :}
\begin{itemize}
    \item Extr√™mement co√ªteux : $n$ entra√Ænements complets
    \item Variance √©lev√©e de l'estimation
    \item Rarement utilis√© en pratique (pr√©f√©rer 5 ou 10-fold)
\end{itemize}

\subsection{Time Series Split}

\begin{definition}{Time Series Split}
Pour les s√©ries temporelles, il faut respecter l'ordre chronologique. Le \textbf{Time Series Split} cr√©e des folds o√π le training set contient toujours des donn√©es ant√©rieures au test set.
\end{definition}

\textbf{Exemple avec 5 splits :}
\begin{verbatim}
Fold 1: train [1:100], test [101:120]
Fold 2: train [1:120], test [121:140]
Fold 3: train [1:140], test [141:160]
Fold 4: train [1:160], test [161:180]
Fold 5: train [1:180], test [181:200]
\end{verbatim}

\begin{lstlisting}[language=Python, caption=Time Series Split]
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

for train_index, test_index in tscv.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    print(f"Score: {score:.3f}")
\end{lstlisting}

\begin{attention}
Pour les s√©ries temporelles, \textbf{JAMAIS de shuffle} ni de K-Fold classique !
Utiliser uniquement Time Series Split ou un simple train/test split chronologique.
\end{attention}

\subsection{Comparaison des Techniques de Validation}

\begin{table}[h]
\centering
\caption{Comparaison des techniques de validation}
\label{tab:validation_comparison}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Technique} & \textbf{Donn√©es utilis√©es} & \textbf{Variance} & \textbf{Co√ªt} & \textbf{Cas d'usage} \\
\midrule
Train/Test & 80\% & √âlev√©e & Faible & Grands datasets, premi√®re it√©ration \\
Train/Val/Test & 60\% & √âlev√©e & Faible & Tuning hyperparam√®tres \\
K-Fold (K=5) & 100\% & Mod√©r√©e & Moyen & Usage g√©n√©ral, petits/moyens datasets \\
K-Fold (K=10) & 100\% & Faible & √âlev√© & Datasets moyens, besoin pr√©cision \\
LOOCV & 100\% & Tr√®s faible & Tr√®s √©lev√© & Tr√®s petits datasets (< 100) \\
Stratified K-Fold & 100\% & Mod√©r√©e & Moyen & Classes d√©s√©quilibr√©es \\
Time Series Split & 100\% & Mod√©r√©e & Moyen & S√©ries temporelles uniquement \\
\bottomrule
\end{tabular}
}
\end{table}

% ===== SECTION 5: PI√àGES COURANTS =====
\section{Pi√®ges Courants et Bonnes Pratiques}

\subsection{Data Leakage (Fuite de donn√©es)}

\begin{attention}
Le \textbf{data leakage} est la principale cause de mod√®les qui performent bien en d√©veloppement mais √©chouent en production.
\end{attention}

\begin{definition}{Data Leakage}
Il y a \textbf{data leakage} quand des informations du test set influencent l'entra√Ænement du mod√®le. Cela conduit √† des performances surestim√©es.
\end{definition}

\textbf{Types de leakage :}

\subsubsection{Leakage via le preprocessing}

\textbf{MAUVAIS :}
\begin{lstlisting}[language=Python, caption=Leakage via normalisation (INCORRECT)]
from sklearn.preprocessing import StandardScaler

# ERREUR: normalisation AVANT le split
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # utilise mean/std de TOUT le dataset

# Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)

# Le test set a influenc√© la normalisation !
\end{lstlisting}

\textbf{BON :}
\begin{lstlisting}[language=Python, caption=Normalisation correcte (CORRECT)]
# Split AVANT preprocessing
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Normalisation bas√©e UNIQUEMENT sur train set
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # fit sur train
X_test_scaled = scaler.transform(X_test)        # transform sur test (sans fit!)
\end{lstlisting}

\textbf{Pourquoi c'est important :}
\begin{itemize}
    \item Le scaler calcul√© sur tout le dataset "voit" les statistiques du test set
    \item En production, vous n'aurez pas acc√®s aux donn√©es futures
    \item Impact : performances surestim√©es de 5-10\% ou plus
\end{itemize}

\subsubsection{Leakage via les features}

\begin{exemple}{Pr√©diction de fraude bancaire (leakage)}
\textbf{Feature probl√©matique :} \texttt{fraud\_score} (score de fraude calcul√© manuellement apr√®s investigation)

\textbf{Probl√®me :} Ce score n'existe qu'apr√®s l'enqu√™te, donc inutilisable en production (au moment de la transaction).

\textbf{Sympt√¥me :} Mod√®le avec 99\% d'accuracy en validation, mais 0\% en production.
\end{exemple}

\subsubsection{Leakage temporel}

\begin{exemple}{Pr√©diction de prix d'actions}
\textbf{Erreur :} Utiliser K-Fold classique sur des donn√©es temporelles.

\textbf{Probl√®me :} Le mod√®le s'entra√Æne sur des donn√©es futures pour pr√©dire le pass√© (impossible en production).

\textbf{Solution :} Utiliser Time Series Split.
\end{exemple}

\subsection{Classes D√©s√©quilibr√©es}

\begin{attention}
Avec des classes tr√®s d√©s√©quilibr√©es (99\% / 1\%), l'accuracy devient inutile.
\end{attention}

\textbf{Strat√©gies :}

\begin{enumerate}
    \item \textbf{Changer la m√©trique :}
    \begin{itemize}
        \item Utiliser Precision, Recall, F1-score, AUC
        \item Ignorer l'accuracy
    \end{itemize}

    \item \textbf{R√©√©chantillonnage :}
    \begin{itemize}
        \item \textbf{Oversampling :} dupliquer la classe minoritaire (risque d'overfitting)
        \item \textbf{Undersampling :} r√©duire la classe majoritaire (perte de donn√©es)
        \item \textbf{SMOTE :} g√©n√©rer des exemples synth√©tiques (meilleure approche)
    \end{itemize}

    \item \textbf{Class weights :}
    \begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression

# P√©naliser plus les erreurs sur la classe minoritaire
model = LogisticRegression(class_weight='balanced')
    \end{lstlisting}

    \item \textbf{Algorithmes robustes :}
    \begin{itemize}
        \item Random Forest, XGBoost (g√®rent bien le d√©s√©quilibre)
    \end{itemize}
\end{enumerate}

\subsection{Overfitting et Underfitting}

\begin{definition}{Overfitting et Underfitting}
\begin{itemize}
    \item \textbf{Overfitting :} Le mod√®le m√©morise les donn√©es d'entra√Ænement (bruit inclus). Performance excellente en train, mauvaise en test.
    \item \textbf{Underfitting :} Le mod√®le est trop simple pour capturer les patterns. Performance m√©diocre en train ET en test.
\end{itemize}
\end{definition}

\textbf{D√©tection :}
\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Situation} & \textbf{Train Score} & \textbf{Test Score} & \textbf{Diagnostic} \\
\midrule
Bon mod√®le & 85\% & 83\% & √âquilibre \\
Overfitting & 99\% & 70\% & Trop complexe \\
Underfitting & 65\% & 63\% & Trop simple \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Solutions :}

\textbf{Contre l'overfitting :}
\begin{itemize}
    \item Augmenter les donn√©es d'entra√Ænement
    \item R√©gularisation (L1, L2, Dropout)
    \item R√©duire la complexit√© du mod√®le
    \item Early stopping
    \item Data augmentation
\end{itemize}

\textbf{Contre l'underfitting :}
\begin{itemize}
    \item Augmenter la complexit√© du mod√®le
    \item Ajouter des features
    \item R√©duire la r√©gularisation
    \item Entra√Æner plus longtemps
\end{itemize}

\subsection{S√©lection de M√©triques : Guide Pratique}

\begin{table}[h]
\centering
\caption{Quelle m√©trique utiliser ?}
\label{tab:metric_selection}
\resizebox{\textwidth}{!}{
\begin{tabular}{lll}
\toprule
\textbf{Contexte} & \textbf{M√©trique recommand√©e} & \textbf{Raison} \\
\midrule
Classification √©quilibr√©e & Accuracy, F1-score & Classes √©galement importantes \\
Classification d√©s√©quilibr√©e & Precision, Recall, F1, AUC & Accuracy trompeuse \\
Spam detection & Precision (+ Recall) & √âviter de perdre emails l√©gitimes \\
D√©tection cancer & Recall (+ Precision) & Ne pas manquer de malades \\
D√©tection fraude & AUC, F1-score & Compromis FP/FN + robustesse seuil \\
Multi-classes √©quilibr√©es & Macro average & Toutes classes √©gales \\
Multi-classes d√©s√©quilibr√©es & Weighted average & Classes r√©alistes \\
R√©gression g√©n√©rale & RMSE, MAE, R¬≤ & Standard, facile √† interpr√©ter \\
R√©gression avec outliers & MAE & Moins sensible aux outliers \\
R√©gression sans outliers & RMSE & P√©nalise les grandes erreurs \\
Comparaison multi-datasets & MAPE, R¬≤ & Ind√©pendant de l'√©chelle \\
S√©ries temporelles & Time Series Split + RMSE/MAE & Respect de la temporalit√© \\
\bottomrule
\end{tabular}
}
\end{table}

% ===== SECTION 6: R√âSUM√â =====
\section{R√©sum√© du Chapitre}

\subsection{Points Cl√©s}

\begin{itemize}
    \item \textbf{M√©triques de classification :}
    \begin{itemize}
        \item Matrice de confusion : TP, TN, FP, FN
        \item Accuracy : proportion de pr√©dictions correctes (attention aux classes d√©s√©quilibr√©es)
        \item Precision : "Quand je pr√©dis positif, ai-je raison ?" (important si FP co√ªteux)
        \item Recall : "Parmi les vrais positifs, combien sont d√©tect√©s ?" (important si FN co√ªteux)
        \item F1-score : compromis entre Precision et Recall
        \item AUC-ROC : performance ind√©pendante du seuil (bon pour comparer des mod√®les)
        \item Precision-Recall curve : meilleure que ROC pour classes d√©s√©quilibr√©es
    \end{itemize}

    \item \textbf{M√©triques de r√©gression :}
    \begin{itemize}
        \item MSE : p√©nalise fortement les grandes erreurs (sensible aux outliers)
        \item RMSE : version interpr√©table du MSE (m√™me unit√© que $y$)
        \item MAE : robuste aux outliers, p√©nalise toutes les erreurs √©galement
        \item R¬≤ : proportion de variance expliqu√©e (0 = mod√®le inutile, 1 = parfait)
        \item MAPE : erreur en pourcentage (ind√©pendant de l'√©chelle, mais probl√®mes si $y=0$)
    \end{itemize}

    \item \textbf{Validation :}
    \begin{itemize}
        \item Ne JAMAIS √©valuer sur les donn√©es d'entra√Ænement
        \item Train/Test split : simple, mais variance √©lev√©e
        \item K-Fold CV : utilise toutes les donn√©es, r√©duit la variance
        \item Stratified K-Fold : pr√©serve la distribution des classes (recommand√©)
        \item Time Series Split : obligatoire pour les s√©ries temporelles
    \end{itemize}

    \item \textbf{Pi√®ges √† √©viter :}
    \begin{itemize}
        \item Data leakage (preprocessing APR√àS split)
        \item Choisir la mauvaise m√©trique (accuracy avec classes d√©s√©quilibr√©es)
        \item Overfitting (train score >> test score)
        \item √âvaluer plusieurs fois sur le test set
    \end{itemize}
\end{itemize}

\subsection{Formules Essentielles}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Formules √† retenir]

\textbf{Classification :}
\begin{align}
    \text{Accuracy} &= \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}} \\
    \text{Precision} &= \frac{\text{TP}}{\text{TP} + \text{FP}} \\
    \text{Recall} &= \frac{\text{TP}}{\text{TP} + \text{FN}} \\
    F_1 &= 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{align}

\textbf{R√©gression :}
\begin{align}
    \text{MSE} &= \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \\
    \text{RMSE} &= \sqrt{\text{MSE}} \\
    \text{MAE} &= \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| \\
    R^2 &= 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\end{align}
\end{tcolorbox}

% ===== SECTION 7: EXERCICES =====
\section{Exercices}

\subsection{Questions de compr√©hension}

\begin{enumerate}
    \item Expliquez pourquoi l'accuracy peut √™tre trompeuse avec des classes d√©s√©quilibr√©es (99\% / 1\%). Donnez un exemple concret.

    \item Un mod√®le de d√©tection de cancer a Precision = 80\% et Recall = 95\%. Que signifient ces chiffres concr√®tement ? Quelle m√©trique est la plus importante dans ce contexte ?

    \item Pourquoi utilise-t-on la moyenne harmonique (F1) plut√¥t que la moyenne arithm√©tique entre Precision et Recall ?

    \item Vous entra√Ænez un mod√®le et obtenez : Train accuracy = 98\%, Test accuracy = 72\%. Quel est le probl√®me ? Quelles solutions proposez-vous ?

    \item Expliquez la diff√©rence entre RMSE et MAE. Dans quel contexte pr√©f√©rer l'un √† l'autre ?

    \item Pourquoi ne peut-on pas utiliser K-Fold classique pour des s√©ries temporelles ? Quelle technique utiliser ?

    \item Qu'est-ce que le data leakage ? Donnez un exemple de leakage via le preprocessing.

    \item Un dataset contient 1\% de fraudes. Vous voulez optimiser un seuil de classification. Quelle courbe utiliser : ROC ou Precision-Recall ? Pourquoi ?
\end{enumerate}

\subsection{Exercices pratiques}

\begin{enumerate}
    \item \textbf{Calcul manuel de m√©triques :}
    \begin{itemize}
        \item Soit une matrice de confusion : TP = 45, FP = 10, FN = 5, TN = 140
        \item Calculez : Accuracy, Precision, Recall, F1-score, Sp√©cificit√©
        \item Interpr√©tez les r√©sultats
    \end{itemize}

    \item \textbf{Comparaison de mod√®les (r√©gression) :}
    \begin{itemize}
        \item Mod√®le A : MSE = 25, MAE = 4, R¬≤ = 0.85
        \item Mod√®le B : MSE = 30, MAE = 3.5, R¬≤ = 0.82
        \item Quel mod√®le choisir ? Justifiez selon le contexte.
    \end{itemize}

    \item \textbf{Impl√©mentation from scratch :}
    \begin{itemize}
        \item Impl√©menter Precision, Recall, F1-score en NumPy pur (sans scikit-learn)
        \item Tester sur des donn√©es synth√©tiques
        \item Comparer avec \texttt{sklearn.metrics}
    \end{itemize}

    \item \textbf{Validation crois√©e :}
    \begin{itemize}
        \item Dataset : Breast Cancer (sklearn)
        \item Comparer : simple train/test, 5-fold CV, 10-fold CV, Stratified 5-fold
        \item Analyser moyenne et √©cart-type des scores
    \end{itemize}

    \item \textbf{Classes d√©s√©quilibr√©es :}
    \begin{itemize}
        \item Cr√©er un dataset d√©s√©quilibr√© (95\% / 5\%)
        \item Entra√Æner un mod√®le baseline
        \item Tester : SMOTE, class weights, undersampling
        \item Comparer les m√©triques (Accuracy, F1, AUC)
    \end{itemize}

    \item \textbf{Visualisations :}
    \begin{itemize}
        \item Tracer une matrice de confusion avec seaborn
        \item Tracer courbes ROC et Precision-Recall
        \item Comparer 3 mod√®les sur le m√™me graphique
    \end{itemize}
\end{enumerate}


% ===== SECTION 8: POUR ALLER PLUS LOIN =====
\section{Pour Aller Plus Loin}

\subsection{Lectures Recommand√©es}

\begin{itemize}
    \item \textbf{Articles :}
    \begin{itemize}
        \item "The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets" (Saito \& Rehmsmeier, 2015)
        \item "A Survey on Deep Learning for Imbalanced Classification" (Zhang et al., 2023)
    \end{itemize}

    \item \textbf{Livres :}
    \begin{itemize}
        \item "Hands-On Machine Learning" (Aur√©lien G√©ron) - Chapitre 3
        \item "The Elements of Statistical Learning" (Hastie et al.) - Chapitre 7
    \end{itemize}

    \item \textbf{Blogs :}
    \begin{itemize}
        \item \url{https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/}
        \item \url{https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc}
    \end{itemize}
\end{itemize}

\subsection{Ressources en Ligne}

\begin{itemize}
    \item Documentation scikit-learn : \url{https://scikit-learn.org/stable/modules/model_evaluation.html}
    \item Imbalanced-learn (SMOTE) : \url{https://imbalanced-learn.org/}
    \item Interactive ROC/PR curves : \url{https://roc.mlvis.com/}
\end{itemize}

\subsection{Notebooks Associ√©s}

\begin{enumerate}
    \item \texttt{02_demo_metriques\_classification.ipynb} : Calcul manuel, visualisations ROC/PR
    \item \texttt{02_demo_metriques\_regression.ipynb} : Comparaison MSE/MAE/R¬≤
\end{enumerate}

\subsection{Prochaines √âtapes}

Chapitre suivant recommand√© : \textbf{Chapitre 03 - R√©gression Lin√©aire}

Vous savez maintenant √©valuer un mod√®le. Le chapitre suivant vous apprendra √† construire votre premier mod√®le de r√©gression, en appliquant toutes les m√©triques et techniques de validation vues ici.

% ===== BIBLIOGRAPHIE =====
\section*{R√©f√©rences}

\begin{enumerate}
    \item G√©ron, A. (2022). \textit{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow} (3rd ed.). O'Reilly Media.
    \item Hastie, T., Tibshirani, R., \& Friedman, J. (2009). \textit{The Elements of Statistical Learning} (2nd ed.). Springer.
    \item Saito, T., \& Rehmsmeier, M. (2015). "The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets". \textit{PLOS ONE}, 10(3).
    \item Pedregosa, F. et al. (2011). "Scikit-learn: Machine Learning in Python". \textit{Journal of Machine Learning Research}, 12, 2825-2830.
    \item Provost, F., \& Fawcett, T. (2013). \textit{Data Science for Business}. O'Reilly Media.
\end{enumerate}

\end{document}
