{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/00_introduction/00_exercices_solutions.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '00_exercices_solutions.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 00 - Solutions des Exercices\n",
    "\n",
    "Ce notebook contient les solutions d√©taill√©es des exercices du Chapitre 00.\n",
    "\n",
    "**Remarque** : Essayez d'abord de r√©soudre les exercices par vous-m√™me avant de consulter les solutions !\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_wine, load_breast_cancer, load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Biblioth√®ques import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 1 : Analyse Exploratoire (EDA)\n",
    "\n",
    "### 1.1 Chargement et Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Chargement du dataset Wine\nwine = load_wine()\n\n# Cr√©ation du DataFrame\ndf_wine = pd.DataFrame(\n    data=wine.data,  # type: ignore\n    columns=wine.feature_names  # type: ignore\n)\ndf_wine['target'] = wine.target  # type: ignore\ndf_wine['target_name'] = df_wine['target'].map(\n    {i: name for i, name in enumerate(wine.target_names)}  # type: ignore\n)\n\nprint(\"Dataset Wine charg√©\")\ndisplay(df_wine.head(10))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# R√©ponses aux questions\n\n# 1. Nombre d'√©chantillons et features\nn_samples, n_features = wine.data.shape  # type: ignore\nprint(f\"1. Nombre d'√©chantillons : {n_samples}\")\nprint(f\"   Nombre de features : {n_features}\")\n\n# 2. Classes cibles\nprint(f\"\\n2. Classes cibles : {wine.target_names}\")  # type: ignore\nprint(f\"   Nombre de classes : {len(wine.target_names)}\")  # type: ignore\n\n# 3. Valeurs manquantes\nprint(f\"\\n3. Valeurs manquantes :\")\nprint(df_wine.isnull().sum())\nprint(\"   ‚Üí Aucune valeur manquante\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Statistiques Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "display(df_wine.describe())\n",
    "\n",
    "print(\"\\nObservations :\")\n",
    "print(\"- Les features ont des √©chelles tr√®s diff√©rentes (ex: proline ~746, malic_acid ~2.3)\")\n",
    "print(\"- La standardisation sera importante pour certains algorithmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Distribution des Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Distribution des classes\nprint(\"Distribution des classes :\")\nprint(df_wine['target_name'].value_counts())\n\n# Visualisation\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Countplot\nsns.countplot(data=df_wine, x='target_name', hue='target_name', ax=ax[0], palette='Set2', legend=False)\nax[0].set_title('Distribution des Classes', fontsize=14, fontweight='bold')\nax[0].set_xlabel('Classe')\nax[0].set_ylabel('Nombre d\\'√©chantillons')\n\n# Pie chart\ncounts = df_wine['target_name'].value_counts()\nax[1].pie(\n    counts,\n    labels=counts.index,\n    autopct='%1.1f%%',\n    colors=sns.color_palette('Set2'),\n    startangle=90\n)\nax[1].set_title('Proportion des Classes', fontsize=14, fontweight='bold')\nax[1].set_ylabel('')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nR√©ponse : Le dataset est l√©g√®rement d√©s√©quilibr√©\")\nprint(\"class_0: 59 √©chantillons (33%), class_1: 71 (40%), class_2: 48 (27%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Matrice de Corr√©lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Matrice de corr√©lation\ncorr_matrix = df_wine[list(wine.feature_names)].corr()  # type: ignore\n\n# Visualisation\nplt.figure(figsize=(12, 10))\nsns.heatmap(\n    corr_matrix,\n    annot=True,\n    cmap='coolwarm',\n    center=0,\n    square=True,\n    linewidths=0.5,\n    cbar_kws={\"shrink\": 0.8},\n    fmt='.2f',\n    annot_kws={'size': 8}\n)\nplt.title('Matrice de Corr√©lation', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Identifier les corr√©lations les plus fortes\nprint(\"\\nPaires de features les plus corr√©l√©es :\")\ncorr_pairs = []\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        corr_pairs.append((\n            corr_matrix.columns[i],\n            corr_matrix.columns[j],\n            abs(corr_matrix.iloc[i, j])\n        ))\n\ncorr_pairs_sorted = sorted(corr_pairs, key=lambda x: x[2], reverse=True)\nfor feat1, feat2, corr in corr_pairs_sorted[:5]:\n    print(f\"  {feat1[:20]:20s} ‚Üî {feat2[:20]:20s} : {corr:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pairplot pour les 4 premi√®res features\nfeatures_to_plot = wine.feature_names[:4]  # type: ignore\ndf_subset = df_wine[list(features_to_plot) + ['target_name']]\n\nsns.pairplot(\n    df_subset,\n    hue='target_name',\n    palette='Set2',\n    diag_kind='kde',\n    markers=['o', 's', 'D'],\n    plot_kws={'alpha': 0.6}\n)\nplt.suptitle('Pairplot - 4 Premi√®res Features', y=1.02, fontsize=14, fontweight='bold')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 2 : Pipeline ML Complet\n",
    "\n",
    "### 2.1 Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Chargement du dataset Breast Cancer\ncancer = load_breast_cancer()\nX = cancer.data  # type: ignore\ny = cancer.target  # type: ignore\n\nprint(f\"Nombre d'√©chantillons : {X.shape[0]}\")\nprint(f\"Nombre de features : {X.shape[1]}\")\nprint(f\"Classes : {cancer.target_names}\")  # type: ignore\nprint(f\"Distribution : {np.bincount(y)} (0=malignant, 1=benign)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 70/30 avec stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set : {X_train.shape[0]} √©chantillons ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set  : {X_test.shape[0]} √©chantillons ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribution train : {np.bincount(y_train)}\")\n",
    "print(f\"Distribution test  : {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation (fit sur train uniquement)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úì Standardisation effectu√©e\")\n",
    "print(f\"Moyennes (train) : {X_train_scaled.mean(axis=0)[:5].round(2)}\")\n",
    "print(f\"√âcart-types (train) : {X_train_scaled.std(axis=0)[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Entra√Ænement de Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire de mod√®les\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=10000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Entra√Ænement des mod√®les...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"‚è≥ {name}...\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    if name in ['Logistic Regression', 'SVM']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Stockage\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"   Accuracy : {accuracy:.4f}\")\n",
    "    print(f\"   CV Score : {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\\n\")\n",
    "\n",
    "print(\"‚úÖ Entra√Ænement termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 √âvaluation et Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de comparaison\n",
    "comparison = pd.DataFrame({\n",
    "    'Mod√®le': list(results.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
    "    'CV Mean': [r['cv_mean'] for r in results.values()],\n",
    "    'CV Std': [r['cv_std'] for r in results.values()]\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"Comparaison des performances :\")\n",
    "display(comparison)\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_pos = np.arange(len(comparison))\n",
    "\n",
    "ax.barh(\n",
    "    x_pos,\n",
    "    comparison['Accuracy'],\n",
    "    color=sns.color_palette('viridis', len(comparison)),\n",
    "    edgecolor='black'\n",
    ")\n",
    "ax.set_yticks(x_pos)\n",
    "ax.set_yticklabels(comparison['Mod√®le'])\n",
    "ax.set_xlabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Comparaison des Performances', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0.9, 1.0])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Annotations\n",
    "for i, v in enumerate(comparison['Accuracy']):\n",
    "    ax.text(v + 0.005, i, f\"{v:.4f}\", va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 √âvaluation D√©taill√©e du Meilleur Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# S√©lection du meilleur mod√®le\nbest_model_name = comparison.iloc[0]['Mod√®le']\nbest_model = results[best_model_name]['model']\nbest_y_pred = results[best_model_name]['y_pred']\n\nprint(f\"üèÜ Meilleur mod√®le : {best_model_name}\\n\")\n\n# Classification report\nprint(\"Classification Report :\")\nprint(classification_report(\n    y_test,\n    best_y_pred,\n    target_names=cancer.target_names  # type: ignore\n))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, best_y_pred)\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 6))\n\n# Matrice absolue\ndisp1 = ConfusionMatrixDisplay(\n    confusion_matrix=cm,\n    display_labels=cancer.target_names  # type: ignore\n)\ndisp1.plot(ax=ax[0], cmap='Blues', values_format='d')\nax[0].set_title('Matrice de Confusion', fontsize=12, fontweight='bold')\n\n# Matrice normalis√©e\ncm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\ndisp2 = ConfusionMatrixDisplay(\n    confusion_matrix=cm_norm,\n    display_labels=cancer.target_names  # type: ignore\n)\ndisp2.plot(ax=ax[1], cmap='Blues', values_format='.2%')\nax[1].set_title('Matrice de Confusion (Normalis√©e)', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 3 : D√©tection d'Overfitting\n",
    "\n",
    "### 3.1 Cr√©ation d'un Dataset avec Peu d'√âchantillons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dataset Wine avec seulement 30 √©chantillons\nwine = load_wine()\n\n# S√©lection al√©atoire de 30 indices\nindices = np.random.choice(len(wine.data), size=30, replace=False)  # type: ignore\n\nX_small = wine.data[indices]  # type: ignore\ny_small = wine.target[indices]  # type: ignore\n\n# Split 20/10\nX_train_small, X_test_small, y_train_small, y_test_small = train_test_split(\n    X_small, y_small,\n    test_size=10,\n    random_state=42,\n    stratify=y_small\n)\n\nprint(f\"Train : {len(X_train_small)} √©chantillons\")\nprint(f\"Test  : {len(X_test_small)} √©chantillons\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entra√Ænement d'un Mod√®le Complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree sans limitation de profondeur\n",
    "tree_overfit = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
    "tree_overfit.fit(X_train_small, y_train_small)\n",
    "\n",
    "# Accuracy sur train et test\n",
    "train_accuracy = tree_overfit.score(X_train_small, y_train_small)\n",
    "test_accuracy = tree_overfit.score(X_test_small, y_test_small)\n",
    "\n",
    "print(f\"Train Accuracy : {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy  : {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Analyse :\")\n",
    "if train_accuracy > test_accuracy + 0.1:\n",
    "    print(\"   ‚ö†Ô∏è OVERFITTING d√©tect√© !\")\n",
    "    print(\"   Le mod√®le a une performance parfaite sur le train (1.00)\")\n",
    "    print(\"   mais beaucoup plus faible sur le test.\")\n",
    "    print(\"   ‚Üí Le mod√®le a m√©moris√© les donn√©es d'entra√Ænement.\")\n",
    "else:\n",
    "    print(\"   ‚úì Pas d'overfitting majeur d√©tect√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 R√©gularisation pour R√©duire l'Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de diff√©rentes profondeurs\n",
    "max_depths = [2, 3, 4, 5, 6, 7, 8, None]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train_small, y_train_small)\n",
    "    \n",
    "    train_scores.append(tree.score(X_train_small, y_train_small))\n",
    "    test_scores.append(tree.score(X_test_small, y_test_small))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Conversion de None en \"No limit\" pour l'affichage\n",
    "depths_labels = [str(d) if d is not None else \"No limit\" for d in max_depths]\n",
    "x_pos = np.arange(len(max_depths))\n",
    "\n",
    "plt.plot(x_pos, train_scores, 'o-', linewidth=2, markersize=8, label='Train', color='blue')\n",
    "plt.plot(x_pos, test_scores, 's-', linewidth=2, markersize=8, label='Test', color='red')\n",
    "\n",
    "plt.xticks(x_pos, depths_labels)\n",
    "plt.xlabel('max_depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Train vs Test Accuracy en fonction de max_depth', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "# Zone d'overfitting\n",
    "plt.axvspan(5.5, 7.5, alpha=0.2, color='red', label='Zone d\\'overfitting')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Meilleure profondeur\n",
    "best_depth_idx = np.argmax(test_scores)\n",
    "best_depth = max_depths[best_depth_idx]\n",
    "print(f\"\\n‚úÖ Meilleure profondeur : {best_depth}\")\n",
    "print(f\"   Test Accuracy : {test_scores[best_depth_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 4 : Feature Engineering\n",
    "\n",
    "### 4.1 Cr√©ation de Nouvelles Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dataset Iris\niris = load_iris()\ndf_iris = pd.DataFrame(iris.data, columns=iris.feature_names)  # type: ignore\ndf_iris['target'] = iris.target  # type: ignore\n\n# Cr√©ation de nouvelles features\ndf_iris['sepal_ratio'] = df_iris['sepal length (cm)'] / df_iris['sepal width (cm)']\ndf_iris['petal_ratio'] = df_iris['petal length (cm)'] / df_iris['petal width (cm)']\ndf_iris['petal_area'] = df_iris['petal length (cm)'] * df_iris['petal width (cm)']\n\nprint(\"Nouvelles features cr√©√©es :\")\ndisplay(df_iris.head(10))\n\n# Visualisation des nouvelles features\nfig, ax = plt.subplots(1, 3, figsize=(16, 5))\n\nfor i, (col, axis) in enumerate(zip(['sepal_ratio', 'petal_ratio', 'petal_area'], ax)):\n    for target in range(3):\n        subset = df_iris[df_iris['target'] == target]\n        axis.hist(\n            subset[col],\n            alpha=0.6,\n            label=iris.target_names[target],  # type: ignore\n            bins=15,\n            edgecolor='black'\n        )\n    axis.set_xlabel(col, fontsize=11)\n    axis.set_ylabel('Fr√©quence', fontsize=11)\n    axis.set_title(f'Distribution : {col}', fontsize=12, fontweight='bold')\n    axis.legend()\n    axis.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Comparaison Avant/Apr√®s Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mod√®le sans nouvelles features\nX_original = iris.data  # type: ignore\ny = iris.target  # type: ignore\n\nmodel_original = RandomForestClassifier(n_estimators=100, random_state=42)\ncv_original = cross_val_score(model_original, X_original, y, cv=5)\n\nprint(\"Sans feature engineering :\")\nprint(f\"  CV Mean : {cv_original.mean():.4f}\")\nprint(f\"  CV Std  : {cv_original.std():.4f}\")\n\n# Mod√®le avec nouvelles features\nfeature_cols = [col for col in df_iris.columns if col != 'target']\nX_engineered = df_iris[feature_cols].values\n\nmodel_engineered = RandomForestClassifier(n_estimators=100, random_state=42)\ncv_engineered = cross_val_score(model_engineered, X_engineered, y, cv=5)\n\nprint(\"\\nAvec feature engineering :\")\nprint(f\"  CV Mean : {cv_engineered.mean():.4f}\")\nprint(f\"  CV Std  : {cv_engineered.std():.4f}\")\n\n# Comparaison\nimprovement = cv_engineered.mean() - cv_original.mean()\nprint(f\"\\n‚úÖ Am√©lioration : {improvement:.4f} ({improvement/cv_original.mean()*100:.2f}%)\")\n\n# Visualisation\nfig, ax = plt.subplots(figsize=(10, 6))\n\nbp = ax.boxplot(\n    [cv_original, cv_engineered],\n    labels=['Sans FE', 'Avec FE'],\n    patch_artist=True,\n    boxprops=dict(facecolor='lightblue', edgecolor='black'),\n    medianprops=dict(color='red', linewidth=2)\n)\n\nax.set_ylabel('CV Accuracy', fontsize=12)\nax.set_title('Impact du Feature Engineering', fontsize=14, fontweight='bold')\nax.grid(alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 5 : Interpr√©tation des R√©sultats\n",
    "\n",
    "### 5.1 Analyse d'une Matrice de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion fictive\n",
    "cm = np.array([\n",
    "    [85, 15],  # Classe 0 (non-spam)\n",
    "    [10, 90]   # Classe 1 (spam)\n",
    "])\n",
    "\n",
    "# Calcul des composantes\n",
    "TN = cm[0, 0]  # True Negatives\n",
    "FP = cm[0, 1]  # False Positives\n",
    "FN = cm[1, 0]  # False Negatives\n",
    "TP = cm[1, 1]  # True Positives\n",
    "\n",
    "print(\"Composantes de la matrice de confusion :\")\n",
    "print(f\"  TN (True Negatives)  : {TN}\")\n",
    "print(f\"  FP (False Positives) : {FP}\")\n",
    "print(f\"  FN (False Negatives) : {FN}\")\n",
    "print(f\"  TP (True Positives)  : {TP}\")\n",
    "\n",
    "# Calcul des m√©triques\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"\\nM√©triques :\")\n",
    "print(f\"  Accuracy  : {accuracy:.4f}\")\n",
    "print(f\"  Precision : {precision:.4f}\")\n",
    "print(f\"  Recall    : {recall:.4f}\")\n",
    "print(f\"  F1-Score  : {f1:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation dans le contexte du spam :\")\n",
    "print(\"\\n  RECALL (Sensibilit√©) est la m√©trique la plus importante car :\")\n",
    "print(\"  - On veut d√©tecter un maximum de spams (minimiser les FN)\")\n",
    "print(\"  - Un spam non d√©tect√© (FN) peut contenir malware/phishing\")\n",
    "print(\"  - Un email l√©gitime class√© spam (FP) est moins grave\")\n",
    "print(\"    (l'utilisateur peut v√©rifier son dossier spam)\")\n",
    "print(f\"\\n  Recall actuel : {recall:.4f} (90% des spams d√©tect√©s)\")\n",
    "print(f\"  ‚Üí 10 spams passent √† travers (10 FN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 6 : Questions de R√©flexion\n",
    "\n",
    "### 6.1 Overfitting vs Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R√©ponse** :\n",
    "\n",
    "**Overfitting (Surapprentissage)** :\n",
    "- Le mod√®le apprend \"par c≈ìur\" les donn√©es d'entra√Ænement, incluant le bruit\n",
    "- Performance excellente sur le train set, mais mauvaise sur le test set\n",
    "- Le mod√®le est trop complexe par rapport aux donn√©es disponibles\n",
    "- **Exemple** : Un arbre de d√©cision avec une profondeur illimit√©e sur un petit dataset m√©morise chaque √©chantillon (accuracy train = 100%, test = 60%)\n",
    "\n",
    "**Underfitting (Sous-apprentissage)** :\n",
    "- Le mod√®le est trop simple pour capturer les patterns des donn√©es\n",
    "- Performance faible sur le train set ET le test set\n",
    "- Le mod√®le manque de capacit√© d'apprentissage\n",
    "- **Exemple** : Utiliser une r√©gression lin√©aire pour mod√©liser une relation non-lin√©aire (ex: relation quadratique). Le mod√®le ne peut pas capturer la courbe.\n",
    "\n",
    "**Solutions** :\n",
    "- Overfitting : R√©gularisation (L1/L2), r√©duction de complexit√©, plus de donn√©es, dropout\n",
    "- Underfitting : Mod√®le plus complexe, plus de features, moins de r√©gularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Choix de Mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R√©ponse** :\n",
    "\n",
    "Pour pr√©dire le prix d'une maison, je recommanderais :\n",
    "\n",
    "**1. Gradient Boosting (XGBoost, LightGBM, CatBoost)** - Choix principal\n",
    "- **Pourquoi** :\n",
    "  - Excellent sur les donn√©es tabulaires\n",
    "  - G√®re bien les relations non-lin√©aires\n",
    "  - Robuste aux outliers\n",
    "  - G√®re automatiquement les features cat√©gorielles (CatBoost)\n",
    "  - Performances state-of-the-art sur ce type de probl√®me\n",
    "- **Inconv√©nients** :\n",
    "  - Moins interpr√©table qu'une r√©gression lin√©aire\n",
    "  - N√©cessite un tuning d'hyperparam√®tres\n",
    "\n",
    "**2. Random Forest Regressor** - Alternative solide\n",
    "- **Pourquoi** :\n",
    "  - Robuste, peu de risque d'overfitting\n",
    "  - Fournit des feature importances\n",
    "  - Moins de tuning n√©cessaire\n",
    "- **Inconv√©nients** :\n",
    "  - Performances l√©g√®rement inf√©rieures au Gradient Boosting\n",
    "\n",
    "**3. Ridge Regression** - Pour la baseline et l'interpr√©tabilit√©\n",
    "- **Pourquoi** :\n",
    "  - Tr√®s interpr√©table (coefficients)\n",
    "  - Rapide √† entra√Æner\n",
    "  - Bon pour comprendre l'impact de chaque feature\n",
    "- **Inconv√©nients** :\n",
    "  - Assume une relation lin√©aire\n",
    "  - Performances limit√©es si la relation est complexe\n",
    "\n",
    "**Approche recommand√©e** :\n",
    "1. Commencer par Ridge (baseline interpr√©table)\n",
    "2. Essayer Random Forest\n",
    "3. Optimiser avec Gradient Boosting (meilleure performance)\n",
    "4. Comparer avec validation crois√©e\n",
    "5. Feature engineering : ratios, interactions, transformations log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 √âthique en ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R√©ponse** :\n",
    "\n",
    "**Biais potentiels dans un syst√®me de tri de CV** :\n",
    "\n",
    "1. **Biais de genre** :\n",
    "   - Historique : Si l'entreprise a historiquement embauch√© plus d'hommes, le mod√®le peut apprendre ce biais\n",
    "   - Indices indirects : Pr√©noms, loisirs genr√©s (\"football\" vs \"yoga\")\n",
    "\n",
    "2. **Biais racial/ethnique** :\n",
    "   - Noms de famille, √©tablissements d'enseignement dans certains quartiers\n",
    "   - Langues parl√©es\n",
    "\n",
    "3. **Biais d'√¢ge** :\n",
    "   - Ann√©e de dipl√¥me, nombre d'ann√©es d'exp√©rience\n",
    "   - Technologies anciennes vs r√©centes\n",
    "\n",
    "4. **Biais socio-√©conomique** :\n",
    "   - Universit√© prestigieuse vs moins connue\n",
    "   - Exp√©riences √† l'√©tranger (co√ªt prohibitif pour certains)\n",
    "\n",
    "**Solutions pour att√©nuer ces biais** :\n",
    "\n",
    "1. **Audit des donn√©es** :\n",
    "   - Analyser la distribution des candidats embauch√©s par d√©mographie\n",
    "   - Identifier les d√©s√©quilibres historiques\n",
    "\n",
    "2. **Anonymisation** :\n",
    "   - Retirer nom, pr√©nom, √¢ge, photo\n",
    "   - Masquer les ann√©es (remplacer par \"3-5 ans d'exp√©rience\")\n",
    "\n",
    "3. **Feature selection prudente** :\n",
    "   - Exclure les features sensibles (genre, ethnie, √¢ge)\n",
    "   - √âviter les proxies (codes postaux, √©tablissements)\n",
    "\n",
    "4. **Donn√©es d'entra√Ænement √©quilibr√©es** :\n",
    "   - R√©√©quilibrage (oversampling/undersampling)\n",
    "   - Utiliser des donn√©es synth√©tiques pour les groupes sous-repr√©sent√©s\n",
    "\n",
    "5. **Fairness constraints** :\n",
    "   - Imposer des contraintes d'√©quit√© pendant l'entra√Ænement\n",
    "   - V√©rifier le taux de s√©lection par groupe d√©mographique\n",
    "\n",
    "6. **Human-in-the-loop** :\n",
    "   - Le mod√®le assiste, ne d√©cide pas seul\n",
    "   - Revue humaine des candidatures recommand√©es\n",
    "   - Diversit√© dans l'√©quipe de recrutement\n",
    "\n",
    "7. **Monitoring continu** :\n",
    "   - Suivre les taux d'acceptation par d√©mographie\n",
    "   - Audits r√©guliers par une √©quipe ind√©pendante\n",
    "   - Feedback des candidats rejet√©s\n",
    "\n",
    "8. **Transparence** :\n",
    "   - Documenter le fonctionnement du syst√®me\n",
    "   - Expliquer aux candidats comment les d√©cisions sont prises\n",
    "   - Droit de contestation\n",
    "\n",
    "**Principe fondamental** : Le ML doit augmenter l'humain, pas le remplacer, surtout pour des d√©cisions impactant la vie des personnes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "F√©licitations pour avoir compl√©t√© ces exercices !\n",
    "\n",
    "Vous avez maintenant une bonne compr√©hension des concepts fondamentaux du Machine Learning :\n",
    "- Pipeline ML complet\n",
    "- EDA et visualisation\n",
    "- D√©tection et pr√©vention de l'overfitting\n",
    "- Validation crois√©e\n",
    "- Feature engineering\n",
    "- Interpr√©tation des m√©triques\n",
    "- Consid√©rations √©thiques\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "1. Pratiquer sur d'autres datasets (Kaggle)\n",
    "2. Approfondir les math√©matiques (Chapitre 01)\n",
    "3. Explorer les algorithmes en d√©tail (Chapitres 02-10)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}