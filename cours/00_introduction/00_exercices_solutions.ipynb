{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/00_introduction/00_exercices_solutions.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '00_exercices_solutions.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 00 - Solutions des Exercices\n",
    "\n",
    "Ce notebook contient les solutions d√©taill√©es des exercices du Chapitre 00.\n",
    "\n",
    "**Remarque** : Essayez d'abord de r√©soudre les exercices par vous-m√™me avant de consulter les solutions !\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_wine, load_breast_cancer, load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Biblioth√®ques import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 1 : Analyse Exploratoire (EDA)\n",
    "\n",
    "### 1.1 Chargement et Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset Wine\n",
    "wine = load_wine()\n",
    "\n",
    "# Cr√©ation du DataFrame\n",
    "df_wine = pd.DataFrame(\n",
    "    data=wine.data,\n",
    "    columns=wine.feature_names\n",
    ")\n",
    "df_wine['target'] = wine.target\n",
    "df_wine['target_name'] = df_wine['target'].map(\n",
    "    {i: name for i, name in enumerate(wine.target_names)}\n",
    ")\n",
    "\n",
    "print(\"Dataset Wine charg√©\")\n",
    "display(df_wine.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©ponses aux questions\n",
    "\n",
    "# 1. Nombre d'√©chantillons et features\n",
    "n_samples, n_features = wine.data.shape\n",
    "print(f\"1. Nombre d'√©chantillons : {n_samples}\")\n",
    "print(f\"   Nombre de features : {n_features}\")\n",
    "\n",
    "# 2. Classes cibles\n",
    "print(f\"\\n2. Classes cibles : {wine.target_names}\")\n",
    "print(f\"   Nombre de classes : {len(wine.target_names)}\")\n",
    "\n",
    "# 3. Valeurs manquantes\n",
    "print(f\"\\n3. Valeurs manquantes :\")\n",
    "print(df_wine.isnull().sum())\n",
    "print(\"   ‚Üí Aucune valeur manquante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Statistiques Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "display(df_wine.describe())\n",
    "\n",
    "print(\"\\nObservations :\")\n",
    "print(\"- Les features ont des √©chelles tr√®s diff√©rentes (ex: proline ~746, malic_acid ~2.3)\")\n",
    "print(\"- La standardisation sera importante pour certains algorithmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Distribution des Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des classes\n",
    "print(\"Distribution des classes :\")\n",
    "print(df_wine['target_name'].value_counts())\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Countplot\n",
    "sns.countplot(data=df_wine, x='target_name', ax=ax[0], palette='Set2')\n",
    "ax[0].set_title('Distribution des Classes', fontsize=14, fontweight='bold')\n",
    "ax[0].set_xlabel('Classe')\n",
    "ax[0].set_ylabel('Nombre d\\'√©chantillons')\n",
    "\n",
    "# Pie chart\n",
    "df_wine['target_name'].value_counts().plot.pie(\n",
    "    autopct='%1.1f%%',\n",
    "    ax=ax[1],\n",
    "    colors=sns.color_palette('Set2'),\n",
    "    startangle=90\n",
    ")\n",
    "ax[1].set_title('Proportion des Classes', fontsize=14, fontweight='bold')\n",
    "ax[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nR√©ponse : Le dataset est l√©g√®rement d√©s√©quilibr√©\")\n",
    "print(\"class_0: 59 √©chantillons (33%), class_1: 71 (40%), class_2: 48 (27%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Matrice de Corr√©lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corr√©lation\n",
    "corr_matrix = df_wine[wine.feature_names].corr()\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    "    fmt='.2f',\n",
    "    annot_kws={'size': 8}\n",
    ")\n",
    "plt.title('Matrice de Corr√©lation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identifier les corr√©lations les plus fortes\n",
    "print(\"\\nPaires de features les plus corr√©l√©es :\")\n",
    "corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append((\n",
    "            corr_matrix.columns[i],\n",
    "            corr_matrix.columns[j],\n",
    "            abs(corr_matrix.iloc[i, j])\n",
    "        ))\n",
    "\n",
    "corr_pairs_sorted = sorted(corr_pairs, key=lambda x: x[2], reverse=True)\n",
    "for feat1, feat2, corr in corr_pairs_sorted[:5]:\n",
    "    print(f\"  {feat1[:20]:20s} ‚Üî {feat2[:20]:20s} : {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot pour les 4 premi√®res features\n",
    "features_to_plot = wine.feature_names[:4]\n",
    "df_subset = df_wine[list(features_to_plot) + ['target_name']]\n",
    "\n",
    "sns.pairplot(\n",
    "    df_subset,\n",
    "    hue='target_name',\n",
    "    palette='Set2',\n",
    "    diag_kind='kde',\n",
    "    markers=['o', 's', 'D'],\n",
    "    plot_kws={'alpha': 0.6}\n",
    ")\n",
    "plt.suptitle('Pairplot - 4 Premi√®res Features', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 2 : Pipeline ML Complet\n",
    "\n",
    "### 2.1 Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset Breast Cancer\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "print(f\"Nombre d'√©chantillons : {X.shape[0]}\")\n",
    "print(f\"Nombre de features : {X.shape[1]}\")\n",
    "print(f\"Classes : {cancer.target_names}\")\n",
    "print(f\"Distribution : {np.bincount(y)} (0=malignant, 1=benign)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 70/30 avec stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set : {X_train.shape[0]} √©chantillons ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set  : {X_test.shape[0]} √©chantillons ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribution train : {np.bincount(y_train)}\")\n",
    "print(f\"Distribution test  : {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation (fit sur train uniquement)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úì Standardisation effectu√©e\")\n",
    "print(f\"Moyennes (train) : {X_train_scaled.mean(axis=0)[:5].round(2)}\")\n",
    "print(f\"√âcart-types (train) : {X_train_scaled.std(axis=0)[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Entra√Ænement de Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire de mod√®les\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=10000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Entra√Ænement des mod√®les...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"‚è≥ {name}...\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    if name in ['Logistic Regression', 'SVM']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Stockage\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"   Accuracy : {accuracy:.4f}\")\n",
    "    print(f\"   CV Score : {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\\n\")\n",
    "\n",
    "print(\"‚úÖ Entra√Ænement termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 √âvaluation et Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de comparaison\n",
    "comparison = pd.DataFrame({\n",
    "    'Mod√®le': list(results.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
    "    'CV Mean': [r['cv_mean'] for r in results.values()],\n",
    "    'CV Std': [r['cv_std'] for r in results.values()]\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"Comparaison des performances :\")\n",
    "display(comparison)\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_pos = np.arange(len(comparison))\n",
    "\n",
    "ax.barh(\n",
    "    x_pos,\n",
    "    comparison['Accuracy'],\n",
    "    color=sns.color_palette('viridis', len(comparison)),\n",
    "    edgecolor='black'\n",
    ")\n",
    "ax.set_yticks(x_pos)\n",
    "ax.set_yticklabels(comparison['Mod√®le'])\n",
    "ax.set_xlabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Comparaison des Performances', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0.9, 1.0])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Annotations\n",
    "for i, v in enumerate(comparison['Accuracy']):\n",
    "    ax.text(v + 0.005, i, f\"{v:.4f}\", va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 √âvaluation D√©taill√©e du Meilleur Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lection du meilleur mod√®le\n",
    "best_model_name = comparison.iloc[0]['Mod√®le']\n",
    "best_model = results[best_model_name]['model']\n",
    "best_y_pred = results[best_model_name]['y_pred']\n",
    "\n",
    "print(f\"üèÜ Meilleur mod√®le : {best_model_name}\\n\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report :\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    best_y_pred,\n",
    "    target_names=cancer.target_names\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Matrice absolue\n",
    "disp1 = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=cancer.target_names\n",
    ")\n",
    "disp1.plot(ax=ax[0], cmap='Blues', values_format='d')\n",
    "ax[0].set_title('Matrice de Confusion', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Matrice normalis√©e\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "disp2 = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_norm,\n",
    "    display_labels=cancer.target_names\n",
    ")\n",
    "disp2.plot(ax=ax[1], cmap='Blues', values_format='.2%')\n",
    "ax[1].set_title('Matrice de Confusion (Normalis√©e)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 3 : D√©tection d'Overfitting\n",
    "\n",
    "### 3.1 Cr√©ation d'un Dataset avec Peu d'√âchantillons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Wine avec seulement 30 √©chantillons\n",
    "wine = load_wine()\n",
    "\n",
    "# S√©lection al√©atoire de 30 indices\n",
    "indices = np.random.choice(len(wine.data), size=30, replace=False)\n",
    "\n",
    "X_small = wine.data[indices]\n",
    "y_small = wine.target[indices]\n",
    "\n",
    "# Split 20/10\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=10,\n",
    "    random_state=42,\n",
    "    stratify=y_small\n",
    ")\n",
    "\n",
    "print(f\"Train : {len(X_train_small)} √©chantillons\")\n",
    "print(f\"Test  : {len(X_test_small)} √©chantillons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entra√Ænement d'un Mod√®le Complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree sans limitation de profondeur\n",
    "tree_overfit = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
    "tree_overfit.fit(X_train_small, y_train_small)\n",
    "\n",
    "# Accuracy sur train et test\n",
    "train_accuracy = tree_overfit.score(X_train_small, y_train_small)\n",
    "test_accuracy = tree_overfit.score(X_test_small, y_test_small)\n",
    "\n",
    "print(f\"Train Accuracy : {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy  : {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Analyse :\")\n",
    "if train_accuracy > test_accuracy + 0.1:\n",
    "    print(\"   ‚ö†Ô∏è OVERFITTING d√©tect√© !\")\n",
    "    print(\"   Le mod√®le a une performance parfaite sur le train (1.00)\")\n",
    "    print(\"   mais beaucoup plus faible sur le test.\")\n",
    "    print(\"   ‚Üí Le mod√®le a m√©moris√© les donn√©es d'entra√Ænement.\")\n",
    "else:\n",
    "    print(\"   ‚úì Pas d'overfitting majeur d√©tect√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 R√©gularisation pour R√©duire l'Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de diff√©rentes profondeurs\n",
    "max_depths = [2, 3, 4, 5, 6, 7, 8, None]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train_small, y_train_small)\n",
    "    \n",
    "    train_scores.append(tree.score(X_train_small, y_train_small))\n",
    "    test_scores.append(tree.score(X_test_small, y_test_small))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Conversion de None en \"No limit\" pour l'affichage\n",
    "depths_labels = [str(d) if d is not None else \"No limit\" for d in max_depths]\n",
    "x_pos = np.arange(len(max_depths))\n",
    "\n",
    "plt.plot(x_pos, train_scores, 'o-', linewidth=2, markersize=8, label='Train', color='blue')\n",
    "plt.plot(x_pos, test_scores, 's-', linewidth=2, markersize=8, label='Test', color='red')\n",
    "\n",
    "plt.xticks(x_pos, depths_labels)\n",
    "plt.xlabel('max_depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Train vs Test Accuracy en fonction de max_depth', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "# Zone d'overfitting\n",
    "plt.axvspan(5.5, 7.5, alpha=0.2, color='red', label='Zone d\\'overfitting')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Meilleure profondeur\n",
    "best_depth_idx = np.argmax(test_scores)\n",
    "best_depth = max_depths[best_depth_idx]\n",
    "print(f\"\\n‚úÖ Meilleure profondeur : {best_depth}\")\n",
    "print(f\"   Test Accuracy : {test_scores[best_depth_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 4 : Validation Crois√©e\n",
    "\n",
    "### 4.1 Comparaison avec et sans Validation Crois√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Wine complet\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# M√©thode 1 : Simple train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "accuracy_simple = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy (simple split) : {accuracy_simple:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 2 : Validation crois√©e 5-fold\n",
    "model_cv = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "cv_scores = cross_val_score(model_cv, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nCV Scores : {cv_scores}\")\n",
    "print(f\"CV Mean   : {cv_scores.mean():.4f}\")\n",
    "print(f\"CV Std    : {cv_scores.std():.4f}\")\n",
    "\n",
    "print(\"\\nüí° R√©ponse :\")\n",
    "print(\"   La validation crois√©e est plus fiable car :\")\n",
    "print(\"   1. Elle utilise toutes les donn√©es (pas de perte)\")\n",
    "print(\"   2. Elle donne une estimation plus robuste (moyenne sur plusieurs splits)\")\n",
    "print(\"   3. Elle r√©duit la variance de l'estimation\")\n",
    "print(\"   4. Elle fournit un intervalle de confiance (std)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Impact du Nombre de Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de diff√©rents nombres de folds\n",
    "folds = [3, 5, 10, 20]\n",
    "cv_results = []\n",
    "\n",
    "for k in folds:\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=k, scoring='accuracy')\n",
    "    cv_results.append({\n",
    "        'k': k,\n",
    "        'scores': scores,\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std()\n",
    "    })\n",
    "    print(f\"k={k:2d} ‚Üí Mean: {scores.mean():.4f}, Std: {scores.std():.4f}\")\n",
    "\n",
    "# Visualisation (boxplot)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "positions = np.arange(len(folds))\n",
    "bp = ax.boxplot(\n",
    "    [r['scores'] for r in cv_results],\n",
    "    positions=positions,\n",
    "    widths=0.6,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightblue', edgecolor='black'),\n",
    "    medianprops=dict(color='red', linewidth=2),\n",
    "    whiskerprops=dict(color='black'),\n",
    "    capprops=dict(color='black')\n",
    ")\n",
    "\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels([f\"{k}-fold\" for k in folds])\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_xlabel('Nombre de Folds', fontsize=12)\n",
    "ax.set_title('Impact du Nombre de Folds sur la CV', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observations :\")\n",
    "print(\"   - Plus k est grand, plus la variance diminue\")\n",
    "print(\"   - k=5 ou k=10 sont des choix standards\")\n",
    "print(\"   - k trop grand augmente le temps de calcul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 5 : Feature Engineering\n",
    "\n",
    "### 5.1 Cr√©ation de Nouvelles Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Iris\n",
    "iris = load_iris()\n",
    "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df_iris['target'] = iris.target\n",
    "\n",
    "# Cr√©ation de nouvelles features\n",
    "df_iris['sepal_ratio'] = df_iris['sepal length (cm)'] / df_iris['sepal width (cm)']\n",
    "df_iris['petal_ratio'] = df_iris['petal length (cm)'] / df_iris['petal width (cm)']\n",
    "df_iris['petal_area'] = df_iris['petal length (cm)'] * df_iris['petal width (cm)']\n",
    "\n",
    "print(\"Nouvelles features cr√©√©es :\")\n",
    "display(df_iris.head(10))\n",
    "\n",
    "# Visualisation des nouvelles features\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for i, (col, axis) in enumerate(zip(['sepal_ratio', 'petal_ratio', 'petal_area'], ax)):\n",
    "    for target in range(3):\n",
    "        subset = df_iris[df_iris['target'] == target]\n",
    "        axis.hist(\n",
    "            subset[col],\n",
    "            alpha=0.6,\n",
    "            label=iris.target_names[target],\n",
    "            bins=15,\n",
    "            edgecolor='black'\n",
    "        )\n",
    "    axis.set_xlabel(col, fontsize=11)\n",
    "    axis.set_ylabel('Fr√©quence', fontsize=11)\n",
    "    axis.set_title(f'Distribution : {col}', fontsize=12, fontweight='bold')\n",
    "    axis.legend()\n",
    "    axis.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Comparaison Avant/Apr√®s Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le sans nouvelles features\n",
    "X_original = iris.data\n",
    "y = iris.target\n",
    "\n",
    "model_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "cv_original = cross_val_score(model_original, X_original, y, cv=5)\n",
    "\n",
    "print(\"Sans feature engineering :\")\n",
    "print(f\"  CV Mean : {cv_original.mean():.4f}\")\n",
    "print(f\"  CV Std  : {cv_original.std():.4f}\")\n",
    "\n",
    "# Mod√®le avec nouvelles features\n",
    "feature_cols = [col for col in df_iris.columns if col != 'target']\n",
    "X_engineered = df_iris[feature_cols].values\n",
    "\n",
    "model_engineered = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "cv_engineered = cross_val_score(model_engineered, X_engineered, y, cv=5)\n",
    "\n",
    "print(\"\\nAvec feature engineering :\")\n",
    "print(f\"  CV Mean : {cv_engineered.mean():.4f}\")\n",
    "print(f\"  CV Std  : {cv_engineered.std():.4f}\")\n",
    "\n",
    "# Comparaison\n",
    "improvement = cv_engineered.mean() - cv_original.mean()\n",
    "print(f\"\\n‚úÖ Am√©lioration : {improvement:.4f} ({improvement/cv_original.mean()*100:.2f}%)\")\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bp = ax.boxplot(\n",
    "    [cv_original, cv_engineered],\n",
    "    labels=['Sans FE', 'Avec FE'],\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightblue', edgecolor='black'),\n",
    "    medianprops=dict(color='red', linewidth=2)\n",
    ")\n",
    "\n",
    "ax.set_ylabel('CV Accuracy', fontsize=12)\n",
    "ax.set_title('Impact du Feature Engineering', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 6 : Interpr√©tation des R√©sultats\n",
    "\n",
    "### 6.1 Analyse d'une Matrice de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion fictive\n",
    "cm = np.array([\n",
    "    [85, 15],  # Classe 0 (non-spam)\n",
    "    [10, 90]   # Classe 1 (spam)\n",
    "])\n",
    "\n",
    "# Calcul des composantes\n",
    "TN = cm[0, 0]  # True Negatives\n",
    "FP = cm[0, 1]  # False Positives\n",
    "FN = cm[1, 0]  # False Negatives\n",
    "TP = cm[1, 1]  # True Positives\n",
    "\n",
    "print(\"Composantes de la matrice de confusion :\")\n",
    "print(f\"  TN (True Negatives)  : {TN}\")\n",
    "print(f\"  FP (False Positives) : {FP}\")\n",
    "print(f\"  FN (False Negatives) : {FN}\")\n",
    "print(f\"  TP (True Positives)  : {TP}\")\n",
    "\n",
    "# Calcul des m√©triques\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"\\nM√©triques :\")\n",
    "print(f\"  Accuracy  : {accuracy:.4f}\")\n",
    "print(f\"  Precision : {precision:.4f}\")\n",
    "print(f\"  Recall    : {recall:.4f}\")\n",
    "print(f\"  F1-Score  : {f1:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation dans le contexte du spam :\")\n",
    "print(\"\\n  RECALL (Sensibilit√©) est la m√©trique la plus importante car :\")\n",
    "print(\"  - On veut d√©tecter un maximum de spams (minimiser les FN)\")\n",
    "print(\"  - Un spam non d√©tect√© (FN) peut contenir malware/phishing\")\n",
    "print(\"  - Un email l√©gitime class√© spam (FP) est moins grave\")\n",
    "print(\"    (l'utilisateur peut v√©rifier son dossier spam)\")\n",
    "print(f\"\\n  Recall actuel : {recall:.4f} (90% des spams d√©tect√©s)\")\n",
    "print(f\"  ‚Üí 10 spams passent √† travers (10 FN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution Exercice 7 : Questions de R√©flexion\n",
    "\n",
    "### 7.1 Overfitting vs Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R√©ponse** :\n",
    "\n",
    "**Overfitting (Surapprentissage)** :\n",
    "- Le mod√®le apprend \"par c≈ìur\" les donn√©es d'entra√Ænement, incluant le bruit\n",
    "- Performance excellente sur le train set, mais mauvaise sur le test set\n",
    "- Le mod√®le est trop complexe par rapport aux donn√©es disponibles\n",
    "- **Exemple** : Un arbre de d√©cision avec une profondeur illimit√©e sur un petit dataset m√©morise chaque √©chantillon (accuracy train = 100%, test = 60%)\n",
    "\n",
    "**Underfitting (Sous-apprentissage)** :\n",
    "- Le mod√®le est trop simple pour capturer les patterns des donn√©es\n",
    "- Performance faible sur le train set ET le test set\n",
    "- Le mod√®le manque de capacit√© d'apprentissage\n",
    "- **Exemple** : Utiliser une r√©gression lin√©aire pour mod√©liser une relation non-lin√©aire (ex: relation quadratique). Le mod√®le ne peut pas capturer la courbe.\n",
    "\n",
    "**Solutions** :\n",
    "- Overfitting : R√©gularisation (L1/L2), r√©duction de complexit√©, plus de donn√©es, dropout\n",
    "- Underfitting : Mod√®le plus complexe, plus de features, moins de r√©gularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Choix de Mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R√©ponse** :\n",
    "\n",
    "Pour pr√©dire le prix d'une maison, je recommanderais :\n",
    "\n",
    "**1. Gradient Boosting (XGBoost, LightGBM, CatBoost)** - Choix principal\n",
    "- **Pourquoi** :\n",
    "  - Excellent sur les donn√©es tabulaires\n",
    "  - G√®re bien les relations non-lin√©aires\n",
    "  - Robuste aux outliers\n",
    "  - G√®re automatiquement les features cat√©gorielles (CatBoost)\n",
    "  - Performances state-of-the-art sur ce type de probl√®me\n",
    "- **Inconv√©nients** :\n",
    "  - Moins interpr√©table qu'une r√©gression lin√©aire\n",
    "  - N√©cessite un tuning d'hyperparam√®tres\n",
    "\n",
    "**2. Random Forest Regressor** - Alternative solide\n",
    "- **Pourquoi** :\n",
    "  - Robuste, peu de risque d'overfitting\n",
    "  - Fournit des feature importances\n",
    "  - Moins de tuning n√©cessaire\n",
    "- **Inconv√©nients** :\n",
    "  - Performances l√©g√®rement inf√©rieures au Gradient Boosting\n",
    "\n",
    "**3. Ridge Regression** - Pour la baseline et l'interpr√©tabilit√©\n",
    "- **Pourquoi** :\n",
    "  - Tr√®s interpr√©table (coefficients)\n",
    "  - Rapide √† entra√Æner\n",
    "  - Bon pour comprendre l'impact de chaque feature\n",
    "- **Inconv√©nients** :\n",
    "  - Assume une relation lin√©aire\n",
    "  - Performances limit√©es si la relation est complexe\n",
    "\n",
    "**Approche recommand√©e** :\n",
    "1. Commencer par Ridge (baseline interpr√©table)\n",
    "2. Essayer Random Forest\n",
    "3. Optimiser avec Gradient Boosting (meilleure performance)\n",
    "4. Comparer avec validation crois√©e\n",
    "5. Feature engineering : ratios, interactions, transformations log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 √âthique en ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R√©ponse** :\n",
    "\n",
    "**Biais potentiels dans un syst√®me de tri de CV** :\n",
    "\n",
    "1. **Biais de genre** :\n",
    "   - Historique : Si l'entreprise a historiquement embauch√© plus d'hommes, le mod√®le peut apprendre ce biais\n",
    "   - Indices indirects : Pr√©noms, loisirs genr√©s (\"football\" vs \"yoga\")\n",
    "\n",
    "2. **Biais racial/ethnique** :\n",
    "   - Noms de famille, √©tablissements d'enseignement dans certains quartiers\n",
    "   - Langues parl√©es\n",
    "\n",
    "3. **Biais d'√¢ge** :\n",
    "   - Ann√©e de dipl√¥me, nombre d'ann√©es d'exp√©rience\n",
    "   - Technologies anciennes vs r√©centes\n",
    "\n",
    "4. **Biais socio-√©conomique** :\n",
    "   - Universit√© prestigieuse vs moins connue\n",
    "   - Exp√©riences √† l'√©tranger (co√ªt prohibitif pour certains)\n",
    "\n",
    "**Solutions pour att√©nuer ces biais** :\n",
    "\n",
    "1. **Audit des donn√©es** :\n",
    "   - Analyser la distribution des candidats embauch√©s par d√©mographie\n",
    "   - Identifier les d√©s√©quilibres historiques\n",
    "\n",
    "2. **Anonymisation** :\n",
    "   - Retirer nom, pr√©nom, √¢ge, photo\n",
    "   - Masquer les ann√©es (remplacer par \"3-5 ans d'exp√©rience\")\n",
    "\n",
    "3. **Feature selection prudente** :\n",
    "   - Exclure les features sensibles (genre, ethnie, √¢ge)\n",
    "   - √âviter les proxies (codes postaux, √©tablissements)\n",
    "\n",
    "4. **Donn√©es d'entra√Ænement √©quilibr√©es** :\n",
    "   - R√©√©quilibrage (oversampling/undersampling)\n",
    "   - Utiliser des donn√©es synth√©tiques pour les groupes sous-repr√©sent√©s\n",
    "\n",
    "5. **Fairness constraints** :\n",
    "   - Imposer des contraintes d'√©quit√© pendant l'entra√Ænement\n",
    "   - V√©rifier le taux de s√©lection par groupe d√©mographique\n",
    "\n",
    "6. **Human-in-the-loop** :\n",
    "   - Le mod√®le assiste, ne d√©cide pas seul\n",
    "   - Revue humaine des candidatures recommand√©es\n",
    "   - Diversit√© dans l'√©quipe de recrutement\n",
    "\n",
    "7. **Monitoring continu** :\n",
    "   - Suivre les taux d'acceptation par d√©mographie\n",
    "   - Audits r√©guliers par une √©quipe ind√©pendante\n",
    "   - Feedback des candidats rejet√©s\n",
    "\n",
    "8. **Transparence** :\n",
    "   - Documenter le fonctionnement du syst√®me\n",
    "   - Expliquer aux candidats comment les d√©cisions sont prises\n",
    "   - Droit de contestation\n",
    "\n",
    "**Principe fondamental** : Le ML doit augmenter l'humain, pas le remplacer, surtout pour des d√©cisions impactant la vie des personnes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "F√©licitations pour avoir compl√©t√© ces exercices !\n",
    "\n",
    "Vous avez maintenant une bonne compr√©hension des concepts fondamentaux du Machine Learning :\n",
    "- Pipeline ML complet\n",
    "- EDA et visualisation\n",
    "- D√©tection et pr√©vention de l'overfitting\n",
    "- Validation crois√©e\n",
    "- Feature engineering\n",
    "- Interpr√©tation des m√©triques\n",
    "- Consid√©rations √©thiques\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "1. Pratiquer sur d'autres datasets (Kaggle)\n",
    "2. Approfondir les math√©matiques (Chapitre 01)\n",
    "3. Explorer les algorithmes en d√©tail (Chapitres 02-10)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}