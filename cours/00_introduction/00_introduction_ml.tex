% Chapitre 00 - Introduction au Machine Learning

\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}

% Math√©matiques
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

% Mise en page
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}

% Graphiques et couleurs
\usepackage{graphicx}
\usepackage{xcolor}

% ===== UNICODE CHARACTERS SUPPORT =====
\usepackage{newunicodechar}

% Emojis et symboles
\newunicodechar{‚úÖ}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚ùå}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚úì}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚úó}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚ö†}{\textcolor{orange!80!black}{\textbf{/!\textbackslash}}}
\newunicodechar{üí°}{\textcolor{blue!70!black}{\textbf{(i)}}}
\newunicodechar{üéØ}{\textcolor{purple!70!black}{\textbf{$\star$}}}
\newunicodechar{üìä}{\textcolor{blue!70!black}{\textbf{[=]}}}

% √âtoiles (pour tableaux)
\newunicodechar{‚òÖ}{\textcolor{orange!80!black}{$\star$}}
\newunicodechar{‚òÜ}{\textcolor{gray!50}{$\star$}}

% Fl√®ches
\newunicodechar{‚Üí}{$\rightarrow$}
\newunicodechar{‚Üê}{$\leftarrow$}
\newunicodechar{‚Üë}{$\uparrow$}
\newunicodechar{‚Üì}{$\downarrow$}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric}

% Tableaux
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}

% Code et algorithmes
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Hyperliens
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 00 - Introduction au Machine Learning},
    pdfauthor={Cours ML},
}

% Boxes color√©es
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}


% ===== TCOLORBOX AVEC EMOJIS =====
\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö† Attention,
    breakable
}

\newtcolorbox{definition}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition,
    breakable
}

\newtcolorbox{astuce}{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

\newtcolorbox{remarque}{
    colback=yellow!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=üí° Remarque,
    breakable
}

\newtcolorbox{important}{
    colback=purple!5!white,
    colframe=purple!75!black,
    fonttitle=\bfseries,
    title=‚ö† Important,
    breakable
}

\newtcolorbox{exemple}{
    colback=gray!5!white,
    colframe=gray!75!black,
    fonttitle=\bfseries,
    title=Exemple,
    breakable
}

% En-t√™tes et pieds de page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 00 - Introduction au Machine Learning}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIGURATION LISTINGS (code Python) =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}
\lstset{style=pythonstyle}

% ===== CONFIGURATION TCOLORBOX =====
% Box pour d√©finitions


% Box pour th√©or√®mes
\newtcolorbox{theoreme}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Th√©or√®me: #1,
    breakable
}

% Box pour exemples


% Box pour attention/warning


% Box pour astuce/tips


% ===== COMMANDES PERSONNALIS√âES =====
\newcommand{\vect}[1]{\mathbf{#1}}  % Vecteur
\newcommand{\mat}[1]{\mathbf{#1}}   % Matrice
\newcommand{\R}{\mathbb{R}}         % R√©els
\newcommand{\N}{\mathbb{N}}         % Naturels
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}

% ===== D√âBUT DU DOCUMENT =====
\begin{document}

% ===== PAGE DE TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]

    \vspace{1cm}

    {\LARGE Chapitre 00}\\[0.3cm]
    {\LARGE\bfseries Introduction au Machine Learning}\\[2cm]

    \vfill

    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Comprendre ce qu'est le Machine Learning et son √©volution historique
        \item Ma√Ætriser les diff√©rents types d'apprentissage automatique
        \item Conna√Ætre le vocabulaire fondamental et le pipeline ML typique
        \item Distinguer ML classique et Deep Learning
        \item Identifier les applications concr√®tes et les enjeux √©thiques
    \end{itemize}
    }

    \vfill

    {\large
    \textbf{Pr√©requis :} Aucun (chapitre d'introduction)\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 4-6 heures (incluant formation Python)\\[0.3cm]
    \textbf{Notebooks :} \texttt{00\_prerequis\_python.ipynb}, \texttt{00\_demo\_ml\_pipeline.ipynb}, \texttt{00\_exercices.ipynb}
    }

    \vfill

    {\large Cours ML - Sandbox-ML\\
    Version 1.0 - 2026}
\end{titlepage}

% ===== TABLE DES MATI√àRES =====
\tableofcontents
\newpage

% ===== SECTION 1: QU'EST-CE QUE LE MACHINE LEARNING ? =====
\section{Qu'est-ce que le Machine Learning ?}

\subsection{D√©finition}

\begin{definition}{Machine Learning (Apprentissage Automatique)}
Le Machine Learning est un sous-domaine de l'intelligence artificielle qui permet aux ordinateurs d'apprendre √† partir de donn√©es sans √™tre explicitement programm√©s. Un programme apprend d'une exp√©rience $E$ par rapport √† une t√¢che $T$ et une mesure de performance $P$, si sa performance sur $T$, mesur√©e par $P$, s'am√©liore avec l'exp√©rience $E$.
\end{definition}

Cette d√©finition classique de Tom Mitchell (1997) met en √©vidence trois composantes essentielles :

\begin{itemize}
    \item \textbf{T√¢che (T)} : Ce que le syst√®me doit accomplir (classification, pr√©diction, recommandation, etc.)
    \item \textbf{Exp√©rience (E)} : Les donn√©es utilis√©es pour l'apprentissage
    \item \textbf{Performance (P)} : La m√©trique utilis√©e pour √©valuer la qualit√© du syst√®me
\end{itemize}

\begin{exemple}{Reconnaissance de spam}
\begin{itemize}
    \item \textbf{T√¢che :} Classifier un email comme spam ou non-spam
    \item \textbf{Exp√©rience :} Base de donn√©es d'emails d√©j√† √©tiquet√©s (spam/non-spam)
    \item \textbf{Performance :} Pourcentage d'emails correctement classifi√©s
\end{itemize}
\end{exemple}

\subsection{Programmation Traditionnelle vs Machine Learning}

Le Machine Learning inverse le paradigme de la programmation traditionnelle :

\begin{center}
\textbf{Programmation Traditionnelle}

\vspace{0.3cm}
\begin{tikzpicture}[
    box/.style={rectangle, draw, minimum width=2cm, minimum height=0.8cm, align=center},
    process/.style={rectangle, draw, fill=blue!10, minimum width=2cm, minimum height=0.8cm, align=center},
    arrow/.style={->, thick}
]
    \node[box] (data1) {Donn√©es};
    \node[process, right=1.5cm of data1] (rules1) {R√®gles\\(code)};
    \node[box, right=1.5cm of rules1] (output1) {R√©sultats};

    \draw[arrow] (data1) -- (rules1);
    \draw[arrow] (rules1) -- (output1);
\end{tikzpicture}

\vspace{0.8cm}
\textbf{Machine Learning}

\vspace{0.3cm}
\begin{tikzpicture}[
    box/.style={rectangle, draw, minimum width=2cm, minimum height=0.8cm, align=center},
    process/.style={rectangle, draw, fill=green!10, minimum width=2cm, minimum height=0.8cm, align=center},
    arrow/.style={->, thick}
]
    \node[box] (data2) {Donn√©es};
    \node[box, below=0.5cm of data2] (labels) {Labels};
    \node[process, right=1.5cm of data2, yshift=-0.4cm] (model) {Algorithme\\ML};
    \node[box, right=1.5cm of model] (output2) {Mod√®le\\(r√®gles)};

    \draw[arrow] (data2.east) -- ([yshift=0.2cm]model.west);
    \draw[arrow] (labels.east) -- ([yshift=-0.2cm]model.west);
    \draw[arrow] (model) -- (output2);
\end{tikzpicture}
\end{center}

\textbf{Programmation traditionnelle :} On √©crit des r√®gles explicites pour transformer les donn√©es en r√©sultats.

\textbf{Machine Learning :} On fournit des donn√©es et des r√©sultats attendus, et l'algorithme apprend les r√®gles automatiquement.

\subsection{Quand utiliser le Machine Learning ?}

\begin{astuce}
Le Machine Learning est particuli√®rement adapt√© quand :
\begin{itemize}
    \item Le probl√®me est trop complexe pour √™tre r√©solu par des r√®gles explicites
    \item Les r√®gles changent fr√©quemment (adaptation dynamique n√©cessaire)
    \item Il existe de grandes quantit√©s de donn√©es disponibles
    \item Le probl√®me n√©cessite de la personnalisation (recommandations, publicit√© cibl√©e)
    \item On cherche √† d√©couvrir des patterns cach√©s dans les donn√©es
\end{itemize}
\end{astuce}

\begin{attention}
√âviter le Machine Learning quand :
\begin{itemize}
    \item Des r√®gles simples et explicites suffisent
    \item Les donn√©es sont insuffisantes ou de mauvaise qualit√©
    \item L'explicabilit√© est critique et le mod√®le doit √™tre interpr√©table
    \item Le co√ªt de calcul est prohibitif pour le b√©n√©fice attendu
\end{itemize}
\end{attention}

% ===== SECTION 2: HISTOIRE DU MACHINE LEARNING =====
\section{Histoire du Machine Learning (1950-2026)}

\subsection{Chronologie des √âv√©nements Majeurs}

\begin{table}[h]
\centering
\caption{Principales √©tapes historiques du Machine Learning}
\label{tab:histoire}
\small
\begin{tabular}{p{2cm}p{10cm}}
\toprule
\textbf{P√©riode} & \textbf{√âv√©nements Marquants} \\
\midrule
\textbf{1950s} &
    \begin{itemize}
        \item 1950 : Test de Turing (Alan Turing)
        \item 1957 : Perceptron (Frank Rosenblatt)
        \item 1959 : Terme "Machine Learning" (Arthur Samuel)
    \end{itemize} \\
\midrule
\textbf{1960-70s} &
    \begin{itemize}
        \item 1969 : Limitations du perceptron (Minsky \& Papert)
        \item Premier "hiver de l'IA" (1974-1980)
        \item 1979 : Backpropagation (Werbos)
    \end{itemize} \\
\midrule
\textbf{1980s} &
    \begin{itemize}
        \item 1986 : Popularisation du backpropagation (Rumelhart et al.)
        \item D√©veloppement des r√©seaux de neurones multicouches
        \item Syst√®mes experts et IA symbolique
    \end{itemize} \\
\midrule
\textbf{1990s} &
    \begin{itemize}
        \item 1995 : Support Vector Machines (Vapnik)
        \item 1997 : Random Forests (Ho), LSTM (Hochreiter \& Schmidhuber)
        \item 1997 : Deep Blue bat Kasparov aux √©checs
    \end{itemize} \\
\midrule
\textbf{2000s} &
    \begin{itemize}
        \item 2001 : Gradient Boosting Machines
        \item 2006 : Deep Learning renaissance (Hinton et al.)
        \item 2009 : ImageNet dataset cr√©√©
    \end{itemize} \\
\midrule
\textbf{2010s} &
    \begin{itemize}
        \item 2012 : AlexNet r√©volutionne la vision par ordinateur
        \item 2014 : GANs (Generative Adversarial Networks - Goodfellow)
        \item 2016 : AlphaGo bat Lee Sedol au Go
        \item 2017 : Transformers (Attention is All You Need)
        \item 2018 : BERT pour le NLP
    \end{itemize} \\
\midrule
\textbf{2020s} &
    \begin{itemize}
        \item 2020 : GPT-3 (175B param√®tres)
        \item 2021 : AlphaFold r√©sout le repliement des prot√©ines
        \item 2022 : ChatGPT, Stable Diffusion, DALL-E 2
        \item 2023 : GPT-4, LLaMA, mod√®les multimodaux
        \item 2024-2026 : D√©mocratisation de l'IA g√©n√©rative, LLMs open-source
    \end{itemize} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Les Trois Vagues du Machine Learning}

\textbf{Premi√®re vague (1950-1980) : L'√®re symbolique}
\begin{itemize}
    \item R√®gles logiques et syst√®mes experts
    \item Limitations : incapacit√© √† g√©rer l'incertitude et la complexit√©
\end{itemize}

\textbf{Deuxi√®me vague (1980-2010) : L'apprentissage statistique}
\begin{itemize}
    \item Approches probabilistes et statistiques
    \item SVMs, Random Forests, Gradient Boosting
    \item M√©thodes bas√©es sur des caract√©ristiques con√ßues manuellement
\end{itemize}

\textbf{Troisi√®me vague (2010-aujourd'hui) : Le Deep Learning}
\begin{itemize}
    \item R√©seaux de neurones profonds
    \item Apprentissage automatique de repr√©sentations hi√©rarchiques
    \item Performances surhumaines sur certaines t√¢ches
\end{itemize}

% ===== SECTION 3: TYPES D'APPRENTISSAGE =====
\section{Types d'Apprentissage Automatique}

\subsection{Apprentissage Supervis√©}

\begin{definition}{Apprentissage Supervis√©}
L'apprentissage supervis√© consiste √† apprendre une fonction de mapping $f: X \to Y$ √† partir d'un ensemble de donn√©es √©tiquet√©es $\{(\vect{x}_i, y_i)\}_{i=1}^n$ o√π $\vect{x}_i$ sont les features (entr√©es) et $y_i$ sont les labels (sorties attendues).
\end{definition}

\textbf{Formulation math√©matique :}

On cherche √† minimiser le risque empirique :
\begin{equation}
    \hat{f} = \argmin_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n L(y_i, f(\vect{x}_i))
\end{equation}

o√π $L$ est une fonction de perte et $\mathcal{F}$ est l'espace des fonctions consid√©r√©es.

\textbf{Deux grandes familles :}

\begin{itemize}
    \item \textbf{R√©gression :} $Y$ est continu (ex: pr√©dire le prix d'une maison)
    \begin{itemize}
        \item R√©gression lin√©aire, Ridge, Lasso
        \item Arbres de d√©cision, Random Forests
        \item SVR (Support Vector Regression)
        \item R√©seaux de neurones
    \end{itemize}

    \item \textbf{Classification :} $Y$ est discret (ex: spam/non-spam)
    \begin{itemize}
        \item R√©gression logistique
        \item K-Nearest Neighbors (KNN)
        \item Arbres de d√©cision, Random Forests
        \item SVM (Support Vector Machines)
        \item R√©seaux de neurones
    \end{itemize}
\end{itemize}

\begin{exemple}{Applications de l'apprentissage supervis√©}
\begin{itemize}
    \item \textbf{Vision :} D√©tection de visages, classification d'images m√©dicales
    \item \textbf{NLP :} Analyse de sentiment, traduction automatique
    \item \textbf{Finance :} Pr√©diction de cours boursiers, scoring cr√©dit
    \item \textbf{Sant√© :} Diagnostic m√©dical, pr√©diction de maladies
\end{itemize}
\end{exemple}

\subsection{Apprentissage Non-Supervis√©}

\begin{definition}{Apprentissage Non-Supervis√©}
L'apprentissage non-supervis√© consiste √† d√©couvrir des structures cach√©es dans des donn√©es non √©tiquet√©es $\{\vect{x}_i\}_{i=1}^n$. Le but est d'apprendre la distribution sous-jacente des donn√©es ou de les organiser de mani√®re significative.
\end{definition}

\textbf{Principales t√¢ches :}

\begin{itemize}
    \item \textbf{Clustering :} Regrouper des donn√©es similaires
    \begin{itemize}
        \item K-Means, DBSCAN, Hierarchical Clustering
        \item Gaussian Mixture Models
    \end{itemize}

    \item \textbf{R√©duction de dimensionnalit√© :} Projeter les donn√©es dans un espace de dimension r√©duite
    \begin{itemize}
        \item PCA (Principal Component Analysis)
        \item t-SNE, UMAP
        \item Autoencoders
    \end{itemize}

    \item \textbf{D√©tection d'anomalies :} Identifier les points atypiques
    \begin{itemize}
        \item Isolation Forest
        \item One-Class SVM
        \item Autoencoders variationnels
    \end{itemize}
\end{itemize}

\begin{exemple}{Applications de l'apprentissage non-supervis√©}
\begin{itemize}
    \item \textbf{Marketing :} Segmentation de clients
    \item \textbf{S√©curit√© :} D√©tection de fraudes, intrusions r√©seau
    \item \textbf{Biologie :} D√©couverte de nouveaux types cellulaires
    \item \textbf{Recommandation :} Syst√®mes de recommandation (collaborative filtering)
\end{itemize}
\end{exemple}

\subsection{Apprentissage Semi-Supervis√©}

\begin{definition}{Apprentissage Semi-Supervis√©}
L'apprentissage semi-supervis√© combine un petit ensemble de donn√©es √©tiquet√©es $\{(\vect{x}_i, y_i)\}_{i=1}^{n_l}$ avec un grand ensemble de donn√©es non √©tiquet√©es $\{\vect{x}_j\}_{j=1}^{n_u}$ o√π g√©n√©ralement $n_u \gg n_l$.
\end{definition}

\textbf{Motivation :} L'√©tiquetage des donn√©es est souvent co√ªteux et chronophage, tandis que les donn√©es non √©tiquet√©es sont abondantes.

\textbf{Approches principales :}
\begin{itemize}
    \item Self-training et co-training
    \item Mod√®les g√©n√©ratifs semi-supervis√©s
    \item Graph-based methods
    \item Consistency regularization (mod√®les modernes)
\end{itemize}

\subsection{Apprentissage par Renforcement}

\begin{definition}{Apprentissage par Renforcement}
L'apprentissage par renforcement consiste √† apprendre une politique $\pi$ qui maximise la r√©compense cumulative d'un agent interagissant avec un environnement. L'agent apprend par essai-erreur en recevant des r√©compenses (positives ou n√©gatives) pour ses actions.
\end{definition}

\textbf{Composantes principales :}
\begin{itemize}
    \item \textbf{Agent :} Entit√© qui prend des d√©cisions
    \item \textbf{Environnement :} Monde avec lequel l'agent interagit
    \item \textbf{√âtat :} Configuration actuelle de l'environnement
    \item \textbf{Action :} Choix effectu√© par l'agent
    \item \textbf{R√©compense :} Signal de feedback (positif/n√©gatif)
    \item \textbf{Politique :} Strat√©gie de l'agent ($\pi: S \to A$)
\end{itemize}

\textbf{Objectif :} Maximiser la r√©compense cumulative esp√©r√©e
\begin{equation}
    J(\pi) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t r_t \mid \pi\right]
\end{equation}
o√π $\gamma \in [0,1]$ est le facteur de discount et $r_t$ la r√©compense au temps $t$.

\begin{exemple}{Applications de l'apprentissage par renforcement}
\begin{itemize}
    \item \textbf{Jeux :} AlphaGo, AlphaZero (Go, √©checs, shogi)
    \item \textbf{Robotique :} Contr√¥le de robots, manipulation d'objets
    \item \textbf{Finance :} Trading automatique
    \item \textbf{V√©hicules autonomes :} Conduite autonome
    \item \textbf{Optimisation :} Gestion de datacenters, allocation de ressources
\end{itemize}
\end{exemple}

% ===== SECTION 4: VOCABULAIRE FONDAMENTAL =====
\section{Vocabulaire Fondamental}

\subsection{Terminologie des Donn√©es}

\begin{table}[h]
\centering
\caption{Vocabulaire cl√© du Machine Learning}
\label{tab:vocab}
\begin{tabular}{p{4.2cm}p{7.8cm}}
\toprule
\textbf{Terme} & \textbf{D√©finition} \\
\midrule
\textbf{Dataset} & Ensemble complet de donn√©es utilis√©es pour le ML \\
\textbf{Sample/Instance} & Une observation individuelle (une ligne du dataset) \\
\textbf{Feature/Variable} & Attribut ou caract√©ristique mesur√©e (colonne) \\
\textbf{Label/Target} & Valeur √† pr√©dire (variable d√©pendante) \\
\textbf{Training Set} & Donn√©es utilis√©es pour entra√Æner le mod√®le (70-80\%) \\
\textbf{Validation Set} & Donn√©es pour ajuster les hyperparam√®tres (10-15\%) \\
\textbf{Test Set} & Donn√©es pour √©valuer la performance finale (10-15\%) \\
\textbf{Feature Vector} & Vecteur $\vect{x} \in \R^d$ repr√©sentant une instance \\
\textbf{Dimensionnalit√©} & Nombre de features ($d$) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Terminologie des Mod√®les}

\begin{table}[h]
\centering
\begin{tabular}{p{4.2cm}p{7.8cm}}
\toprule
\textbf{Terme} & \textbf{D√©finition} \\
\midrule
\textbf{Mod√®le} & Fonction math√©matique apprise $f: X \to Y$ \\
\textbf{Param√®tres} & Variables internes apprises par le mod√®le (poids $\vect{w}$) \\
\textbf{Hyperparam√®tres} & Param√®tres fix√©s avant l'entra√Ænement (learning rate, etc.) \\
\textbf{Entra√Ænement/Fit} & Processus d'apprentissage des param√®tres \\
\textbf{Inf√©rence/Pr√©diction} & Utilisation du mod√®le entra√Æn√© sur de nouvelles donn√©es \\
\textbf{Fonction de perte} & Mesure de l'erreur du mod√®le $L(y, \hat{y})$ \\
\textbf{Optimisation} & Processus de minimisation de la fonction de perte \\
\textbf{Epoch} & Une passe compl√®te sur tout le dataset d'entra√Ænement \\
\textbf{Batch} & Sous-ensemble de donn√©es trait√© simultan√©ment \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Probl√©matiques Cl√©s}

\begin{definition}{Overfitting (Surapprentissage)}
Un mod√®le en overfitting apprend trop bien les donn√©es d'entra√Ænement, incluant le bruit, et g√©n√©ralise mal sur de nouvelles donn√©es. Performance √©lev√©e sur le train set, faible sur le test set.
\end{definition}

\begin{definition}{Underfitting (Sous-apprentissage)}
Un mod√®le en underfitting est trop simple pour capturer les patterns des donn√©es. Performance faible sur le train set et le test set.
\end{definition}

\begin{definition}{Compromis Biais-Variance}
\begin{itemize}
    \item \textbf{Biais √©lev√© :} Le mod√®le fait des hypoth√®ses trop fortes (underfitting)
    \item \textbf{Variance √©lev√©e :} Le mod√®le est trop sensible aux variations des donn√©es (overfitting)
    \item \textbf{Objectif :} Trouver le bon √©quilibre entre biais et variance
\end{itemize}
\end{definition}

\begin{center}
\begin{tikzpicture}
    \draw[->] (0,0) -- (8,0) node[right] {Complexit√© du mod√®le};
    \draw[->] (0,0) -- (0,5) node[above] {Erreur};

    % Courbes
    \draw[blue, thick] (0.5,4) .. controls (2,2) and (4,1) .. (7.5,1.5) node[right] {Biais};
    \draw[red, thick] (0.5,1) .. controls (2,0.8) and (4,1) .. (7.5,4) node[right] {Variance};
    \draw[green!50!black, thick] (0.5,2.5) .. controls (2,1.5) and (4,1) .. (5,1.2) .. controls (6,1.5) and (7,2.5) .. (7.5,3);

    \node[green!50!black] at (4,0.5) {$\downarrow$ Zone optimale};
    \draw[dashed] (4,0) -- (4,1);

    \node[align=center] at (1.5,5.5) {\small Underfitting\\(biais √©lev√©)};
    \node[align=center] at (6.5,5.5) {\small Overfitting\\(variance √©lev√©e)};
\end{tikzpicture}
\end{center}

% ===== SECTION 5: PIPELINE ML TYPIQUE =====
\section{Pipeline Typique d'un Projet Machine Learning}

Un projet ML suit g√©n√©ralement les √©tapes suivantes :

\begin{center}
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw, rounded corners, minimum width=3cm, minimum height=0.8cm, align=center, fill=blue!10},
    arrow/.style={->, thick}
]
    \node[box] (prob) {1. D√©finition du\\probl√®me};
    \node[box, below of=prob] (data) {2. Collecte et\\exploration des donn√©es};
    \node[box, below of=data] (prep) {3. Pr√©paration\\des donn√©es};
    \node[box, below of=prep] (feat) {4. Feature\\Engineering};
    \node[box, below of=feat] (model) {5. S√©lection et\\entra√Ænement du mod√®le};
    \node[box, below of=model] (eval) {6. √âvaluation};
    \node[box, below of=eval] (deploy) {7. D√©ploiement et\\monitoring};

    \draw[arrow] (prob) -- (data);
    \draw[arrow] (data) -- (prep);
    \draw[arrow] (prep) -- (feat);
    \draw[arrow] (feat) -- (model);
    \draw[arrow] (model) -- (eval);
    \draw[arrow] (eval) -- (deploy);

    % Boucle de r√©troaction
    \draw[arrow, dashed, red] (eval.east) -- ++(1,0) |- (feat.east);
    \node[red, right] at (6,-6) {\small It√©ration};
\end{tikzpicture}
\end{center}

\subsection{D√©finition du Probl√®me}

\textbf{Questions √† se poser :}
\begin{itemize}
    \item Quel est le probl√®me business/scientifique √† r√©soudre ?
    \item Est-ce un probl√®me de classification, r√©gression, clustering, etc. ?
    \item Quelles m√©triques de succ√®s utiliser ?
    \item Quelles sont les contraintes (temps, ressources, explicabilit√©) ?
\end{itemize}

\subsection{Collecte et Exploration des Donn√©es}

\textbf{Analyse exploratoire des donn√©es (EDA) :}
\begin{itemize}
    \item Taille du dataset ($n$ instances, $d$ features)
    \item Types de variables (num√©riques, cat√©gorielles, textuelles)
    \item Distribution des variables
    \item Valeurs manquantes
    \item Outliers (valeurs aberrantes)
    \item Corr√©lations entre features
    \item D√©s√©quilibre des classes (pour la classification)
\end{itemize}

\subsection{Pr√©paration des Donn√©es}

\textbf{Nettoyage des donn√©es :}
\begin{itemize}
    \item \textbf{Gestion des valeurs manquantes :}
    \begin{itemize}
        \item Suppression des lignes/colonnes
        \item Imputation (moyenne, m√©diane, mode, KNN, etc.)
    \end{itemize}

    \item \textbf{Gestion des outliers :}
    \begin{itemize}
        \item D√©tection (IQR, Z-score)
        \item Suppression ou transformation
    \end{itemize}

    \item \textbf{Encodage des variables cat√©gorielles :}
    \begin{itemize}
        \item One-Hot Encoding
        \item Label Encoding
        \item Target Encoding
    \end{itemize}

    \item \textbf{Normalisation/Standardisation :}
    \begin{itemize}
        \item Min-Max scaling: $x' = \frac{x - x_{min}}{x_{max} - x_{min}}$
        \item Z-score standardization: $x' = \frac{x - \mu}{\sigma}$
    \end{itemize}
\end{itemize}

\subsection{Feature Engineering}

Cr√©ation de nouvelles features pertinentes √† partir des donn√©es brutes :

\begin{itemize}
    \item Combinaisons de features existantes
    \item Transformations math√©matiques (log, sqrt, polynomiales)
    \item Extraction de features temporelles (jour, mois, jour de la semaine)
    \item Agr√©gations et statistiques
    \item S√©lection de features (√©limination des redondantes)
\end{itemize}

\subsection{Entra√Ænement du Mod√®le}

\begin{lstlisting}[language=Python, caption=Sch√©ma g√©n√©ral d'entra√Ænement]
# 1. Split train/validation/test (80% / 10% / 10%)
from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42  # 80% train, 20% temp
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42  # 10% val, 10% test
)

# 2. Choix et instanciation du mod√®le
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)

# 3. Entra√Ænement
model.fit(X_train, y_train)

# 4. Pr√©dictions
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)
\end{lstlisting}

\subsection{√âvaluation}

\textbf{M√©triques pour la classification :}
\begin{itemize}
    \item Accuracy, Precision, Recall, F1-score
    \item Confusion Matrix
    \item ROC-AUC, PR-AUC
\end{itemize}

\textbf{M√©triques pour la r√©gression :}
\begin{itemize}
    \item MAE (Mean Absolute Error)
    \item MSE (Mean Squared Error)
    \item RMSE (Root Mean Squared Error)
    \item R¬≤ (Coefficient de d√©termination)
\end{itemize}

\textbf{Validation crois√©e :}
\begin{lstlisting}[language=Python]
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"Accuracy: {scores.mean():.3f} (+/- {scores.std():.3f})")
\end{lstlisting}

\subsection{D√©ploiement et Monitoring}

\begin{itemize}
    \item Sauvegarde du mod√®le (pickle, joblib, ONNX)
    \item Mise en production (API REST, batch processing)
    \item Monitoring des performances en production
    \item R√©entra√Ænement p√©riodique (drift des donn√©es)
\end{itemize}

% ===== SECTION 6: ML CLASSIQUE vs DEEP LEARNING =====
\section{Machine Learning Classique vs Deep Learning}

\subsection{Machine Learning Classique}

\textbf{Caract√©ristiques :}
\begin{itemize}
    \item Caract√©ristiques (features) con√ßues manuellement
    \item Mod√®les relativement simples (arbres, SVMs, r√©gression)
    \item N√©cessite expertise domaine pour feature engineering
    \item Moins de donn√©es n√©cessaires
    \item Plus rapide √† entra√Æner
    \item Plus interpr√©table
\end{itemize}

\textbf{Algorithmes typiques :}
\begin{itemize}
    \item R√©gression lin√©aire/logistique
    \item Arbres de d√©cision, Random Forests, Gradient Boosting (XGBoost, LightGBM)
    \item Support Vector Machines
    \item K-Nearest Neighbors
    \item Naive Bayes
\end{itemize}

\subsection{Deep Learning}

\textbf{Caract√©ristiques :}
\begin{itemize}
    \item Apprentissage automatique de features hi√©rarchiques
    \item R√©seaux de neurones multicouches (profonds)
    \item N√©cessite beaucoup de donn√©es
    \item Gourmand en ressources de calcul (GPU/TPU)
    \item Performances sup√©rieures sur donn√©es non structur√©es (images, texte, audio)
    \item Moins interpr√©table ("bo√Æte noire")
\end{itemize}

\textbf{Architectures typiques :}
\begin{itemize}
    \item \textbf{MLP (Multi-Layer Perceptron)} : R√©seaux fully-connected
    \item \textbf{CNN (Convolutional Neural Networks)} : Vision par ordinateur
    \item \textbf{RNN/LSTM/GRU} : S√©quences temporelles
    \item \textbf{Transformers} : NLP moderne, vision, multimodal
    \item \textbf{Autoencoders, GANs} : G√©n√©ration et repr√©sentation
\end{itemize}

\subsection{Comparaison}

\begin{table}[h]
\centering
\caption{ML Classique vs Deep Learning}
\begin{tabular}{p{4cm}p{5cm}p{5cm}}
\toprule
\textbf{Crit√®re} & \textbf{ML Classique} & \textbf{Deep Learning} \\
\midrule
\textbf{Donn√©es} & Fonctionne avec peu de donn√©es & N√©cessite beaucoup de donn√©es \\
\textbf{Features} & Engineering manuel & Apprises automatiquement \\
\textbf{Performance} & Plateaux rapidement & S'am√©liore avec plus de donn√©es \\
\textbf{Temps d'entra√Ænement} & Rapide (minutes-heures) & Lent (heures-jours) \\
\textbf{Hardware} & CPU suffit & GPU/TPU recommand√© \\
\textbf{Interpr√©tabilit√©} & Bonne & Faible \\
\textbf{Donn√©es structur√©es} & Excellent & Bon \\
\textbf{Donn√©es non structur√©es} & Limit√© & Excellent \\
\bottomrule
\end{tabular}
\end{table}

\begin{astuce}
\textbf{Quand utiliser quoi ?}
\begin{itemize}
    \item \textbf{ML Classique :} Donn√©es tabulaires, peu de donn√©es, besoin d'interpr√©tabilit√©, ressources limit√©es
    \item \textbf{Deep Learning :} Images, texte, audio, grandes quantit√©s de donn√©es, performance maximale
\end{itemize}
\end{astuce}

% ===== SECTION 7: APPLICATIONS CONCR√àTES =====
\section{Applications Concr√®tes du Machine Learning}

\subsection{Vision par Ordinateur}

\textbf{T√¢ches :}
\begin{itemize}
    \item \textbf{Classification d'images :} Reconna√Ætre des objets, animaux, sc√®nes
    \item \textbf{D√©tection d'objets :} Localiser et identifier plusieurs objets (YOLO, R-CNN)
    \item \textbf{Segmentation :} D√©limiter pr√©cis√©ment les objets pixel par pixel
    \item \textbf{Reconnaissance faciale :} Identification biom√©trique
    \item \textbf{G√©n√©ration d'images :} GANs, Diffusion Models (Stable Diffusion, DALL-E)
\end{itemize}

\textbf{Applications :}
\begin{itemize}
    \item V√©hicules autonomes (d√©tection pi√©tons, panneaux, v√©hicules)
    \item Diagnostic m√©dical (radiologie, dermatologie)
    \item Surveillance et s√©curit√©
    \item Contr√¥le qualit√© industriel
    \item R√©alit√© augment√©e
\end{itemize}

\subsection{Natural Language Processing (NLP)}

\textbf{T√¢ches :}
\begin{itemize}
    \item \textbf{Classification de texte :} Analyse de sentiment, d√©tection de spam
    \item \textbf{Named Entity Recognition :} Extraction d'entit√©s (personnes, lieux, organisations)
    \item \textbf{Traduction automatique :} Google Translate, DeepL
    \item \textbf{Question-Answering :} Syst√®mes de Q\&A
    \item \textbf{G√©n√©ration de texte :} GPT-4, LLaMA, chatbots
    \item \textbf{R√©sum√© automatique :} Synth√®se de documents
\end{itemize}

\textbf{Applications :}
\begin{itemize}
    \item Assistants virtuels (Alexa, Siri, Google Assistant)
    \item Chatbots customer service
    \item Traduction en temps r√©el
    \item Analyse de documents l√©gaux/m√©dicaux
    \item G√©n√©ration de code (GitHub Copilot)
\end{itemize}

\subsection{Syst√®mes de Recommandation}

\textbf{Approches :}
\begin{itemize}
    \item \textbf{Filtrage collaboratif :} Bas√© sur les pr√©f√©rences d'utilisateurs similaires
    \item \textbf{Filtrage bas√© contenu :} Bas√© sur les caract√©ristiques des items
    \item \textbf{Approches hybrides :} Combinaison des deux
\end{itemize}

\textbf{Applications :}
\begin{itemize}
    \item Netflix, YouTube (recommandations de vid√©os)
    \item Amazon, Alibaba (produits)
    \item Spotify, Apple Music (musique)
    \item Publicit√© cibl√©e
\end{itemize}

\subsection{Jeux et Strat√©gie}

\textbf{Succ√®s remarquables :}
\begin{itemize}
    \item 1997 : Deep Blue bat Kasparov aux √©checs
    \item 2016 : AlphaGo bat Lee Sedol au Go
    \item 2017 : AlphaZero ma√Ætrise √©checs, shogi, Go sans connaissance humaine
    \item 2019 : AlphaStar atteint niveau grand-ma√Ætre √† StarCraft II
    \item 2021 : AlphaCode r√©sout des probl√®mes de programmation comp√©titive
\end{itemize}

\subsection{Autres Domaines}

\begin{itemize}
    \item \textbf{Finance :} Trading algorithmique, d√©tection de fraude, scoring cr√©dit
    \item \textbf{Sant√© :} D√©couverte de m√©dicaments (AlphaFold), diagnostic, m√©decine personnalis√©e
    \item \textbf{Climatologie :} Pr√©visions m√©t√©orologiques, mod√©lisation climatique
    \item \textbf{Agriculture :} Optimisation de rendements, d√©tection de maladies
    \item \textbf{√ânergie :} Optimisation de r√©seaux √©lectriques, pr√©vision de consommation
    \item \textbf{Cybers√©curit√© :} D√©tection d'intrusions, malware analysis
\end{itemize}

% ===== SECTION 8: √âTHIQUE ET BIAIS =====
\section{√âthique et Biais en Machine Learning}

\subsection{Probl√©matiques √âthiques}

\begin{attention}
Le Machine Learning soul√®ve des questions √©thiques importantes :
\begin{itemize}
    \item \textbf{Biais algorithmiques :} Discrimination bas√©e sur race, genre, √¢ge
    \item \textbf{Vie priv√©e :} Collecte et utilisation de donn√©es personnelles
    \item \textbf{Transparence :} Mod√®les opaques ("bo√Ætes noires")
    \item \textbf{Responsabilit√© :} Qui est responsable des erreurs ?
    \item \textbf{S√©curit√© :} Attaques adverses, deepfakes
    \item \textbf{Impact social :} Automatisation et emploi
\end{itemize}
\end{attention}

\subsection{Biais dans les Donn√©es et les Mod√®les}

\textbf{Sources de biais :}

\begin{enumerate}
    \item \textbf{Biais de collecte :} Donn√©es non repr√©sentatives de la population
    \item \textbf{Biais de labeling :} √âtiquetage subjectif ou inconsistant
    \item \textbf{Biais historique :} Donn√©es refl√©tant des discriminations pass√©es
    \item \textbf{Biais de s√©lection :} √âchantillonnage non al√©atoire
    \item \textbf{Biais de confirmation :} Chercher des patterns qui confirment des pr√©jug√©s
\end{enumerate}

\begin{exemple}{Cas concrets de biais}
\begin{itemize}
    \item \textbf{COMPAS :} Syst√®me de pr√©diction de r√©cidive aux USA biais√© contre les minorit√©s
    \item \textbf{Recrutement :} Amazon a d√ª abandonner un outil de tri de CV biais√© contre les femmes
    \item \textbf{Reconnaissance faciale :} Taux d'erreur plus √©lev√©s pour les personnes de couleur
    \item \textbf{Traduction :} Associations st√©r√©otyp√©es (ex: "doctor" $\to$ "il", "nurse" $\to$ "elle")
\end{itemize}
\end{exemple}

\subsection{Fairness et √âquit√©}

\textbf{D√©finitions de fairness :}
\begin{itemize}
    \item \textbf{Demographic parity :} Pr√©dictions ind√©pendantes des attributs sensibles
    \item \textbf{Equalized odds :} Taux de vrais/faux positifs √©gaux entre groupes
    \item \textbf{Individual fairness :} Individus similaires trait√©s de mani√®re similaire
\end{itemize}

\textbf{Techniques de mitigation :}
\begin{itemize}
    \item Pr√©-traitement : Correction des biais dans les donn√©es
    \item In-processing : Contraintes de fairness pendant l'entra√Ænement
    \item Post-processing : Ajustement des pr√©dictions pour assurer l'√©quit√©
\end{itemize}

\subsection{Bonnes Pratiques}

\begin{astuce}
\textbf{D√©veloppement responsable de ML :}
\begin{itemize}
    \item \textbf{Auditer les donn√©es :} V√©rifier la repr√©sentativit√© et les biais
    \item \textbf{Tester la fairness :} √âvaluer les performances par sous-groupes
    \item \textbf{Documenter :} Model cards, datasheets pour les datasets
    \item \textbf{Explicabilit√© :} Utiliser LIME, SHAP pour comprendre les pr√©dictions
    \item \textbf{Human-in-the-loop :} Validation humaine pour d√©cisions critiques
    \item \textbf{Monitoring continu :} Surveiller les performances en production
    \item \textbf{Diversit√© :} √âquipes diverses pour identifier les angles morts
\end{itemize}
\end{astuce}

\subsection{R√©glementation}

\textbf{Cadres l√©gaux √©mergents :}
\begin{itemize}
    \item \textbf{RGPD (Europe)} : Protection des donn√©es personnelles, droit √† l'explication
    \item \textbf{AI Act (UE)} : Classification des syst√®mes IA par niveau de risque
    \item \textbf{Algorithmic Accountability Act (USA)} : Transparence des algorithmes
    \item \textbf{IEEE, ISO :} Standards techniques pour l'IA √©thique
\end{itemize}

% ===== SECTION 9: √âCOSYST√àME ET OUTILS =====
\section{√âcosyst√®me et Outils}

\subsection{Biblioth√®ques Python Essentielles}

\textbf{Manipulation de donn√©es :}
\begin{itemize}
    \item \texttt{NumPy} : Calcul num√©rique, arrays multidimensionnels
    \item \texttt{Pandas} : DataFrames, manipulation de donn√©es tabulaires
    \item \texttt{Polars} : Alternative rapide √† Pandas
\end{itemize}

\textbf{Machine Learning classique :}
\begin{itemize}
    \item \texttt{scikit-learn} : ML g√©n√©ral (classification, r√©gression, clustering)
    \item \texttt{XGBoost, LightGBM, CatBoost} : Gradient boosting optimis√©
    \item \texttt{statsmodels} : Mod√®les statistiques
\end{itemize}

\textbf{Deep Learning :}
\begin{itemize}
    \item \texttt{PyTorch} : Framework DL flexible et pythonic
    \item \texttt{TensorFlow/Keras} : Framework DL production-ready
    \item \texttt{JAX} : Calcul num√©rique haute performance
    \item \texttt{Hugging Face Transformers} : NLP state-of-the-art
\end{itemize}

\textbf{Visualisation :}
\begin{itemize}
    \item \texttt{Matplotlib} : Graphiques standard
    \item \texttt{Seaborn} : Visualisations statistiques
    \item \texttt{Plotly} : Graphiques interactifs
\end{itemize}

\subsection{Plateformes et Environnements}

\begin{itemize}
    \item \textbf{Jupyter Notebooks} : D√©veloppement interactif
    \item \textbf{Google Colab} : Jupyter gratuit avec GPU/TPU
    \item \textbf{Kaggle} : Comp√©titions, datasets, notebooks
    \item \textbf{Weights \& Biases, MLflow} : Tracking d'exp√©riences
    \item \textbf{TensorBoard} : Visualisation d'entra√Ænement
\end{itemize}

% ===== PR√âREQUIS PYTHON =====
\section{Pr√©requis Python pour le Machine Learning}

Avant de vous lancer dans le Machine Learning, il est essentiel de ma√Ætriser les outils Python de base. Cette section vous guide √† travers les biblioth√®ques fondamentales que vous utiliserez tout au long du cours.

\begin{tcolorbox}[colback=green!5!white, colframe=green!75!black, title=Notebook Pratique]
Le notebook \texttt{00\_prerequis\_python.ipynb} couvre en d√©tail tous ces pr√©requis avec des exemples ex√©cutables et des exercices pratiques.

\textbf{Si vous connaissez d√©j√† Python, NumPy et Pandas}, vous pouvez passer cette section et ce notebook.
\end{tcolorbox}

\subsection{Python : Syntaxe Essentielle}

\textbf{Types de base et structures :}
\begin{itemize}
    \item Types : \texttt{int}, \texttt{float}, \texttt{str}, \texttt{bool}
    \item Listes : collections ordonn√©es modifiables
    \item Dictionnaires : paires cl√©-valeur
    \item Tuples : collections immuables
\end{itemize}

\textbf{List comprehensions :} Construction compacte de listes, tr√®s utilis√©e en ML.
\begin{lstlisting}[language=Python]
# Exemple : normaliser des scores
scores = [0.85, 0.92, 0.78, 0.95, 0.88]
scores_percent = [score * 100 for score in scores]
# [85.0, 92.0, 78.0, 95.0, 88.0]

# Filtrage
bons_scores = [s for s in scores if s >= 0.90]
# [0.92, 0.95]
\end{lstlisting}

\textbf{Fonctions :} Encapsulation de logique r√©utilisable.
\begin{lstlisting}[language=Python]
def normaliser(valeur, min_val, max_val):
    """Normalise une valeur entre 0 et 1"""
    return (valeur - min_val) / (max_val - min_val)
\end{lstlisting}

\subsection{NumPy : Calcul Num√©rique}

NumPy est \textbf{la biblioth√®que fondamentale} pour le calcul scientifique en Python. Tous les frameworks ML (scikit-learn, PyTorch, TensorFlow) sont construits sur NumPy.

\textbf{Arrays multidimensionnels :}
\begin{lstlisting}[language=Python]
import numpy as np

# Vecteur (1D)
v = np.array([1, 2, 3, 4, 5])

# Matrice (2D) : dataset avec 3 personnes, 4 features
X = np.array([
    [25, 50000, 1, 1],  # age, salaire, exp, diplome
    [30, 60000, 3, 2],
    [35, 75000, 5, 3]
])

print(X.shape)  # (3, 4) : 3 lignes, 4 colonnes
\end{lstlisting}

\textbf{Op√©rations vectoris√©es (rapides) :}
\begin{lstlisting}[language=Python]
# Element-wise operations
a = np.array([1, 2, 3, 4])
b = np.array([10, 20, 30, 40])

print(a + b)   # [11 22 33 44]
print(a * b)   # [10 40 90 160]
print(a ** 2)  # [1 4 9 16]
\end{lstlisting}

\textbf{Indexation et slicing :}
\begin{lstlisting}[language=Python]
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# Selection
X[0, :]     # Premiere ligne : [1, 2, 3]
X[:, 1]     # Deuxieme colonne : [2, 5, 8]
X[0, 1]     # Element : 2

# Masquage booleen (tres utilise en ML)
X[X > 5]    # [6, 7, 8, 9]
\end{lstlisting}

\textbf{Broadcasting :} Op√©rations entre arrays de shapes diff√©rentes.
\begin{lstlisting}[language=Python]
matrice = np.array([[1, 2, 3], [4, 5, 6]])
vecteur = np.array([10, 20, 30])

# Le vecteur est "diffuse" sur chaque ligne
resultat = matrice + vecteur
# [[11 22 33]
#  [14 25 36]]
\end{lstlisting}

\textbf{Fonctions d'agr√©gation :}
\begin{lstlisting}[language=Python]
scores = np.array([0.75, 0.82, 0.91, 0.88, 0.79])

print(scores.mean())     # 0.83
print(scores.std())      # 0.059
print(scores.min())      # 0.75
print(scores.max())      # 0.91

# Agregation par axe
X.mean(axis=0)  # Moyenne par colonne
X.mean(axis=1)  # Moyenne par ligne
\end{lstlisting}

\subsection{Pandas : Manipulation de Donn√©es}

Pandas permet de manipuler des donn√©es tabulaires (comme Excel ou CSV). C'est l'outil principal pour l'exploration et le preprocessing de donn√©es.

\textbf{DataFrames :} Tableaux avec labels de colonnes et lignes.
\begin{lstlisting}[language=Python]
import pandas as pd

# Creation
data = {
    'nom': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'salaire': [45000, 55000, 65000]
}
df = pd.DataFrame(data)

#       nom  age  salaire
# 0   Alice   25    45000
# 1     Bob   30    55000
# 2 Charlie   35    65000
\end{lstlisting}

\textbf{S√©lection et filtrage :}
\begin{lstlisting}[language=Python]
# Selection de colonnes
df['age']              # Serie (1 colonne)
df[['nom', 'age']]     # DataFrame (plusieurs colonnes)

# Filtrage
seniors = df[df['age'] >= 30]
riches = df[(df['age'] >= 30) & (df['salaire'] > 50000)]
\end{lstlisting}

\textbf{Statistiques descriptives :}
\begin{lstlisting}[language=Python]
df.describe()          # Statistiques sur colonnes numeriques
df['salaire'].mean()   # Salaire moyen
df['age'].median()     # Age median
df['nom'].value_counts()  # Comptage des valeurs
\end{lstlisting}

\textbf{Groupby (SQL GROUP BY) :}
\begin{lstlisting}[language=Python]
# Salaire moyen par diplome
df.groupby('diplome')['salaire'].mean()
\end{lstlisting}

\textbf{Lecture de fichiers :}
\begin{lstlisting}[language=Python]
# CSV (le plus courant)
df = pd.read_csv('data.csv')

# Excel
df = pd.read_excel('data.xlsx')

# JSON
df = pd.read_json('data.json')
\end{lstlisting}

\subsection{Matplotlib : Visualisation}

Matplotlib est la biblioth√®que de base pour cr√©er des graphiques. Elle est utilis√©e pour visualiser les donn√©es et les r√©sultats des mod√®les.

\textbf{Graphiques de base :}
\begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt

# Courbe
plt.plot([1, 2, 3, 4], [1, 4, 2, 3])
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Mon graphique')
plt.show()

# Nuage de points
plt.scatter(X, y)
plt.show()

# Histogramme
plt.hist(scores, bins=20)
plt.show()
\end{lstlisting}

\textbf{Subplots (plusieurs graphiques) :}
\begin{lstlisting}[language=Python]
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].plot(x, y)        # Haut gauche
axes[0, 1].scatter(x, y)     # Haut droite
axes[1, 0].hist(scores)      # Bas gauche
axes[1, 1].bar(['A','B'], [1, 2])  # Bas droite

plt.tight_layout()
plt.show()
\end{lstlisting}

\subsection{Installation et V√©rification}

Si vous utilisez ce cours avec Docker, toutes les biblioth√®ques sont d√©j√† install√©es. Sinon, installez-les avec :

\begin{lstlisting}[language=bash]
pip install numpy pandas matplotlib scikit-learn jupyter
\end{lstlisting}

\textbf{V√©rification :}
\begin{lstlisting}[language=Python]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

print(f"NumPy: {np.__version__}")
print(f"Pandas: {pd.__version__}")
print("Tout est pret !")
\end{lstlisting}

\begin{attention}{Ordre d'apprentissage recommand√©}
Pour bien d√©marrer ce cours :
\begin{enumerate}
    \item Commencez par le notebook \texttt{00\_prerequis\_python.ipynb}
    \item Faites tous les exercices (normalisation, analyse de dataset)
    \item Une fois √† l'aise, passez au chapitre 01 (Fondamentaux Math√©matiques)
\end{enumerate}

\textbf{Temps estim√© :} 1-2 heures pour le notebook Python si vous √™tes d√©butant, 15-30 minutes si vous connaissez d√©j√† les bases.
\end{attention}

% ===== R√âSUM√â =====
\section{R√©sum√© du Chapitre}

\subsection{Points Cl√©s}

\begin{itemize}
    \item \textbf{Machine Learning :} Apprendre √† partir de donn√©es sans programmation explicite
    \item \textbf{Types d'apprentissage :} Supervis√© (labels), non-supervis√© (patterns), semi-supervis√© (mix), renforcement (r√©compenses)
    \item \textbf{Pipeline ML :} Probl√®me $\to$ Donn√©es $\to$ Pr√©paration $\to$ Features $\to$ Mod√®le $\to$ √âvaluation $\to$ D√©ploiement
    \item \textbf{ML vs DL :} ML classique (peu de donn√©es, interpr√©table) vs Deep Learning (beaucoup de donn√©es, performance)
    \item \textbf{Applications :} Vision, NLP, recommandation, jeux, sant√©, finance...
    \item \textbf{√âthique :} Attention aux biais, fairness, transparence, responsabilit√©
\end{itemize}

\subsection{Formules Essentielles}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Concepts √† retenir]
\textbf{Apprentissage supervis√© :}
\begin{equation}
    \hat{f} = \argmin_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n L(y_i, f(\vect{x}_i))
\end{equation}

\textbf{Compromis biais-variance :}
\begin{equation}
    \text{Erreur totale} = \text{Biais}^2 + \text{Variance} + \text{Bruit irr√©ductible}
\end{equation}

\textbf{Train/Validation/Test :} Typiquement 70\% / 15\% / 15\%
\end{tcolorbox}

% ===== EXERCICES =====
\section{Exercices}

\subsection{Questions de Compr√©hension}

\begin{enumerate}
    \item Expliquez la diff√©rence entre un param√®tre et un hyperparam√®tre. Donnez des exemples.

    \item Pourquoi divise-t-on les donn√©es en ensembles d'entra√Ænement, validation et test ?

    \item Qu'est-ce que l'overfitting ? Comment peut-on le d√©tecter et le pr√©venir ?

    \item Comparez l'apprentissage supervis√© et non-supervis√©. Donnez un exemple d'application pour chaque.

    \item Dans quelles situations privil√©gieriez-vous le Deep Learning par rapport au ML classique ?

    \item Expliquez comment les biais dans les donn√©es peuvent mener √† des mod√®les discriminatoires.

    \item D√©crivez les √©tapes du pipeline ML pour un probl√®me de pr√©diction de prix immobiliers.

    \item Pourquoi la standardisation des features est-elle importante pour certains algorithmes (ex: SVM, KNN) ?
\end{enumerate}

\subsection{Exercices Pratiques}

\begin{enumerate}
    \item \textbf{Pipeline ML complet sur Iris :}
    \begin{itemize}
        \item Charger le dataset Iris depuis scikit-learn
        \item Effectuer une analyse exploratoire (EDA)
        \item S√©parer train/test
        \item Entra√Æner un classifieur (KNN, Decision Tree, ou Random Forest)
        \item √âvaluer avec accuracy, confusion matrix, et classification report
    \end{itemize}

    \textit{Voir le notebook} \texttt{00\_demo\_ml\_pipeline.ipynb}

    \item \textbf{Comparaison de mod√®les :}
    \begin{itemize}
        \item Sur le dataset Iris, comparer 3 algorithmes diff√©rents
        \item Utiliser la validation crois√©e
        \item Analyser les r√©sultats et expliquer les diff√©rences
    \end{itemize}

    \item \textbf{D√©tection d'overfitting :}
    \begin{itemize}
        \item Cr√©er un dataset synth√©tique avec peu d'√©chantillons
        \item Entra√Æner un mod√®le complexe (ex: Decision Tree sans limitation de profondeur)
        \item Comparer les performances train vs test
        \item Appliquer des techniques de r√©gularisation et observer l'impact
    \end{itemize}
\end{enumerate}

\textit{Solutions d√©taill√©es disponibles dans} \texttt{00\_exercices\_solutions.ipynb}

% ===== POUR ALLER PLUS LOIN =====
\section{Pour Aller Plus Loin}

\subsection{Lectures Recommand√©es}

\textbf{Livres :}
\begin{itemize}
    \item \textit{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow} (3e √©d., 2022) - Aur√©lien G√©ron
    \item \textit{Pattern Recognition and Machine Learning} (2006) - Christopher Bishop
    \item \textit{The Elements of Statistical Learning} (2009) - Hastie, Tibshirani, Friedman
    \item \textit{Deep Learning} (2016) - Goodfellow, Bengio, Courville
\end{itemize}

\textbf{Cours en ligne :}
\begin{itemize}
    \item Andrew Ng - Machine Learning (Coursera)
    \item Fast.ai - Practical Deep Learning for Coders
    \item Stanford CS229 - Machine Learning
    \item MIT 6.S191 - Introduction to Deep Learning
\end{itemize}

\subsection{Ressources en Ligne}

\begin{itemize}
    \item Documentation scikit-learn : \url{https://scikit-learn.org/}
    \item PyTorch Tutorials : \url{https://pytorch.org/tutorials/}
    \item Papers With Code : \url{https://paperswithcode.com/}
    \item Kaggle Datasets \& Competitions : \url{https://www.kaggle.com/}
    \item Hugging Face : \url{https://huggingface.co/}
    \item Distill.pub (visualisations ML) : \url{https://distill.pub/}
\end{itemize}

\subsection{Datasets pour Pratiquer}

\textbf{D√©butants :}
\begin{itemize}
    \item Iris, Wine, Digits (scikit-learn)
    \item Titanic, House Prices (Kaggle)
    \item MNIST (images de chiffres manuscrits)
\end{itemize}

\textbf{Interm√©diaires :}
\begin{itemize}
    \item CIFAR-10/100 (images couleur)
    \item IMDB Reviews (analyse de sentiment)
    \item UCI Machine Learning Repository
\end{itemize}

\subsection{Prochaines √âtapes}

Chapitre suivant recommand√© : \textbf{Chapitre 01 - Fondamentaux Math√©matiques}

\textit{Pr√©requis pour approfondir : alg√®bre lin√©aire, calcul diff√©rentiel, probabilit√©s, statistiques}

% ===== BIBLIOGRAPHIE =====
\section*{R√©f√©rences}

\begin{enumerate}
    \item Mitchell, T. (1997). \textit{Machine Learning}. McGraw-Hill.

    \item G√©ron, A. (2022). \textit{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow} (3e √©d.). O'Reilly Media.

    \item Goodfellow, I., Bengio, Y., \& Courville, A. (2016). \textit{Deep Learning}. MIT Press.

    \item Hastie, T., Tibshirani, R., \& Friedman, J. (2009). \textit{The Elements of Statistical Learning} (2e √©d.). Springer.

    \item Bishop, C. M. (2006). \textit{Pattern Recognition and Machine Learning}. Springer.

    \item Samuel, A. L. (1959). "Some Studies in Machine Learning Using the Game of Checkers". \textit{IBM Journal of Research and Development}, 3(3), 210-229.

    \item Rumelhart, D. E., Hinton, G. E., \& Williams, R. J. (1986). "Learning representations by back-propagating errors". \textit{Nature}, 323(6088), 533-536.

    \item Barocas, S., Hardt, M., \& Narayanan, A. (2019). \textit{Fairness and Machine Learning: Limitations and Opportunities}. MIT Press.
\end{enumerate}

\end{document}
