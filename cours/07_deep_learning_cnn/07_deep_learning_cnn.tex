\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}

% Math√©matiques
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

% Mise en page
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}

% Graphiques et couleurs
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, calc, matrix}

% Tableaux
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}

% Code et algorithmes
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Hyperliens
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 07 - Deep Learning : CNN},
    pdfauthor={Cours ML},
}

% Boxes color√©es
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}

% En-t√™tes et pieds de page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 07 - Deep Learning : CNN}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIGURATION LISTINGS =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}
\lstset{style=pythonstyle}

% ===== CONFIGURATION TCOLORBOX =====
\newtcolorbox{definition}[1]{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition: #1,
    breakable
}

\newtcolorbox{theoreme}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Th√©or√®me: #1,
    breakable
}

\newtcolorbox{exemple}[1]{
    colback=orange!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=Exemple: #1,
    breakable
}

\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö†Ô∏è Attention,
    breakable
}

\newtcolorbox{astuce}{
    colback=yellow!10!white,
    colframe=yellow!75!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

% ===== COMMANDES PERSONNALIS√âES =====
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\relu}{\operatorname{ReLU}}
\newcommand{\softmax}{\operatorname{softmax}}

\begin{document}

% ===== PAGE DE TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]

    \vspace{1cm}

    {\LARGE Chapitre 07}\\[0.3cm]
    {\LARGE\bfseries Deep Learning : R√©seaux de Neurones Convolutifs (CNN)}\\[2cm]

    \vfill

    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Comprendre l'op√©ration de convolution et son utilit√© pour les images
        \item Ma√Ætriser les couches fondamentales : convolution, pooling, fully-connected
        \item √âtudier les architectures classiques : LeNet, AlexNet, VGG, ResNet
        \item Appliquer le transfer learning et le fine-tuning
        \item Impl√©menter un CNN avec PyTorch/TensorFlow
    \end{itemize}
    }

    \vfill

    {\large
    \textbf{Pr√©requis :} Chapitre 06 (R√©seaux de Neurones Fondamentaux)\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 8-10 heures\\[0.3cm]
    \textbf{Notebooks :} \texttt{07_demo_*.ipynb}
    }

    \vfill

    {\large Cours ML - Sandbox-ML\\
    Version 1.0 - 2026}
\end{titlepage}

\tableofcontents
\newpage

% ===== SECTION 1: INTRODUCTION =====
\section{Introduction aux CNN}

\subsection{Motivation : Limitations des MLP pour les images}

Un MLP classique pr√©sente des probl√®mes majeurs pour traiter les images :

\begin{enumerate}
    \item \textbf{Nombre de param√®tres explosif} : Image 224√ó224 RGB ‚Üí 150K inputs ‚Üí MLP(512) n√©cessite 77M param√®tres pour la premi√®re couche !
    \item \textbf{Pas d'invariance spatiale} : Un chat en haut √† gauche vs en bas √† droite sont des patterns compl√®tement diff√©rents
    \item \textbf{Perte de structure 2D} : Aplatir l'image en vecteur 1D d√©truit les relations spatiales locales
\end{enumerate}

\begin{exemple}{MNIST avec MLP vs CNN}
\begin{itemize}
    \item MLP (784 ‚Üí 128 ‚Üí 10) : ~100K param√®tres, ~98\% accuracy
    \item CNN simple (2 conv + pooling) : ~10K param√®tres, ~99\% accuracy
\end{itemize}
Le CNN est 10√ó plus compact et plus performant !
\end{exemple}

\subsection{Principes des CNN}

Les CNN exploitent trois id√©es cl√©s :

\begin{enumerate}
    \item \textbf{Connexions locales (local connectivity)} : Chaque neurone ne "regarde" qu'une petite r√©gion de l'image (champ r√©cepteur)
    \item \textbf{Partage de poids (weight sharing)} : Le m√™me filtre est appliqu√© sur toute l'image ‚Üí invariance par translation
    \item \textbf{Hi√©rarchie de features} : Couches successives d√©tectent des features de plus en plus complexes
\end{enumerate}

\subsection{Hi√©rarchie de repr√©sentations}

\begin{itemize}
    \item \textbf{Couche 1} : D√©tecte bords, contours, gradients (features bas niveau)
    \item \textbf{Couche 2-3} : D√©tecte textures, motifs simples (coins, cercles)
    \item \textbf{Couche 4-5} : D√©tecte parties d'objets (yeux, roues, fen√™tres)
    \item \textbf{Couche finale} : D√©tecte objets complets (chat, voiture, visage)
\end{itemize}

% ===== SECTION 2: OP√âRATION DE CONVOLUTION =====
\section{Op√©ration de Convolution}

\subsection{Convolution 2D}

\begin{definition}{Convolution Discr√®te 2D}
Pour une image $\mat{I} \in \R^{H \times W}$ et un filtre (kernel) $\mat{K} \in \R^{k \times k}$, la convolution produit une feature map $\mat{O}$ :
\begin{equation}
    \mat{O}[i, j] = (\mat{I} * \mat{K})[i, j] = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} \mat{I}[i+m, j+n] \cdot \mat{K}[m, n]
\end{equation}
\end{definition}

\textbf{Interpr√©tation :} Le filtre "glisse" sur l'image et calcule un produit scalaire local √† chaque position.

\subsection{Exemple concret}

Image 5√ó5 et filtre 3√ó3 :
\begin{equation*}
\mat{I} = \begin{bmatrix}
1 & 2 & 3 & 0 & 1 \\
0 & 1 & 2 & 3 & 0 \\
1 & 0 & 1 & 2 & 3 \\
2 & 1 & 0 & 1 & 2 \\
3 & 2 & 1 & 0 & 1
\end{bmatrix}, \quad
\mat{K} = \begin{bmatrix}
1 & 0 & -1 \\
1 & 0 & -1 \\
1 & 0 & -1
\end{bmatrix} \text{ (d√©tecteur de bord vertical)}
\end{equation*}

Calcul de $\mat{O}[0, 0]$ :
\begin{align*}
\mat{O}[0, 0] &= 1 \cdot 1 + 2 \cdot 0 + 3 \cdot (-1) \\
              &+ 0 \cdot 1 + 1 \cdot 0 + 2 \cdot (-1) \\
              &+ 1 \cdot 1 + 0 \cdot 0 + 1 \cdot (-1) \\
              &= 1 + 0 - 3 + 0 + 0 - 2 + 1 + 0 - 1 = -4
\end{align*}

\subsection{Filtres classiques}

\textbf{D√©tecteur de bord vertical :}
\begin{equation*}
\mat{K}_{\text{vert}} = \begin{bmatrix}
1 & 0 & -1 \\
1 & 0 & -1 \\
1 & 0 & -1
\end{bmatrix}
\end{equation*}

\textbf{D√©tecteur de bord horizontal :}
\begin{equation*}
\mat{K}_{\text{horiz}} = \begin{bmatrix}
1 & 1 & 1 \\
0 & 0 & 0 \\
-1 & -1 & -1
\end{bmatrix}
\end{equation*}

\textbf{Filtre de flou (blur) :}
\begin{equation*}
\mat{K}_{\text{blur}} = \frac{1}{9} \begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1
\end{bmatrix}
\end{equation*}

\textbf{Sobel (d√©tection de contours) :}
\begin{equation*}
\mat{K}_{\text{Sobel}_x} = \begin{bmatrix}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1
\end{bmatrix}, \quad
\mat{K}_{\text{Sobel}_y} = \begin{bmatrix}
-1 & -2 & -1 \\
0 & 0 & 0 \\
1 & 2 & 1
\end{bmatrix}
\end{equation*}

\subsection{Hyperparam√®tres de la convolution}

\subsubsection{Padding}

\begin{definition}{Padding}
Ajout de z√©ros (ou autres valeurs) autour de l'image pour contr√¥ler la taille de sortie.
\begin{itemize}
    \item \textbf{Valid padding} (no padding) : $p = 0$
    \item \textbf{Same padding} : $p = \lfloor k/2 \rfloor$ (sortie m√™me taille que l'entr√©e)
\end{itemize}
\end{definition}

\subsubsection{Stride}

\begin{definition}{Stride}
Pas de d√©placement du filtre. Stride $s = 1$ : d√©placement de 1 pixel. Stride $s = 2$ : d√©placement de 2 pixels (sous-√©chantillonnage).
\end{definition}

\subsubsection{Taille de sortie}

Pour une entr√©e $H \times W$, filtre $k \times k$, padding $p$, stride $s$ :
\begin{equation}
    H_{out} = \left\lfloor \frac{H + 2p - k}{s} \right\rfloor + 1, \quad
    W_{out} = \left\lfloor \frac{W + 2p - k}{s} \right\rfloor + 1
\end{equation}

\begin{exemple}{Calcul de taille}
Entr√©e 32√ó32, filtre 5√ó5, padding 2, stride 1 :
\begin{equation*}
    H_{out} = \frac{32 + 2 \cdot 2 - 5}{1} + 1 = \frac{31}{1} + 1 = 32
\end{equation*}
Sortie : 32√ó32 (same padding)
\end{exemple}

\subsection{Convolution multi-canal}

Pour une image RGB (3 canaux) :
\begin{itemize}
    \item Entr√©e : $\mat{I} \in \R^{H \times W \times C_{in}}$ ($C_{in} = 3$ pour RGB)
    \item Filtre : $\mat{K} \in \R^{k \times k \times C_{in} \times C_{out}}$
    \item Sortie : $\mat{O} \in \R^{H' \times W' \times C_{out}}$
\end{itemize}

Pour chaque canal de sortie $c$, on applique un filtre 3D :
\begin{equation}
    \mat{O}[i, j, c] = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} \sum_{c'=0}^{C_{in}-1} \mat{I}[i+m, j+n, c'] \cdot \mat{K}[m, n, c', c] + b_c
\end{equation}

\begin{astuce}
Un CNN apprend automatiquement les meilleurs filtres pendant l'entra√Ænement, contrairement aux filtres manuels (Sobel, etc.) !
\end{astuce}

% ===== SECTION 3: COUCHES CNN =====
\section{Couches d'un CNN}

\subsection{Couche de Convolution (Conv Layer)}

\begin{definition}{Couche de Convolution}
Une couche Conv applique $C_{out}$ filtres apprenables sur l'entr√©e pour produire $C_{out}$ feature maps.
\end{definition}

\textbf{Nombre de param√®tres :}
\begin{equation}
    \text{Params} = (k \times k \times C_{in} + 1) \times C_{out}
\end{equation}
Le $+1$ correspond au biais par filtre.

\begin{exemple}{Conv2D(3, 64, kernel=3)}
Entr√©e RGB (3 canaux), 64 filtres de taille 3√ó3 :
\begin{equation*}
    \text{Params} = (3 \times 3 \times 3 + 1) \times 64 = 28 \times 64 = 1{,}792
\end{equation*}
\end{exemple}

\subsection{Fonction d'Activation}

Apr√®s chaque convolution, on applique une activation non-lin√©aire (typiquement ReLU) :
\begin{equation}
    \mat{A} = \relu(\mat{O}) = \max(0, \mat{O})
\end{equation}

\subsection{Couche de Pooling}

\begin{definition}{Pooling}
Op√©ration de sous-√©chantillonnage qui r√©duit la dimension spatiale des feature maps.
\end{definition}

\subsubsection{Max Pooling}

Prend le maximum dans chaque r√©gion :
\begin{equation}
    \mat{O}[i, j] = \max_{m, n \in \text{pool}} \mat{I}[i \cdot s + m, j \cdot s + n]
\end{equation}

\textbf{Max Pooling 2√ó2 avec stride 2 :}
\begin{equation*}
\begin{bmatrix}
1 & 3 & 2 & 4 \\
5 & 6 & 7 & 8 \\
3 & 2 & 1 & 0 \\
1 & 2 & 3 & 4
\end{bmatrix}
\xrightarrow{\text{MaxPool 2√ó2}}
\begin{bmatrix}
6 & 8 \\
3 & 4
\end{bmatrix}
\end{equation*}

\subsubsection{Average Pooling}

Prend la moyenne :
\begin{equation}
    \mat{O}[i, j] = \frac{1}{k^2} \sum_{m, n \in \text{pool}} \mat{I}[i \cdot s + m, j \cdot s + n]
\end{equation}

\subsubsection{Avantages du Pooling}

\begin{itemize}
    \item ‚úÖ R√©duit la taille spatiale ‚Üí moins de param√®tres dans les couches suivantes
    \item ‚úÖ Invariance locale par translation (petits d√©placements)
    \item ‚úÖ Augmente le champ r√©cepteur
    \item ‚úÖ R√©gularisation (r√©duit overfitting)
\end{itemize}

\begin{attention}
Le pooling \textbf{n'a pas de param√®tres apprenables}. C'est une op√©ration d√©terministe.
\end{attention}

\subsection{Couche Fully-Connected (FC)}

En fin de r√©seau, on "aplatit" les feature maps et on applique un MLP classique :
\begin{align}
    \vect{x}_{\text{flat}} &= \text{Flatten}(\mat{A}^{[L-1]}) \quad \in \R^d \\
    \vect{z}^{[L]} &= \mat{W}^{[L]} \vect{x}_{\text{flat}} + \vect{b}^{[L]} \\
    \hat{\vect{y}} &= \softmax(\vect{z}^{[L]})
\end{align}

\subsection{Architecture typique}

Un CNN classique suit le pattern :
\begin{equation*}
\boxed{\text{INPUT}} \to \boxed{\text{[CONV + ReLU + POOL]} \times N} \to \boxed{\text{FC}} \to \boxed{\text{SOFTMAX}}
\end{equation*}

\begin{exemple}{CNN simple pour MNIST}
\begin{align*}
&\text{Input: } 28 \times 28 \times 1 \\
&\text{Conv1: } 32 \text{ filtres } 3 \times 3 \to 28 \times 28 \times 32 \\
&\text{ReLU + MaxPool } 2 \times 2 \to 14 \times 14 \times 32 \\
&\text{Conv2: } 64 \text{ filtres } 3 \times 3 \to 14 \times 14 \times 64 \\
&\text{ReLU + MaxPool } 2 \times 2 \to 7 \times 7 \times 64 \\
&\text{Flatten: } 7 \times 7 \times 64 = 3{,}136 \\
&\text{FC: } 3{,}136 \to 10 \text{ (classes)}
\end{align*}
\end{exemple}

% ===== SECTION 4: BACKPROPAGATION DANS LES CNN =====
\section{Backpropagation dans les CNN}

\subsection{Gradient de la convolution}

Pour une convolution $\mat{O} = \mat{I} * \mat{K}$ :

\textbf{Gradient par rapport √† l'entr√©e :}
\begin{equation}
    \frac{\partial L}{\partial \mat{I}} = \frac{\partial L}{\partial \mat{O}} * \mat{K}_{\text{rot180}}
\end{equation}
o√π $\mat{K}_{\text{rot180}}$ est le filtre $\mat{K}$ tourn√© de 180¬∞.

\textbf{Gradient par rapport au filtre :}
\begin{equation}
    \frac{\partial L}{\partial \mat{K}} = \mat{I} * \frac{\partial L}{\partial \mat{O}}
\end{equation}

\subsection{Gradient du Max Pooling}

Le gradient ne se propage qu'√† travers l'√©l√©ment qui √©tait le maximum :
\begin{equation}
    \frac{\partial L}{\partial \mat{I}[i, j]} = \begin{cases}
        \frac{\partial L}{\partial \mat{O}[i', j']} & \text{si } \mat{I}[i, j] = \max \text{ dans sa r√©gion} \\
        0 & \text{sinon}
    \end{cases}
\end{equation}

\begin{astuce}
En pratique, PyTorch et TensorFlow calculent automatiquement tous ces gradients gr√¢ce √† l'autodiff√©rentiation !
\end{astuce}

% ===== SECTION 5: ARCHITECTURES CLASSIQUES =====
\section{Architectures CNN Classiques}

\subsection{LeNet-5 (1998)}

\textbf{Auteurs :} Yann LeCun et al.
\textbf{Application :} Reconnaissance de chiffres manuscrits (MNIST)

\textbf{Architecture :}
\begin{align*}
&\text{Input: } 32 \times 32 \times 1 \\
&\text{C1: Conv } 6 @ 5 \times 5 \to 28 \times 28 \times 6 \\
&\text{S2: AvgPool } 2 \times 2 \to 14 \times 14 \times 6 \\
&\text{C3: Conv } 16 @ 5 \times 5 \to 10 \times 10 \times 16 \\
&\text{S4: AvgPool } 2 \times 2 \to 5 \times 5 \times 16 \\
&\text{C5: Conv } 120 @ 5 \times 5 \to 1 \times 1 \times 120 \\
&\text{F6: FC } 120 \to 84 \\
&\text{Output: FC } 84 \to 10
\end{align*}

\textbf{Param√®tres :} ~60K
\textbf{Fonction d'activation :} Tanh (√† l'√©poque, ReLU n'√©tait pas encore populaire)

\subsection{AlexNet (2012)}

\textbf{Auteurs :} Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton
\textbf{Dataset :} ImageNet (1.2M images, 1000 classes)
\textbf{Impact :} R√©volution du deep learning (top-5 error : 15.3\% vs 26\% avant)

\textbf{Architecture :}
\begin{align*}
&\text{Input: } 227 \times 227 \times 3 \\
&\text{Conv1: } 96 @ 11 \times 11, \text{stride } 4 \to 55 \times 55 \times 96 \\
&\text{MaxPool } 3 \times 3, \text{stride } 2 \to 27 \times 27 \times 96 \\
&\text{Conv2: } 256 @ 5 \times 5 \to 27 \times 27 \times 256 \\
&\text{MaxPool } 3 \times 3, \text{stride } 2 \to 13 \times 13 \times 256 \\
&\text{Conv3: } 384 @ 3 \times 3 \to 13 \times 13 \times 384 \\
&\text{Conv4: } 384 @ 3 \times 3 \to 13 \times 13 \times 384 \\
&\text{Conv5: } 256 @ 3 \times 3 \to 13 \times 13 \times 256 \\
&\text{MaxPool } 3 \times 3 \to 6 \times 6 \times 256 \\
&\text{FC6: } 9{,}216 \to 4{,}096 \\
&\text{FC7: } 4{,}096 \to 4{,}096 \\
&\text{FC8: } 4{,}096 \to 1{,}000
\end{align*}

\textbf{Param√®tres :} ~60M
\textbf{Innovations :}
\begin{itemize}
    \item ReLU activation (au lieu de tanh)
    \item Dropout (0.5 dans les FC)
    \item Data augmentation (crop, flip, color jitter)
    \item GPU training (2√ó GTX 580)
\end{itemize}

\subsection{VGGNet (2014)}

\textbf{Auteurs :} Simonyan \& Zisserman (Oxford)
\textbf{Principe :} Utiliser des filtres 3√ó3 exclusivement, empiler beaucoup de couches

\textbf{VGG-16 Architecture :}
\begin{itemize}
    \item \textbf{Block 1 :} 2√ó Conv(64, 3√ó3) + MaxPool
    \item \textbf{Block 2 :} 2√ó Conv(128, 3√ó3) + MaxPool
    \item \textbf{Block 3 :} 3√ó Conv(256, 3√ó3) + MaxPool
    \item \textbf{Block 4 :} 3√ó Conv(512, 3√ó3) + MaxPool
    \item \textbf{Block 5 :} 3√ó Conv(512, 3√ó3) + MaxPool
    \item \textbf{FC :} 4096 ‚Üí 4096 ‚Üí 1000
\end{itemize}

\textbf{Param√®tres :} 138M (tr√®s lourd !)
\textbf{Insight :} 2 convolutions 3√ó3 = champ r√©cepteur 5√ó5, mais avec moins de param√®tres et plus de non-lin√©arit√©

\begin{equation*}
\text{Params}(5 \times 5) = 25C^2 \quad \text{vs} \quad \text{Params}(3 \times 3 \times 2) = 18C^2
\end{equation*}

\subsection{ResNet (2015)}

\textbf{Auteurs :} He et al. (Microsoft Research)
\textbf{Innovation :} Residual connections (skip connections)

\textbf{Probl√®me des r√©seaux profonds :} Degradation problem (r√©seaux tr√®s profonds difficiles √† entra√Æner, m√™me avec BN)

\begin{definition}{Residual Block}
Au lieu d'apprendre $\mathcal{H}(\vect{x})$, on apprend le r√©sidu $\mathcal{F}(\vect{x}) = \mathcal{H}(\vect{x}) - \vect{x}$ :
\begin{equation}
    \vect{y} = \mathcal{F}(\vect{x}, \{W_i\}) + \vect{x}
\end{equation}
La connexion $+\vect{x}$ est appel√©e "skip connection" ou "shortcut".
\end{definition}

\textbf{Architecture d'un bloc r√©siduel :}
\begin{align*}
\vect{x} &\to \text{Conv } 3 \times 3 \to \text{BN} \to \text{ReLU} \\
&\to \text{Conv } 3 \times 3 \to \text{BN} \to (+\vect{x}) \to \text{ReLU} \to \vect{y}
\end{align*}

\textbf{Avantages :}
\begin{itemize}
    \item Permet d'entra√Æner des r√©seaux tr√®s profonds (50, 101, 152, voire 1000 couches)
    \item Gradient flow am√©lior√© (√©vite vanishing gradient)
    \item Si n√©cessaire, le r√©seau peut "copier" l'identit√© en mettant $\mathcal{F}(\vect{x}) = 0$
\end{itemize}

\textbf{ResNet-50 :} 50 couches, ~25M param√®tres, top-5 error ImageNet : 3.6\%

\subsection{Autres architectures modernes}

\begin{table}[h]
\centering
\caption{Comparaison des architectures CNN}
\label{tab:cnn_architectures}
\begin{tabular}{lcccc}
\toprule
\textbf{Mod√®le} & \textbf{Ann√©e} & \textbf{Param√®tres} & \textbf{Top-5 Error} & \textbf{Innovation} \\
\midrule
LeNet-5 & 1998 & 60K & - & Premier CNN \\
AlexNet & 2012 & 60M & 15.3\% & ReLU, Dropout, GPU \\
VGG-16 & 2014 & 138M & 7.3\% & Filtres 3√ó3 profonds \\
ResNet-50 & 2015 & 25M & 3.6\% & Skip connections \\
Inception v3 & 2015 & 24M & 3.5\% & Multi-scale filters \\
EfficientNet & 2019 & 5-66M & 2.9\% & Scaling optimal \\
Vision Transformer & 2020 & 86M & 2.3\% & Self-attention \\
\bottomrule
\end{tabular}
\end{table}

% ===== SECTION 6: IMPL√âMENTATION =====
\section{Impl√©mentation avec PyTorch}

\subsection{CNN simple from scratch}

\begin{lstlisting}[language=Python, caption=CNN simple pour MNIST]
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()

        # Couches de convolution
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32,
                               kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,
                               kernel_size=3, padding=1)

        # Pooling
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # Couches fully-connected
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)

        # Dropout
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # Block 1: Conv + ReLU + Pool
        x = self.pool(F.relu(self.conv1(x)))  # 28x28x1 -> 14x14x32

        # Block 2: Conv + ReLU + Pool
        x = self.pool(F.relu(self.conv2(x)))  # 14x14x32 -> 7x7x64

        # Flatten
        x = x.view(-1, 64 * 7 * 7)  # (batch, 3136)

        # FC layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

# Instancier le mod√®le
model = SimpleCNN()
print(model)

# Compter les param√®tres
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")
\end{lstlisting}

\subsection{Entra√Ænement}

\begin{lstlisting}[language=Python, caption=Training loop]
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Dataset et DataLoader
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST('./data', train=True, download=True,
                               transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Loss et optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        # Forward
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)

        # Backward
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

print("Training complete!")
\end{lstlisting}

\subsection{VGG-like architecture}

\begin{lstlisting}[language=Python, caption=VGG-style CNN]
class VGGBlock(nn.Module):
    def __init__(self, in_channels, out_channels, num_convs):
        super(VGGBlock, self).__init__()
        layers = []
        for _ in range(num_convs):
            layers.append(nn.Conv2d(in_channels, out_channels,
                                    kernel_size=3, padding=1))
            layers.append(nn.ReLU(inplace=True))
            in_channels = out_channels
        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)

class TinyVGG(nn.Module):
    def __init__(self, num_classes=10):
        super(TinyVGG, self).__init__()

        self.features = nn.Sequential(
            VGGBlock(3, 64, 2),   # 2x Conv64 + Pool
            VGGBlock(64, 128, 2), # 2x Conv128 + Pool
            VGGBlock(128, 256, 3) # 3x Conv256 + Pool
        )

        self.classifier = nn.Sequential(
            nn.Linear(256 * 4 * 4, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
\end{lstlisting}

\subsection{ResNet block}

\begin{lstlisting}[language=Python, caption=Residual Block]
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()

        self.conv1 = nn.Conv2d(in_channels, out_channels,
                               kernel_size=3, stride=stride, padding=1,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels,
                               kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # Skip connection (shortcut)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)  # Skip connection
        out = F.relu(out)
        return out

# Utilisation
class TinyResNet(nn.Module):
    def __init__(self, num_classes=10):
        super(TinyResNet, self).__init__()

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)

        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)

        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)

    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride))
        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels, out_channels, 1))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
\end{lstlisting}

% ===== SECTION 7: TRANSFER LEARNING =====
\section{Transfer Learning et Fine-Tuning}

\subsection{Principe}

\textbf{Id√©e :} Utiliser un r√©seau pr√©-entra√Æn√© sur ImageNet (1.2M images) comme point de d√©part pour une nouvelle t√¢che.

\textbf{Pourquoi √ßa marche ?}
\begin{itemize}
    \item Les features bas niveau (bords, textures) sont universelles
    \item Pr√©-entra√Æner sur un grand dataset capture ces features
    \item On peut r√©utiliser ces features pour une nouvelle t√¢che (m√™me avec peu de donn√©es)
\end{itemize}

\subsection{Strat√©gies}

\subsubsection{Feature Extraction (Frozen Backbone)}

\begin{enumerate}
    \item Charger un mod√®le pr√©-entra√Æn√© (ResNet, VGG, etc.)
    \item \textbf{Geler} toutes les couches convolutionnelles
    \item Remplacer la derni√®re couche FC par une nouvelle (taille = nb de classes)
    \item Entra√Æner uniquement la nouvelle couche FC
\end{enumerate}

\textbf{Avantages :} Tr√®s rapide, peu de donn√©es n√©cessaires
\textbf{Inconv√©nients :} Features pas adapt√©es √† la nouvelle t√¢che

\subsubsection{Fine-Tuning}

\begin{enumerate}
    \item Charger mod√®le pr√©-entra√Æn√©
    \item Remplacer derni√®re couche FC
    \item \textbf{Entra√Æner tout le r√©seau} avec un learning rate tr√®s faible
    \item Optionnel : d√©geler progressivement les couches (shallow ‚Üí deep)
\end{enumerate}

\textbf{Avantages :} Meilleure performance
\textbf{Inconv√©nients :} N√©cessite plus de donn√©es, risque d'overfitting

\subsection{Impl√©mentation PyTorch}

\begin{lstlisting}[language=Python, caption=Transfer Learning avec ResNet]
import torchvision.models as models

# Charger ResNet-18 pr√©-entra√Æn√©
model = models.resnet18(pretrained=True)

# Option 1 : Feature Extraction (geler toutes les couches)
for param in model.parameters():
    param.requires_grad = False

# Remplacer la derni√®re couche FC
num_classes = 10  # Notre nouvelle t√¢che
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Seule la derni√®re couche sera entra√Æn√©e
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

# -------------------------------------------

# Option 2 : Fine-Tuning (tout entra√Æner)
model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)

# Entra√Æner tout le r√©seau avec un petit LR
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# -------------------------------------------

# Option 3 : Fine-Tuning progressif
model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)

# Geler d'abord toutes les couches sauf FC
for param in model.parameters():
    param.requires_grad = False
for param in model.fc.parameters():
    param.requires_grad = True

# Entra√Æner FC pendant quelques epochs
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)
# ... train ...

# Puis d√©geler tout et fine-tune avec petit LR
for param in model.parameters():
    param.requires_grad = True
optimizer = optim.Adam(model.parameters(), lr=1e-5)
# ... train ...
\end{lstlisting}

\subsection{Data Augmentation}

Pour √©viter l'overfitting avec peu de donn√©es :

\begin{lstlisting}[language=Python, caption=Data Augmentation]
from torchvision import transforms

train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),      # Crop al√©atoire
    transforms.RandomHorizontalFlip(),      # Flip horizontal
    transforms.ColorJitter(                 # Perturbations couleur
        brightness=0.2,
        contrast=0.2,
        saturation=0.2
    ),
    transforms.RandomRotation(15),          # Rotation ¬±15¬∞
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])
\end{lstlisting}

% ===== SECTION 8: VISUALISATION =====
\section{Visualisation et Interpr√©tation}

\subsection{Visualiser les filtres}

\begin{lstlisting}[language=Python, caption=Visualiser les filtres Conv1]
import matplotlib.pyplot as plt

# Extraire les poids de la premi√®re couche conv
conv1_weights = model.conv1.weight.data.cpu()
# Shape: (out_channels, in_channels, k, k)

# Visualiser les 16 premiers filtres
fig, axes = plt.subplots(4, 4, figsize=(10, 10))
for i, ax in enumerate(axes.flat):
    if i < conv1_weights.shape[0]:
        # Si RGB, prendre le premier canal
        if conv1_weights.shape[1] == 3:
            filter_img = conv1_weights[i, 0, :, :]
        else:
            filter_img = conv1_weights[i, 0, :, :]
        ax.imshow(filter_img, cmap='gray')
        ax.axis('off')
plt.suptitle('Filtres Conv1')
plt.show()
\end{lstlisting}

\subsection{Visualiser les feature maps}

\begin{lstlisting}[language=Python, caption=Visualiser les activations]
def visualize_feature_maps(model, image, layer_name='conv1'):
    # Hook pour capturer l'output d'une couche
    activations = {}
    def hook_fn(module, input, output):
        activations['feature_maps'] = output

    # Enregistrer le hook
    layer = dict(model.named_modules())[layer_name]
    hook = layer.register_forward_hook(hook_fn)

    # Forward pass
    model.eval()
    with torch.no_grad():
        _ = model(image.unsqueeze(0))

    # Supprimer le hook
    hook.remove()

    # Visualiser
    feature_maps = activations['feature_maps'][0].cpu()
    num_maps = min(16, feature_maps.shape[0])

    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    for i, ax in enumerate(axes.flat):
        if i < num_maps:
            ax.imshow(feature_maps[i], cmap='viridis')
            ax.set_title(f'Map {i}')
            ax.axis('off')
    plt.suptitle(f'Feature Maps - {layer_name}')
    plt.show()
\end{lstlisting}

\subsection{Grad-CAM (Class Activation Mapping)}

Technique pour visualiser quelles r√©gions de l'image influencent la pr√©diction.

\begin{lstlisting}[language=Python, caption=Grad-CAM simplifi√©]
def grad_cam(model, image, target_class):
    model.eval()
    image.requires_grad = True

    # Forward
    output = model(image.unsqueeze(0))
    class_score = output[0, target_class]

    # Backward
    model.zero_grad()
    class_score.backward()

    # R√©cup√©rer gradients de la derni√®re conv
    gradients = image.grad.data

    # Calculer importance (moyenne des gradients)
    weights = torch.mean(gradients, dim=(2, 3), keepdim=True)

    # Combiner avec feature maps
    cam = torch.sum(weights * image, dim=1, keepdim=True)
    cam = F.relu(cam)  # ReLU pour garder activations positives

    # Normaliser
    cam = (cam - cam.min()) / (cam.max() - cam.min())

    return cam.squeeze().cpu().numpy()
\end{lstlisting}

% ===== SECTION 9: APPLICATIONS =====
\section{Applications des CNN}

\subsection{Computer Vision}

\begin{enumerate}
    \item \textbf{Classification d'images}
    \begin{itemize}
        \item ImageNet : 1000 classes (chiens, chats, avions, etc.)
        \item Diagnostic m√©dical : d√©tection cancer, COVID-19 sur radiographies
    \end{itemize}

    \item \textbf{D√©tection d'objets} (Object Detection)
    \begin{itemize}
        \item YOLO, Faster R-CNN, SSD
        \item Applications : voitures autonomes, surveillance
    \end{itemize}

    \item \textbf{Segmentation s√©mantique}
    \begin{itemize}
        \item U-Net, Mask R-CNN
        \item Applications : imagerie m√©dicale, √©dition photo
    \end{itemize}

    \item \textbf{Reconnaissance faciale}
    \begin{itemize}
        \item FaceNet, DeepFace
        \item Applications : d√©verrouillage t√©l√©phone, s√©curit√©
    \end{itemize}

    \item \textbf{G√©n√©ration d'images}
    \begin{itemize}
        \item GANs (Generative Adversarial Networks)
        \item Style Transfer, Super-Resolution
    \end{itemize}
\end{enumerate}

\subsection{Au-del√† de la vision}

Les CNN peuvent aussi traiter d'autres donn√©es spatiales :

\begin{itemize}
    \item \textbf{Traitement du signal audio} : spectrogrammes (convolution 1D ou 2D)
    \item \textbf{S√©ries temporelles} : Temporal CNN (TCN)
    \item \textbf{Texte} : CNN 1D pour classification de textes (moins utilis√© que Transformers)
    \item \textbf{Graphes} : Graph Convolutional Networks (GCN)
\end{itemize}

% ===== SECTION 10: BONNES PRATIQUES =====
\section{Bonnes Pratiques}

\subsection{Architecture}

\begin{itemize}
    \item Utiliser des filtres 3√ó3 (standard moderne)
    \item Doubler les canaux quand on divise la r√©solution spatiale par 2
    \item Batch Normalization apr√®s chaque Conv (avant ReLU)
    \item Utiliser Global Average Pooling au lieu de FC massifs (r√©duit overfitting)
    \item Pr√©f√©rer des r√©seaux profonds mais avec skip connections (ResNet-style)
\end{itemize}

\subsection{Entra√Ænement}

\begin{itemize}
    \item \textbf{Optimizer :} Adam ou SGD avec momentum (0.9)
    \item \textbf{Learning Rate :} 1e-3 pour Adam, 1e-1 pour SGD
    \item \textbf{LR Scheduling :} ReduceLROnPlateau ou Cosine Annealing
    \item \textbf{Batch Size :} 32-128 (compromis vitesse/g√©n√©ralisation)
    \item \textbf{R√©gularisation :} L2 (1e-4), Dropout (0.5), Data Augmentation
    \item \textbf{Early Stopping :} Patience de 10-20 epochs
\end{itemize}

\subsection{Transfer Learning}

\begin{astuce}
\textbf{R√®gle g√©n√©rale :}
\begin{itemize}
    \item \textbf{Peu de donn√©es (< 1000)} : Feature extraction
    \item \textbf{Donn√©es moyennes (1K-10K)} : Fine-tuning des derni√®res couches
    \item \textbf{Beaucoup de donn√©es (> 10K)} : Fine-tuning complet ou entra√Ænement from scratch
\end{itemize}
\end{astuce}

% ===== SECTION 11: AVANTAGES ET LIMITES =====
\section{Avantages et Limites}

\subsection{Avantages}

\begin{itemize}
    \item ‚úÖ Exploitation de la structure spatiale des images
    \item ‚úÖ Invariance par translation (weight sharing)
    \item ‚úÖ Beaucoup moins de param√®tres qu'un MLP √©quivalent
    \item ‚úÖ Hi√©rarchie automatique de features (bas niveau ‚Üí haut niveau)
    \item ‚úÖ State-of-the-art en vision par ordinateur
    \item ‚úÖ Transfer learning tr√®s efficace
\end{itemize}

\subsection{Limites}

\begin{itemize}
    \item ‚ùå N√©cessite beaucoup de donn√©es (ou transfer learning)
    \item ‚ùå Pas invariant aux rotations/√©chelles (n√©cessite data augmentation)
    \item ‚ùå Sensible aux adversarial examples (perturbations imperceptibles)
    \item ‚ùå Co√ªt computationnel √©lev√© (GPU indispensable)
    \item ‚ùå Difficile √† interpr√©ter (bo√Æte noire)
    \item ‚ùå Pas optimal pour donn√©es non-spatiales (tableaux, graphes)
\end{itemize}

% ===== SECTION 12: R√âSUM√â =====
\section{R√©sum√© du Chapitre}

\subsection{Points Cl√©s}

\begin{itemize}
    \item \textbf{Convolution} : Produit scalaire local avec partage de poids ‚Üí invariance translation
    \item \textbf{Pooling} : Sous-√©chantillonnage pour r√©duire dimension et augmenter champ r√©cepteur
    \item \textbf{Architecture} : [Conv + ReLU + Pool] √ó N ‚Üí Flatten ‚Üí FC ‚Üí Softmax
    \item \textbf{LeNet} (1998) : Premier CNN (MNIST)
    \item \textbf{AlexNet} (2012) : R√©volution deep learning (ImageNet)
    \item \textbf{VGG} (2014) : Empilage profond de Conv 3√ó3
    \item \textbf{ResNet} (2015) : Skip connections ‚Üí r√©seaux tr√®s profonds
    \item \textbf{Transfer Learning} : R√©utilisation de mod√®les pr√©-entra√Æn√©s
    \item \textbf{Data Augmentation} : Flip, crop, rotation, color jitter
\end{itemize}

\subsection{Formules Essentielles}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Formules √† retenir]
\textbf{Convolution 2D :}
\begin{equation*}
    \mat{O}[i, j] = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} \mat{I}[i+m, j+n] \cdot \mat{K}[m, n] + b
\end{equation*}

\textbf{Taille de sortie :}
\begin{equation*}
    H_{out} = \left\lfloor \frac{H + 2p - k}{s} \right\rfloor + 1
\end{equation*}

\textbf{Nombre de param√®tres Conv :}
\begin{equation*}
    \text{Params} = (k \times k \times C_{in} + 1) \times C_{out}
\end{equation*}

\textbf{Residual Block :}
\begin{equation*}
    \vect{y} = \mathcal{F}(\vect{x}, \{W_i\}) + \vect{x}
\end{equation*}
\end{tcolorbox}

% ===== SECTION 13: EXERCICES =====
\section{Exercices}

\subsection{Questions de compr√©hension}

\begin{enumerate}
    \item Pourquoi un CNN a-t-il moins de param√®tres qu'un MLP pour traiter des images ?
    \item Expliquer l'intuition derri√®re le max pooling.
    \item Quelle est la diff√©rence entre valid padding et same padding ?
    \item Pourquoi VGG utilise exclusivement des filtres 3√ó3 ?
    \item Comment les skip connections de ResNet aident-elles √† entra√Æner des r√©seaux profonds ?
    \item Quelle est la diff√©rence entre feature extraction et fine-tuning ?
\end{enumerate}

\subsection{Exercices pratiques}

\begin{enumerate}
    \item \textbf{CNN pour CIFAR-10}
    \begin{itemize}
        \item Impl√©menter un CNN from scratch pour CIFAR-10
        \item Architecture libre, objectif : > 75\% accuracy
        \item Utiliser data augmentation
    \end{itemize}

    \item \textbf{Transfer Learning}
    \begin{itemize}
        \item Charger ResNet-18 pr√©-entra√Æn√©
        \item Fine-tuner sur un petit dataset (ex: Cats vs Dogs)
        \item Comparer feature extraction vs fine-tuning
    \end{itemize}

    \item \textbf{Visualisation}
    \begin{itemize}
        \item Visualiser les filtres de Conv1
        \item Visualiser les feature maps pour diff√©rentes couches
        \item Impl√©menter Grad-CAM
    \end{itemize}

    \item \textbf{Architecture ResNet}
    \begin{itemize}
        \item Impl√©menter un ResNet-18 from scratch
        \item Comparer avec un CNN classique de m√™me profondeur (sans skip connections)
    \end{itemize}
\end{enumerate}

\textit{Solutions disponibles dans} \texttt{07_exercices.ipynb} \textit{(solutions int√©gr√©es dans le notebook)}

% ===== SECTION 14: POUR ALLER PLUS LOIN =====
\section{Pour Aller Plus Loin}

\subsection{Lectures Recommand√©es}

\begin{itemize}
    \item LeCun et al. (1998) - "Gradient-based learning applied to document recognition" (LeNet)
    \item Krizhevsky et al. (2012) - "ImageNet Classification with Deep CNNs" (AlexNet)
    \item Simonyan \& Zisserman (2014) - "Very Deep Convolutional Networks" (VGG)
    \item He et al. (2015) - "Deep Residual Learning for Image Recognition" (ResNet)
    \item Szegedy et al. (2015) - "Going Deeper with Convolutions" (Inception)
\end{itemize}

\subsection{Ressources en Ligne}

\begin{itemize}
    \item CS231n Stanford : \url{http://cs231n.stanford.edu/}
    \item PyTorch Tutorials : \url{https://pytorch.org/tutorials/}
    \item Papers With Code : \url{https://paperswithcode.com/}
    \item Distill.pub : Articles interactifs sur les CNN
\end{itemize}

\subsection{Architectures Avanc√©es}

\begin{itemize}
    \item \textbf{EfficientNet} : Scaling optimal (width, depth, resolution)
    \item \textbf{MobileNet} : CNN l√©gers pour mobile/edge devices
    \item \textbf{Vision Transformers (ViT)} : Alternatives aux CNN bas√©es sur attention
    \item \textbf{YOLO, Faster R-CNN} : D√©tection d'objets
    \item \textbf{U-Net, Mask R-CNN} : Segmentation
\end{itemize}

\subsection{Prochaines √âtapes}

Chapitre suivant recommand√© : \textbf{Chapitre 08 - Deep Learning : RNN et Transformers}

Les RNN et Transformers sont des architectures pour les donn√©es s√©quentielles (texte, s√©ries temporelles, audio).

% ===== BIBLIOGRAPHIE =====
\section*{R√©f√©rences}

\begin{enumerate}
    \item LeCun, Y., et al. (1998). "Gradient-based learning applied to document recognition". \textit{Proceedings of the IEEE}, 86(11), 2278-2324.
    \item Krizhevsky, A., Sutskever, I., \& Hinton, G. E. (2012). "ImageNet classification with deep convolutional neural networks". \textit{NIPS}.
    \item Simonyan, K., \& Zisserman, A. (2014). "Very deep convolutional networks for large-scale image recognition". \textit{arXiv:1409.1556}.
    \item He, K., et al. (2015). "Deep residual learning for image recognition". \textit{CVPR}.
    \item Szegedy, C., et al. (2015). "Going deeper with convolutions". \textit{CVPR}.
    \item Goodfellow, I., Bengio, Y., \& Courville, A. (2016). \textit{Deep Learning}. MIT Press.
\end{enumerate}

\end{document}
