{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/09_reinforcement_learning/09_exercices.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('üì¶ Installation des packages...')\n",
    "    !pip install -q numpy pandas matplotlib seaborn\n",
    "    !pip install -q gymnasium[classic-control]\n",
    "    !pip install -q torch torchvision\n",
    "    print('‚úÖ Installation termin√©e !')\n",
    "else:\n",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 09 - Exercices : Reinforcement Learning\n",
    "\n",
    "Ce notebook contient des exercices pratiques pour consolider les concepts du Chapitre 09.\n",
    "\n",
    "**Instructions** :\n",
    "- Compl√©tez les cellules marqu√©es `# VOTRE CODE ICI`\n",
    "- Les solutions sont disponibles dans `09_exercices_solutions.ipynb`\n",
    "- N'h√©sitez pas √† consulter la documentation (Gymnasium, PyTorch)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports n√©cessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úì Biblioth√®ques import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 1 : Q-Learning sur FrozenLake\n",
    "\n",
    "**Environnement** : FrozenLake-v1 (4x4 grid)\n",
    "\n",
    "**Objectif** : Impl√©menter l'algorithme Q-Learning pour apprendre √† naviguer sur un lac gel√©.\n",
    "\n",
    "### 1.1 Exploration de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ez l'environnement FrozenLake (non-slippery pour commencer)\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "env = None  # gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "# Affichez les informations sur l'environnement\n",
    "print(f\"Espace d'√©tats : {env.observation_space}\")\n",
    "print(f\"Espace d'actions : {env.action_space}\")\n",
    "print(f\"\\nActions possibles :\")\n",
    "print(\"  0 = LEFT, 1 = DOWN, 2 = RIGHT, 3 = UP\")\n",
    "\n",
    "# Question : Combien y a-t-il d'√©tats et d'actions possibles ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Impl√©mentation de Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impl√©mentez l'algorithme Q-Learning\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, learning_rate=0.1, gamma=0.99, epsilon=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_states: Nombre d'√©tats\n",
    "            n_actions: Nombre d'actions\n",
    "            learning_rate: Taux d'apprentissage (alpha)\n",
    "            gamma: Facteur de discount\n",
    "            epsilon: Probabilit√© d'exploration initiale\n",
    "        \"\"\"\n",
    "        # TODO: Initialisez la Q-table\n",
    "        self.q_table = None  # np.zeros((n_states, n_actions))\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Strat√©gie epsilon-greedy.\"\"\"\n",
    "        # TODO: Impl√©mentez epsilon-greedy\n",
    "        # Avec probabilit√© epsilon : action al√©atoire\n",
    "        # Sinon : argmax Q(state, action)\n",
    "        pass\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Mise √† jour Q-Learning.\"\"\"\n",
    "        # TODO: Impl√©mentez la formule de mise √† jour Q-Learning\n",
    "        # Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]\n",
    "        pass\n",
    "    \n",
    "    def decay_epsilon(self, decay_rate=0.995):\n",
    "        \"\"\"D√©croissance d'epsilon.\"\"\"\n",
    "        # TODO: R√©duisez epsilon\n",
    "        pass\n",
    "\n",
    "# Cr√©ez l'agent\n",
    "agent = QLearningAgent(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n,\n",
    "    learning_rate=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Entra√Ænement de l'Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entra√Ænement\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "n_episodes = 10000\n",
    "rewards_history = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    # TODO: R√©initialisez l'environnement\n",
    "    state, info = None, None  # env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # TODO: Choisissez une action\n",
    "        action = None\n",
    "        \n",
    "        # TODO: Ex√©cutez l'action dans l'environnement\n",
    "        next_state, reward, terminated, truncated, info = None, None, None, None, None\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # TODO: Mettez √† jour la Q-table\n",
    "        \n",
    "        # TODO: Passez √† l'√©tat suivant\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    # TODO: D√©croissance d'epsilon\n",
    "    \n",
    "    rewards_history.append(episode_reward)\n",
    "    \n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        avg_reward = np.mean(rewards_history[-100:])\n",
    "        print(f\"Episode {episode + 1}/{n_episodes} | Avg Reward (last 100): {avg_reward:.3f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "print(\"\\n‚úì Entra√Ænement termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Visualisation de l'Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisez la courbe d'apprentissage\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Calculez la moyenne mobile sur 100 √©pisodes\n",
    "window = 100\n",
    "moving_avg = None  # np.convolve(...)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1 : R√©compenses brutes\n",
    "plt.subplot(1, 2, 1)\n",
    "# TODO: Tracez rewards_history\n",
    "\n",
    "# Subplot 2 : Moyenne mobile\n",
    "plt.subplot(1, 2, 2)\n",
    "# TODO: Tracez moving_avg\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 √âvaluation de la Politique Apprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluez l'agent sur 100 √©pisodes (sans exploration)\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "n_eval_episodes = 100\n",
    "eval_rewards = []\n",
    "\n",
    "for _ in range(n_eval_episodes):\n",
    "    # TODO: Testez l'agent avec epsilon=0 (exploitation pure)\n",
    "    pass\n",
    "\n",
    "success_rate = None  # Pourcentage d'√©pisodes avec reward=1\n",
    "print(f\"Taux de succ√®s : {success_rate:.1f}%\")\n",
    "print(f\"R√©compense moyenne : {np.mean(eval_rewards):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Visualisation de la Politique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisez la politique apprise (heatmap des actions)\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Cr√©ez une grille 4x4 avec l'action optimale pour chaque √©tat\n",
    "policy_grid = None  # np.argmax(agent.q_table, axis=1).reshape(4, 4)\n",
    "\n",
    "# Symboles pour les actions\n",
    "action_symbols = {0: '‚Üê', 1: '‚Üì', 2: '‚Üí', 3: '‚Üë'}\n",
    "\n",
    "# TODO: Affichez la politique avec plt.imshow et annotez avec les symboles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 2 : Q-Learning sur FrozenLake Slippery\n",
    "\n",
    "**Objectif** : Adapter l'agent √† un environnement stochastique (le lac est glissant).\n",
    "\n",
    "### 2.1 Cr√©ation de l'Environnement Stochastique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ez l'environnement avec is_slippery=True\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "env_slippery = None  # gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "# Question : Qu'est-ce qui change avec is_slippery=True ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Entra√Ænement sur Environnement Stochastique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænez un nouvel agent sur l'environnement slippery\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Conseil : Vous aurez besoin de plus d'√©pisodes et/ou d'ajuster les hyperparam√®tres\n",
    "\n",
    "agent_slippery = QLearningAgent(\n",
    "    n_states=env_slippery.observation_space.n,\n",
    "    n_actions=env_slippery.action_space.n,\n",
    "    learning_rate=0.1,  # Peut-√™tre ajuster\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0\n",
    ")\n",
    "\n",
    "# TODO: Boucle d'entra√Ænement (similaire √† 1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparaison des Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparez les taux de succ√®s entre l'environnement d√©terministe et stochastique\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Question : Pourquoi le taux de succ√®s est-il plus faible en stochastique ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 3 : Deep Q-Network (DQN) sur CartPole\n",
    "\n",
    "**Environnement** : CartPole-v1 (espace d'√©tats continu)\n",
    "\n",
    "**Objectif** : Impl√©menter un DQN avec Experience Replay et Target Network.\n",
    "\n",
    "### 3.1 Architecture du R√©seau de Neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impl√©mentez le r√©seau Q-Network\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # TODO: D√©finissez les couches du r√©seau\n",
    "        # Input : state_dim ‚Üí Hidden : hidden_dim ‚Üí Output : action_dim\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Impl√©mentez le forward pass\n",
    "        # Utilisez ReLU pour les couches cach√©es\n",
    "        pass\n",
    "\n",
    "# Test du r√©seau\n",
    "env_cartpole = gym.make('CartPole-v1')\n",
    "state_dim = env_cartpole.observation_space.shape[0]\n",
    "action_dim = env_cartpole.action_space.n\n",
    "\n",
    "print(f\"State dim: {state_dim}, Action dim: {action_dim}\")\n",
    "\n",
    "# Cr√©ez le r√©seau\n",
    "q_net = QNetwork(state_dim, action_dim)\n",
    "print(q_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impl√©mentez le Replay Buffer\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        # TODO: Utilisez collections.deque pour stocker les transitions\n",
    "        self.buffer = None  # deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Ajoute une transition au buffer.\"\"\"\n",
    "        # TODO: Ajoutez la transition\n",
    "        pass\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"√âchantillonne un batch de transitions.\"\"\"\n",
    "        # TODO: √âchantillonnez al√©atoirement batch_size transitions\n",
    "        # Retournez des tensors PyTorch s√©par√©s pour chaque √©l√©ment\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Test du buffer\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "print(f\"Buffer cr√©√© avec capacit√© 1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Agent DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impl√©mentez l'agent DQN\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0):\n",
    "        # TODO: Cr√©ez le policy network et le target network\n",
    "        self.policy_net = None  # QNetwork(...)\n",
    "        self.target_net = None  # QNetwork(...)\n",
    "        \n",
    "        # TODO: Copiez les poids du policy vers target\n",
    "        # self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        # TODO: Cr√©ez l'optimizer et le replay buffer\n",
    "        self.optimizer = None  # optim.Adam(...)\n",
    "        self.buffer = ReplayBuffer(capacity=10000)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        # TODO: Impl√©mentez epsilon-greedy avec le r√©seau de neurones\n",
    "        pass\n",
    "    \n",
    "    def train_step(self, batch_size=64):\n",
    "        \"\"\"Mise √† jour du r√©seau via gradient descent.\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "        \n",
    "        # TODO: √âchantillonnez un batch du buffer\n",
    "        states, actions, rewards, next_states, dones = None, None, None, None, None\n",
    "        \n",
    "        # TODO: Calculez les Q-values actuelles\n",
    "        # q_values = self.policy_net(states)\n",
    "        # q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # TODO: Calculez les Q-values cibles avec le target network\n",
    "        # next_q_values = self.target_net(next_states)\n",
    "        # max_next_q_values = next_q_values.max(1)[0]\n",
    "        # target_q_value = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "        \n",
    "        # TODO: Calculez la loss (MSE) et mettez √† jour le r√©seau\n",
    "        # loss = nn.functional.mse_loss(q_value, target_q_value.detach())\n",
    "        # self.optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # self.optimizer.step()\n",
    "        pass\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copie les poids du policy vers le target network.\"\"\"\n",
    "        # TODO: Mettez √† jour le target network\n",
    "        pass\n",
    "    \n",
    "    def decay_epsilon(self, decay_rate=0.995, min_epsilon=0.01):\n",
    "        # TODO: D√©croissance d'epsilon\n",
    "        pass\n",
    "\n",
    "# Cr√©ez l'agent\n",
    "dqn_agent = DQNAgent(state_dim, action_dim)\n",
    "print(\"‚úì Agent DQN cr√©√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Entra√Ænement du DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entra√Ænement DQN\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "n_episodes = 500\n",
    "target_update_freq = 10  # Fr√©quence de mise √† jour du target network\n",
    "rewards_history = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    # TODO: Boucle d'entra√Ænement\n",
    "    # 1. Reset environment\n",
    "    # 2. Boucle dans l'√©pisode\n",
    "    # 3. Choose action\n",
    "    # 4. Execute action\n",
    "    # 5. Store transition in buffer\n",
    "    # 6. Train network\n",
    "    # 7. Update target network every target_update_freq episodes\n",
    "    # 8. Decay epsilon\n",
    "    pass\n",
    "\n",
    "print(\"\\n‚úì Entra√Ænement DQN termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualisation et √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisez la courbe d'apprentissage du DQN\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Calculez la moyenne mobile\n",
    "# Tracez les r√©compenses\n",
    "\n",
    "# Question : CartPole est r√©solu quand la moyenne sur 100 √©pisodes atteint 195. Avez-vous r√©solu l'environnement ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 4 : Comparaison Q-Learning vs DQN\n",
    "\n",
    "**Objectif** : Comparer les deux approches.\n",
    "\n",
    "### 4.1 Questions de R√©flexion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** : Quels sont les avantages du DQN par rapport au Q-Learning tabulaire ?\n",
    "\n",
    "**VOTRE R√âPONSE ICI**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 2** : Pourquoi utilise-t-on un Experience Replay Buffer ?\n",
    "\n",
    "**VOTRE R√âPONSE ICI**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 3** : √Ä quoi sert le Target Network ?\n",
    "\n",
    "**VOTRE R√âPONSE ICI**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 4** : Pourquoi ne peut-on pas utiliser Q-Learning tabulaire sur CartPole ?\n",
    "\n",
    "**VOTRE R√âPONSE ICI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 5 : Am√©liorations du DQN (Bonus)\n",
    "\n",
    "**Objectif** : Impl√©menter des am√©liorations de l'algorithme DQN.\n",
    "\n",
    "### 5.1 Double DQN\n",
    "\n",
    "Le Double DQN r√©duit la surestimation des Q-values en utilisant le policy network pour s√©lectionner l'action, et le target network pour √©valuer cette action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impl√©mentez Double DQN\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Modifiez la m√©thode train_step pour utiliser :\n",
    "# 1. Policy network pour s√©lectionner l'action : argmax Q_policy(s', a')\n",
    "# 2. Target network pour √©valuer cette action : Q_target(s', argmax a')\n",
    "\n",
    "# Conseil : Modifiez uniquement le calcul de target_q_value dans train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prioritized Experience Replay\n",
    "\n",
    "√âchantillonnez les transitions avec une probabilit√© proportionnelle √† leur erreur TD (Temporal Difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impl√©mentez un Prioritized Replay Buffer (simplifi√©)\n",
    "# VOTRE CODE ICI\n",
    "\n",
    "# Conseil : Stockez √©galement les priorit√©s dans le buffer\n",
    "# Utilisez np.random.choice avec probabilit√©s pour l'√©chantillonnage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "F√©licitations pour avoir compl√©t√© ces exercices !\n",
    "\n",
    "**Points cl√©s √† retenir** :\n",
    "- Q-Learning fonctionne bien pour des espaces d'√©tats discrets et petits\n",
    "- DQN √©tend Q-Learning aux espaces continus via approximation par r√©seaux de neurones\n",
    "- Experience Replay brise la corr√©lation temporelle et am√©liore la stabilit√©\n",
    "- Target Network stabilise l'entra√Ænement en figeant temporairement les cibles\n",
    "- L'exploration (epsilon-greedy) est cruciale pour d√©couvrir de bonnes politiques\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "- Consultez les solutions dans `09_exercices_solutions.ipynb`\n",
    "- Testez vos agents sur d'autres environnements Gymnasium\n",
    "- Explorez d'autres algorithmes RL : Policy Gradient, Actor-Critic, PPO\n",
    "- Passez au Chapitre 10 (Algorithmes G√©n√©tiques)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
