{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz d'Auto-√âvaluation - Chapitre 09 : Reinforcement Learning\n",
    "\n",
    "**Instructions** :\n",
    "- Ce quiz contient 15 questions pour tester votre compr√©hension du chapitre\n",
    "- R√©pondez aux questions par vous-m√™me avant de regarder les r√©ponses\n",
    "- Les r√©ponses sont dans une cellule masqu√©e √† la fin\n",
    "- Comptez 1 point par bonne r√©ponse\n",
    "\n",
    "**Bar√®me** :\n",
    "- 13-15 : Excellent ! Vous ma√Ætrisez le chapitre üí™\n",
    "- 10-12 : Bien, relisez les sections o√π vous avez des lacunes\n",
    "- 7-9 : Moyen, relisez le chapitre attentivement\n",
    "- < 7 : Insuffisant, reprenez le chapitre depuis le d√©but\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Question 1 : RL vs Supervised Learning\n",
    "Quelle est la principale diff√©rence entre Reinforcement Learning et Supervised Learning ?\n",
    "\n",
    "A) Le RL utilise plus de donn√©es  \n",
    "B) Le RL apprend via r√©compenses diff√©r√©es au lieu de labels explicites  \n",
    "C) Le RL est plus rapide √† entra√Æner  \n",
    "D) Le RL ne n√©cessite pas de donn√©es  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : Markov Decision Process (MDP)\n",
    "Un MDP est d√©fini par $(S, A, P, R, \\gamma)$. Que repr√©sente $P$ ?\n",
    "\n",
    "A) La policy de l'agent  \n",
    "B) Les probabilit√©s de transition entre √©tats  \n",
    "C) Les param√®tres du mod√®le  \n",
    "D) La performance  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 : Discount Factor $\\gamma$\n",
    "Que se passe-t-il si $\\gamma = 0$ dans la formule $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ ?\n",
    "\n",
    "A) L'agent maximise la r√©compense cumulative totale  \n",
    "B) L'agent ne consid√®re que la r√©compense imm√©diate (myopie)  \n",
    "C) L'agent ne peut pas apprendre  \n",
    "D) L'agent explore infiniment  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 : Q-Function\n",
    "Que repr√©sente $Q(s, a)$ ?\n",
    "\n",
    "A) La probabilit√© de prendre l'action $a$ dans l'√©tat $s$  \n",
    "B) La r√©compense cumulative attendue en prenant $a$ dans $s$  \n",
    "C) Le nombre de fois que l'action $a$ a √©t√© prise dans $s$  \n",
    "D) La value function de l'√©tat $s$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 : √âquation de Bellman\n",
    "Quelle est la forme de l'√©quation de Bellman pour la Q-function ?\n",
    "\n",
    "A) $Q(s, a) = R(s, a) + \\gamma Q(s', a')$  \n",
    "B) $Q(s, a) = R(s, a) + \\gamma \\max_{a'} Q(s', a')$  \n",
    "C) $Q(s, a) = R(s, a) + \\sum_{a'} Q(s', a')$  \n",
    "D) $Q(s, a) = \\max_{a'} R(s', a')$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : Exploration vs Exploitation\n",
    "Dans la strat√©gie $\\epsilon$-greedy, que fait l'agent avec probabilit√© $\\epsilon$ ?\n",
    "\n",
    "A) Il choisit l'action optimale (exploit)  \n",
    "B) Il choisit une action al√©atoire (explore)  \n",
    "C) Il arr√™te d'apprendre  \n",
    "D) Il r√©initialise la Q-table  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 : Q-Learning Update\n",
    "Dans Q-Learning, quelle est la formule de mise √† jour ?\n",
    "\n",
    "A) $Q(s, a) \\leftarrow R(s, a)$  \n",
    "B) $Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$  \n",
    "C) $Q(s, a) \\leftarrow r + \\gamma Q(s', a')$  \n",
    "D) $Q(s, a) \\leftarrow \\alpha Q(s, a)$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 : Limite du Q-Learning Tabulaire\n",
    "Pourquoi le Q-Learning tabulaire ne fonctionne pas pour des √©tats continus (ex: images) ?\n",
    "\n",
    "A) Parce qu'il est trop lent  \n",
    "B) Parce qu'il est impossible de stocker $Q(s, a)$ pour tous les √©tats  \n",
    "C) Parce qu'il n√©cessite trop de m√©moire GPU  \n",
    "D) Parce qu'il ne peut pas utiliser de gradient descent  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 : Deep Q-Network (DQN)\n",
    "Quelle est l'innovation principale du DQN ?\n",
    "\n",
    "A) Utiliser un r√©seau de neurones pour approximer $Q(s, a)$  \n",
    "B) Utiliser des arbres de d√©cision  \n",
    "C) Utiliser des SVM  \n",
    "D) Utiliser des k-NN  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 : Experience Replay\n",
    "√Ä quoi sert l'Experience Replay dans DQN ?\n",
    "\n",
    "A) Acc√©l√©rer l'entra√Ænement  \n",
    "B) Briser la corr√©lation temporelle entre transitions cons√©cutives  \n",
    "C) R√©duire le nombre de param√®tres  \n",
    "D) Augmenter l'exploration  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 : Target Network\n",
    "Pourquoi utiliser un target network $Q(s, a; \\theta^-)$ dans DQN ?\n",
    "\n",
    "A) Pour r√©duire le nombre de param√®tres  \n",
    "B) Pour stabiliser l'entra√Ænement en figeant les targets temporairement  \n",
    "C) Pour acc√©l√©rer la convergence  \n",
    "D) Pour √©viter l'exploration  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12 : Policy Gradient\n",
    "Quelle est la diff√©rence entre Q-Learning et Policy Gradient ?\n",
    "\n",
    "A) Q-Learning apprend une value function, Policy Gradient apprend directement la policy  \n",
    "B) Q-Learning est plus rapide  \n",
    "C) Policy Gradient ne fonctionne que sur des jeux  \n",
    "D) Il n'y a pas de diff√©rence  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13 : REINFORCE\n",
    "Dans l'algorithme REINFORCE, que repr√©sente $G_t$ ?\n",
    "\n",
    "A) Le gradient de la policy  \n",
    "B) La r√©compense cumulative (return) √† partir du temps $t$  \n",
    "C) Le learning rate  \n",
    "D) La value function  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14 : Actor-Critic\n",
    "Dans Actor-Critic, quel est le r√¥le du Critic ?\n",
    "\n",
    "A) Choisir les actions  \n",
    "B) √âvaluer la qualit√© des actions via une value function  \n",
    "C) Explorer l'environnement  \n",
    "D) Calculer les r√©compenses  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15 : Applications du RL\n",
    "Parmi ces applications, laquelle N'utilise PAS typiquement le Reinforcement Learning ?\n",
    "\n",
    "A) AlphaGo (jeu de Go)  \n",
    "B) Contr√¥le de robots  \n",
    "C) Classification d'images statiques  \n",
    "D) Trading algorithmique  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Auto-Correction\n",
    "\n",
    "Avant de regarder les r√©ponses, comptez combien de r√©ponses vous avez donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrez vos r√©ponses ici (ex: ['D', 'B', 'A', ...])\n",
    "mes_reponses = []  # TODO: remplir avec vos r√©ponses\n",
    "\n",
    "# R√©ponses correctes (masqu√©es)\n",
    "reponses_correctes = ['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'C']\n",
    "\n",
    "if len(mes_reponses) == 15:\n",
    "    score = sum([1 for i, r in enumerate(mes_reponses) if r.upper() == reponses_correctes[i]])\n",
    "    print(f\"Votre score : {score}/15\")\n",
    "    \n",
    "    if score >= 13:\n",
    "        print(\"\\nüéâ Excellent ! Vous ma√Ætrisez le chapitre !\")\n",
    "    elif score >= 10:\n",
    "        print(\"\\n‚úÖ Bien ! Relisez les sections o√π vous avez des lacunes.\")\n",
    "    elif score >= 7:\n",
    "        print(\"\\n‚ö†Ô∏è  Moyen. Relisez le chapitre attentivement.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Insuffisant. Reprenez le chapitre depuis le d√©but.\")\n",
    "    \n",
    "    # Afficher les erreurs\n",
    "    print(\"\\nD√©tail :\")\n",
    "    for i, (ma_rep, bonne_rep) in enumerate(zip(mes_reponses, reponses_correctes), 1):\n",
    "        if ma_rep.upper() == bonne_rep:\n",
    "            print(f\"Q{i}: ‚úì Correct\")\n",
    "        else:\n",
    "            print(f\"Q{i}: ‚úó Votre r√©ponse: {ma_rep}, Correcte: {bonne_rep}\")\n",
    "else:\n",
    "    print(\"Veuillez remplir toutes les r√©ponses (15 lettres A, B, C ou D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Explications des R√©ponses\n",
    "\n",
    "### Q1 : B\n",
    "En **Supervised Learning**, on a des labels explicites pour chaque exemple. En **Reinforcement Learning**, l'agent apprend via des **r√©compenses diff√©r√©es** (feedback delayed, pas de labels directs).\n",
    "\n",
    "### Q2 : B\n",
    "$P$ repr√©sente les **probabilit√©s de transition** : $P(s' | s, a)$ = probabilit√© de passer de l'√©tat $s$ √† $s'$ en prenant l'action $a$.\n",
    "\n",
    "### Q3 : B\n",
    "Si $\\gamma = 0$, alors $G_t = R_{t+1}$ (seule la r√©compense imm√©diate compte). L'agent devient **myope** et ne planifie pas √† long terme.\n",
    "\n",
    "### Q4 : B\n",
    "$Q(s, a)$ repr√©sente la **r√©compense cumulative attendue** (expected return) en prenant l'action $a$ dans l'√©tat $s$, puis en suivant la policy $\\pi$.\n",
    "\n",
    "### Q5 : B\n",
    "L'**√©quation de Bellman** pour $Q$ est : $Q(s, a) = R(s, a) + \\gamma \\max_{a'} Q(s', a')$. Le $\\max$ vient du fait qu'on suppose une policy optimale.\n",
    "\n",
    "### Q6 : B\n",
    "Dans $\\epsilon$-greedy, avec probabilit√© $\\epsilon$, l'agent **explore** (choisit une action al√©atoire). Avec probabilit√© $1-\\epsilon$, il **exploite** (choisit $\\argmax_a Q(s, a)$).\n",
    "\n",
    "### Q7 : B\n",
    "La **mise √† jour Q-Learning** est : $Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$, o√π $\\alpha$ est le learning rate.\n",
    "\n",
    "### Q8 : B\n",
    "Pour des √©tats continus (images, positions continues), il est **impossible de stocker** $Q(s, a)$ pour tous les √©tats possibles dans une table. Solution : approximation avec r√©seau de neurones (DQN).\n",
    "\n",
    "### Q9 : A\n",
    "Le **DQN** utilise un **r√©seau de neurones** pour approximer la Q-function : $Q(s, a; \\theta)$, permettant de g√©rer des espaces d'√©tats larges/continus.\n",
    "\n",
    "### Q10 : B\n",
    "L'**Experience Replay** stocke les transitions $(s, a, r, s')$ dans un buffer et √©chantillonne des mini-batches al√©atoires, **brisant la corr√©lation temporelle** et stabilisant l'entra√Ænement.\n",
    "\n",
    "### Q11 : B\n",
    "Le **target network** $Q(s, a; \\theta^-)$ est une copie gel√©e du policy network, mise √† jour p√©riodiquement. Cela **stabilise les targets** et √©vite les oscillations.\n",
    "\n",
    "### Q12 : A\n",
    "**Q-Learning** apprend une **value function** ($Q(s, a)$) puis d√©rive la policy. **Policy Gradient** apprend **directement** la policy $\\pi_\\theta(a|s)$.\n",
    "\n",
    "### Q13 : B\n",
    "Dans REINFORCE, $G_t = \\sum_{k=t}^T \\gamma^{k-t} r_k$ est la **r√©compense cumulative** (return) √† partir du temps $t$ jusqu'√† la fin de l'√©pisode.\n",
    "\n",
    "### Q14 : B\n",
    "Dans **Actor-Critic**, l'**Actor** choisit les actions (policy $\\pi_\\theta$), et le **Critic** √©value leur qualit√© via une **value function** $V_\\phi(s)$ ou $Q_\\phi(s, a)$.\n",
    "\n",
    "### Q15 : C\n",
    "La **classification d'images statiques** est typiquement un probl√®me de **Supervised Learning** (pas de d√©cisions s√©quentielles, pas de r√©compenses diff√©r√©es). Les autres (AlphaGo, robotique, trading) utilisent bien le RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaines √âtapes\n",
    "\n",
    "- **Score < 10** : Relisez le chapitre 09 attentivement, en particulier :\n",
    "  - Section 1 : Paradigme RL, MDP, r√©compenses diff√©r√©es\n",
    "  - Section 2 : Q-Learning (Q-function, √©quation de Bellman, epsilon-greedy)\n",
    "  - Section 3 : DQN (experience replay, target network)\n",
    "  - Section 4-5 : Policy Gradient, Actor-Critic\n",
    "\n",
    "- **Score >= 10** : Passez au Chapitre 10 (Algorithmes G√©n√©tiques)\n",
    "\n",
    "- **Notebooks recommand√©s** :\n",
    "  - `09_demo_qlearning.ipynb` : Q-Learning sur FrozenLake (OpenAI Gym)\n",
    "  - `09_demo_dqn.ipynb` : Deep Q-Network sur CartPole\n",
    "\n",
    "- **Pour aller plus loin** :\n",
    "  - Sutton & Barto - *Reinforcement Learning: An Introduction* (bible du RL)\n",
    "  - OpenAI Gym : environnements pour tester vos agents\n",
    "  - Stable Baselines3 : impl√©mentations RL pr√™tes √† l'emploi\n",
    "\n",
    "- **R√©vision** : Refaites le quiz dans 2-3 jours pour ancrer les connaissances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
