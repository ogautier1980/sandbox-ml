{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/09_reinforcement_learning/09_demo_qlearning.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '09_demo_qlearning.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 09 - Q-Learning : Introduction au Reinforcement Learning\n",
    "\n",
    "Ce notebook d√©montre l'algorithme Q-Learning sur deux environnements Gym.\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre les concepts de RL: √©tat, action, r√©compense, politique\n",
    "- Impl√©menter Q-Learning avec Q-table\n",
    "- Entra√Æner un agent sur FrozenLake et CartPole\n",
    "- Visualiser l'apprentissage et les performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction √† Gym\n",
    "\n",
    "OpenAI Gym fournit des environnements standardis√©s pour tester les algorithmes de RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un environnement FrozenLake\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)  # D√©terministe pour commencer\n",
    "\n",
    "print(\"FrozenLake Environment:\")\n",
    "print(f\"  State space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  Number of states: {env.observation_space.n}\")\n",
    "print(f\"  Number of actions: {env.action_space.n}\")\n",
    "print(\"\\nActions: 0=Left, 1=Down, 2=Right, 3=Up\")\n",
    "print(\"\\nGrid: S=Start, F=Frozen, H=Hole, G=Goal\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agent Al√©atoire (Baseline)\n",
    "\n",
    "Testons d'abord un agent qui choisit des actions au hasard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_agent(env, num_episodes=100):\n",
    "    \"\"\"Teste un agent al√©atoire.\"\"\"\n",
    "    wins = 0\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Action al√©atoire\n",
    "            state, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        if reward > 0:  # Win\n",
    "            wins += 1\n",
    "    \n",
    "    return wins / num_episodes, total_rewards\n",
    "\n",
    "random_win_rate, random_rewards = test_random_agent(env, 1000)\n",
    "print(f\"Random Agent Win Rate: {random_win_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-Learning Algorithm\n",
    "\n",
    "### √âquation de Bellman:\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "O√π:\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\gamma$ = discount factor\n",
    "- $r$ = reward\n",
    "- $s, a$ = current state, action\n",
    "- $s', a'$ = next state, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_factor=0.99, \n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Q-table: [states x actions]\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    def choose_action(self, state, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Exploit\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Met √† jour la Q-table avec l'√©quation de Bellman.\"\"\"\n",
    "        current_q = self.q_table[state, action]\n",
    "        \n",
    "        if done:\n",
    "            # Si terminal, pas de futur reward\n",
    "            target_q = reward\n",
    "        else:\n",
    "            # Meilleure action future\n",
    "            max_future_q = np.max(self.q_table[next_state])\n",
    "            target_q = reward + self.gamma * max_future_q\n",
    "        \n",
    "        # Mise √† jour Q-learning\n",
    "        self.q_table[state, action] += self.lr * (target_q - current_q)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Diminue epsilon progressivement.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"Q-Learning Agent defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entra√Ænement sur FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, agent, num_episodes=5000, eval_interval=100):\n",
    "    \"\"\"Entra√Æne l'agent Q-Learning.\"\"\"\n",
    "    rewards_history = []\n",
    "    win_rates = []\n",
    "    epsilons = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Choisir action\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Ex√©cuter action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Mettre √† jour Q-table\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        rewards_history.append(episode_reward)\n",
    "        \n",
    "        # √âvaluation p√©riodique\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            win_rate = evaluate_agent(env, agent, num_eval=100)\n",
    "            win_rates.append(win_rate)\n",
    "            epsilons.append(agent.epsilon)\n",
    "            \n",
    "            if (episode + 1) % 1000 == 0:\n",
    "                print(f\"Episode {episode+1}/{num_episodes} - Win Rate: {win_rate:.2%} - Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return rewards_history, win_rates, epsilons\n",
    "\n",
    "def evaluate_agent(env, agent, num_eval=100):\n",
    "    \"\"\"√âvalue l'agent sans exploration.\"\"\"\n",
    "    wins = 0\n",
    "    for _ in range(num_eval):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state, training=False)\n",
    "            state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "    \n",
    "    return wins / num_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er et entra√Æner l'agent\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "agent = QLearningAgent(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995\n",
    ")\n",
    "\n",
    "print(\"Training Q-Learning Agent on FrozenLake...\\n\")\n",
    "rewards, win_rates, epsilons = train_q_learning(env, agent, num_episodes=5000)\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Win rate progression\n",
    "eval_episodes = np.arange(100, 5001, 100)\n",
    "axes[0].plot(eval_episodes, win_rates, linewidth=2)\n",
    "axes[0].axhline(y=random_win_rate, color='red', linestyle='--', label='Random Agent', linewidth=2)\n",
    "axes[0].set_xlabel('Episode', fontsize=12)\n",
    "axes[0].set_ylabel('Win Rate', fontsize=12)\n",
    "axes[0].set_title('Win Rate Evolution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon decay\n",
    "axes[1].plot(eval_episodes, epsilons, linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Episode', fontsize=12)\n",
    "axes[1].set_ylabel('Epsilon', fontsize=12)\n",
    "axes[1].set_title('Exploration Rate (Epsilon)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Rewards distribution (last 1000 episodes)\n",
    "recent_rewards = rewards[-1000:]\n",
    "axes[2].hist(recent_rewards, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[2].set_xlabel('Reward', fontsize=12)\n",
    "axes[2].set_ylabel('Frequency', fontsize=12)\n",
    "axes[2].set_title('Rewards Distribution (Last 1000 Episodes)', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation de la Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape Q-table pour visualisation 4x4\n",
    "q_table_grid = agent.q_table.max(axis=1).reshape(4, 4)\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(q_table_grid, annot=True, fmt='.2f', cmap='YlGnBu', \n",
    "            cbar_kws={'label': 'Max Q-Value'})\n",
    "plt.title('Q-Table Visualization (Max Q-Value per State)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Column', fontsize=12)\n",
    "plt.ylabel('Row', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Politique apprise (meilleure action par √©tat)\n",
    "action_names = ['‚Üê', '‚Üì', '‚Üí', '‚Üë']\n",
    "policy = np.array([action_names[a] for a in agent.q_table.argmax(axis=1)]).reshape(4, 4)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=policy, cellLoc='center', loc='center',\n",
    "                colWidths=[0.2]*4, cellColours=[['lightblue']*4]*4)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(20)\n",
    "table.scale(1, 3)\n",
    "plt.title('Learned Policy (Best Action per State)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test sur FrozenLake Slippery\n",
    "\n",
    "Environnement stochastique o√π l'agent peut glisser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environnement avec glissement\n",
    "env_slippery = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "agent_slippery = QLearningAgent(\n",
    "    n_states=env_slippery.observation_space.n,\n",
    "    n_actions=env_slippery.action_space.n,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.999,  # D√©croissance plus lente pour stochastique\n",
    "    epsilon_min=0.05\n",
    ")\n",
    "\n",
    "print(\"Training on Slippery FrozenLake...\\n\")\n",
    "rewards_slip, win_rates_slip, epsilons_slip = train_q_learning(\n",
    "    env_slippery, agent_slippery, num_episodes=10000, eval_interval=200\n",
    ")\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison Deterministic vs Slippery\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Win rates\n",
    "eval_episodes_det = np.arange(100, 5001, 100)\n",
    "eval_episodes_slip = np.arange(200, 10001, 200)\n",
    "\n",
    "axes[0].plot(eval_episodes_det, win_rates, label='Deterministic', linewidth=2)\n",
    "axes[0].plot(eval_episodes_slip, win_rates_slip, label='Slippery', linewidth=2)\n",
    "axes[0].set_xlabel('Episode', fontsize=12)\n",
    "axes[0].set_ylabel('Win Rate', fontsize=12)\n",
    "axes[0].set_title('Win Rate Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-tables comparison\n",
    "q_diff = np.abs(agent.q_table - agent_slippery.q_table).max(axis=1).reshape(4, 4)\n",
    "sns.heatmap(q_diff, annot=True, fmt='.2f', cmap='Reds', ax=axes[1],\n",
    "            cbar_kws={'label': 'Q-Value Difference'})\n",
    "axes[1].set_title('Q-Table Difference (Abs Max)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. D√©monstration de l'Agent Entra√Æn√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_agent(env, agent, num_demos=5):\n",
    "    \"\"\"Montre quelques √©pisodes de l'agent entra√Æn√©.\"\"\"\n",
    "    action_names = ['Left', 'Down', 'Right', 'Up']\n",
    "    \n",
    "    for demo in range(num_demos):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Demo {demo + 1}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        print(f\"Initial state: {state}\")\n",
    "        \n",
    "        while not done and steps < 100:\n",
    "            action = agent.choose_action(state, training=False)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            print(f\"Step {steps+1}: Action={action_names[action]}, Next State={next_state}, Reward={reward}\")\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        result = \"WIN!\" if total_reward > 0 else \"LOSS\"\n",
    "        print(f\"\\nResult: {result} - Total Reward: {total_reward} - Steps: {steps}\")\n",
    "\n",
    "demonstrate_agent(env, agent, num_demos=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyse de la Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyenne mobile du win rate\n",
    "def moving_average(data, window=10):\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Convertir rewards en win rate par 100 √©pisodes\n",
    "win_rate_per_100 = []\n",
    "for i in range(0, len(rewards), 100):\n",
    "    batch = rewards[i:i+100]\n",
    "    win_rate_per_100.append(sum(batch) / len(batch))\n",
    "\n",
    "ma_win_rate = moving_average(win_rate_per_100, window=5)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(win_rate_per_100, alpha=0.3, label='Win Rate (per 100 episodes)')\n",
    "plt.plot(np.arange(4, len(ma_win_rate)+4), ma_win_rate, linewidth=2, label='Moving Average (5)')\n",
    "plt.axhline(y=1.0, color='green', linestyle='--', label='Optimal', linewidth=2)\n",
    "plt.xlabel('Batch (100 episodes)', fontsize=12)\n",
    "plt.ylabel('Win Rate', fontsize=12)\n",
    "plt.title('Learning Convergence Analysis', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Ce que nous avons appris:\n",
    "1. Concepts fondamentaux du RL: √©tat, action, r√©compense, politique\n",
    "2. Algorithme Q-Learning et √©quation de Bellman\n",
    "3. Exploration vs Exploitation (epsilon-greedy)\n",
    "4. Diff√©rence entre environnements d√©terministes et stochastiques\n",
    "5. Visualisation de l'apprentissage et de la politique\n",
    "\n",
    "### Points cl√©s:\n",
    "- Q-Learning converge vers la politique optimale\n",
    "- Epsilon doit d√©cro√Ætre pour passer d'exploration √† exploitation\n",
    "- Les environnements stochastiques n√©cessitent plus d'exploration\n",
    "- La Q-table stocke les valeurs pour chaque paire (√©tat, action)\n",
    "\n",
    "### Limitations de Q-Learning:\n",
    "- Limit√© aux espaces d'√©tats discrets et petits\n",
    "- Ne scale pas pour des environnements complexes\n",
    "- Solution: Deep Q-Learning (DQN) pour espaces continus\n",
    "\n",
    "### Pour aller plus loin:\n",
    "- Deep Q-Network (DQN) pour CartPole, Atari\n",
    "- Policy Gradient methods (REINFORCE, A3C)\n",
    "- Actor-Critic algorithms\n",
    "- Multi-agent RL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}