{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/09_reinforcement_learning/09_demo_dqn.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '09_demo_dqn.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 09 - Deep Q-Network (DQN)\n",
    "\n",
    "Ce notebook d√©montre l'algorithme DQN pour r√©soudre des environnements avec espaces d'√©tats continus.\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre les limitations de Q-Learning tabulaire\n",
    "- Impl√©menter DQN avec PyTorch\n",
    "- Utiliser Experience Replay et Target Network\n",
    "- Entra√Æner un agent sur CartPole-v1\n",
    "- Visualiser l'apprentissage et les performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Pour reproductibilit√©\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environnement CartPole\n",
    "\n",
    "CartPole: √©quilibrer un poteau sur un chariot mobile.\n",
    "\n",
    "- √âtat: 4 valeurs continues (position, vitesse, angle, vitesse angulaire)\n",
    "- Actions: 2 discr√®tes (gauche, droite)\n",
    "- R√©compense: +1 √† chaque timestep o√π le poteau reste debout\n",
    "- Terminal: poteau tombe (angle > 12¬∞) ou chariot sort de la zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(\"CartPole Environment:\")\n",
    "print(f\"  State space: {env.observation_space}\")\n",
    "print(f\"  State shape: {env.observation_space.shape}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  Number of actions: {env.action_space.n}\")\n",
    "print(\"\\nState variables:\")\n",
    "print(\"  0: Cart Position\")\n",
    "print(\"  1: Cart Velocity\")\n",
    "print(\"  2: Pole Angle\")\n",
    "print(\"  3: Pole Angular Velocity\")\n",
    "print(\"\\nActions: 0=Push Left, 1=Push Right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network pour Q-Function\n",
    "\n",
    "Au lieu d'une Q-table, nous utilisons un r√©seau de neurones pour approximer Q(s, a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            state: (batch_size, state_dim)\n",
    "        Returns:\n",
    "            Q-values: (batch_size, action_dim)\n",
    "        \"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "# Tester le mod√®le\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "test_network = DQN(state_dim, action_dim)\n",
    "print(f\"\\nDQN Architecture:\")\n",
    "print(test_network)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in test_network.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experience Replay Buffer\n",
    "\n",
    "Stocke les transitions pour casser la corr√©lation temporelle et am√©liorer la stabilit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition structure\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        \"\"\"Ajoute une transition.\"\"\"\n",
    "        self.buffer.append(Transition(state, action, next_state, reward, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"√âchantillonne un batch al√©atoire.\"\"\"\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Convertir en tenseurs\n",
    "        states = torch.FloatTensor(np.array(batch.state)).to(device)\n",
    "        actions = torch.LongTensor(batch.action).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(batch.next_state)).to(device)\n",
    "        rewards = torch.FloatTensor(batch.reward).to(device)\n",
    "        dones = torch.FloatTensor(batch.done).to(device)\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"Replay Buffer implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent avec Experience Replay et Target Network.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99,\n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "                 buffer_size=10000, batch_size=64, target_update_freq=10):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Networks: policy et target\n",
    "        self.policy_net = DQN(state_dim, action_dim).to(device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # Target network en mode eval\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Compteur pour update target\n",
    "        self.update_count = 0\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy network avec batch from replay buffer.\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, next_states, rewards, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Compute current Q-values\n",
    "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values avec target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Loss (MSE)\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping pour stabilit√©\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network p√©riodiquement\n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"DQN Agent implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, agent, num_episodes=500, eval_interval=10):\n",
    "    \"\"\"Entra√Æne l'agent DQN.\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    eval_rewards = []\n",
    "    epsilons = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        episode_losses = []\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.replay_buffer.push(state, action, next_state, reward, float(done))\n",
    "            \n",
    "            # Update\n",
    "            loss = agent.update()\n",
    "            if loss is not None:\n",
    "                episode_losses.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Logging\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "        if episode_losses:\n",
    "            losses.append(np.mean(episode_losses))\n",
    "        \n",
    "        # Evaluation\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            eval_reward = evaluate_agent(env, agent, num_eval=5)\n",
    "            eval_rewards.append(eval_reward)\n",
    "            epsilons.append(agent.epsilon)\n",
    "            \n",
    "            print(f\"Episode {episode+1}/{num_episodes} - \"\n",
    "                  f\"Reward: {episode_reward:.1f} - \"\n",
    "                  f\"Eval Reward: {eval_reward:.1f} - \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths, losses, eval_rewards, epsilons\n",
    "\n",
    "def evaluate_agent(env, agent, num_eval=10):\n",
    "    \"\"\"√âvalue l'agent sans exploration.\"\"\"\n",
    "    total_reward = 0\n",
    "    for _ in range(num_eval):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state, training=False)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "    \n",
    "    return total_reward / num_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er agent\n",
    "agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    learning_rate=0.001,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    buffer_size=10000,\n",
    "    batch_size=64,\n",
    "    target_update_freq=10\n",
    ")\n",
    "\n",
    "# Entra√Æner\n",
    "print(\"Training DQN Agent on CartPole-v1...\\n\")\n",
    "rewards, lengths, losses, eval_rewards, epsilons = train_dqn(\n",
    "    env, agent, num_episodes=500, eval_interval=10\n",
    ")\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Episode rewards\n",
    "axes[0, 0].plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "# Moving average\n",
    "window = 20\n",
    "if len(rewards) >= window:\n",
    "    ma_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    axes[0, 0].plot(np.arange(window-1, len(rewards)), ma_rewards, \n",
    "                    linewidth=2, label=f'MA({window})')\n",
    "axes[0, 0].axhline(y=195, color='green', linestyle='--', label='Solved (195)', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Episode', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Reward', fontsize=12)\n",
    "axes[0, 0].set_title('Training Rewards', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Evaluation rewards\n",
    "eval_episodes = np.arange(10, 501, 10)\n",
    "axes[0, 1].plot(eval_episodes, eval_rewards, linewidth=2, marker='o')\n",
    "axes[0, 1].axhline(y=195, color='green', linestyle='--', label='Solved', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Episode', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Average Reward', fontsize=12)\n",
    "axes[0, 1].set_title('Evaluation Performance', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "if losses:\n",
    "    axes[1, 0].plot(losses, linewidth=1, alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Episode', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1, 0].set_title('Training Loss (MSE)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon decay\n",
    "axes[1, 1].plot(eval_episodes, epsilons, linewidth=2, color='purple')\n",
    "axes[1, 1].set_xlabel('Episode', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Epsilon', fontsize=12)\n",
    "axes[1, 1].set_title('Exploration Rate', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test de l'Agent Entra√Æn√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final\n",
    "test_rewards = []\n",
    "num_tests = 100\n",
    "\n",
    "for _ in range(num_tests):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state, training=False)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "\n",
    "print(f\"\\nFinal Test Results ({num_tests} episodes):\")\n",
    "print(f\"  Mean Reward: {np.mean(test_rewards):.2f}\")\n",
    "print(f\"  Std Reward: {np.std(test_rewards):.2f}\")\n",
    "print(f\"  Min Reward: {np.min(test_rewards):.2f}\")\n",
    "print(f\"  Max Reward: {np.max(test_rewards):.2f}\")\n",
    "print(f\"  Success Rate (>195): {(np.array(test_rewards) > 195).mean():.2%}\")\n",
    "\n",
    "# Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(test_rewards, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=195, color='green', linestyle='--', label='Solved Threshold', linewidth=2)\n",
    "plt.axvline(x=np.mean(test_rewards), color='red', linestyle='--', label='Mean', linewidth=2)\n",
    "plt.xlabel('Reward', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Test Rewards Distribution', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse des Q-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser les Q-values pour diff√©rents √©tats\n",
    "def analyze_q_values(agent, num_samples=100):\n",
    "    \"\"\"Analyse la distribution des Q-values.\"\"\"\n",
    "    all_q_values = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        state = env.reset()\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = agent.policy_net(state_tensor).cpu().numpy()[0]\n",
    "            all_q_values.append(q_values)\n",
    "    \n",
    "    all_q_values = np.array(all_q_values)\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Distribution par action\n",
    "    for action in range(action_dim):\n",
    "        axes[0].hist(all_q_values[:, action], bins=20, alpha=0.6, \n",
    "                     label=f'Action {action}')\n",
    "    axes[0].set_xlabel('Q-Value', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('Q-Values Distribution by Action', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Boxplot\n",
    "    axes[1].boxplot([all_q_values[:, i] for i in range(action_dim)],\n",
    "                     labels=[f'Action {i}' for i in range(action_dim)])\n",
    "    axes[1].set_ylabel('Q-Value', fontsize=12)\n",
    "    axes[1].set_title('Q-Values Boxplot', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return all_q_values\n",
    "\n",
    "q_values = analyze_q_values(agent, num_samples=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparaison avec Agent Al√©atoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent al√©atoire\n",
    "random_rewards = []\n",
    "for _ in range(100):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    \n",
    "    random_rewards.append(episode_reward)\n",
    "\n",
    "# Comparaison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogrammes\n",
    "axes[0].hist(random_rewards, bins=20, alpha=0.6, label='Random', edgecolor='black')\n",
    "axes[0].hist(test_rewards, bins=20, alpha=0.6, label='DQN', edgecolor='black')\n",
    "axes[0].set_xlabel('Reward', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Rewards Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot([random_rewards, test_rewards], labels=['Random', 'DQN'])\n",
    "axes[1].axhline(y=195, color='green', linestyle='--', label='Solved', linewidth=2)\n",
    "axes[1].set_ylabel('Reward', fontsize=12)\n",
    "axes[1].set_title('Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRandom Agent: {np.mean(random_rewards):.2f} ¬± {np.std(random_rewards):.2f}\")\n",
    "print(f\"DQN Agent: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\")\n",
    "print(f\"Improvement: {((np.mean(test_rewards) - np.mean(random_rewards)) / np.mean(random_rewards) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Ce que nous avons appris:\n",
    "1. Deep Q-Network pour espaces d'√©tats continus\n",
    "2. Experience Replay pour casser la corr√©lation temporelle\n",
    "3. Target Network pour stabiliser l'apprentissage\n",
    "4. Epsilon-greedy pour balance exploration/exploitation\n",
    "5. Gradient clipping pour stabilit√©\n",
    "\n",
    "### Am√©liorations de DQN:\n",
    "- **Double DQN**: R√©duire la surestimation des Q-values\n",
    "- **Dueling DQN**: S√©parer value et advantage\n",
    "- **Prioritized Experience Replay**: √âchantillonner les transitions importantes\n",
    "- **Rainbow DQN**: Combiner toutes les am√©liorations\n",
    "\n",
    "### Limitations:\n",
    "- Actions discr√®tes uniquement\n",
    "- Peut √™tre instable sur certains environnements\n",
    "- N√©cessite beaucoup d'interactions\n",
    "\n",
    "### Pour aller plus loin:\n",
    "- Policy Gradient methods (REINFORCE, A2C, PPO)\n",
    "- Actor-Critic algorithms (DDPG, TD3, SAC)\n",
    "- Multi-agent RL\n",
    "- Model-based RL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}