{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup",
    "",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/06_reseaux_neurones_fondamentaux/06_exercices_solutions.ipynb)",
    "",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)import sysIN_COLAB = 'google.colab' in sys.modulesif IN_COLAB:    print('üì¶ Installation des packages...')        # Packages ML de base    !pip install -q numpy pandas matplotlib seaborn scikit-learn        # D√©tection du chapitre et installation des d√©pendances sp√©cifiques    notebook_name = '06_exercices.ipynb'  # Sera remplac√© automatiquement        # Ch 06-08 : Deep Learning    if any(x in notebook_name for x in ['06_', '07_', '08_']):        !pip install -q torch torchvision torchaudio        # Ch 08 : NLP    if '08_' in notebook_name:        !pip install -q transformers datasets tokenizers        if 'rag' in notebook_name:            !pip install -q sentence-transformers faiss-cpu rank-bm25        # Ch 09 : Reinforcement Learning    if '09_' in notebook_name:        !pip install -q gymnasium[classic-control]        # Ch 04 : Boosting    if '04_' in notebook_name and 'boosting' in notebook_name:        !pip install -q xgboost lightgbm catboost        # Ch 05 : Clustering avanc√©    if '05_' in notebook_name:        !pip install -q umap-learn        # Ch 11 : S√©ries temporelles    if '11_' in notebook_name:        !pip install -q statsmodels prophet        # Ch 12 : Vision avanc√©e    if '12_' in notebook_name:        !pip install -q ultralytics timm segmentation-models-pytorch        # Ch 13 : Recommandation    if '13_' in notebook_name:        !pip install -q scikit-surprise implicit        # Ch 14 : MLOps    if '14_' in notebook_name:        !pip install -q mlflow fastapi pydantic        print('‚úÖ Installation termin√©e !')else:    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 06 - Solutions : R√©seaux de Neurones (MLP)",
    "",
    "**Objectifs** :",
    "1. Impl√©menter des variantes de MLP",
    "2. Exp√©rimenter avec hyperparam√®tres",
    "3. R√©gularisation et optimisation",
    "4. Comparaison NumPy vs PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np",
    "import torch",
    "import torch.nn as nn",
    "import torch.optim as optim",
    "from torch.utils.data import DataLoader, TensorDataset",
    "import matplotlib.pyplot as plt",
    "from sklearn.datasets import make_classification, make_circles, fetch_openml",
    "from sklearn.model_selection import train_test_split",
    "from sklearn.preprocessing import StandardScaler",
    "from sklearn.metrics import accuracy_score, classification_report",
    "import seaborn as sns",
    "",
    "np.random.seed(42)",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 : Impl√©mentation Perceptron simple (NumPy)",
    "",
    "**Objectif** : Impl√©menter un perceptron monocouche pour classification binaire.",
    "",
    "**TODO** :",
    "1. Impl√©menter la classe `Perceptron`",
    "2. M√©thodes : `fit()`, `predict()`",
    "3. Tester sur dataset lin√©airement s√©parable",
    "4. Visualiser la fronti√®re de d√©cision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:",
    "    \"\"\"Perceptron simple pour classification binaire.\"\"\"",
    "    ",
    "    def __init__(self, learning_rate=0.01, n_epochs=100):",
    "        self.lr = learning_rate",
    "        self.n_epochs = n_epochs",
    "        self.weights = None",
    "        self.bias = None",
    "    ",
    "    def fit(self, X, y):",
    "        \"\"\"Entra√Æne le perceptron.\"\"\"",
    "        # SOLUTION: Impl√©menter l'algorithme du perceptron",
    "        # - Initialiser weights et bias",
    "        # - Pour chaque epoch:",
    "        #     - Pour chaque √©chantillon:",
    "        #         - Calculer pr√©diction (z = X @ w + b, pred = 1 if z >= 0 else 0)",
    "        #         - Mettre √† jour poids si erreur: w += lr * (y - pred) * X",
    "        pass",
    "    ",
    "    def predict(self, X):",
    "        \"\"\"Pr√©diction (0 ou 1).\"\"\"",
    "        # SOLUTION: Impl√©menter la pr√©diction",
    "        pass",
    "",
    "# Test sur dataset lin√©airement s√©parable",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, ",
    "                           n_informative=2, n_clusters_per_class=1, random_state=42)",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
    "",
    "# SOLUTION: Entra√Æner le perceptron et √©valuer",
    "# perceptron = Perceptron(learning_rate=0.01, n_epochs=100)",
    "# perceptron.fit(X_train, y_train)",
    "# y_pred = perceptron.predict(X_test)",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")",
    "",
    "# SOLUTION: Visualiser la fronti√®re de d√©cision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : XOR Problem avec MLP",
    "",
    "**Contexte** : Le perceptron simple ne peut pas r√©soudre XOR (non lin√©airement s√©parable).",
    "",
    "**TODO** :",
    "1. Cr√©er dataset XOR",
    "2. Entra√Æner MLP PyTorch avec une couche cach√©e",
    "3. Comparer avec perceptron simple",
    "4. Visualiser la fronti√®re de d√©cision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset XOR",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)",
    "y_xor = np.array([0, 1, 1, 0], dtype=np.int64)",
    "",
    "print(\"Dataset XOR:\")",
    "for x, y in zip(X_xor, y_xor):",
    "    print(f\"Input: {x} -> Output: {y}\")",
    "",
    "# SOLUTION: Cr√©er MLP PyTorch pour r√©soudre XOR",
    "# Architecture sugg√©r√©e: 2 -> 4 -> 1 (ou 2 -> 4 -> 2 avec Softmax)",
    "# Indices:",
    "# - Utiliser nn.Sequential",
    "# - Activation: ReLU ou Sigmoid",
    "# - Optimizer: Adam ou SGD",
    "# - Loss: BCEWithLogitsLoss ou CrossEntropyLoss",
    "# - Entra√Æner sur ~1000 epochs",
    "",
    "# SOLUTION: Tester si le MLP pr√©dit correctement les 4 cas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 : Impact du Dropout",
    "",
    "**Objectif** : Comparer MLP avec et sans Dropout pour comprendre l'effet de r√©gularisation.",
    "",
    "**TODO** :",
    "1. Entra√Æner 2 mod√®les sur MNIST (avec/sans Dropout)",
    "2. Comparer overfitting (train vs val accuracy)",
    "3. Visualiser les courbes d'apprentissage",
    "4. Conclure sur l'efficacit√© du Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement MNIST (petit subset)",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')",
    "X = mnist.data.astype('float32').values[:5000] / 255.0  # type: ignore",
    "y = mnist.target.astype('int').values[:5000]  # type: ignore",
    "",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
    "",
    "# SOLUTION: Cr√©er 2 mod√®les identiques sauf Dropout",
    "# Mod√®le 1: 784 -> 256 -> 128 -> 10 (sans Dropout)",
    "# Mod√®le 2: 784 -> 256 -> Dropout(0.3) -> 128 -> Dropout(0.3) -> 10",
    "",
    "# SOLUTION: Entra√Æner les 2 mod√®les sur 50 epochs",
    "",
    "# SOLUTION: Comparer:",
    "# - Train accuracy finale",
    "# - Val accuracy finale",
    "# - Gap train-val (indicateur d'overfitting)",
    "",
    "# SOLUTION: Visualiser les courbes d'apprentissage c√¥te √† c√¥te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 : Comparaison d'Optimizers",
    "",
    "**Objectif** : Comparer SGD, SGD+Momentum, Adam sur m√™me architecture.",
    "",
    "**TODO** :",
    "1. Entra√Æner 3 mod√®les identiques avec optimizers diff√©rents",
    "2. Comparer vitesse de convergence",
    "3. Comparer accuracy finale",
    "4. Visualiser les courbes de loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Make Circles (non lin√©aire)",
    "X, y = make_circles(n_samples=1000, noise=0.1, factor=0.5, random_state=42)",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
    "",
    "# Conversion PyTorch",
    "X_train_t = torch.FloatTensor(X_train)",
    "y_train_t = torch.LongTensor(y_train)",
    "X_test_t = torch.FloatTensor(X_test)",
    "y_test_t = torch.LongTensor(y_test)",
    "",
    "train_dataset = TensorDataset(X_train_t, y_train_t)",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)",
    "",
    "# SOLUTION: Cr√©er 3 mod√®les identiques (2 -> 32 -> 16 -> 2)",
    "",
    "# SOLUTION: D√©finir 3 optimizers:",
    "# - SGD (lr=0.1)",
    "# - SGD + Momentum (lr=0.1, momentum=0.9)",
    "# - Adam (lr=0.01)",
    "",
    "# SOLUTION: Entra√Æner les 3 mod√®les sur 100 epochs",
    "# Stocker l'historique de loss pour chaque mod√®le",
    "",
    "# SOLUTION: Comparer les 3 optimizers:",
    "# - Courbes de loss (m√™me graphique)",
    "# - Accuracy finale sur test set",
    "# - Nombre d'epochs pour atteindre 95% accuracy (si atteint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5 : Batch Normalization vs Layer Normalization",
    "",
    "**Objectif** : Comprendre l'effet de BatchNorm sur la stabilit√© de l'entra√Ænement.",
    "",
    "**TODO** :",
    "1. Entra√Æner 3 mod√®les:",
    "   - Sans normalisation",
    "   - Avec Batch Normalization",
    "   - Avec Layer Normalization",
    "2. Comparer stabilit√© et vitesse de convergence",
    "3. Tester sur architecture profonde (5 couches cach√©es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Cr√©er 3 architectures profondes:",
    "# Architecture: 784 -> 512 -> 256 -> 128 -> 64 -> 32 -> 10",
    "",
    "# Mod√®le 1: Sans normalisation",
    "# model1 = nn.Sequential(",
    "#     nn.Linear(784, 512), nn.ReLU(),",
    "#     nn.Linear(512, 256), nn.ReLU(),",
    "#     ...",
    "# )",
    "",
    "# Mod√®le 2: Avec BatchNorm apr√®s chaque Linear",
    "# model2 = nn.Sequential(",
    "#     nn.Linear(784, 512), nn.BatchNorm1d(512), nn.ReLU(),",
    "#     ...",
    "# )",
    "",
    "# Mod√®le 3: Avec LayerNorm",
    "# model3 = nn.Sequential(",
    "#     nn.Linear(784, 512), nn.LayerNorm(512), nn.ReLU(),",
    "#     ...",
    "# )",
    "",
    "# SOLUTION: Entra√Æner les 3 mod√®les sur MNIST (50 epochs)",
    "",
    "# SOLUTION: Comparer:",
    "# - Convergence (loss apr√®s 10 epochs)",
    "# - Stabilit√© (variance de la loss)",
    "# - Accuracy finale",
    "",
    "# SOLUTION: Visualiser l'√©volution de la loss pour les 3 mod√®les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 6 : Learning Rate Scheduling",
    "",
    "**Objectif** : Exp√©rimenter avec diff√©rents schedulers pour optimiser la convergence.",
    "",
    "**TODO** :",
    "1. Tester 4 strategies:",
    "   - Learning rate constant",
    "   - StepLR (r√©duction par paliers)",
    "   - ExponentialLR (r√©duction exponentielle)",
    "   - ReduceLROnPlateau (r√©duction adaptative)",
    "2. Comparer vitesse de convergence",
    "3. Visualiser l'√©volution du learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Cr√©er 4 mod√®les identiques",
    "",
    "# SOLUTION: D√©finir 4 schedulers:",
    "# 1. Pas de scheduler (lr constant = 0.1)",
    "# 2. StepLR: lr *= 0.5 tous les 10 epochs",
    "# 3. ExponentialLR: lr *= 0.95 chaque epoch",
    "# 4. ReduceLROnPlateau: lr *= 0.5 si val_loss stagne 5 epochs",
    "",
    "# SOLUTION: Entra√Æner les 4 mod√®les sur 50 epochs",
    "# Stocker: loss, accuracy, learning_rate",
    "",
    "# SOLUTION: Visualiser:",
    "# - Graphique 1: Loss vs Epoch (4 courbes)",
    "# - Graphique 2: Learning Rate vs Epoch (4 courbes)",
    "# - Graphique 3: Test Accuracy finale (bar plot)",
    "",
    "# SOLUTION: Conclure sur la meilleure strat√©gie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 7 : Weight Initialization",
    "",
    "**Objectif** : Comprendre l'impact de l'initialisation des poids.",
    "",
    "**TODO** :",
    "1. Tester 4 m√©thodes d'initialisation:",
    "   - Zeros (tous les poids √† 0)",
    "   - Random uniforme [-1, 1]",
    "   - Xavier/Glorot",
    "   - He/Kaiming",
    "2. Comparer vitesse de convergence",
    "3. Analyser la distribution des gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_zeros(m):",
    "    if isinstance(m, nn.Linear):",
    "        m.weight.data.fill_(0.0)",
    "        m.bias.data.fill_(0.0)",
    "",
    "def init_weights_uniform(m):",
    "    if isinstance(m, nn.Linear):",
    "        nn.init.uniform_(m.weight, -1, 1)",
    "        nn.init.uniform_(m.bias, -1, 1)",
    "",
    "def init_weights_xavier(m):",
    "    if isinstance(m, nn.Linear):",
    "        nn.init.xavier_uniform_(m.weight)",
    "        nn.init.zeros_(m.bias)",
    "",
    "def init_weights_he(m):",
    "    if isinstance(m, nn.Linear):",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')",
    "        nn.init.zeros_(m.bias)",
    "",
    "# SOLUTION: Cr√©er 4 mod√®les identiques et appliquer les initialisations",
    "# model1.apply(init_weights_zeros)",
    "# model2.apply(init_weights_uniform)",
    "# model3.apply(init_weights_xavier)",
    "# model4.apply(init_weights_he)",
    "",
    "# SOLUTION: Entra√Æner les 4 mod√®les et comparer:",
    "# - Loss apr√®s 5 epochs (indicateur de vitesse initiale)",
    "# - Accuracy finale apr√®s 50 epochs",
    "# - Distribution des poids apr√®s initialisation (histogramme)",
    "",
    "# SOLUTION: Visualiser l'√©volution de la loss pour chaque initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 8 : Mini-Projet - Fashion MNIST",
    "",
    "**Objectif** : Appliquer toutes les techniques apprises sur Fashion MNIST.",
    "",
    "**TODO** :",
    "1. Charger Fashion MNIST (10 classes de v√™tements)",
    "2. Construire architecture optimale (exp√©rimenter)",
    "3. Appliquer les meilleures pratiques:",
    "   - BatchNorm",
    "   - Dropout",
    "   - Adam optimizer",
    "   - Learning rate scheduler",
    "   - Early stopping",
    "4. Atteindre >88% accuracy sur test set",
    "5. Analyser les erreurs de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement Fashion MNIST",
    "fashion_mnist = fetch_openml('Fashion-MNIST', version=1, parser='auto')",
    "X_fashion = fashion_mnist.data.astype('float32').values / 255.0  # type: ignore",
    "y_fashion = fashion_mnist.target.astype('int').values  # type: ignore",
    "",
    "# Labels Fashion MNIST",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']",
    "",
    "# SOLUTION: Split train/val/test",
    "",
    "# SOLUTION: Construire votre meilleure architecture MLP",
    "# Suggestions:",
    "# - 3-4 couches cach√©es",
    "# - BatchNorm apr√®s chaque couche",
    "# - Dropout 0.3-0.5",
    "# - ReLU activation",
    "# - Adam optimizer (lr=0.001)",
    "# - ReduceLROnPlateau scheduler",
    "",
    "# SOLUTION: Entra√Æner avec early stopping",
    "",
    "# SOLUTION: √âvaluation compl√®te:",
    "# - Accuracy par classe",
    "# - Matrice de confusion",
    "# - Classification report",
    "# - Visualisation des erreurs",
    "",
    "# SOLUTION: Analyser:",
    "# - Quelles classes sont les plus difficiles?",
    "# - Quelles confusions sont fr√©quentes? (ex: Shirt vs T-shirt)",
    "# - Visualiser 10 pr√©dictions incorrectes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions et Indices",
    "",
    "### Exercice 1 - Perceptron",
    "```python",
    "# Initialisation",
    "self.weights = np.random.randn(X.shape[1]) * 0.01",
    "self.bias = 0.0",
    "",
    "# Update rule",
    "for epoch in range(self.n_epochs):",
    "    for xi, yi in zip(X, y):",
    "        z = np.dot(xi, self.weights) + self.bias",
    "        pred = 1 if z >= 0 else 0",
    "        error = yi - pred",
    "        self.weights += self.lr * error * xi",
    "        self.bias += self.lr * error",
    "```",
    "",
    "### Exercice 2 - XOR avec MLP",
    "```python",
    "model = nn.Sequential(",
    "    nn.Linear(2, 4),",
    "    nn.ReLU(),",
    "    nn.Linear(4, 2)",
    ")",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)",
    "criterion = nn.CrossEntropyLoss()",
    "```",
    "",
    "### Exercice 3 - Dropout",
    "- Avec Dropout: moins d'overfitting, meilleure g√©n√©ralisation",
    "- Sans Dropout: train accuracy > val accuracy (signe d'overfitting)",
    "",
    "### Exercice 4 - Optimizers",
    "- **SGD** : Convergence lente, peut osciller",
    "- **SGD + Momentum** : Convergence plus rapide, moins d'oscillations",
    "- **Adam** : Convergence la plus rapide, adapte le learning rate",
    "",
    "### Exercice 8 - Fashion MNIST",
    "Architecture sugg√©r√©e:",
    "```python",
    "model = nn.Sequential(",
    "    nn.Linear(784, 512),",
    "    nn.BatchNorm1d(512),",
    "    nn.ReLU(),",
    "    nn.Dropout(0.4),",
    "    ",
    "    nn.Linear(512, 256),",
    "    nn.BatchNorm1d(256),",
    "    nn.ReLU(),",
    "    nn.Dropout(0.3),",
    "    ",
    "    nn.Linear(256, 128),",
    "    nn.BatchNorm1d(128),",
    "    nn.ReLU(),",
    "    nn.Dropout(0.2),",
    "    ",
    "    nn.Linear(128, 10)",
    ")",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}