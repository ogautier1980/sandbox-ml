\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}

% Math√©matiques
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

% Mise en page
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}

% Graphiques et couleurs
\usepackage{graphicx}
\usepackage{xcolor}

% ===== UNICODE CHARACTERS SUPPORT =====
\usepackage{newunicodechar}

% Emojis et symboles
\newunicodechar{‚úÖ}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚ùå}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚úì}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚úó}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚ö†}{\textcolor{orange!80!black}{\textbf{/!\textbackslash}}}
\newunicodechar{üí°}{\textcolor{blue!70!black}{\textbf{(i)}}}
\newunicodechar{üéØ}{\textcolor{purple!70!black}{\textbf{$\star$}}}
\newunicodechar{üìä}{\textcolor{blue!70!black}{\textbf{[=]}}}

% √âtoiles (pour tableaux)
\newunicodechar{‚òÖ}{\textcolor{orange!80!black}{$\star$}}
\newunicodechar{‚òÜ}{\textcolor{gray!50}{$\star$}}

% Fl√®ches
\newunicodechar{‚Üí}{$\rightarrow$}
\newunicodechar{‚Üê}{$\leftarrow$}
\newunicodechar{‚Üë}{$\uparrow$}
\newunicodechar{‚Üì}{$\downarrow$}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, calc}

% Tableaux
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}

% Code et algorithmes
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Hyperliens
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 06 - R√©seaux de Neurones Fondamentaux},
    pdfauthor={Cours ML},
}

% Boxes color√©es
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}


% ===== TCOLORBOX AVEC EMOJIS =====
\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö† Attention,
    breakable
}

\newtcolorbox{definition}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition,
    breakable
}

\newtcolorbox{astuce}{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

\newtcolorbox{remarque}{
    colback=yellow!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=üí° Remarque,
    breakable
}

\newtcolorbox{important}{
    colback=purple!5!white,
    colframe=purple!75!black,
    fonttitle=\bfseries,
    title=‚ö† Important,
    breakable
}

\newtcolorbox{exemple}{
    colback=gray!5!white,
    colframe=gray!75!black,
    fonttitle=\bfseries,
    title=Exemple,
    breakable
}

% En-t√™tes et pieds de page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 06 - R√©seaux de Neurones Fondamentaux}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIGURATION LISTINGS (code Python) =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}
\lstset{style=pythonstyle}

% ===== CONFIGURATION TCOLORBOX =====


\newtcolorbox{theoreme}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Th√©or√®me: #1,
    breakable
}







% ===== COMMANDES PERSONNALIS√âES =====
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\sigmoid}{\operatorname{sigmoid}}
\newcommand{\relu}{\operatorname{ReLU}}
\newcommand{\softmax}{\operatorname{softmax}}

\begin{document}

% ===== PAGE DE TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]

    \vspace{1cm}

    {\LARGE Chapitre 06}\\[0.3cm]
    {\LARGE\bfseries R√©seaux de Neurones Fondamentaux}\\[2cm]

    \vfill

    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Comprendre le fonctionnement du perceptron et du perceptron multi-couches (MLP)
        \item Ma√Ætriser le forward pass et le backpropagation
        \item D√©couvrir les fonctions d'activation et leur r√¥le
        \item Impl√©menter un r√©seau de neurones from scratch
        \item Appliquer les techniques de r√©gularisation (dropout, batch norm)
    \end{itemize}
    }

    \vfill

    {\large
    \textbf{Pr√©requis :} Chapitres 01 (Math), 02 (M√©triques), 03 (R√©gression)\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 6-8 heures\\[0.3cm]
    \textbf{Notebooks :} \texttt{06\_demo\_*.ipynb}
    }

    \vfill

    {\large Cours ML - Sandbox-ML\\
    Version 1.0 - 2026}
\end{titlepage}

\tableofcontents
\newpage

% ===== SECTION 1: INTRODUCTION =====
\section{Introduction aux R√©seaux de Neurones}

\subsection{Motivation}

Les r√©seaux de neurones artificiels sont inspir√©s du fonctionnement du cerveau humain. Ils permettent de mod√©liser des relations non-lin√©aires complexes entre les donn√©es d'entr√©e et les sorties.

\begin{exemple}{Classification d'images manuscrites}
Reconna√Ætre des chiffres manuscrits (MNIST) : chaque pixel est une entr√©e, et le r√©seau doit pr√©dire le chiffre (0-9). Une r√©gression logistique simple obtient ~92\% de pr√©cision, tandis qu'un r√©seau de neurones atteint ~98\%.
\end{exemple}

\subsection{Historique}

\begin{itemize}
    \item \textbf{1943} : McCulloch-Pitts - premier mod√®le de neurone formel
    \item \textbf{1958} : Rosenblatt - Perceptron (classification binaire lin√©aire)
    \item \textbf{1969} : Minsky \& Papert - limites du perceptron (XOR)
    \item \textbf{1986} : Rumelhart, Hinton, Williams - Backpropagation
    \item \textbf{2012} : AlexNet - r√©volution deep learning (ImageNet)
\end{itemize}

\subsection{Analogie biologique}

Un neurone artificiel est une simplification extr√™me d'un neurone biologique :

\begin{itemize}
    \item \textbf{Dendrites} ‚Üí Entr√©es pond√©r√©es ($\vect{x} \odot \vect{w}$)
    \item \textbf{Corps cellulaire} ‚Üí Sommation ($\sum w_i x_i + b$)
    \item \textbf{Axone} ‚Üí Fonction d'activation ($\sigma(z)$)
    \item \textbf{Synapses} ‚Üí Poids ajustables ($\vect{w}$)
\end{itemize}

% ===== SECTION 2: LE PERCEPTRON =====
\section{Le Perceptron}

\subsection{D√©finition}

\begin{definition}{Perceptron}
Le perceptron est un mod√®le de classification binaire lin√©aire. Pour une entr√©e $\vect{x} \in \R^d$, il calcule :
\begin{equation}
    y = \begin{cases}
        1 & \text{si } \vect{w}^T \vect{x} + b > 0 \\
        0 & \text{sinon}
    \end{cases}
\end{equation}
o√π $\vect{w} \in \R^d$ sont les poids et $b \in \R$ est le biais.
\end{definition}

Le perceptron d√©finit un \textbf{hyperplan de s√©paration} dans l'espace des features :
\begin{equation}
    \vect{w}^T \vect{x} + b = 0
\end{equation}

\subsection{Algorithme d'apprentissage}

\begin{algorithm}[H]
\caption{Perceptron Learning Algorithm}
\label{alg:perceptron}
\begin{algorithmic}[1]
\REQUIRE Donn√©es $\{(\vect{x}_i, y_i)\}_{i=1}^n$, $y_i \in \{0, 1\}$
\REQUIRE Learning rate $\alpha$, nombre d'epochs $T$
\ENSURE Poids $\vect{w}$, biais $b$
\STATE Initialiser $\vect{w} = \vect{0}$, $b = 0$
\FOR{$epoch = 1$ \TO $T$}
    \FOR{$i = 1$ \TO $n$}
        \STATE Pr√©dire : $\hat{y}_i = \mathbb{1}[\vect{w}^T \vect{x}_i + b > 0]$
        \IF{$\hat{y}_i \neq y_i$}
            \STATE $\vect{w} \leftarrow \vect{w} + \alpha (y_i - \hat{y}_i) \vect{x}_i$
            \STATE $b \leftarrow b + \alpha (y_i - \hat{y}_i)$
        \ENDIF
    \ENDFOR
\ENDFOR
\RETURN $\vect{w}, b$
\end{algorithmic}
\end{algorithm}

\subsection{Th√©or√®me de convergence}

\begin{theoreme}{Convergence du Perceptron}
Si les donn√©es sont lin√©airement s√©parables, l'algorithme du perceptron converge en un nombre fini d'it√©rations.
\end{theoreme}

\begin{attention}
Le perceptron \textbf{ne peut pas} r√©soudre le probl√®me XOR (non lin√©airement s√©parable). C'est une limitation majeure qui a motiv√© le d√©veloppement des r√©seaux multi-couches.
\end{attention}

\subsection{Impl√©mentation}

\begin{lstlisting}[language=Python, caption=Perceptron from scratch]
import numpy as np

class Perceptron:
    def __init__(self, learning_rate=0.01, n_epochs=100):
        self.lr = learning_rate
        self.n_epochs = n_epochs
        self.w = None
        self.b = 0

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.w = np.zeros(n_features)

        for epoch in range(self.n_epochs):
            for i in range(n_samples):
                # Forward pass
                z = np.dot(X[i], self.w) + self.b
                y_pred = 1 if z > 0 else 0

                # Update weights si erreur
                if y_pred != y[i]:
                    update = self.lr * (y[i] - y_pred)
                    self.w += update * X[i]
                    self.b += update

        return self

    def predict(self, X):
        z = np.dot(X, self.w) + self.b
        return (z > 0).astype(int)
\end{lstlisting}

% ===== SECTION 3: FONCTIONS D'ACTIVATION =====
\section{Fonctions d'Activation}

Les fonctions d'activation introduisent la \textbf{non-lin√©arit√©} dans les r√©seaux de neurones. Sans elles, un r√©seau multi-couches serait √©quivalent √† une r√©gression lin√©aire.

\subsection{Sigmoid}

\begin{definition}{Fonction Sigmoid}
\begin{equation}
    \sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation}
Propri√©t√©s :
\begin{itemize}
    \item Sortie entre 0 et 1 (interpr√©table comme probabilit√©)
    \item D√©riv√©e : $\sigma'(z) = \sigma(z)(1 - \sigma(z))$
    \item Probl√®me : \textbf{vanishing gradient} pour $|z|$ grand
\end{itemize}
\end{definition}

\subsection{Tanh (Tangente Hyperbolique)}

\begin{equation}
    \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\sigma(2z) - 1
\end{equation}

Propri√©t√©s :
\begin{itemize}
    \item Sortie entre -1 et 1 (centr√© √† 0, meilleur que sigmoid)
    \item D√©riv√©e : $\tanh'(z) = 1 - \tanh^2(z)$
    \item Aussi sujet au vanishing gradient
\end{itemize}

\subsection{ReLU (Rectified Linear Unit)}

\begin{definition}{ReLU}
\begin{equation}
    \relu(z) = \max(0, z) = \begin{cases}
        z & \text{si } z > 0 \\
        0 & \text{sinon}
    \end{cases}
\end{equation}
Propri√©t√©s :
\begin{itemize}
    \item Calcul tr√®s rapide
    \item Pas de vanishing gradient pour $z > 0$
    \item D√©riv√©e : $\relu'(z) = \mathbb{1}[z > 0]$
    \item Probl√®me : \textbf{dying ReLU} (neurones inactifs si $z < 0$)
\end{itemize}
\end{definition}

\textbf{ReLU est la fonction d'activation par d√©faut} dans les r√©seaux modernes.

\subsection{Variantes de ReLU}

\textbf{Leaky ReLU :}
\begin{equation}
    \text{LeakyReLU}(z) = \begin{cases}
        z & \text{si } z > 0 \\
        \alpha z & \text{sinon}
    \end{cases} \quad (\alpha \approx 0.01)
\end{equation}

\textbf{ELU (Exponential Linear Unit) :}
\begin{equation}
    \text{ELU}(z) = \begin{cases}
        z & \text{si } z > 0 \\
        \alpha(e^z - 1) & \text{sinon}
    \end{cases}
\end{equation}

\textbf{Swish / SiLU :}
\begin{equation}
    \text{Swish}(z) = z \cdot \sigma(z)
\end{equation}

\subsection{Softmax (pour classification multi-classe)}

\begin{definition}{Softmax}
Pour un vecteur $\vect{z} \in \R^K$, la fonction softmax renvoie un vecteur de probabilit√©s :
\begin{equation}
    \softmax(\vect{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \quad \text{pour } j = 1, \ldots, K
\end{equation}
Propri√©t√© : $\sum_{j=1}^K \softmax(\vect{z})_j = 1$ (distribution de probabilit√©)
\end{definition}

\begin{table}[h]
\centering
\caption{Comparaison des fonctions d'activation}
\label{tab:activations}
\begin{tabular}{lccc}
\toprule
\textbf{Fonction} & \textbf{Plage} & \textbf{Vanishing Gradient} & \textbf{Usage} \\
\midrule
Sigmoid & $(0, 1)$ & ‚úó Oui & Output binaire \\
Tanh & $(-1, 1)$ & ‚úó Oui & RNN (historique) \\
ReLU & $[0, \infty)$ & ‚úì Non & Hidden layers (d√©faut) \\
Leaky ReLU & $(-\infty, \infty)$ & ‚úì Non & Alternative ReLU \\
Softmax & $[0, 1]^K$ & - & Output multi-classe \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.1]
    % Subplot 1: Sigmoid
    \begin{scope}[xshift=0cm]
        % Axes
        \draw[->] (-3,0) -- (3,0) node[right, font=\footnotesize] {$z$};
        \draw[->] (0,-0.3) -- (0,3.2) node[above, font=\footnotesize] {$\sigma(z)$};

        % Grid
        \draw[gray!30, very thin] (-3,0) grid[step=1] (3,3);

        % Sigmoid curve: œÉ(z) = 1/(1+e^(-z))
        \draw[blue, very thick, smooth, domain=-3:3, samples=100]
            plot (\x, {3/(1+exp(-2*\x))});

        % Asymptotes
        \draw[dashed, red!60, thin] (-3,0) -- (3,0);
        \draw[dashed, red!60, thin] (-3,3) -- (3,3);

        % Annotations
        \node[font=\footnotesize, left] at (0,3) {1};
        \node[font=\footnotesize, left] at (0,1.5) {0.5};
        \node[font=\footnotesize, below] at (0,0) {0};
        \node[font=\small, blue, above] at (0,3.5) {\textbf{Sigmoid}};
        \node[font=\tiny, align=center] at (0,-1) {$\sigma(z) = \frac{1}{1+e^{-z}}$};

        % Saturation zones
        \node[font=\tiny, red] at (-2.5, 0.3) {Saturation};
        \node[font=\tiny, red] at (2.5, 2.7) {Saturation};
    \end{scope}

    % Subplot 2: Tanh
    \begin{scope}[xshift=7cm]
        % Axes
        \draw[->] (-3,0) -- (3,0) node[right, font=\footnotesize] {$z$};
        \draw[->] (0,-0.3) -- (0,3.2);

        % Grid
        \draw[gray!30, very thin] (-3,0) grid[step=1] (3,3);

        % Tanh curve: tanh(z) = (e^z - e^(-z))/(e^z + e^(-z))
        % Shifted to [0,3] range: (tanh(z)+1)*1.5
        \draw[orange!80!black, very thick, smooth, domain=-3:3, samples=100]
            plot (\x, {1.5*(tanh(\x)+1)});

        % Asymptotes
        \draw[dashed, red!60, thin] (-3,0) -- (3,0);
        \draw[dashed, red!60, thin] (-3,3) -- (3,3);

        % Annotations
        \node[font=\footnotesize, left] at (0,3) {1};
        \node[font=\footnotesize, left] at (0,1.5) {0};
        \node[font=\footnotesize, left] at (0,0) {-1};
        \node[font=\small, orange!80!black, above] at (0,3.5) {\textbf{Tanh}};
        \node[font=\tiny, align=center] at (0,-1) {$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$};

        % Saturation zones
        \node[font=\tiny, red] at (-2.5, 0.3) {Saturation};
        \node[font=\tiny, red] at (2.5, 2.7) {Saturation};
    \end{scope}

    % Subplot 3: ReLU
    \begin{scope}[xshift=0cm, yshift=-5.5cm]
        % Axes
        \draw[->] (-3,0) -- (3,0) node[right, font=\footnotesize] {$z$};
        \draw[->] (0,-0.3) -- (0,3.2) node[above, font=\footnotesize] {$\text{ReLU}(z)$};

        % Grid
        \draw[gray!30, very thin] (-3,0) grid[step=1] (3,3);

        % ReLU: max(0, z)
        \draw[green!60!black, very thick] (-3,0) -- (0,0);
        \draw[green!60!black, very thick] (0,0) -- (3,3);

        % Dead zone
        \fill[red!20, opacity=0.3] (-3,0) rectangle (0,3);
        \node[font=\tiny, red!70!black] at (-1.5, 2.5) {Dead zone};
        \node[font=\tiny, red!70!black] at (-1.5, 2) {$z \leq 0$};

        % Linear zone
        \node[font=\tiny, green!40!black] at (1.5, 2.5) {Active zone};
        \node[font=\tiny, green!40!black] at (1.5, 2) {$z > 0$};

        % Annotations
        \node[font=\footnotesize, below] at (0,0) {0};
        \node[font=\small, green!60!black, above] at (0,3.5) {\textbf{ReLU}};
        \node[font=\tiny, align=center] at (0,-1) {$\text{ReLU}(z) = \max(0, z)$};
    \end{scope}

    % Subplot 4: Leaky ReLU
    \begin{scope}[xshift=7cm, yshift=-5.5cm]
        % Axes
        \draw[->] (-3,0) -- (3,0) node[right, font=\footnotesize] {$z$};
        \draw[->] (0,-0.3) -- (0,3.2);

        % Grid
        \draw[gray!30, very thin] (-3,0) grid[step=1] (3,3);

        % Leaky ReLU: max(0.1z, z)
        \draw[purple!70!black, very thick] (-3,-0.3) -- (0,0);
        \draw[purple!70!black, very thick] (0,0) -- (3,3);

        % Small gradient zone
        \fill[orange!20, opacity=0.3] (-3,0) rectangle (0,3);
        \node[font=\tiny, orange!70!black] at (-1.5, 2.5) {Small gradient};
        \node[font=\tiny, orange!70!black] at (-1.5, 2) {$\alpha z$ ($\alpha=0.01$)};

        % Full gradient zone
        \node[font=\tiny, purple!40!black] at (1.5, 2.5) {Full gradient};
        \node[font=\tiny, purple!40!black] at (1.5, 2) {$z$};

        % Annotations
        \node[font=\footnotesize, below] at (0,0) {0};
        \node[font=\small, purple!70!black, above] at (0,3.5) {\textbf{Leaky ReLU}};
        \node[font=\tiny, align=center] at (0,-1) {$\text{LReLU}(z) = \begin{cases} \alpha z & z \leq 0 \\ z & z > 0 \end{cases}$};
    \end{scope}

\end{tikzpicture}
\caption{Comparaison des principales fonctions d'activation. \textbf{Sigmoid et Tanh} souffrent de saturation (gradients $\approx 0$ pour $|z|$ grand), causant le \textit{vanishing gradient}. \textbf{ReLU} r√©sout ce probl√®me mais peut mourir (dead neurons) si $z \leq 0$ toujours. \textbf{Leaky ReLU} √©vite la mort neuronale avec un petit gradient n√©gatif ($\alpha=0.01$). En pratique, ReLU est le choix par d√©faut pour les couches cach√©es.}
\label{fig:activation_functions}
\end{figure}

\clearpage

% ===== SECTION 4: MLP (MULTI-LAYER PERCEPTRON) =====
\section{Perceptron Multi-Couches (MLP)}

\subsection{Architecture}

Un MLP est compos√© de plusieurs couches de neurones :
\begin{itemize}
    \item \textbf{Couche d'entr√©e (input layer)} : re√ßoit les features $\vect{x}$
    \item \textbf{Couches cach√©es (hidden layers)} : transformations non-lin√©aires
    \item \textbf{Couche de sortie (output layer)} : pr√©dictions $\hat{y}$
\end{itemize}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    neuron/.style={circle, draw, minimum size=0.8cm, fill=blue!20},
    input/.style={circle, draw, minimum size=0.8cm, fill=green!20},
    output/.style={circle, draw, minimum size=0.8cm, fill=red!20}
]

    % Input layer
    \foreach \y in {1,2,3,4}
        \node[input] (I\y) at (0, -\y*1.2) {};
    \node[above] at (I1.north) {Input Layer};
    \node[left, font=\small] at (I2.west) {$x_1$};
    \node[left, font=\small] at (I3.west) {$x_2$};
    \node[left, font=\small] at (I4.west) {$x_3$};

    % Hidden layer 1
    \foreach \y in {1,2,3,4,5}
        \node[neuron] (H1\y) at (3, -\y*1) {};
    \node[above] at (H11.north) {Hidden Layer 1};
    \node[right, font=\tiny] at (H13.east) {ReLU};

    % Hidden layer 2
    \foreach \y in {1,2,3}
        \node[neuron] (H2\y) at (6, -\y*1.5-0.5) {};
    \node[above] at (H21.north) {Hidden Layer 2};
    \node[right, font=\tiny] at (H22.east) {ReLU};

    % Output layer
    \foreach \y in {1,2}
        \node[output] (O\y) at (9, -\y*2-1) {};
    \node[above] at (O1.north) {Output Layer};
    \node[right, font=\small] at (O1.east) {$\hat{y}_1$};
    \node[right, font=\small] at (O2.east) {$\hat{y}_2$};
    \node[right, font=\tiny] at (O1.south east) {Softmax};

    % Connections Input -> Hidden1
    \foreach \i in {1,2,3,4}
        \foreach \j in {1,2,3,4,5}
            \draw[->] (I\i) -- (H1\j);

    % Connections Hidden1 -> Hidden2
    \foreach \i in {1,2,3,4,5}
        \foreach \j in {1,2,3}
            \draw[->] (H1\i) -- (H2\j);

    % Connections Hidden2 -> Output
    \foreach \i in {1,2,3}
        \foreach \j in {1,2}
            \draw[->] (H2\i) -- (O\j);

    % Annotations dimensions
    \node[below, font=\small, align=center] at (0, -5.5) {$d_0 = 3$\\(features)};
    \node[below, font=\small, align=center] at (3, -5.5) {$d_1 = 5$};
    \node[below, font=\small, align=center] at (6, -5.5) {$d_2 = 3$};
    \node[below, font=\small, align=center] at (9, -5.5) {$d_3 = 2$\\(classes)};

    % Forward pass annotation
    \draw[->, very thick, red] (0, -6.5) -- (9, -6.5);
    \node[red, below] at (4.5, -6.5) {Forward Pass $\rightarrow$};

\end{tikzpicture}
\caption{Architecture MLP (Multi-Layer Perceptron): les neurones d'une couche sont connect√©s √† tous les neurones de la couche suivante (fully-connected). Les activations se propagent de gauche √† droite via transformations lin√©aires ($\mat{W}\vect{a} + \vect{b}$) suivies de fonctions d'activation non-lin√©aires (ReLU, Softmax).}
\label{fig:mlp_architecture}
\end{figure}

\begin{definition}{MLP √† $L$ couches}
Pour une entr√©e $\vect{x} \in \R^{d_0}$, un MLP calcule :
\begin{align}
    \vect{a}^{[0]} &= \vect{x} \\
    \vect{z}^{[l]} &= \mat{W}^{[l]} \vect{a}^{[l-1]} + \vect{b}^{[l]} \quad \text{pour } l = 1, \ldots, L \\
    \vect{a}^{[l]} &= \sigma^{[l]}(\vect{z}^{[l]}) \\
    \hat{y} &= \vect{a}^{[L]}
\end{align}
o√π :
\begin{itemize}
    \item $\mat{W}^{[l]} \in \R^{d_l \times d_{l-1}}$ : matrice de poids de la couche $l$
    \item $\vect{b}^{[l]} \in \R^{d_l}$ : vecteur de biais de la couche $l$
    \item $\sigma^{[l]}$ : fonction d'activation de la couche $l$
    \item $\vect{a}^{[l]}$ : activations de la couche $l$
    \item $\vect{z}^{[l]}$ : pr√©-activations (avant fonction d'activation)
\end{itemize}
\end{definition}

\subsection{Th√©or√®me d'approximation universelle}

\begin{theoreme}{Approximation Universelle}
Un MLP avec une seule couche cach√©e de taille suffisante et une fonction d'activation non-lin√©aire peut approximer n'importe quelle fonction continue sur un compact de $\R^d$ avec une pr√©cision arbitraire.
\end{theoreme}

\begin{attention}
Ce th√©or√®me est \textbf{th√©orique} : en pratique, les r√©seaux profonds (deep learning) avec plusieurs couches sont plus efficaces et n√©cessitent moins de neurones.
\end{attention}

\subsection{Exemple : MLP 3 couches}

Architecture : 784 (input) ‚Üí 128 (hidden) ‚Üí 64 (hidden) ‚Üí 10 (output)

\begin{align}
    \vect{x} &\in \R^{784} \quad \text{(image 28√ó28)} \\
    \vect{z}^{[1]} &= \mat{W}^{[1]} \vect{x} + \vect{b}^{[1]} \quad (\mat{W}^{[1]} \in \R^{128 \times 784}) \\
    \vect{a}^{[1]} &= \relu(\vect{z}^{[1]}) \quad \in \R^{128} \\
    \vect{z}^{[2]} &= \mat{W}^{[2]} \vect{a}^{[1]} + \vect{b}^{[2]} \quad (\mat{W}^{[2]} \in \R^{64 \times 128}) \\
    \vect{a}^{[2]} &= \relu(\vect{z}^{[2]}) \quad \in \R^{64} \\
    \vect{z}^{[3]} &= \mat{W}^{[3]} \vect{a}^{[2]} + \vect{b}^{[3]} \quad (\mat{W}^{[3]} \in \R^{10 \times 64}) \\
    \hat{\vect{y}} &= \softmax(\vect{z}^{[3]}) \quad \in \R^{10}
\end{align}

Nombre total de param√®tres :
\begin{equation}
    (784 \times 128 + 128) + (128 \times 64 + 64) + (64 \times 10 + 10) = 109{,}386
\end{equation}

% ===== SECTION 5: FORWARD PASS =====
\section{Forward Pass (Propagation Avant)}

\subsection{Principe}

Le forward pass consiste √† calculer la sortie du r√©seau pour une entr√©e donn√©e en propageant les activations couche par couche.

\begin{algorithm}[H]
\caption{Forward Pass}
\label{alg:forward}
\begin{algorithmic}[1]
\REQUIRE Entr√©e $\vect{x}$, poids $\{\mat{W}^{[l]}, \vect{b}^{[l]}\}_{l=1}^L$
\ENSURE Pr√©diction $\hat{y}$, cache des activations $\{\vect{a}^{[l]}, \vect{z}^{[l]}\}$
\STATE $\vect{a}^{[0]} = \vect{x}$
\FOR{$l = 1$ \TO $L$}
    \STATE $\vect{z}^{[l]} = \mat{W}^{[l]} \vect{a}^{[l-1]} + \vect{b}^{[l]}$ \quad \COMMENT{Combinaison lin√©aire}
    \STATE $\vect{a}^{[l]} = \sigma^{[l]}(\vect{z}^{[l]})$ \quad \COMMENT{Activation non-lin√©aire}
    \STATE Stocker $\vect{z}^{[l]}, \vect{a}^{[l]}$ dans le cache \quad \COMMENT{Pour backprop}
\ENDFOR
\STATE $\hat{y} = \vect{a}^{[L]}$
\RETURN $\hat{y}$, cache
\end{algorithmic}
\end{algorithm}

\subsection{Impl√©mentation vectoris√©e}

Pour un batch de $m$ exemples $\mat{X} \in \R^{m \times d_0}$ :

\begin{align}
    \mat{A}^{[0]} &= \mat{X} \quad \in \R^{m \times d_0} \\
    \mat{Z}^{[l]} &= \mat{A}^{[l-1]} (\mat{W}^{[l]})^T + \vect{b}^{[l]} \quad \in \R^{m \times d_l} \\
    \mat{A}^{[l]} &= \sigma^{[l]}(\mat{Z}^{[l]})
\end{align}

\begin{lstlisting}[language=Python, caption=Forward pass vectoris√©]
def forward_pass(X, parameters):
    """
    X : (m, d0) - batch de m exemples
    parameters : dict avec W[l], b[l] pour chaque couche l
    """
    cache = {}
    A = X
    L = len(parameters) // 2  # Nombre de couches

    for l in range(1, L + 1):
        A_prev = A
        W = parameters[f'W{l}']
        b = parameters[f'b{l}']

        # Linear forward
        Z = np.dot(A_prev, W.T) + b

        # Activation
        if l < L:  # Hidden layers : ReLU
            A = np.maximum(0, Z)
        else:  # Output layer : softmax
            A = softmax(Z)

        # Cache pour backprop
        cache[f'Z{l}'] = Z
        cache[f'A{l}'] = A
        cache[f'A{l-1}'] = A_prev

    return A, cache
\end{lstlisting}

% ===== SECTION 6: FONCTION DE CO√õT =====
\section{Fonctions de Co√ªt}

\subsection{Pour la r√©gression : MSE}

\begin{equation}
    L(\vect{\theta}) = \frac{1}{m} \sum_{i=1}^m (\hat{y}_i - y_i)^2
\end{equation}

\subsection{Pour classification binaire : Binary Cross-Entropy}

\begin{equation}
    L(\vect{\theta}) = -\frac{1}{m} \sum_{i=1}^m \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
\end{equation}

\subsection{Pour classification multi-classe : Categorical Cross-Entropy}

\begin{definition}{Cross-Entropy Loss}
Pour $K$ classes, avec labels one-hot $\vect{y} \in \{0, 1\}^K$ :
\begin{equation}
    L(\vect{\theta}) = -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_{i,k} \log(\hat{y}_{i,k})
\end{equation}
o√π $\hat{\vect{y}}_i = \softmax(\vect{z}_i^{[L]})$.
\end{definition}

En pratique, on utilise souvent la formulation :
\begin{equation}
    L = -\frac{1}{m} \sum_{i=1}^m \log(\hat{y}_{i, c_i})
\end{equation}
o√π $c_i$ est la classe correcte pour l'exemple $i$.

% ===== SECTION 7: BACKPROPAGATION =====
\section{Backpropagation}

\subsection{Principe}

La \textbf{backpropagation} (r√©tropropagation) est l'algorithme qui permet de calculer efficacement les gradients de la fonction de co√ªt par rapport √† tous les param√®tres du r√©seau.

\textbf{Id√©e cl√© :} Utiliser la \textbf{r√®gle de la cha√Æne (chain rule)} pour propager les gradients de la sortie vers l'entr√©e.

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.9, every node/.style={font=\small}]
    % Layer boxes
    \node[draw, rectangle, minimum width=1.8cm, minimum height=2.5cm, fill=green!15] (L0) at (0,0) {};
    \node[draw, rectangle, minimum width=1.8cm, minimum height=2.5cm, fill=blue!15] (L1) at (3.5,0) {};
    \node[draw, rectangle, minimum width=1.8cm, minimum height=2.5cm, fill=blue!15] (L2) at (7,0) {};
    \node[draw, rectangle, minimum width=1.8cm, minimum height=2.5cm, fill=red!15] (L3) at (10.5,0) {};

    % Layer titles
    \node[above, font=\footnotesize\bfseries] at (L0.north) {Input};
    \node[above, font=\footnotesize\bfseries] at (L1.north) {Hidden 1};
    \node[above, font=\footnotesize\bfseries] at (L2.north) {Hidden 2};
    \node[above, font=\footnotesize\bfseries] at (L3.north) {Output};

    % Layer labels
    \node at (L0.center) {$\vect{a}^{[0]}$};
    \node[above=0.3cm] at (L0.center) {$\vect{x}$};

    \node at (L1.center) {$\vect{a}^{[1]}$};
    \node[above=0.3cm, font=\tiny] at (L1.center) {$\vect{z}^{[1]}$};

    \node at (L2.center) {$\vect{a}^{[2]}$};
    \node[above=0.3cm, font=\tiny] at (L2.center) {$\vect{z}^{[2]}$};

    \node at (L3.center) {$\vect{a}^{[3]}$};
    \node[above=0.3cm] at (L3.center) {$\hat{\vect{y}}$};

    % Forward pass arrows (top)
    \draw[->, very thick, blue!70!black] (L0.east) ++ (0,0.6) -- ++ (1.5,0)
        node[midway, above, font=\tiny] {$\mat{W}^{[1]}\vect{a}^{[0]} + \vect{b}^{[1]}$}
        node[midway, below, font=\tiny] {$\sigma^{[1]}(\vect{z}^{[1]})$};

    \draw[->, very thick, blue!70!black] (L1.east) ++ (0,0.6) -- ++ (1.5,0)
        node[midway, above, font=\tiny] {$\mat{W}^{[2]}\vect{a}^{[1]} + \vect{b}^{[2]}$}
        node[midway, below, font=\tiny] {$\sigma^{[2]}(\vect{z}^{[2]})$};

    \draw[->, very thick, blue!70!black] (L2.east) ++ (0,0.6) -- ++ (1.5,0)
        node[midway, above, font=\tiny] {$\mat{W}^{[3]}\vect{a}^{[2]} + \vect{b}^{[3]}$}
        node[midway, below, font=\tiny] {$\sigma^{[3]}(\vect{z}^{[3]})$};

    % Backward pass arrows (bottom)
    \draw[<-, very thick, red!70!black] (L0.east) ++ (0,-0.6) -- ++ (1.5,0)
        node[midway, below, font=\tiny] {$\delta^{[1]} (\vect{a}^{[0]})^T$};

    \draw[<-, very thick, red!70!black] (L1.east) ++ (0,-0.6) -- ++ (1.5,0)
        node[midway, above, font=\tiny] {$(\mat{W}^{[2]})^T \delta^{[2]} \odot \sigma'^{[1]}$}
        node[midway, below, font=\tiny] {$\delta^{[2]} (\vect{a}^{[1]})^T$};

    \draw[<-, very thick, red!70!black] (L2.east) ++ (0,-0.6) -- ++ (1.5,0)
        node[midway, above, font=\tiny] {$(\mat{W}^{[3]})^T \delta^{[3]} \odot \sigma'^{[2]}$}
        node[midway, below, font=\tiny] {$\delta^{[3]} (\vect{a}^{[2]})^T$};

    % Loss function
    \node[draw, circle, minimum size=1.2cm, fill=orange!20] (Loss) at (13,0) {$L$};
    \draw[->, very thick, blue!70!black] (L3.east) ++ (0,0.4) -- (Loss.west |- L3.east) ++ (0,0.4)
        node[midway, above, font=\tiny] {$L(\hat{\vect{y}}, \vect{y})$};

    % Gradient from loss
    \draw[<-, very thick, red!70!black] (L3.east) ++ (0,-0.4) -- (Loss.west |- L3.east) ++ (0,-0.4)
        node[midway, below, font=\tiny] {$\delta^{[3]} = \hat{\vect{y}} - \vect{y}$};

    % Legend
    \node[draw, rectangle, minimum width=3.5cm, minimum height=1.8cm, dashed] (legend) at (13,-3.5) {};
    \node[above left, font=\footnotesize\bfseries] at (legend.north west) {L√©gende:};

    \draw[->, very thick, blue!70!black] (11.5,-3) -- ++ (1,0);
    \node[right, font=\footnotesize] at (12.5,-3) {Forward Pass};

    \draw[<-, very thick, red!70!black] (11.5,-4) -- ++ (1,0);
    \node[right, font=\footnotesize] at (12.5,-4) {Backward Pass};

    % Annotations
    \node[blue!70!black, font=\footnotesize, align=center] at (5.25, 2.2) {FORWARD PROPAGATION};
    \node[blue!70!black, font=\tiny, align=center] at (5.25, 1.7) {Calcul des pr√©dictions $\hat{\vect{y}}$};

    \node[red!70!black, font=\footnotesize, align=center] at (5.25, -2.2) {BACKWARD PROPAGATION};
    \node[red!70!black, font=\tiny, align=center] at (5.25, -2.7) {Calcul des gradients $\frac{\partial L}{\partial \mat{W}^{[l]}}, \frac{\partial L}{\partial \vect{b}^{[l]}}$};

    % Cache annotation
    \node[font=\tiny, align=center, fill=yellow!20, draw, dashed] at (5.25, 0) {Cache: $\vect{z}^{[l]}, \vect{a}^{[l]}$};

\end{tikzpicture}
\caption{Flux bidirectionnel Forward et Backward Propagation dans un MLP √† 2 couches cach√©es. \textbf{Forward Pass (bleu):} les activations se propagent de gauche √† droite pour calculer $\hat{\vect{y}}$, puis la loss $L(\hat{\vect{y}}, \vect{y})$. Les valeurs interm√©diaires ($\vect{z}^{[l]}, \vect{a}^{[l]}$) sont stock√©es dans un cache. \textbf{Backward Pass (rouge):} les gradients se propagent de droite √† gauche via la r√®gle de la cha√Æne, calculant $\frac{\partial L}{\partial \mat{W}^{[l]}}$ et $\frac{\partial L}{\partial \vect{b}^{[l]}}$ pour chaque couche. Le gradient initial $\delta^{[L]} = \hat{\vect{y}} - \vect{y}$ (pour cross-entropy + softmax).}
\label{fig:forward_backward_propagation}
\end{figure}

\clearpage

\subsection{Notations}

D√©finissons :
\begin{align}
    \delta^{[l]} &= \frac{\partial L}{\partial \vect{z}^{[l]}} \quad \text{(gradient par rapport aux pr√©-activations)} \\
    \frac{\partial L}{\partial \mat{W}^{[l]}} &\quad \text{(gradient par rapport aux poids)} \\
    \frac{\partial L}{\partial \vect{b}^{[l]}} &\quad \text{(gradient par rapport aux biais)}
\end{align}

\subsection{D√©rivation des √©quations}

\textbf{Couche de sortie ($l = L$) :}

Pour cross-entropy avec softmax :
\begin{equation}
    \delta^{[L]} = \hat{\vect{y}} - \vect{y} \quad \text{(formule simplifi√©e !)}
\end{equation}

\textbf{Couches cach√©es ($l < L$) :}

Par la r√®gle de la cha√Æne :
\begin{align}
    \delta^{[l]} &= \frac{\partial L}{\partial \vect{z}^{[l]}} \\
    &= \frac{\partial L}{\partial \vect{a}^{[l]}} \odot \frac{\partial \vect{a}^{[l]}}{\partial \vect{z}^{[l]}} \\
    &= \left[ (\mat{W}^{[l+1]})^T \delta^{[l+1]} \right] \odot \sigma'^{[l]}(\vect{z}^{[l]})
\end{align}

o√π $\odot$ est le produit √©l√©ment par √©l√©ment (Hadamard).

\textbf{Gradients des param√®tres :}
\begin{align}
    \frac{\partial L}{\partial \mat{W}^{[l]}} &= \delta^{[l]} (\vect{a}^{[l-1]})^T \\
    \frac{\partial L}{\partial \vect{b}^{[l]}} &= \delta^{[l]}
\end{align}

Pour un batch de $m$ exemples :
\begin{align}
    \frac{\partial L}{\partial \mat{W}^{[l]}} &= \frac{1}{m} \mat{\Delta}^{[l]} (\mat{A}^{[l-1]})^T \\
    \frac{\partial L}{\partial \vect{b}^{[l]}} &= \frac{1}{m} \sum_{i=1}^m \delta_i^{[l]}
\end{align}

\subsection{Algorithme complet}

\begin{algorithm}[H]
\caption{Backpropagation}
\label{alg:backprop}
\begin{algorithmic}[1]
\REQUIRE Cache du forward pass $\{\vect{a}^{[l]}, \vect{z}^{[l]}\}$
\REQUIRE Gradient de la loss $\frac{\partial L}{\partial \vect{a}^{[L]}}$
\ENSURE Gradients $\{\frac{\partial L}{\partial \mat{W}^{[l]}}, \frac{\partial L}{\partial \vect{b}^{[l]}}\}$
\STATE Calculer $\delta^{[L]} = \hat{\vect{y}} - \vect{y}$ \quad \COMMENT{Output layer}
\FOR{$l = L$ \TO $1$} \COMMENT{Parcourir en sens inverse}
    \STATE $\frac{\partial L}{\partial \mat{W}^{[l]}} = \frac{1}{m} \delta^{[l]} (\vect{a}^{[l-1]})^T$
    \STATE $\frac{\partial L}{\partial \vect{b}^{[l]}} = \frac{1}{m} \sum_i \delta_i^{[l]}$
    \IF{$l > 1$}
        \STATE $\delta^{[l-1]} = [(\mat{W}^{[l]})^T \delta^{[l]}] \odot \sigma'^{[l-1]}(\vect{z}^{[l-1]})$
    \ENDIF
\ENDFOR
\RETURN Gradients
\end{algorithmic}
\end{algorithm}

\subsection{D√©riv√©es des fonctions d'activation}

\begin{itemize}
    \item \textbf{Sigmoid :} $\sigma'(z) = \sigma(z)(1 - \sigma(z))$
    \item \textbf{Tanh :} $\tanh'(z) = 1 - \tanh^2(z)$
    \item \textbf{ReLU :} $\relu'(z) = \mathbb{1}[z > 0]$
    \item \textbf{Leaky ReLU :} $\text{LReLU}'(z) = \begin{cases} 1 & z > 0 \\ \alpha & z \leq 0 \end{cases}$
\end{itemize}

\subsection{Impl√©mentation}

\begin{lstlisting}[language=Python, caption=Backpropagation]
def backward_pass(y_true, cache, parameters):
    """
    y_true : (m, K) - labels one-hot
    cache : dict des activations du forward pass
    parameters : dict des poids W[l], b[l]
    """
    m = y_true.shape[0]
    grads = {}
    L = len(parameters) // 2

    # Output layer gradient (cross-entropy + softmax)
    y_pred = cache[f'A{L}']
    dZ = y_pred - y_true  # Shape: (m, K)

    # Backprop √† travers les couches
    for l in reversed(range(1, L + 1)):
        A_prev = cache[f'A{l-1}']

        # Gradients des param√®tres
        grads[f'dW{l}'] = (1/m) * np.dot(dZ.T, A_prev)
        grads[f'db{l}'] = (1/m) * np.sum(dZ, axis=0, keepdims=True)

        if l > 1:
            # Gradient pour la couche pr√©c√©dente
            W = parameters[f'W{l}']
            dA_prev = np.dot(dZ, W)

            # Gradient √† travers l'activation (ReLU)
            Z_prev = cache[f'Z{l-1}']
            dZ = dA_prev * (Z_prev > 0)  # ReLU derivative

    return grads
\end{lstlisting}

% ===== SECTION 8: ENTRA√éNEMENT =====
\section{Entra√Ænement d'un MLP}

\subsection{Optimiseurs}

\subsubsection{Gradient Descent}

\begin{equation}
    \vect{\theta}_{t+1} = \vect{\theta}_t - \alpha \nabla L(\vect{\theta}_t)
\end{equation}

\subsubsection{Stochastic Gradient Descent (SGD)}

Mise √† jour sur un seul exemple (ou un mini-batch) :
\begin{equation}
    \vect{\theta}_{t+1} = \vect{\theta}_t - \alpha \nabla L_i(\vect{\theta}_t)
\end{equation}

\textbf{Avec momentum :}
\begin{align}
    \vect{v}_t &= \beta \vect{v}_{t-1} + (1 - \beta) \nabla L(\vect{\theta}_t) \\
    \vect{\theta}_{t+1} &= \vect{\theta}_t - \alpha \vect{v}_t
\end{align}

Typiquement $\beta = 0.9$.

\subsubsection{Adam (Adaptive Moment Estimation)}

Combine momentum et RMSprop :
\begin{align}
    \vect{m}_t &= \beta_1 \vect{m}_{t-1} + (1 - \beta_1) \nabla L(\vect{\theta}_t) \quad \text{(1st moment)} \\
    \vect{v}_t &= \beta_2 \vect{v}_{t-1} + (1 - \beta_2) (\nabla L(\vect{\theta}_t))^2 \quad \text{(2nd moment)} \\
    \hat{\vect{m}}_t &= \frac{\vect{m}_t}{1 - \beta_1^t} \quad \text{(bias correction)} \\
    \hat{\vect{v}}_t &= \frac{\vect{v}_t}{1 - \beta_2^t} \\
    \vect{\theta}_{t+1} &= \vect{\theta}_t - \alpha \frac{\hat{\vect{m}}_t}{\sqrt{\hat{\vect{v}}_t} + \epsilon}
\end{align}

Hyperparam√®tres par d√©faut : $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$.

\begin{astuce}
\textbf{Adam est l'optimiseur par d√©faut} pour la plupart des probl√®mes deep learning. Il est robuste et n√©cessite peu de tuning.
\end{astuce}

\subsection{Algorithme d'entra√Ænement complet}

\begin{algorithm}[H]
\caption{Entra√Ænement MLP}
\label{alg:mlp_training}
\begin{algorithmic}[1]
\REQUIRE Dataset $(X, y)$, architecture $\{d_0, d_1, \ldots, d_L\}$
\REQUIRE Learning rate $\alpha$, batch size $B$, nombre d'epochs $T$
\ENSURE Param√®tres entra√Æn√©s $\vect{\theta}^*$
\STATE Initialiser al√©atoirement $\mat{W}^{[l]}, \vect{b}^{[l]}$ pour $l = 1, \ldots, L$
\FOR{$epoch = 1$ \TO $T$}
    \STATE M√©langer les donn√©es
    \FOR{chaque mini-batch $(X_{batch}, y_{batch})$}
        \STATE \COMMENT{Forward pass}
        \STATE $\hat{y}_{batch}, cache = \text{forward\_pass}(X_{batch}, \vect{\theta})$
        \STATE $L_{batch} = \text{compute\_loss}(\hat{y}_{batch}, y_{batch})$
        \STATE \COMMENT{Backward pass}
        \STATE $grads = \text{backward\_pass}(y_{batch}, cache, \vect{\theta})$
        \STATE \COMMENT{Update parameters}
        \FOR{chaque param√®tre $\theta$}
            \STATE $\theta \leftarrow \theta - \alpha \cdot \frac{\partial L}{\partial \theta}$
        \ENDFOR
    \ENDFOR
    \STATE √âvaluer sur validation set
\ENDFOR
\RETURN $\vect{\theta}^*$
\end{algorithmic}
\end{algorithm}

\subsection{Initialisation des poids}

\begin{attention}
Ne JAMAIS initialiser tous les poids √† 0 ! Les neurones seraient tous identiques (sym√©trie).
\end{attention}

\textbf{Xavier/Glorot initialization (pour Sigmoid/Tanh) :}
\begin{equation}
    W^{[l]}_{ij} \sim \mathcal{N}\left(0, \frac{2}{d_{l-1} + d_l}\right)
\end{equation}

\textbf{He initialization (pour ReLU) :}
\begin{equation}
    W^{[l]}_{ij} \sim \mathcal{N}\left(0, \frac{2}{d_{l-1}}\right)
\end{equation}

\begin{lstlisting}[language=Python, caption=Initialisation He]
def initialize_parameters(layer_dims):
    """
    layer_dims : [d0, d1, d2, ..., dL]
    """
    parameters = {}
    L = len(layer_dims) - 1

    for l in range(1, L + 1):
        # He initialization
        parameters[f'W{l}'] = np.random.randn(
            layer_dims[l], layer_dims[l-1]
        ) * np.sqrt(2 / layer_dims[l-1])

        parameters[f'b{l}'] = np.zeros((1, layer_dims[l]))

    return parameters
\end{lstlisting}

% ===== SECTION 9: R√âGULARISATION =====
\section{R√©gularisation}

\subsection{L2 Regularization (Weight Decay)}

Ajouter un terme de p√©nalit√© sur les poids :
\begin{equation}
    L_{reg}(\vect{\theta}) = L(\vect{\theta}) + \frac{\lambda}{2m} \sum_{l=1}^L \|\mat{W}^{[l]}\|_F^2
\end{equation}

o√π $\|\mat{W}\|_F^2 = \sum_{i,j} W_{ij}^2$ est la norme de Frobenius.

Le gradient devient :
\begin{equation}
    \frac{\partial L_{reg}}{\partial \mat{W}^{[l]}} = \frac{\partial L}{\partial \mat{W}^{[l]}} + \frac{\lambda}{m} \mat{W}^{[l]}
\end{equation}

\subsection{Dropout}

\begin{definition}{Dropout}
Pendant l'entra√Ænement, chaque neurone est d√©sactiv√© avec probabilit√© $p$ (typiquement $p = 0.5$). Au test, on utilise tous les neurones mais on multiplie les sorties par $(1-p)$.
\end{definition}

\textbf{Impl√©mentation (inverted dropout) :}
\begin{lstlisting}[language=Python]
def dropout_forward(A, keep_prob=0.5, training=True):
    if training:
        mask = np.random.rand(*A.shape) < keep_prob
        A = A * mask / keep_prob  # Inverted dropout
        return A, mask
    else:
        return A, None
\end{lstlisting}

\begin{astuce}
Le dropout agit comme un \textbf{ensemble de r√©seaux} : √† chaque it√©ration, on entra√Æne un sous-r√©seau diff√©rent.
\end{astuce}

\subsection{Batch Normalization}

Normaliser les activations √† chaque couche :
\begin{align}
    \mu_B &= \frac{1}{m} \sum_{i=1}^m z_i^{[l]} \\
    \sigma_B^2 &= \frac{1}{m} \sum_{i=1}^m (z_i^{[l]} - \mu_B)^2 \\
    \hat{z}_i^{[l]} &= \frac{z_i^{[l]} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \\
    \tilde{z}_i^{[l]} &= \gamma \hat{z}_i^{[l]} + \beta
\end{align}

o√π $\gamma, \beta$ sont des param√®tres apprenables.

\textbf{Avantages :}
\begin{itemize}
    \item Acc√©l√®re l'entra√Ænement (learning rates plus √©lev√©s)
    \item R√©duit la sensibilit√© √† l'initialisation
    \item Effet r√©gularisant (similaire au dropout)
\end{itemize}

\subsection{Early Stopping}

Surveiller la loss sur le validation set et arr√™ter l'entra√Ænement quand elle commence √† augmenter.

\begin{lstlisting}[language=Python]
best_val_loss = float('inf')
patience = 10
counter = 0

for epoch in range(n_epochs):
    train_model()
    val_loss = evaluate_validation()

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_checkpoint()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping")
            break
\end{lstlisting}

% ===== SECTION 10: IMPL√âMENTATION COMPL√àTE =====
\section{Impl√©mentation Compl√®te avec NumPy}

\begin{lstlisting}[language=Python, caption=MLP complet from scratch]
import numpy as np

class MLP:
    def __init__(self, layer_dims, learning_rate=0.01):
        self.layer_dims = layer_dims
        self.lr = learning_rate
        self.parameters = self._initialize_parameters()
        self.L = len(layer_dims) - 1

    def _initialize_parameters(self):
        """He initialization"""
        params = {}
        for l in range(1, len(self.layer_dims)):
            params[f'W{l}'] = np.random.randn(
                self.layer_dims[l], self.layer_dims[l-1]
            ) * np.sqrt(2 / self.layer_dims[l-1])
            params[f'b{l}'] = np.zeros((1, self.layer_dims[l]))
        return params

    def _relu(self, Z):
        return np.maximum(0, Z)

    def _softmax(self, Z):
        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))
        return expZ / np.sum(expZ, axis=1, keepdims=True)

    def forward(self, X):
        """Forward pass"""
        cache = {'A0': X}
        A = X

        for l in range(1, self.L + 1):
            Z = np.dot(A, self.parameters[f'W{l}'].T) + \
                self.parameters[f'b{l}']

            if l < self.L:
                A = self._relu(Z)
            else:
                A = self._softmax(Z)

            cache[f'Z{l}'] = Z
            cache[f'A{l}'] = A

        return A, cache

    def backward(self, y_true, cache):
        """Backpropagation"""
        m = y_true.shape[0]
        grads = {}

        # Output layer
        dZ = cache[f'A{self.L}'] - y_true

        for l in reversed(range(1, self.L + 1)):
            A_prev = cache[f'A{l-1}']
            grads[f'dW{l}'] = (1/m) * np.dot(dZ.T, A_prev)
            grads[f'db{l}'] = (1/m) * np.sum(dZ, axis=0, keepdims=True)

            if l > 1:
                W = self.parameters[f'W{l}']
                dA_prev = np.dot(dZ, W)
                dZ = dA_prev * (cache[f'Z{l-1}'] > 0)

        return grads

    def update_parameters(self, grads):
        """Gradient descent update"""
        for l in range(1, self.L + 1):
            self.parameters[f'W{l}'] -= self.lr * grads[f'dW{l}']
            self.parameters[f'b{l}'] -= self.lr * grads[f'db{l}']

    def compute_loss(self, y_pred, y_true):
        """Cross-entropy loss"""
        m = y_true.shape[0]
        loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m
        return loss

    def fit(self, X, y, epochs=100, batch_size=32, verbose=True):
        """Training loop"""
        m = X.shape[0]
        history = {'loss': []}

        for epoch in range(epochs):
            # Shuffle
            indices = np.random.permutation(m)
            X_shuffled = X[indices]
            y_shuffled = y[indices]

            epoch_loss = 0
            n_batches = m // batch_size

            for i in range(n_batches):
                start = i * batch_size
                end = start + batch_size
                X_batch = X_shuffled[start:end]
                y_batch = y_shuffled[start:end]

                # Forward
                y_pred, cache = self.forward(X_batch)
                loss = self.compute_loss(y_pred, y_batch)
                epoch_loss += loss

                # Backward
                grads = self.backward(y_batch, cache)

                # Update
                self.update_parameters(grads)

            avg_loss = epoch_loss / n_batches
            history['loss'].append(avg_loss)

            if verbose and (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        return history

    def predict(self, X):
        """Predictions"""
        y_pred, _ = self.forward(X)
        return np.argmax(y_pred, axis=1)

# Exemple d'utilisation
if __name__ == "__main__":
    from sklearn.datasets import load_digits
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import OneHotEncoder

    # Charger donn√©es
    digits = load_digits()
    X, y = digits.data, digits.target

    # Normaliser
    X = X / 16.0

    # One-hot encoding
    y_onehot = np.zeros((len(y), 10))
    y_onehot[np.arange(len(y)), y] = 1

    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_onehot, test_size=0.2, random_state=42
    )

    # Entra√Æner
    mlp = MLP(layer_dims=[64, 128, 64, 10], learning_rate=0.1)
    history = mlp.fit(X_train, y_train, epochs=100, batch_size=32)

    # √âvaluer
    y_pred = mlp.predict(X_test)
    y_test_labels = np.argmax(y_test, axis=1)
    accuracy = np.mean(y_pred == y_test_labels)
    print(f"\nTest Accuracy: {accuracy:.4f}")
\end{lstlisting}

% ===== SECTION 11: AVEC PYTORCH =====
\section{Impl√©mentation avec PyTorch}

\begin{lstlisting}[language=Python, caption=MLP avec PyTorch]
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class MLP_PyTorch(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        super(MLP_PyTorch, self).__init__()

        layers = []
        prev_dim = input_dim

        # Hidden layers
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.2))
            prev_dim = hidden_dim

        # Output layer
        layers.append(nn.Linear(prev_dim, output_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Exemple d'utilisation
model = MLP_PyTorch(input_dim=64, hidden_dims=[128, 64], output_dim=10)

# Loss et optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
def train_pytorch(model, train_loader, epochs=50):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for X_batch, y_batch in train_loader:
            # Forward
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")

# Utilisation
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.LongTensor(np.argmax(y_train, axis=1))

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

train_pytorch(model, train_loader, epochs=50)
\end{lstlisting}

% ===== SECTION 12: DIAGNOSTIC =====
\section{Diagnostic et Debugging}

\subsection{Probl√®mes courants}

\begin{table}[h]
\centering
\caption{Diagnostic des probl√®mes d'entra√Ænement}
\label{tab:debugging}
\begin{tabular}{lll}
\toprule
\textbf{Sympt√¥me} & \textbf{Cause probable} & \textbf{Solution} \\
\midrule
Loss = NaN & Explosion gradients & R√©duire learning rate \\
 & & Gradient clipping \\
 & & V√©rifier normalisation \\
\midrule
Loss ne diminue pas & Learning rate trop faible & Augmenter LR \\
 & Mauvaise init & He/Xavier init \\
 & Architecture inadapt√©e & Changer nb couches \\
\midrule
Overfitting & Trop de param√®tres & Dropout, L2 reg \\
 & Pas assez de donn√©es & Data augmentation \\
 & & Early stopping \\
\midrule
Underfitting & Mod√®le trop simple & Ajouter couches/neurones \\
 & R√©gularisation trop forte & R√©duire $\lambda$, dropout \\
\midrule
Vanishing gradients & Sigmoid/Tanh profond & Utiliser ReLU \\
 & & Batch normalization \\
 & & Residual connections \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Gradient checking}

V√©rifier l'impl√©mentation de backprop en comparant avec gradient num√©rique :
\begin{equation}
    \frac{\partial L}{\partial \theta} \approx \frac{L(\theta + \epsilon) - L(\theta - \epsilon)}{2\epsilon}
\end{equation}

\begin{lstlisting}[language=Python, caption=Gradient checking]
def gradient_check(model, X, y, epsilon=1e-7):
    """V√©rifie backprop avec gradients num√©riques"""
    # Calculer gradients analytiques (backprop)
    y_pred, cache = model.forward(X)
    grads_analytic = model.backward(y, cache)

    # Calculer gradients num√©riques
    for param_name in model.parameters:
        param = model.parameters[param_name]
        grad_numeric = np.zeros_like(param)

        it = np.nditer(param, flags=['multi_index'])
        while not it.finished:
            idx = it.multi_index
            old_value = param[idx]

            # f(theta + epsilon)
            param[idx] = old_value + epsilon
            y_pred_plus, _ = model.forward(X)
            loss_plus = model.compute_loss(y_pred_plus, y)

            # f(theta - epsilon)
            param[idx] = old_value - epsilon
            y_pred_minus, _ = model.forward(X)
            loss_minus = model.compute_loss(y_pred_minus, y)

            # Gradient num√©rique
            grad_numeric[idx] = (loss_plus - loss_minus) / (2 * epsilon)

            param[idx] = old_value
            it.iternext()

        # Comparer
        grad_analytic = grads_analytic[f'd{param_name}']
        diff = np.linalg.norm(grad_numeric - grad_analytic) / \
               (np.linalg.norm(grad_numeric) + np.linalg.norm(grad_analytic))

        print(f"{param_name}: diff = {diff:.2e}")
        if diff < 1e-7:
            print("  ‚úì Gradient correct")
        else:
            print("  ‚úó Erreur dans backprop!")
\end{lstlisting}

% ===== SECTION 13: AVANTAGES ET LIMITES =====
\section{Avantages et Limites}

\subsection{Avantages}
\begin{itemize}
    \item ‚úÖ Mod√®le les relations non-lin√©aires complexes
    \item ‚úÖ Approximation universelle (th√©oriquement)
    \item ‚úÖ Flexible : r√©gression, classification, s√©ries temporelles
    \item ‚úÖ Apprentissage de repr√©sentations (features automatiques)
    \item ‚úÖ Parall√©lisation sur GPU
\end{itemize}

\subsection{Limites}
\begin{itemize}
    \item ‚ùå Bo√Æte noire (difficile √† interpr√©ter)
    \item ‚ùå N√©cessite beaucoup de donn√©es
    \item ‚ùå Sensible aux hyperparam√®tres
    \item ‚ùå Risque d'overfitting
    \item ‚ùå Temps d'entra√Ænement long
    \item ‚ùå Optima locaux (pas de garantie de convergence globale)
\end{itemize}

\subsection{Quand utiliser un MLP ?}

\begin{astuce}
Un MLP est particuli√®rement adapt√© quand :
\begin{itemize}
    \item Les donn√©es sont tabulaires (non structur√©es spatialement)
    \item Relations non-lin√©aires complexes
    \item Beaucoup de donn√©es disponibles
    \item Performance > interpr√©tabilit√©
\end{itemize}
\end{astuce}

\begin{attention}
Pr√©f√©rer d'autres mod√®les dans les cas suivants :
\begin{itemize}
    \item Petites donn√©es ($< 1000$ exemples) ‚Üí Random Forest, SVM
    \item Images ‚Üí CNN (Chapitre 07)
    \item S√©quences/texte ‚Üí RNN, Transformers (Chapitre 08)
    \item Besoin d'interpr√©tabilit√© ‚Üí Arbres de d√©cision, r√©gression lin√©aire
\end{itemize}
\end{attention}

% ===== SECTION 14: HYPERPARAM√àTRES =====
\section{Hyperparam√®tres et Tuning}

\begin{table}[h]
\centering
\caption{Hyperparam√®tres principaux d'un MLP}
\label{tab:hyperparams_mlp}
\begin{tabular}{lll}
\toprule
\textbf{Param√®tre} & \textbf{Valeurs typiques} & \textbf{Impact} \\
\midrule
\textbf{Architecture} & & \\
Nombre de couches & 2-5 & Capacit√© du mod√®le \\
Neurones par couche & 64, 128, 256, 512 & Capacit√©, overfitting \\
\midrule
\textbf{Optimisation} & & \\
Learning rate & $10^{-4}$ √† $10^{-1}$ & Vitesse/stabilit√© \\
Batch size & 32, 64, 128, 256 & Vitesse, g√©n√©ralisation \\
Optimizer & SGD, Adam & Convergence \\
\midrule
\textbf{R√©gularisation} & & \\
Dropout & 0.2, 0.5 & Overfitting \\
L2 weight decay & $10^{-5}$ √† $10^{-3}$ & Overfitting \\
\midrule
\textbf{Autres} & & \\
Activation & ReLU, Leaky ReLU & Gradient flow \\
Initialisation & He, Xavier & Convergence initiale \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Strat√©gies de tuning}

\begin{enumerate}
    \item \textbf{Grid Search} : Tester toutes les combinaisons (co√ªteux)
    \item \textbf{Random Search} : √âchantillonner al√©atoirement (souvent meilleur)
    \item \textbf{Bayesian Optimization} : Optimisation intelligente (Optuna, Hyperopt)
\end{enumerate}

\begin{lstlisting}[language=Python, caption=Random search avec scikit-learn]
from sklearn.model_selection import RandomizedSearchCV
from sklearn.neural_network import MLPClassifier

param_distributions = {
    'hidden_layer_sizes': [(64,), (128,), (64, 32), (128, 64)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001, 0.01],  # L2 regularization
    'learning_rate_init': [0.001, 0.01, 0.1],
    'batch_size': [32, 64, 128]
}

mlp = MLPClassifier(max_iter=100)
random_search = RandomizedSearchCV(
    mlp, param_distributions, n_iter=20, cv=3, n_jobs=-1
)

random_search.fit(X_train, y_train)
print("Best params:", random_search.best_params_)
\end{lstlisting}

% ===== SECTION 15: APPLICATIONS =====
\section{Applications Pratiques}

\begin{enumerate}
    \item \textbf{Classification d'images} : MNIST, CIFAR-10 (avant CNN)
    \item \textbf{Reconnaissance vocale} : Phon√®mes, commandes vocales
    \item \textbf{Pr√©diction de s√©ries temporelles} : Finance, m√©t√©o
    \item \textbf{Syst√®mes de recommandation} : Collaborative filtering
    \item \textbf{D√©tection de fraude} : Transactions bancaires
    \item \textbf{Diagnostic m√©dical} : Pr√©diction de maladies √† partir de biomarqueurs
    \item \textbf{NLP} : Sentiment analysis, classification de textes
\end{enumerate}

% ===== SECTION 16: R√âSUM√â =====
\section{R√©sum√© du Chapitre}

\subsection{Points Cl√©s}
\begin{itemize}
    \item \textbf{Perceptron} : Classification binaire lin√©aire (limit√© au lin√©airement s√©parable)
    \item \textbf{MLP} : Empilage de couches avec activations non-lin√©aires (approximation universelle)
    \item \textbf{Forward pass} : Propagation des activations de l'entr√©e √† la sortie
    \item \textbf{Backpropagation} : Calcul efficace des gradients via la cha√Æne rule
    \item \textbf{Fonctions d'activation} : ReLU (d√©faut), sigmoid (output binaire), softmax (multi-classe)
    \item \textbf{Optimiseurs} : SGD, Adam (d√©faut), momentum
    \item \textbf{R√©gularisation} : Dropout, L2, batch norm, early stopping
    \item \textbf{Initialisation} : He (ReLU), Xavier (sigmoid/tanh)
\end{itemize}

\subsection{Formules Essentielles}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Formules √† retenir]
\textbf{Forward pass (couche $l$) :}
\begin{align*}
    \vect{z}^{[l]} &= \mat{W}^{[l]} \vect{a}^{[l-1]} + \vect{b}^{[l]} \\
    \vect{a}^{[l]} &= \sigma^{[l]}(\vect{z}^{[l]})
\end{align*}

\textbf{Backpropagation :}
\begin{align*}
    \delta^{[l]} &= [(\mat{W}^{[l+1]})^T \delta^{[l+1]}] \odot \sigma'^{[l]}(\vect{z}^{[l]}) \\
    \frac{\partial L}{\partial \mat{W}^{[l]}} &= \delta^{[l]} (\vect{a}^{[l-1]})^T \\
    \frac{\partial L}{\partial \vect{b}^{[l]}} &= \delta^{[l]}
\end{align*}

\textbf{Cross-Entropy + Softmax :}
\begin{align*}
    L &= -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_{i,k} \log(\hat{y}_{i,k}) \\
    \delta^{[L]} &= \hat{\vect{y}} - \vect{y}
\end{align*}

\textbf{Gradient Descent :}
\begin{equation*}
    \vect{\theta} \leftarrow \vect{\theta} - \alpha \nabla L(\vect{\theta})
\end{equation*}
\end{tcolorbox}

% ===== SECTION 17: EXERCICES =====
\section{Exercices}

\subsection{Questions de compr√©hension}
\begin{enumerate}
    \item Pourquoi le perceptron ne peut-il pas r√©soudre le probl√®me XOR ?
    \item Que se passe-t-il si on initialise tous les poids √† 0 ?
    \item Expliquer le probl√®me du vanishing gradient avec la fonction sigmoid.
    \item Pourquoi ReLU est-il pr√©f√©r√© √† sigmoid dans les couches cach√©es ?
    \item Quelle est la diff√©rence entre batch, epoch et iteration ?
    \item Pourquoi utilise-t-on softmax + cross-entropy pour la classification multi-classe ?
\end{enumerate}

\subsection{Exercices pratiques}

\begin{enumerate}
    \item \textbf{Perceptron from scratch}
    \begin{itemize}
        \item Impl√©menter le perceptron en NumPy
        \item Tester sur un dataset lin√©airement s√©parable (make\_classification)
        \item Visualiser la fronti√®re de d√©cision
    \end{itemize}

    \item \textbf{MLP pour MNIST}
    \begin{itemize}
        \item Charger le dataset MNIST (sklearn.datasets.load\_digits)
        \item Entra√Æner un MLP from scratch (architecture libre)
        \item Comparer avec MLPClassifier de scikit-learn
        \item Objectif : > 95\% accuracy
    \end{itemize}

    \item \textbf{Gradient checking}
    \begin{itemize}
        \item Impl√©menter le gradient checking
        \item V√©rifier votre impl√©mentation de backprop
    \end{itemize}

    \item \textbf{R√©gularisation}
    \begin{itemize}
        \item Comparer MLP avec/sans dropout
        \item Tester diff√©rentes valeurs de L2 regularization
        \item Tracer les courbes d'apprentissage
    \end{itemize}

    \item \textbf{Hyperparameter tuning}
    \begin{itemize}
        \item Utiliser RandomizedSearchCV pour trouver les meilleurs hyperparam√®tres
        \item Comparer GridSearch vs RandomSearch
    \end{itemize}
\end{enumerate}

\textit{Solutions disponibles dans} \texttt{06\_exercices.ipynb} \textit{(solutions int√©gr√©es dans le notebook)}

% ===== SECTION 18: POUR ALLER PLUS LOIN =====
\section{Pour Aller Plus Loin}

\subsection{Lectures Recommand√©es}
\begin{itemize}
    \item \textbf{Deep Learning Book} (Goodfellow et al., 2016) - Chapitres 6-8
    \item \textbf{Neural Networks and Deep Learning} (Michael Nielsen) - En ligne gratuit
    \item Rumelhart et al. (1986) - "Learning representations by back-propagating errors"
    \item Glorot \& Bengio (2010) - "Understanding the difficulty of training deep feedforward neural networks"
\end{itemize}

\subsection{Ressources en Ligne}
\begin{itemize}
    \item Playground TensorFlow : \url{https://playground.tensorflow.org/}
    \item 3Blue1Brown - Neural Networks : \url{https://www.youtube.com/watch?v=aircAruvnKk}
    \item Documentation scikit-learn MLP : \url{https://scikit-learn.org/stable/modules/neural_networks_supervised.html}
    \item PyTorch Tutorials : \url{https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html}
\end{itemize}

\subsection{Extensions}
\begin{itemize}
    \item \textbf{Residual Networks (ResNet)} : Connexions r√©siduelles pour r√©seaux tr√®s profonds
    \item \textbf{Batch Normalization} : Normalisation des activations
    \item \textbf{Autoencoders} : Apprentissage de repr√©sentations non supervis√©
    \item \textbf{Transfer Learning} : R√©utilisation de r√©seaux pr√©-entra√Æn√©s
\end{itemize}

\subsection{Prochaines √âtapes}
Chapitre suivant recommand√© : \textbf{Chapitre 07 - Deep Learning : R√©seaux de Neurones Convolutifs (CNN)}

Les CNN sont une architecture sp√©cialis√©e pour les donn√©es spatiales (images) qui exploite la localit√© et l'invariance par translation.

% ===== BIBLIOGRAPHIE =====
\section*{R√©f√©rences}
\begin{enumerate}
    \item Goodfellow, I., Bengio, Y., \& Courville, A. (2016). \textit{Deep Learning}. MIT Press.
    \item Rumelhart, D. E., Hinton, G. E., \& Williams, R. J. (1986). "Learning representations by back-propagating errors". \textit{Nature}, 323(6088), 533-536.
    \item LeCun, Y., Bengio, Y., \& Hinton, G. (2015). "Deep learning". \textit{Nature}, 521(7553), 436-444.
    \item Kingma, D. P., \& Ba, J. (2014). "Adam: A method for stochastic optimization". \textit{arXiv preprint arXiv:1412.6980}.
    \item Srivastava, N., et al. (2014). "Dropout: A simple way to prevent neural networks from overfitting". \textit{JMLR}, 15(1), 1929-1958.
    \item He, K., et al. (2015). "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification". \textit{ICCV}.
\end{enumerate}

\end{document}
