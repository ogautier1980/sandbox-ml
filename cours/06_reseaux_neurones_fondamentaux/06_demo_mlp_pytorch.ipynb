{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/06_reseaux_neurones_fondamentaux/06_demo_mlp_pytorch.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '06_demo_mlp_pytorch.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 06 - D√©monstration : MLP avec PyTorch\n",
    "\n",
    "**Objectif** : Impl√©menter un MLP avec PyTorch en utilisant les outils modernes (DataLoader, Optimizer, GPU).\n",
    "\n",
    "**Contenu** :\n",
    "1. Architecture avec `nn.Module`\n",
    "2. DataLoader et augmentation\n",
    "3. Optimizers (Adam, SGD)\n",
    "4. Dropout et Batch Normalization\n",
    "5. Early stopping et checkpointing\n",
    "6. TensorBoard pour visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device (GPU si disponible)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Seed pour reproductibilit√©\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et pr√©paration des donn√©es (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement MNIST\n",
    "print(\"Chargement MNIST...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X = mnist.data.astype('float32').values / 255.0  # Normalisation\n",
    "y = mnist.target.astype('int').values\n",
    "\n",
    "# Sous-√©chantillon (20% pour vitesse)\n",
    "X_small = X[:14000]\n",
    "y_small = y[:14000]\n",
    "\n",
    "# Split train/val/test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_small, y_small, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Val: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en tensors PyTorch\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Nombre de batches train: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture MLP avec PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron avec Dropout et Batch Normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_sizes=[256, 128], num_classes=10, dropout=0.3):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            # Linear -> BatchNorm -> ReLU -> Dropout\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Couche de sortie\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Instanciation\n",
    "model = MLP(input_size=784, hidden_sizes=[256, 128, 64], num_classes=10, dropout=0.3).to(device)\n",
    "\n",
    "# R√©sum√© du mod√®le\n",
    "print(model)\n",
    "print(f\"\\nNombre de param√®tres: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fonction d'entra√Ænement et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Entra√Æne le mod√®le sur une epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # M√©triques\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"√âvalue le mod√®le sur le set de validation.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "print(\"Fonctions d'entra√Ænement d√©finies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entra√Ænement avec Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam√®tres\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# TensorBoard\n",
    "writer = SummaryWriter('runs/mlp_mnist')\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "epochs = 50\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"\\nD√©but de l'entra√Ænement...\")\n",
    "for epoch in range(epochs):\n",
    "    # Entra√Ænement\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Historique\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Sauvegarde du meilleur mod√®le\n",
    "        torch.save(model.state_dict(), 'best_mlp_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Affichage\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Stop si patience d√©pass√©e\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping d√©clench√© √† l'epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "writer.close()\n",
    "print(\"\\nEntra√Ænement termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation des courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss pendant l\\'entra√Ænement')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy pendant l\\'entra√Ænement')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. √âvaluation sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du meilleur mod√®le\n",
    "model.load_state_dict(torch.load('best_mlp_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Pr√©dictions sur test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\nAccuracy test: {test_acc:.4f}\")\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.title('Matrice de Confusion - MNIST')\n",
    "plt.xlabel('Pr√©diction')\n",
    "plt.ylabel('V√©rit√©')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions sur 20 √©chantillons al√©atoires\n",
    "n_samples = 20\n",
    "indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = indices[i]\n",
    "    image = X_test[idx].reshape(28, 28)\n",
    "    true_label = y_test[idx]\n",
    "    \n",
    "    # Pr√©diction\n",
    "    with torch.no_grad():\n",
    "        X_sample = torch.FloatTensor(X_test[idx:idx+1]).to(device)\n",
    "        output = model(X_sample)\n",
    "        probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "        pred_label = np.argmax(probs)\n",
    "        confidence = probs[pred_label]\n",
    "    \n",
    "    # Affichage\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f\"True: {true_label} | Pred: {pred_label}\\nConf: {confidence:.2f}\", color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver les erreurs de classification\n",
    "errors_idx = [i for i, (true, pred) in enumerate(zip(all_labels, all_preds)) if true != pred]\n",
    "print(f\"Nombre d'erreurs: {len(errors_idx)} / {len(all_labels)} ({len(errors_idx)/len(all_labels)*100:.2f}%)\")\n",
    "\n",
    "# Visualiser 10 erreurs\n",
    "n_errors = min(10, len(errors_idx))\n",
    "error_samples = np.random.choice(errors_idx, n_errors, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < n_errors:\n",
    "        idx = error_samples[i]\n",
    "        image = X_test[idx].reshape(28, 28)\n",
    "        true_label = all_labels[idx]\n",
    "        pred_label = all_preds[idx]\n",
    "        \n",
    "        # Probabilit√©s\n",
    "        with torch.no_grad():\n",
    "            X_sample = torch.FloatTensor(X_test[idx:idx+1]).to(device)\n",
    "            output = model(X_sample)\n",
    "            probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.set_title(f\"True: {true_label} | Pred: {pred_label}\\nConf: {probs[pred_label]:.2f}\", color='red')\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Erreurs de classification', fontsize=16, color='red')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyse des activations (feature maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook pour capturer les activations\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Enregistrer hooks sur les couches cach√©es\n",
    "model.network[0].register_forward_hook(get_activation('layer1'))\n",
    "model.network[4].register_forward_hook(get_activation('layer2'))\n",
    "\n",
    "# Forward pass sur un √©chantillon\n",
    "sample_idx = 0\n",
    "with torch.no_grad():\n",
    "    X_sample = torch.FloatTensor(X_test[sample_idx:sample_idx+1]).to(device)\n",
    "    output = model(X_sample)\n",
    "\n",
    "# Visualisation des activations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Image originale\n",
    "axes[0].imshow(X_test[sample_idx].reshape(28, 28), cmap='gray')\n",
    "axes[0].set_title(f\"Image originale (Label: {y_test[sample_idx]})\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Activations Layer 1 (256 neurones)\n",
    "act1 = activations['layer1'].cpu().numpy()[0]\n",
    "axes[1].bar(range(len(act1[:50])), act1[:50])  # Afficher 50 premiers neurones\n",
    "axes[1].set_title('Activations Layer 1 (50 premiers)')\n",
    "axes[1].set_xlabel('Neurone')\n",
    "axes[1].set_ylabel('Activation')\n",
    "\n",
    "# Activations Layer 2 (128 neurones)\n",
    "act2 = activations['layer2'].cpu().numpy()[0]\n",
    "axes[2].bar(range(len(act2[:50])), act2[:50])\n",
    "axes[2].set_title('Activations Layer 2 (50 premiers)')\n",
    "axes[2].set_xlabel('Neurone')\n",
    "axes[2].set_ylabel('Activation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Avantages PyTorch vs NumPy** :\n",
    "1. **Automatisation** : `autograd` calcule automatiquement les gradients\n",
    "2. **GPU** : Acc√©l√©ration mat√©rielle transparente\n",
    "3. **DataLoader** : Gestion efficace des mini-batches\n",
    "4. **Optimizers** : Adam, SGD, RMSprop impl√©ment√©s\n",
    "5. **R√©gularisation** : Dropout, BatchNorm int√©gr√©s\n",
    "6. **Checkpointing** : Sauvegarde/chargement facile\n",
    "\n",
    "**R√©sultats MNIST** :\n",
    "- Accuracy ~97-98% avec architecture (784-256-128-64-10)\n",
    "- Early stopping √©vite l'overfitting\n",
    "- Learning rate scheduler am√©liore la convergence\n",
    "- BatchNorm + Dropout = stabilit√© + g√©n√©ralisation\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "- Tester d'autres architectures (plus profondes)\n",
    "- Utiliser **Weight Decay** (L2 regularization)\n",
    "- Impl√©menter **Data Augmentation**\n",
    "- Passer aux **Convolutional Neural Networks (CNN)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}