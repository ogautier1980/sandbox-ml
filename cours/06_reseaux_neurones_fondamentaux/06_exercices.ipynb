{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/06_reseaux_neurones_fondamentaux/06_exercices.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '06_exercices.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 06 - Exercices : R√©seaux de Neurones (MLP)\n",
    "\n",
    "**Objectifs** :\n",
    "1. Impl√©menter des variantes de MLP\n",
    "2. Exp√©rimenter avec hyperparam√®tres\n",
    "3. R√©gularisation et optimisation\n",
    "4. Comparaison NumPy vs PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_circles, fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 : Impl√©mentation Perceptron simple (NumPy)\n",
    "\n",
    "**Objectif** : Impl√©menter un perceptron monocouche pour classification binaire.\n",
    "\n",
    "**TODO** :\n",
    "1. Impl√©menter la classe `Perceptron`\n",
    "2. M√©thodes : `fit()`, `predict()`\n",
    "3. Tester sur dataset lin√©airement s√©parable\n",
    "4. Visualiser la fronti√®re de d√©cision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Perceptron simple pour classification binaire.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_epochs=100):\n",
    "        self.lr = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entra√Æne le perceptron.\"\"\"\n",
    "        # TODO: Impl√©menter l'algorithme du perceptron\n",
    "        # - Initialiser weights et bias\n",
    "        # - Pour chaque epoch:\n",
    "        #     - Pour chaque √©chantillon:\n",
    "        #         - Calculer pr√©diction (z = X @ w + b, pred = 1 if z >= 0 else 0)\n",
    "        #         - Mettre √† jour poids si erreur: w += lr * (y - pred) * X\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Pr√©diction (0 ou 1).\"\"\"\n",
    "        # TODO: Impl√©menter la pr√©diction\n",
    "        pass\n",
    "\n",
    "# Test sur dataset lin√©airement s√©parable\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
    "                           n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: Entra√Æner le perceptron et √©valuer\n",
    "# perceptron = Perceptron(learning_rate=0.01, n_epochs=100)\n",
    "# perceptron.fit(X_train, y_train)\n",
    "# y_pred = perceptron.predict(X_test)\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# TODO: Visualiser la fronti√®re de d√©cision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : XOR Problem avec MLP\n",
    "\n",
    "**Contexte** : Le perceptron simple ne peut pas r√©soudre XOR (non lin√©airement s√©parable).\n",
    "\n",
    "**TODO** :\n",
    "1. Cr√©er dataset XOR\n",
    "2. Entra√Æner MLP PyTorch avec une couche cach√©e\n",
    "3. Comparer avec perceptron simple\n",
    "4. Visualiser la fronti√®re de d√©cision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset XOR\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_xor = np.array([0, 1, 1, 0], dtype=np.int64)\n",
    "\n",
    "print(\"Dataset XOR:\")\n",
    "for x, y in zip(X_xor, y_xor):\n",
    "    print(f\"Input: {x} -> Output: {y}\")\n",
    "\n",
    "# TODO: Cr√©er MLP PyTorch pour r√©soudre XOR\n",
    "# Architecture sugg√©r√©e: 2 -> 4 -> 1 (ou 2 -> 4 -> 2 avec Softmax)\n",
    "# Indices:\n",
    "# - Utiliser nn.Sequential\n",
    "# - Activation: ReLU ou Sigmoid\n",
    "# - Optimizer: Adam ou SGD\n",
    "# - Loss: BCEWithLogitsLoss ou CrossEntropyLoss\n",
    "# - Entra√Æner sur ~1000 epochs\n",
    "\n",
    "# TODO: Tester si le MLP pr√©dit correctement les 4 cas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 : Impact du Dropout\n",
    "\n",
    "**Objectif** : Comparer MLP avec et sans Dropout pour comprendre l'effet de r√©gularisation.\n",
    "\n",
    "**TODO** :\n",
    "1. Entra√Æner 2 mod√®les sur MNIST (avec/sans Dropout)\n",
    "2. Comparer overfitting (train vs val accuracy)\n",
    "3. Visualiser les courbes d'apprentissage\n",
    "4. Conclure sur l'efficacit√© du Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement MNIST (petit subset)\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X = mnist.data.astype('float32').values[:5000] / 255.0  # type: ignore\n",
    "y = mnist.target.astype('int').values[:5000]  # type: ignore\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: Cr√©er 2 mod√®les identiques sauf Dropout\n",
    "# Mod√®le 1: 784 -> 256 -> 128 -> 10 (sans Dropout)\n",
    "# Mod√®le 2: 784 -> 256 -> Dropout(0.3) -> 128 -> Dropout(0.3) -> 10\n",
    "\n",
    "# TODO: Entra√Æner les 2 mod√®les sur 50 epochs\n",
    "\n",
    "# TODO: Comparer:\n",
    "# - Train accuracy finale\n",
    "# - Val accuracy finale\n",
    "# - Gap train-val (indicateur d'overfitting)\n",
    "\n",
    "# TODO: Visualiser les courbes d'apprentissage c√¥te √† c√¥te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 : Comparaison d'Optimizers\n",
    "\n",
    "**Objectif** : Comparer SGD, SGD+Momentum, Adam sur m√™me architecture.\n",
    "\n",
    "**TODO** :\n",
    "1. Entra√Æner 3 mod√®les identiques avec optimizers diff√©rents\n",
    "2. Comparer vitesse de convergence\n",
    "3. Comparer accuracy finale\n",
    "4. Visualiser les courbes de loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Make Circles (non lin√©aire)\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, factor=0.5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Conversion PyTorch\n",
    "X_train_t = torch.FloatTensor(X_train)\n",
    "y_train_t = torch.LongTensor(y_train)\n",
    "X_test_t = torch.FloatTensor(X_test)\n",
    "y_test_t = torch.LongTensor(y_test)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# TODO: Cr√©er 3 mod√®les identiques (2 -> 32 -> 16 -> 2)\n",
    "\n",
    "# TODO: D√©finir 3 optimizers:\n",
    "# - SGD (lr=0.1)\n",
    "# - SGD + Momentum (lr=0.1, momentum=0.9)\n",
    "# - Adam (lr=0.01)\n",
    "\n",
    "# TODO: Entra√Æner les 3 mod√®les sur 100 epochs\n",
    "# Stocker l'historique de loss pour chaque mod√®le\n",
    "\n",
    "# TODO: Comparer les 3 optimizers:\n",
    "# - Courbes de loss (m√™me graphique)\n",
    "# - Accuracy finale sur test set\n",
    "# - Nombre d'epochs pour atteindre 95% accuracy (si atteint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5 : Batch Normalization vs Layer Normalization\n",
    "\n",
    "**Objectif** : Comprendre l'effet de BatchNorm sur la stabilit√© de l'entra√Ænement.\n",
    "\n",
    "**TODO** :\n",
    "1. Entra√Æner 3 mod√®les:\n",
    "   - Sans normalisation\n",
    "   - Avec Batch Normalization\n",
    "   - Avec Layer Normalization\n",
    "2. Comparer stabilit√© et vitesse de convergence\n",
    "3. Tester sur architecture profonde (5 couches cach√©es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cr√©er 3 architectures profondes:\n",
    "# Architecture: 784 -> 512 -> 256 -> 128 -> 64 -> 32 -> 10\n",
    "\n",
    "# Mod√®le 1: Sans normalisation\n",
    "# model1 = nn.Sequential(\n",
    "#     nn.Linear(784, 512), nn.ReLU(),\n",
    "#     nn.Linear(512, 256), nn.ReLU(),\n",
    "#     ...\n",
    "# )\n",
    "\n",
    "# Mod√®le 2: Avec BatchNorm apr√®s chaque Linear\n",
    "# model2 = nn.Sequential(\n",
    "#     nn.Linear(784, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
    "#     ...\n",
    "# )\n",
    "\n",
    "# Mod√®le 3: Avec LayerNorm\n",
    "# model3 = nn.Sequential(\n",
    "#     nn.Linear(784, 512), nn.LayerNorm(512), nn.ReLU(),\n",
    "#     ...\n",
    "# )\n",
    "\n",
    "# TODO: Entra√Æner les 3 mod√®les sur MNIST (50 epochs)\n",
    "\n",
    "# TODO: Comparer:\n",
    "# - Convergence (loss apr√®s 10 epochs)\n",
    "# - Stabilit√© (variance de la loss)\n",
    "# - Accuracy finale\n",
    "\n",
    "# TODO: Visualiser l'√©volution de la loss pour les 3 mod√®les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 6 : Learning Rate Scheduling\n",
    "\n",
    "**Objectif** : Exp√©rimenter avec diff√©rents schedulers pour optimiser la convergence.\n",
    "\n",
    "**TODO** :\n",
    "1. Tester 4 strategies:\n",
    "   - Learning rate constant\n",
    "   - StepLR (r√©duction par paliers)\n",
    "   - ExponentialLR (r√©duction exponentielle)\n",
    "   - ReduceLROnPlateau (r√©duction adaptative)\n",
    "2. Comparer vitesse de convergence\n",
    "3. Visualiser l'√©volution du learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cr√©er 4 mod√®les identiques\n",
    "\n",
    "# TODO: D√©finir 4 schedulers:\n",
    "# 1. Pas de scheduler (lr constant = 0.1)\n",
    "# 2. StepLR: lr *= 0.5 tous les 10 epochs\n",
    "# 3. ExponentialLR: lr *= 0.95 chaque epoch\n",
    "# 4. ReduceLROnPlateau: lr *= 0.5 si val_loss stagne 5 epochs\n",
    "\n",
    "# TODO: Entra√Æner les 4 mod√®les sur 50 epochs\n",
    "# Stocker: loss, accuracy, learning_rate\n",
    "\n",
    "# TODO: Visualiser:\n",
    "# - Graphique 1: Loss vs Epoch (4 courbes)\n",
    "# - Graphique 2: Learning Rate vs Epoch (4 courbes)\n",
    "# - Graphique 3: Test Accuracy finale (bar plot)\n",
    "\n",
    "# TODO: Conclure sur la meilleure strat√©gie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 7 : Weight Initialization\n",
    "\n",
    "**Objectif** : Comprendre l'impact de l'initialisation des poids.\n",
    "\n",
    "**TODO** :\n",
    "1. Tester 4 m√©thodes d'initialisation:\n",
    "   - Zeros (tous les poids √† 0)\n",
    "   - Random uniforme [-1, 1]\n",
    "   - Xavier/Glorot\n",
    "   - He/Kaiming\n",
    "2. Comparer vitesse de convergence\n",
    "3. Analyser la distribution des gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_zeros(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.fill_(0.0)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "def init_weights_uniform(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.uniform_(m.weight, -1, 1)\n",
    "        nn.init.uniform_(m.bias, -1, 1)\n",
    "\n",
    "def init_weights_xavier(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "def init_weights_he(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# TODO: Cr√©er 4 mod√®les identiques et appliquer les initialisations\n",
    "# model1.apply(init_weights_zeros)\n",
    "# model2.apply(init_weights_uniform)\n",
    "# model3.apply(init_weights_xavier)\n",
    "# model4.apply(init_weights_he)\n",
    "\n",
    "# TODO: Entra√Æner les 4 mod√®les et comparer:\n",
    "# - Loss apr√®s 5 epochs (indicateur de vitesse initiale)\n",
    "# - Accuracy finale apr√®s 50 epochs\n",
    "# - Distribution des poids apr√®s initialisation (histogramme)\n",
    "\n",
    "# TODO: Visualiser l'√©volution de la loss pour chaque initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 8 : Mini-Projet - Fashion MNIST\n",
    "\n",
    "**Objectif** : Appliquer toutes les techniques apprises sur Fashion MNIST.\n",
    "\n",
    "**TODO** :\n",
    "1. Charger Fashion MNIST (10 classes de v√™tements)\n",
    "2. Construire architecture optimale (exp√©rimenter)\n",
    "3. Appliquer les meilleures pratiques:\n",
    "   - BatchNorm\n",
    "   - Dropout\n",
    "   - Adam optimizer\n",
    "   - Learning rate scheduler\n",
    "   - Early stopping\n",
    "4. Atteindre >88% accuracy sur test set\n",
    "5. Analyser les erreurs de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement Fashion MNIST\n",
    "fashion_mnist = fetch_openml('Fashion-MNIST', version=1, parser='auto')\n",
    "X_fashion = fashion_mnist.data.astype('float32').values / 255.0  # type: ignore\n",
    "y_fashion = fashion_mnist.target.astype('int').values  # type: ignore\n",
    "\n",
    "# Labels Fashion MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# TODO: Split train/val/test\n",
    "\n",
    "# TODO: Construire votre meilleure architecture MLP\n",
    "# Suggestions:\n",
    "# - 3-4 couches cach√©es\n",
    "# - BatchNorm apr√®s chaque couche\n",
    "# - Dropout 0.3-0.5\n",
    "# - ReLU activation\n",
    "# - Adam optimizer (lr=0.001)\n",
    "# - ReduceLROnPlateau scheduler\n",
    "\n",
    "# TODO: Entra√Æner avec early stopping\n",
    "\n",
    "# TODO: √âvaluation compl√®te:\n",
    "# - Accuracy par classe\n",
    "# - Matrice de confusion\n",
    "# - Classification report\n",
    "# - Visualisation des erreurs\n",
    "\n",
    "# TODO: Analyser:\n",
    "# - Quelles classes sont les plus difficiles?\n",
    "# - Quelles confusions sont fr√©quentes? (ex: Shirt vs T-shirt)\n",
    "# - Visualiser 10 pr√©dictions incorrectes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions et Indices\n",
    "\n",
    "### Exercice 1 - Perceptron\n",
    "```python\n",
    "# Initialisation\n",
    "self.weights = np.random.randn(X.shape[1]) * 0.01\n",
    "self.bias = 0.0\n",
    "\n",
    "# Update rule\n",
    "for epoch in range(self.n_epochs):\n",
    "    for xi, yi in zip(X, y):\n",
    "        z = np.dot(xi, self.weights) + self.bias\n",
    "        pred = 1 if z >= 0 else 0\n",
    "        error = yi - pred\n",
    "        self.weights += self.lr * error * xi\n",
    "        self.bias += self.lr * error\n",
    "```\n",
    "\n",
    "### Exercice 2 - XOR avec MLP\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 2)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "### Exercice 3 - Dropout\n",
    "- Avec Dropout: moins d'overfitting, meilleure g√©n√©ralisation\n",
    "- Sans Dropout: train accuracy > val accuracy (signe d'overfitting)\n",
    "\n",
    "### Exercice 4 - Optimizers\n",
    "- **SGD** : Convergence lente, peut osciller\n",
    "- **SGD + Momentum** : Convergence plus rapide, moins d'oscillations\n",
    "- **Adam** : Convergence la plus rapide, adapte le learning rate\n",
    "\n",
    "### Exercice 8 - Fashion MNIST\n",
    "Architecture sugg√©r√©e:\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    \n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    \n",
    "    nn.Linear(256, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    \n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}