{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/06_reseaux_neurones_fondamentaux/06_demo_mlp_numpy.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '06_demo_mlp_numpy.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 06 - D√©monstration : MLP from Scratch avec NumPy\n",
    "\n",
    "**Objectif** : Impl√©menter un r√©seau de neurones multicouche (MLP) complet en NumPy pur pour comprendre les m√©canismes internes.\n",
    "\n",
    "**Contenu** :\n",
    "1. Forward pass (propagation avant)\n",
    "2. Backward pass (r√©tropropagation)\n",
    "3. Optimisation SGD avec momentum\n",
    "4. Classification MNIST\n",
    "5. Visualisation des poids et d√©cision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fonctions d'activation et leurs d√©riv√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\"Fonctions d'activation et leurs d√©riv√©es.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"Sigmoid: œÉ(z) = 1 / (1 + e^(-z))\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip pour stabilit√©\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(a):\n",
    "        \"\"\"D√©riv√©e: œÉ'(z) = œÉ(z) * (1 - œÉ(z))\"\"\"\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"ReLU: max(0, z)\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(a):\n",
    "        \"\"\"D√©riv√©e: 1 si z > 0, 0 sinon\"\"\"\n",
    "        return (a > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"Softmax: exp(z_i) / sum(exp(z_j))\"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stabilit√© num√©rique\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Test des activations\n",
    "z = np.linspace(-5, 5, 100)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(z, Activation.sigmoid(z), label='Sigmoid')\n",
    "axes[0].plot(z, Activation.sigmoid_derivative(Activation.sigmoid(z)), label=\"D√©riv√©e\", linestyle='--')\n",
    "axes[0].set_title('Sigmoid')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# ReLU\n",
    "axes[1].plot(z, Activation.relu(z), label='ReLU')\n",
    "axes[1].plot(z, Activation.relu_derivative(Activation.relu(z)), label=\"D√©riv√©e\", linestyle='--')\n",
    "axes[1].set_title('ReLU')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Softmax\n",
    "z_soft = np.array([[1, 2, 3], [1, 2, 3]]).T\n",
    "softmax_out = Activation.softmax(z_soft)\n",
    "axes[2].bar(range(3), softmax_out[0])\n",
    "axes[2].set_title('Softmax (z=[1,2,3])')\n",
    "axes[2].set_ylabel('Probabilit√©')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Softmax([1, 2, 3]) = {softmax_out[0]}\")\n",
    "print(f\"Somme = {softmax_out[0].sum():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classe MLP from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"Multi-Layer Perceptron avec r√©tropropagation.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='relu', learning_rate=0.01, momentum=0.9):\n",
    "        \"\"\"\n",
    "        Initialise le MLP.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer_sizes : list\n",
    "            [input_size, hidden1, hidden2, ..., output_size]\n",
    "        activation : str\n",
    "            'relu' ou 'sigmoid'\n",
    "        learning_rate : float\n",
    "            Taux d'apprentissage\n",
    "        momentum : float\n",
    "            Momentum SGD (0.0 = SGD vanilla)\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialisation Xavier/He\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.velocity_w = []  # Pour momentum\n",
    "        self.velocity_b = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Xavier init pour Sigmoid, He init pour ReLU\n",
    "            if activation == 'relu':\n",
    "                scale = np.sqrt(2.0 / layer_sizes[i])\n",
    "            else:\n",
    "                scale = np.sqrt(1.0 / layer_sizes[i])\n",
    "            \n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * scale\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "            self.velocity_w.append(np.zeros_like(w))\n",
    "            self.velocity_b.append(np.zeros_like(b))\n",
    "        \n",
    "        self.history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    def _activate(self, z, layer_idx):\n",
    "        \"\"\"Applique l'activation (softmax pour derni√®re couche).\"\"\"\n",
    "        if layer_idx == len(self.weights) - 1:  # Derni√®re couche\n",
    "            return Activation.softmax(z)\n",
    "        elif self.activation == 'relu':\n",
    "            return Activation.relu(z)\n",
    "        else:\n",
    "            return Activation.sigmoid(z)\n",
    "    \n",
    "    def _activate_derivative(self, a, layer_idx):\n",
    "        \"\"\"Calcule la d√©riv√©e de l'activation.\"\"\"\n",
    "        if layer_idx == len(self.weights) - 1:  # Softmax g√©r√© dans backprop\n",
    "            return 1\n",
    "        elif self.activation == 'relu':\n",
    "            return Activation.relu_derivative(a)\n",
    "        else:\n",
    "            return Activation.sigmoid_derivative(a)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Propagation avant.\"\"\"\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            z = self.activations[-1] @ w + b\n",
    "            a = self._activate(z, i)\n",
    "            \n",
    "            self.z_values.append(z)\n",
    "            self.activations.append(a)\n",
    "        \n",
    "        return self.activations[-1]\n",
    "    \n",
    "    def backward(self, X, y_true):\n",
    "        \"\"\"R√©tropropagation.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        y_pred = self.activations[-1]\n",
    "        \n",
    "        # Gradient de la derni√®re couche (Softmax + Cross-Entropy)\n",
    "        delta = y_pred - y_true  # Simplification √©l√©gante!\n",
    "        \n",
    "        # R√©tropropagation\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # Gradients\n",
    "            grad_w = self.activations[i].T @ delta / m\n",
    "            grad_b = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            \n",
    "            # Mise √† jour avec momentum\n",
    "            self.velocity_w[i] = self.momentum * self.velocity_w[i] - self.learning_rate * grad_w\n",
    "            self.velocity_b[i] = self.momentum * self.velocity_b[i] - self.learning_rate * grad_b\n",
    "            \n",
    "            self.weights[i] += self.velocity_w[i]\n",
    "            self.biases[i] += self.velocity_b[i]\n",
    "            \n",
    "            # Propager le gradient\n",
    "            if i > 0:\n",
    "                delta = (delta @ self.weights[i].T) * self._activate_derivative(self.activations[i], i-1)\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, batch_size=32, X_val=None, y_val=None, verbose=True):\n",
    "        \"\"\"Entra√Æne le MLP.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Mini-batch SGD\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Forward + Backward\n",
    "                self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch)\n",
    "            \n",
    "            # √âvaluation\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                y_pred = self.forward(X)\n",
    "                loss = -np.mean(y * np.log(y_pred + 1e-8))  # Cross-entropy\n",
    "                acc = accuracy_score(np.argmax(y, axis=1), np.argmax(y_pred, axis=1))\n",
    "                \n",
    "                self.history['loss'].append(loss)\n",
    "                self.history['accuracy'].append(acc)\n",
    "                \n",
    "                if verbose:\n",
    "                    val_str = \"\"\n",
    "                    if X_val is not None:\n",
    "                        y_val_pred = self.forward(X_val)\n",
    "                        val_acc = accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1))\n",
    "                        val_str = f\" - Val Acc: {val_acc:.4f}\"\n",
    "                    \n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Acc: {acc:.4f}{val_str}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Pr√©diction (classe la plus probable).\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"Classe MLP impl√©ment√©e avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test sur dataset simple (Make Moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration dataset\n",
    "X_moons, y_moons = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_moons, y_moons, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_onehot = np.eye(2)[y_train]\n",
    "y_test_onehot = np.eye(2)[y_test]\n",
    "\n",
    "# Standardisation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement MLP\n",
    "mlp = MLP(layer_sizes=[2, 16, 16, 2], activation='relu', learning_rate=0.1, momentum=0.9)\n",
    "mlp.fit(X_train_scaled, y_train_onehot, epochs=200, batch_size=32, \n",
    "        X_val=X_test_scaled, y_val=y_test_onehot, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des courbes d'apprentissage\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(range(0, 200, 10), mlp.history['loss'], marker='o')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (Cross-Entropy)')\n",
    "axes[0].set_title('Loss pendant l\\'entra√Ænement')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(range(0, 200, 10), mlp.history['accuracy'], marker='o', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy pendant l\\'entra√Ænement')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision boundary\n",
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"Visualise la fronti√®re de d√©cision du MLP.\"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolors='k', cmap='coolwarm')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.colorbar(label='Classe')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(mlp, X_test_scaled, y_test, title=\"MLP Decision Boundary (Make Moons)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification MNIST (chiffres manuscrits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement MNIST\n",
    "print(\"Chargement MNIST...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X_mnist = mnist.data.astype('float32') / 255.0  # Normalisation [0, 1]  # type: ignore\n",
    "y_mnist = mnist.target.astype('int')  # type: ignore\n",
    "\n",
    "# Sous-√©chantillon pour acc√©l√©rer (10% du dataset)\n",
    "X_mnist_small = X_mnist[:7000]\n",
    "y_mnist_small = y_mnist[:7000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mnist_small, y_mnist_small, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_onehot = np.eye(10)[y_train]\n",
    "y_test_onehot = np.eye(10)[y_test]\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation √©chantillons\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_train[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle('√âchantillons MNIST')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement MLP (784 -> 128 -> 64 -> 10)\n",
    "mlp_mnist = MLP(\n",
    "    layer_sizes=[784, 128, 64, 10], \n",
    "    activation='relu', \n",
    "    learning_rate=0.1, \n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "print(\"\\nEntra√Ænement MLP sur MNIST...\")\n",
    "mlp_mnist.fit(\n",
    "    X_train, y_train_onehot, \n",
    "    epochs=50, \n",
    "    batch_size=64, \n",
    "    X_val=X_test, \n",
    "    y_val=y_test_onehot, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation finale\n",
    "y_pred = mlp_mnist.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy test finale: {test_acc:.4f}\")\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matrice de Confusion - MNIST')\n",
    "plt.xlabel('Pr√©diction')\n",
    "plt.ylabel('V√©rit√©')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation des poids de la premi√®re couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des 64 premiers neurones de la couche cach√©e 1\n",
    "weights_layer1 = mlp_mnist.weights[0]  # Shape: (784, 128)\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < 64:\n",
    "        weight_image = weights_layer1[:, i].reshape(28, 28)\n",
    "        ax.imshow(weight_image, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Poids des 64 premiers neurones (Layer 1)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pr√©dictions sur nouveaux √©chantillons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©diction sur 10 √©chantillons de test\n",
    "n_samples = 10\n",
    "indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = indices[i]\n",
    "    image = X_test[idx].reshape(28, 28)\n",
    "    true_label = y_test[idx]\n",
    "    \n",
    "    # Pr√©diction\n",
    "    pred_probs = mlp_mnist.forward(X_test[idx:idx+1])[0]\n",
    "    pred_label = np.argmax(pred_probs)\n",
    "    confidence = pred_probs[pred_label]\n",
    "    \n",
    "    # Affichage\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f\"True: {true_label} | Pred: {pred_label} ({confidence:.2f})\", color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Points cl√©s** :\n",
    "1. **Forward pass** : Calcul s√©quentiel des activations\n",
    "2. **Backward pass** : R√©tropropagation du gradient avec la chain rule\n",
    "3. **Optimisation SGD + Momentum** : Acc√©l√®re la convergence\n",
    "4. **Initialisation Xavier/He** : Stabilise l'entra√Ænement\n",
    "5. **Softmax + Cross-Entropy** : Simplifie le gradient (y_pred - y_true)\n",
    "\n",
    "**R√©sultats MNIST** :\n",
    "- Accuracy ~95% avec architecture simple (784-128-64-10)\n",
    "- Les poids de la couche 1 apprennent des \"features\" (contours, formes)\n",
    "- Mini-batch SGD acc√©l√®re l'entra√Ænement vs full-batch\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "- Ajouter **Dropout** pour r√©gularisation\n",
    "- Impl√©menter **Batch Normalization**\n",
    "- Utiliser **Adam optimizer** au lieu de SGD\n",
    "- Tester sur datasets plus complexes (CIFAR-10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}