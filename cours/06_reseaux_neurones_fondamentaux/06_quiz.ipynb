{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz d'Auto-√âvaluation - Chapitre 06 : R√©seaux de Neurones Fondamentaux\n",
    "\n",
    "**Instructions** :\n",
    "- Ce quiz contient 15 questions pour tester votre compr√©hension du chapitre\n",
    "- R√©pondez aux questions par vous-m√™me avant de regarder les r√©ponses\n",
    "- Les r√©ponses sont dans une cellule masqu√©e √† la fin\n",
    "- Comptez 1 point par bonne r√©ponse\n",
    "\n",
    "**Bar√®me** :\n",
    "- 13-15 : Excellent ! Vous ma√Ætrisez le chapitre üí™\n",
    "- 10-12 : Bien, relisez les sections o√π vous avez des lacunes\n",
    "- 7-9 : Moyen, relisez le chapitre attentivement\n",
    "- < 7 : Insuffisant, reprenez le chapitre depuis le d√©but\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Question 1 : Perceptron - Limitation\n",
    "Quelle limitation majeure du perceptron a motiv√© le d√©veloppement des r√©seaux multi-couches ?\n",
    "\n",
    "A) Il est trop lent  \n",
    "B) Il ne peut pas r√©soudre le probl√®me XOR (non lin√©airement s√©parable)  \n",
    "C) Il n√©cessite trop de m√©moire  \n",
    "D) Il fonctionne uniquement avec des images  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : Fonctions d'Activation - R√¥le\n",
    "Pourquoi les fonctions d'activation sont-elles essentielles dans les r√©seaux de neurones ?\n",
    "\n",
    "A) Pour acc√©l√©rer l'entra√Ænement  \n",
    "B) Pour introduire la non-lin√©arit√©  \n",
    "C) Pour r√©duire la dimension  \n",
    "D) Pour normaliser les donn√©es  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 : Fonction Sigmoid\n",
    "Quel est le principal PROBL√àME de la fonction sigmoid dans les r√©seaux profonds ?\n",
    "\n",
    "A) Elle est trop rapide √† calculer  \n",
    "B) Elle cause le vanishing gradient pour $|z|$ grand  \n",
    "C) Elle donne toujours la m√™me sortie  \n",
    "D) Elle ne fonctionne que pour la classification binaire  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 : ReLU\n",
    "Pourquoi ReLU est-il la fonction d'activation par d√©faut dans les r√©seaux modernes ?\n",
    "\n",
    "A) Il est le plus complexe √† calculer  \n",
    "B) Il r√©sout compl√®tement le probl√®me du vanishing gradient pour $z > 0$ et est tr√®s rapide  \n",
    "C) Il fonctionne uniquement avec des valeurs positives  \n",
    "D) Il garantit toujours la convergence  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 : Softmax\n",
    "√Ä quoi sert la fonction softmax ?\n",
    "\n",
    "A) √Ä acc√©l√©rer le forward pass  \n",
    "B) √Ä convertir un vecteur de scores en distribution de probabilit√©s pour classification multi-classe  \n",
    "C) √Ä r√©gulariser le mod√®le  \n",
    "D) √Ä normaliser les features  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : Th√©or√®me d'Approximation Universelle\n",
    "Que dit le th√©or√®me d'approximation universelle ?\n",
    "\n",
    "A) Un MLP √† 10 couches est toujours meilleur qu'un √† 2 couches  \n",
    "B) Un MLP avec une seule couche cach√©e suffisante peut approximer n'importe quelle fonction continue  \n",
    "C) Les r√©seaux de neurones convergent toujours  \n",
    "D) Plus de neurones = toujours meilleure performance  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 : Forward Pass\n",
    "Dans le forward pass d'un MLP, pour chaque couche $l$, on calcule (dans l'ordre) :\n",
    "\n",
    "A) Activation ‚Üí Combinaison lin√©aire  \n",
    "B) Combinaison lin√©aire $z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$ ‚Üí Activation $a^{[l]} = \\sigma(z^{[l]})$  \n",
    "C) Gradient ‚Üí Poids  \n",
    "D) Normalisation ‚Üí Activation  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 : Fonction de Co√ªt - Classification Multi-Classe\n",
    "Quelle fonction de co√ªt est utilis√©e pour la classification multi-classe avec softmax ?\n",
    "\n",
    "A) Mean Squared Error (MSE)  \n",
    "B) Categorical Cross-Entropy  \n",
    "C) Hinge Loss  \n",
    "D) R¬≤ Score  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 : Backpropagation - Principe\n",
    "La backpropagation utilise quelle r√®gle math√©matique pour calculer les gradients ?\n",
    "\n",
    "A) La r√®gle de Bayes  \n",
    "B) La r√®gle de la cha√Æne (chain rule)  \n",
    "C) Le th√©or√®me de Pythagore  \n",
    "D) La descente de gradient uniquement  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 : Gradient de Sortie\n",
    "Pour la cross-entropy avec softmax, le gradient de la couche de sortie $\\delta^{[L]}$ a une formule simplifi√©e. Laquelle ?\n",
    "\n",
    "A) $\\delta^{[L]} = \\hat{y}$  \n",
    "B) $\\delta^{[L]} = \\hat{y} - y$ (pr√©diction - v√©rit√©)  \n",
    "C) $\\delta^{[L]} = y - \\hat{y}$  \n",
    "D) $\\delta^{[L]} = \\log(\\hat{y})$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 : Optimiseurs - Adam\n",
    "Pourquoi Adam est-il l'optimiseur par d√©faut en deep learning ?\n",
    "\n",
    "A) Il est le plus simple √† impl√©menter  \n",
    "B) Il combine momentum et adaptation du learning rate par param√®tre (RMSprop)  \n",
    "C) Il ne n√©cessite aucun hyperparam√®tre  \n",
    "D) Il garantit toujours la convergence globale  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12 : Initialisation des Poids\n",
    "Pourquoi NE doit-on JAMAIS initialiser tous les poids √† 0 ?\n",
    "\n",
    "A) Cela ralentit l'entra√Ænement  \n",
    "B) Tous les neurones seraient identiques (sym√©trie) et apprendraient la m√™me chose  \n",
    "C) Cela cause l'explosion des gradients  \n",
    "D) Cela ne fonctionne qu'avec sigmoid  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13 : He Initialization\n",
    "Quelle initialisation est recommand√©e pour les r√©seaux utilisant ReLU ?\n",
    "\n",
    "A) Tous les poids √† 0  \n",
    "B) Xavier/Glorot initialization  \n",
    "C) He initialization : $W \\sim \\mathcal{N}(0, \\frac{2}{d_{l-1}})$  \n",
    "D) Poids uniformes entre -1 et 1  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14 : Dropout\n",
    "Comment fonctionne le dropout ?\n",
    "\n",
    "A) Il supprime des couches enti√®res  \n",
    "B) Il d√©sactive al√©atoirement des neurones pendant l'entra√Ænement avec probabilit√© $p$  \n",
    "C) Il r√©duit le learning rate  \n",
    "D) Il augmente le nombre de neurones  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15 : Diagnostic - Loss ne diminue pas\n",
    "Si la loss ne diminue pas du tout pendant l'entra√Ænement, quelle N'est PAS une cause probable ?\n",
    "\n",
    "A) Learning rate trop faible  \n",
    "B) Mauvaise initialisation  \n",
    "C) Dropout trop √©lev√© (> 0.8)  \n",
    "D) Trop de donn√©es d'entra√Ænement  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Auto-Correction\n",
    "\n",
    "Avant de regarder les r√©ponses, comptez combien de r√©ponses vous avez donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrez vos r√©ponses ici (ex: ['D', 'B', 'A', ...])\n",
    "mes_reponses = []  # TODO: remplir avec vos r√©ponses\n",
    "\n",
    "# R√©ponses correctes (masqu√©es)\n",
    "reponses_correctes = ['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'C', 'B', 'D']\n",
    "\n",
    "if len(mes_reponses) == 15:\n",
    "    score = sum([1 for i, r in enumerate(mes_reponses) if r.upper() == reponses_correctes[i]])\n",
    "    print(f\"Votre score : {score}/15\")\n",
    "    \n",
    "    if score >= 13:\n",
    "        print(\"\\nüéâ Excellent ! Vous ma√Ætrisez le chapitre !\")\n",
    "    elif score >= 10:\n",
    "        print(\"\\n‚úÖ Bien ! Relisez les sections o√π vous avez des lacunes.\")\n",
    "    elif score >= 7:\n",
    "        print(\"\\n‚ö†Ô∏è  Moyen. Relisez le chapitre attentivement.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Insuffisant. Reprenez le chapitre depuis le d√©but.\")\n",
    "    \n",
    "    # Afficher les erreurs\n",
    "    print(\"\\nD√©tail :\")\n",
    "    for i, (ma_rep, bonne_rep) in enumerate(zip(mes_reponses, reponses_correctes), 1):\n",
    "        if ma_rep.upper() == bonne_rep:\n",
    "            print(f\"Q{i}: ‚úì Correct\")\n",
    "        else:\n",
    "            print(f\"Q{i}: ‚úó Votre r√©ponse: {ma_rep}, Correcte: {bonne_rep}\")\n",
    "else:\n",
    "    print(\"Veuillez remplir toutes les r√©ponses (15 lettres A, B, C ou D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Explications des R√©ponses\n",
    "\n",
    "### Q1 : B\n",
    "Le perceptron ne peut r√©soudre que des probl√®mes **lin√©airement s√©parables**. Le probl√®me **XOR** (non-lin√©aire) a motiv√© les r√©seaux multi-couches.\n",
    "\n",
    "### Q2 : B\n",
    "Les fonctions d'activation introduisent la **non-lin√©arit√©**. Sans elles, un MLP multi-couches serait √©quivalent √† une r√©gression lin√©aire.\n",
    "\n",
    "### Q3 : B\n",
    "Sigmoid cause le **vanishing gradient** : pour $|z|$ grand, la d√©riv√©e $\\sigma'(z) \\approx 0$, les gradients deviennent tr√®s petits et l'apprentissage s'arr√™te.\n",
    "\n",
    "### Q4 : B\n",
    "**ReLU** : pas de vanishing gradient pour $z > 0$ (d√©riv√©e = 1), calcul tr√®s rapide ($\\max(0, z)$), devient la norme.\n",
    "\n",
    "### Q5 : B\n",
    "**Softmax** convertit un vecteur de scores en **distribution de probabilit√©s** : $\\sum_k \\text{softmax}(z)_k = 1$, utilis√© pour classification multi-classe.\n",
    "\n",
    "### Q6 : B\n",
    "Th√©or√®me : un MLP avec **une seule couche cach√©e** de taille suffisante peut approximer **n'importe quelle fonction continue**. En pratique, les r√©seaux profonds sont plus efficaces.\n",
    "\n",
    "### Q7 : B\n",
    "Forward pass : **Combinaison lin√©aire** $z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$ puis **activation** $a^{[l]} = \\sigma(z^{[l]})$.\n",
    "\n",
    "### Q8 : B\n",
    "**Categorical Cross-Entropy** : $L = -\\frac{1}{m} \\sum_i \\sum_k y_{i,k} \\log(\\hat{y}_{i,k})$. Utilis√©e avec softmax pour multi-classe.\n",
    "\n",
    "### Q9 : B\n",
    "Backpropagation utilise la **r√®gle de la cha√Æne** (chain rule) pour propager les gradients de la sortie vers l'entr√©e.\n",
    "\n",
    "### Q10 : B\n",
    "Formule simplifi√©e (cross-entropy + softmax) : $\\delta^{[L]} = \\hat{y} - y$ (pr√©diction - v√©rit√©). Tr√®s √©l√©gant !\n",
    "\n",
    "### Q11 : B\n",
    "**Adam** combine **momentum** (1er moment) et **RMSprop** (2√®me moment, adaptation par param√®tre). Robuste et peu de tuning n√©cessaire.\n",
    "\n",
    "### Q12 : B\n",
    "Si tous les poids = 0, tous les neurones sont **identiques** (sym√©trie) et apprendraient exactement la m√™me chose. Il faut briser la sym√©trie.\n",
    "\n",
    "### Q13 : C\n",
    "**He initialization** pour ReLU : $W \\sim \\mathcal{N}(0, \\frac{2}{d_{l-1}})$. Xavier/Glorot pour sigmoid/tanh.\n",
    "\n",
    "### Q14 : B\n",
    "**Dropout** : d√©sactive al√©atoirement des neurones avec probabilit√© $p$ (typiquement 0.5) pendant l'entra√Ænement. R√©duit overfitting.\n",
    "\n",
    "### Q15 : D\n",
    "**Trop de donn√©es** n'emp√™che PAS l'apprentissage, au contraire ! Causes probables : LR trop faible, mauvaise init, architecture inadapt√©e, dropout trop fort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaines √âtapes\n",
    "\n",
    "- **Score < 10** : Relisez le chapitre 06 attentivement\n",
    "- **Score >= 10** : Passez au Chapitre 07 (Deep Learning : CNN)\n",
    "- **R√©vision recommand√©e** : Refaites le quiz dans 2-3 jours pour ancrer les connaissances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
