{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/03_regression/03_exercices.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '03_exercices.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 03 - Exercices de R√©gression\n",
    "\n",
    "Ce notebook contient des exercices pratiques sur la r√©gression lin√©aire, polynomiale et la r√©gularisation.\n",
    "\n",
    "## Objectifs\n",
    "- Appliquer la r√©gression lin√©aire sur des donn√©es r√©elles\n",
    "- Diagnostiquer les probl√®mes de r√©gression\n",
    "- Utiliser la r√©gularisation pour am√©liorer les mod√®les\n",
    "- Comparer diff√©rentes approches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 : R√©gression Lin√©aire sur le Dataset California Housing\n",
    "\n",
    "**Objectif** : Pr√©dire le prix m√©dian des maisons en Californie.\n",
    "\n",
    "**Consignes** :\n",
    "1. Charger le dataset California Housing\n",
    "2. Explorer les donn√©es (statistiques descriptives, corr√©lations)\n",
    "3. Entra√Æner un mod√®le de r√©gression lin√©aire\n",
    "4. √âvaluer les performances (MSE, RMSE, R¬≤, MAE)\n",
    "5. Analyser les r√©sidus\n",
    "6. Identifier les features les plus importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chargement des donn√©es\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = housing.target  # Prix m√©dian des maisons (en 100k$)\n",
    "\n",
    "print(f\"Shape: {X.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "print(f\"\\nTarget range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Exploration des donn√©es\n",
    "print(\"Statistiques descriptives:\")\n",
    "print(X.describe())\n",
    "\n",
    "# Matrice de corr√©lation\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = X.corrwith(pd.Series(y, name='Target')).sort_values(ascending=False)\n",
    "print(\"\\nCorr√©lations avec la target:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(X.columns):\n",
    "    axes[idx].scatter(X[col], y, alpha=0.3, s=5)\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Price')\n",
    "    axes[idx].set_title(f'Corr: {X[col].corr(pd.Series(y)):.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pr√©paration et entra√Ænement\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardisation (importante pour la r√©gression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entra√Ænement\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Mod√®le entra√Æn√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. √âvaluation des performances\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# M√©triques\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Performances du mod√®le:\")\n",
    "print(f\"\\nTrain RMSE: {np.sqrt(train_mse):.4f}\")\n",
    "print(f\"Test RMSE:  {np.sqrt(test_mse):.4f}\")\n",
    "print(f\"\\nTrain R¬≤: {train_r2:.4f}\")\n",
    "print(f\"Test R¬≤:  {test_r2:.4f}\")\n",
    "print(f\"\\nTest MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "\n",
    "# Visualisation pr√©dictions vs r√©alit√©\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_train_pred, alpha=0.3, s=10)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "plt.xlabel('Valeurs r√©elles')\n",
    "plt.ylabel('Pr√©dictions')\n",
    "plt.title(f'Train Set (R¬≤={train_r2:.3f})')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.3, s=10)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "plt.xlabel('Valeurs r√©elles')\n",
    "plt.ylabel('Pr√©dictions')\n",
    "plt.title(f'Test Set (R¬≤={test_r2:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Analyse des r√©sidus\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# R√©sidus vs pr√©dictions\n",
    "axes[0, 0].scatter(y_test_pred, residuals, alpha=0.3, s=10)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Pr√©dictions')\n",
    "axes[0, 0].set_ylabel('R√©sidus')\n",
    "axes[0, 0].set_title('R√©sidus vs Pr√©dictions')\n",
    "\n",
    "# Distribution des r√©sidus\n",
    "axes[0, 1].hist(residuals, bins=50, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('R√©sidus')\n",
    "axes[0, 1].set_ylabel('Fr√©quence')\n",
    "axes[0, 1].set_title('Distribution des R√©sidus')\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot')\n",
    "\n",
    "# R√©sidus absolus vs pr√©dictions\n",
    "axes[1, 1].scatter(y_test_pred, np.abs(residuals), alpha=0.3, s=10)\n",
    "axes[1, 1].set_xlabel('Pr√©dictions')\n",
    "axes[1, 1].set_ylabel('|R√©sidus|')\n",
    "axes[1, 1].set_title('R√©sidus Absolus vs Pr√©dictions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Moyenne des r√©sidus: {residuals.mean():.6f}\")\n",
    "print(f\"√âcart-type des r√©sidus: {residuals.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Importance des features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_\n",
    "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Importance des features (coefficients):\")\n",
    "print(feature_importance)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'])\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Importance des Features (R√©gression Lin√©aire)')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : R√©gression Polynomiale et Overfitting\n",
    "\n",
    "**Objectif** : Explorer l'impact du degr√© polynomial sur les performances.\n",
    "\n",
    "**Consignes** :\n",
    "1. Cr√©er un dataset synth√©tique avec bruit\n",
    "2. Tester des r√©gressions polynomiales de degr√©s 1, 3, 5, 10, 15\n",
    "3. Comparer les performances train/test\n",
    "4. Visualiser l'overfitting\n",
    "5. Identifier le degr√© optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cr√©ation du dataset synth√©tique\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X_synth = np.sort(np.random.uniform(-3, 3, n_samples))\n",
    "y_true = np.sin(X_synth) + 0.5 * X_synth  # Fonction vraie\n",
    "y_synth = y_true + np.random.normal(0, 0.5, n_samples)  # Avec bruit\n",
    "\n",
    "X_synth = X_synth.reshape(-1, 1)\n",
    "\n",
    "# Split\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(\n",
    "    X_synth, y_synth, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_s_train, y_s_train, alpha=0.6, label='Train', s=50)\n",
    "plt.scatter(X_s_test, y_s_test, alpha=0.6, label='Test', s=50)\n",
    "plt.plot(np.sort(X_synth, axis=0), np.sin(np.sort(X_synth, axis=0)) + 0.5 * np.sort(X_synth, axis=0), \n",
    "         'r--', label='Fonction vraie', linewidth=2)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Dataset Synth√©tique')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3. Test de diff√©rents degr√©s polynomiaux\n",
    "degrees = [1, 3, 5, 10, 15]\n",
    "results = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "X_plot = np.linspace(-3, 3, 300).reshape(-1, 1)\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    # Pipeline: Polynomial Features + Linear Regression\n",
    "    pipeline = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_s_train, y_s_train)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_train_pred = pipeline.predict(X_s_train)\n",
    "    y_test_pred = pipeline.predict(X_s_test)\n",
    "    y_plot = pipeline.predict(X_plot)\n",
    "    \n",
    "    # M√©triques\n",
    "    train_mse = mean_squared_error(y_s_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_s_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_s_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_s_test, y_test_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Degree': degree,\n",
    "        'Train RMSE': np.sqrt(train_mse),\n",
    "        'Test RMSE': np.sqrt(test_mse),\n",
    "        'Train R¬≤': train_r2,\n",
    "        'Test R¬≤': test_r2\n",
    "    })\n",
    "    \n",
    "    # Visualisation\n",
    "    axes[idx].scatter(X_s_train, y_s_train, alpha=0.6, label='Train', s=30)\n",
    "    axes[idx].scatter(X_s_test, y_s_test, alpha=0.6, label='Test', s=30)\n",
    "    axes[idx].plot(X_plot, y_plot, 'g-', label='Mod√®le', linewidth=2)\n",
    "    axes[idx].plot(np.sort(X_synth, axis=0), \n",
    "                   np.sin(np.sort(X_synth, axis=0)) + 0.5 * np.sort(X_synth, axis=0),\n",
    "                   'r--', label='Vrai', linewidth=1, alpha=0.7)\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].set_title(f'Degr√© {degree}\\nTest R¬≤={test_r2:.3f}, RMSE={np.sqrt(test_mse):.3f}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].set_ylim(-5, 5)\n",
    "\n",
    "# Supprimer le dernier subplot vide\n",
    "fig.delaxes(axes[-1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# R√©sultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nR√©sultats par degr√© polynomial:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualisation de l'overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "axes[0].plot(results_df['Degree'], results_df['Train RMSE'], 'o-', label='Train RMSE', linewidth=2)\n",
    "axes[0].plot(results_df['Degree'], results_df['Test RMSE'], 's-', label='Test RMSE', linewidth=2)\n",
    "axes[0].set_xlabel('Degr√© Polynomial')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE vs Degr√© Polynomial')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤\n",
    "axes[1].plot(results_df['Degree'], results_df['Train R¬≤'], 'o-', label='Train R¬≤', linewidth=2)\n",
    "axes[1].plot(results_df['Degree'], results_df['Test R¬≤'], 's-', label='Test R¬≤', linewidth=2)\n",
    "axes[1].set_xlabel('Degr√© Polynomial')\n",
    "axes[1].set_ylabel('R¬≤')\n",
    "axes[1].set_title('R¬≤ vs Degr√© Polynomial')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Identification du degr√© optimal\n",
    "best_idx = results_df['Test R¬≤'].idxmax()\n",
    "best_degree = results_df.loc[best_idx, 'Degree']\n",
    "\n",
    "print(f\"\\nDegr√© optimal: {best_degree}\")\n",
    "print(\"\\nPerformances du meilleur mod√®le:\")\n",
    "print(results_df.loc[best_idx].to_string())\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"- Degr√© 1 (lin√©aire): Underfitting - trop simple\")\n",
    "print(\"- Degr√©s 3-5: Bon compromis biais-variance\")\n",
    "print(\"- Degr√©s 10-15: Overfitting - trop flexible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 : R√©gularisation (Ridge, Lasso, ElasticNet)\n",
    "\n",
    "**Objectif** : Utiliser la r√©gularisation pour contr√¥ler l'overfitting.\n",
    "\n",
    "**Consignes** :\n",
    "1. Utiliser le dataset Diabetes de sklearn\n",
    "2. Cr√©er des features polynomiales (degr√© 3)\n",
    "3. Comparer Linear, Ridge, Lasso, ElasticNet\n",
    "4. Optimiser les hyperparam√®tres alpha\n",
    "5. Analyser la s√©lection de features par Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chargement du dataset Diabetes\n",
    "diabetes = load_diabetes()\n",
    "X_diab = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y_diab = diabetes.target\n",
    "\n",
    "print(f\"Shape: {X_diab.shape}\")\n",
    "print(f\"Features: {list(X_diab.columns)}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"Mean: {y_diab.mean():.2f}, Std: {y_diab.std():.2f}\")\n",
    "print(f\"Range: [{y_diab.min():.2f}, {y_diab.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cr√©ation de features polynomiales (degr√© 3)\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_diab_poly = poly.fit_transform(X_diab)\n",
    "\n",
    "print(f\"Nombre de features originales: {X_diab.shape[1]}\")\n",
    "print(f\"Nombre de features polynomiales: {X_diab_poly.shape[1]}\")\n",
    "\n",
    "# Split\n",
    "X_d_train, X_d_test, y_d_train, y_d_test = train_test_split(\n",
    "    X_diab_poly, y_diab, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardisation\n",
    "scaler_d = StandardScaler()\n",
    "X_d_train_scaled = scaler_d.fit_transform(X_d_train)\n",
    "X_d_test_scaled = scaler_d.transform(X_d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Comparaison des mod√®les\n",
    "models = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'Ridge (Œ±=1)': Ridge(alpha=1.0),\n",
    "    'Ridge (Œ±=10)': Ridge(alpha=10.0),\n",
    "    'Lasso (Œ±=1)': Lasso(alpha=1.0, max_iter=10000),\n",
    "    'Lasso (Œ±=5)': Lasso(alpha=5.0, max_iter=10000),\n",
    "    'ElasticNet (Œ±=1)': ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=10000)\n",
    "}\n",
    "\n",
    "results_reg = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Entra√Ænement\n",
    "    model.fit(X_d_train_scaled, y_d_train)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_train_pred = model.predict(X_d_train_scaled)\n",
    "    y_test_pred = model.predict(X_d_test_scaled)\n",
    "    \n",
    "    # M√©triques\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_d_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_d_test, y_test_pred))\n",
    "    train_r2 = r2_score(y_d_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_d_test, y_test_pred)\n",
    "    \n",
    "    # Nombre de coefficients non nuls\n",
    "    non_zero_coefs = np.sum(np.abs(model.coef_) > 1e-10)\n",
    "    \n",
    "    results_reg.append({\n",
    "        'Model': name,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Train R¬≤': train_r2,\n",
    "        'Test R¬≤': test_r2,\n",
    "        'Non-Zero Coefs': non_zero_coefs\n",
    "    })\n",
    "\n",
    "results_reg_df = pd.DataFrame(results_reg)\n",
    "print(\"Comparaison des mod√®les de r√©gularisation:\")\n",
    "print(results_reg_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des r√©sultats\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "x_pos = np.arange(len(results_reg_df))\n",
    "\n",
    "# RMSE\n",
    "width = 0.35\n",
    "axes[0].bar(x_pos - width/2, results_reg_df['Train RMSE'], width, label='Train', alpha=0.8)\n",
    "axes[0].bar(x_pos + width/2, results_reg_df['Test RMSE'], width, label='Test', alpha=0.8)\n",
    "axes[0].set_xlabel('Mod√®le')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE par Mod√®le')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(results_reg_df['Model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤\n",
    "axes[1].bar(x_pos - width/2, results_reg_df['Train R¬≤'], width, label='Train', alpha=0.8)\n",
    "axes[1].bar(x_pos + width/2, results_reg_df['Test R¬≤'], width, label='Test', alpha=0.8)\n",
    "axes[1].set_xlabel('Mod√®le')\n",
    "axes[1].set_ylabel('R¬≤')\n",
    "axes[1].set_title('R¬≤ par Mod√®le')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(results_reg_df['Model'], rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Optimisation des hyperparam√®tres avec courbes d'apprentissage\n",
    "alphas = np.logspace(-3, 2, 50)\n",
    "ridge_scores = []\n",
    "lasso_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Ridge\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_d_train_scaled, y_d_train)\n",
    "    ridge_scores.append(r2_score(y_d_test, ridge.predict(X_d_test_scaled)))\n",
    "    \n",
    "    # Lasso\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_d_train_scaled, y_d_train)\n",
    "    lasso_scores.append(r2_score(y_d_test, lasso.predict(X_d_test_scaled)))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogx(alphas, ridge_scores, 'o-', linewidth=2)\n",
    "best_alpha_ridge = alphas[np.argmax(ridge_scores)]\n",
    "plt.axvline(best_alpha_ridge, color='r', linestyle='--', label=f'Best Œ±={best_alpha_ridge:.3f}')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('R¬≤ Score (Test)')\n",
    "plt.title('Ridge: R¬≤ vs Alpha')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogx(alphas, lasso_scores, 'o-', linewidth=2)\n",
    "best_alpha_lasso = alphas[np.argmax(lasso_scores)]\n",
    "plt.axvline(best_alpha_lasso, color='r', linestyle='--', label=f'Best Œ±={best_alpha_lasso:.3f}')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('R¬≤ Score (Test)')\n",
    "plt.title('Lasso: R¬≤ vs Alpha')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Meilleur alpha Ridge: {best_alpha_ridge:.4f} (R¬≤={max(ridge_scores):.4f})\")\n",
    "print(f\"Meilleur alpha Lasso: {best_alpha_lasso:.4f} (R¬≤={max(lasso_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Analyse de la s√©lection de features par Lasso\n",
    "# Entra√Æner Lasso avec diff√©rents alphas\n",
    "alphas_lasso = [0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "lasso_models = {}\n",
    "\n",
    "for alpha in alphas_lasso:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_d_train_scaled, y_d_train)\n",
    "    lasso_models[alpha] = lasso\n",
    "\n",
    "# Visualisation des coefficients\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for alpha, model in lasso_models.items():\n",
    "    non_zero = np.sum(np.abs(model.coef_) > 1e-10)\n",
    "    plt.plot(model.coef_, alpha=0.7, marker='o', markersize=2, \n",
    "             label=f'Œ±={alpha} ({non_zero} features)')\n",
    "\n",
    "plt.xlabel('Index de Feature')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.title('Coefficients Lasso pour diff√©rents Alpha')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau r√©sum√©\n",
    "print(\"\\nS√©lection de features par Lasso:\")\n",
    "for alpha, model in lasso_models.items():\n",
    "    non_zero = np.sum(np.abs(model.coef_) > 1e-10)\n",
    "    test_r2 = r2_score(y_d_test, model.predict(X_d_test_scaled))\n",
    "    print(f\"Alpha={alpha:5.1f}: {non_zero:3d}/{len(model.coef_)} features, Test R¬≤={test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 : Learning Curves et Cross-Validation\n",
    "\n",
    "**Objectif** : Diagnostiquer les probl√®mes de biais/variance avec les courbes d'apprentissage.\n",
    "\n",
    "**Consignes** :\n",
    "1. Utiliser le dataset California Housing\n",
    "2. G√©n√©rer des learning curves pour Linear, Ridge, Polynomial (degr√© 5)\n",
    "3. Utiliser la validation crois√©e pour estimer les performances\n",
    "4. Identifier les probl√®mes de biais/variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pr√©paration des donn√©es\n",
    "X_lc = X[:5000]  # Sous-√©chantillon pour vitesse\n",
    "y_lc = y[:5000]\n",
    "\n",
    "X_lc_train, X_lc_test, y_lc_train, y_lc_test = train_test_split(\n",
    "    X_lc, y_lc, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_lc = StandardScaler()\n",
    "X_lc_train_scaled = scaler_lc.fit_transform(X_lc_train)\n",
    "X_lc_test_scaled = scaler_lc.transform(X_lc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. G√©n√©ration des learning curves\n",
    "models_lc = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'Ridge (Œ±=10)': Ridge(alpha=10.0),\n",
    "    'Polynomial (d=5)': Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=5)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate(models_lc.items()):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X_lc_train_scaled, y_lc_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convertir en RMSE\n",
    "    train_scores = np.sqrt(-train_scores)\n",
    "    val_scores = np.sqrt(-val_scores)\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    axes[idx].plot(train_sizes, train_mean, 'o-', label='Train', linewidth=2)\n",
    "    axes[idx].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
    "    \n",
    "    axes[idx].plot(train_sizes, val_mean, 's-', label='Validation', linewidth=2)\n",
    "    axes[idx].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2)\n",
    "    \n",
    "    axes[idx].set_xlabel('Taille du Training Set')\n",
    "    axes[idx].set_ylabel('RMSE')\n",
    "    axes[idx].set_title(f'Learning Curve: {name}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Validation crois√©e\n",
    "print(\"Scores de validation crois√©e (5-fold):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models_lc.items():\n",
    "    scores = cross_val_score(model, X_lc_train_scaled, y_lc_train, \n",
    "                             cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  RMSE moyen: {rmse_scores.mean():.4f} (+/- {rmse_scores.std():.4f})\")\n",
    "    print(f\"  RMSE par fold: {[f'{s:.4f}' for s in rmse_scores]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Diagnostic biais-variance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC BIAIS-VARIANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "1. Linear Regression:\n",
    "   - Train RMSE √©lev√©, proche de Val RMSE\n",
    "   - Les courbes convergent rapidement\n",
    "   - Diagnostic: UNDERFITTING (biais √©lev√©)\n",
    "   - Solution: Ajouter des features ou utiliser un mod√®le plus complexe\n",
    "\n",
    "2. Ridge (Œ±=10):\n",
    "   - Train RMSE l√©g√®rement plus √©lev√© que Linear\n",
    "   - Val RMSE similaire ou l√©g√®rement meilleur\n",
    "   - Diagnostic: BON COMPROMIS (r√©gularisation appropri√©e)\n",
    "   - Solution: Optimiser alpha pour am√©liorer l√©g√®rement\n",
    "\n",
    "3. Polynomial (d=5):\n",
    "   - Train RMSE tr√®s faible\n",
    "   - Grand √©cart entre Train et Val RMSE\n",
    "   - Les courbes ne convergent pas\n",
    "   - Diagnostic: OVERFITTING (variance √©lev√©e)\n",
    "   - Solution: R√©gularisation ou r√©duire la complexit√©\n",
    "\"\"\")\n",
    "\n",
    "print(\"Points cl√©s:\")\n",
    "print(\"- Biais √©lev√©: Train et Val RMSE √©lev√©s, proches l'un de l'autre\")\n",
    "print(\"- Variance √©lev√©e: Train RMSE bas, Val RMSE √©lev√©, grand √©cart\")\n",
    "print(\"- Bon mod√®le: Train et Val RMSE proches et relativement bas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©capitulatif\n",
    "\n",
    "### Points cl√©s abord√©s\n",
    "\n",
    "1. **R√©gression Lin√©aire**\n",
    "   - Analyse exploratoire des donn√©es\n",
    "   - Entra√Ænement et √©valuation (MSE, RMSE, R¬≤, MAE)\n",
    "   - Diagnostic des r√©sidus\n",
    "   - Importance des features\n",
    "\n",
    "2. **R√©gression Polynomiale**\n",
    "   - Impact du degr√© sur les performances\n",
    "   - Visualisation de l'overfitting\n",
    "   - Compromis biais-variance\n",
    "\n",
    "3. **R√©gularisation**\n",
    "   - Ridge (L2): P√©nalit√© sur la magnitude des coefficients\n",
    "   - Lasso (L1): S√©lection de features automatique\n",
    "   - ElasticNet: Combinaison L1 + L2\n",
    "   - Optimisation des hyperparam√®tres\n",
    "\n",
    "4. **Diagnostic et Validation**\n",
    "   - Learning curves pour d√©tecter biais/variance\n",
    "   - Validation crois√©e pour estimer les performances\n",
    "   - Identification des probl√®mes et solutions\n",
    "\n",
    "### Recommandations pratiques\n",
    "\n",
    "1. Toujours explorer les donn√©es avant de mod√©liser\n",
    "2. Standardiser les features pour la r√©gression\n",
    "3. Analyser les r√©sidus pour valider les hypoth√®ses\n",
    "4. Utiliser la validation crois√©e pour √©valuer\n",
    "5. Choisir la r√©gularisation adapt√©e au probl√®me:\n",
    "   - Ridge: Multicollin√©arit√©, toutes les features utiles\n",
    "   - Lasso: S√©lection de features, sparsit√©\n",
    "   - ElasticNet: Compromis Ridge/Lasso\n",
    "\n",
    "### M√©triques de r√©gression\n",
    "\n",
    "- **MSE/RMSE**: Sensible aux outliers, m√™me unit√© que la target\n",
    "- **MAE**: Moins sensible aux outliers\n",
    "- **R¬≤**: Proportion de variance expliqu√©e (0-1, peut √™tre n√©gatif)\n",
    "- **Adjusted R¬≤**: P√©nalise le nombre de features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}