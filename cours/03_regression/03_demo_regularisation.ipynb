{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/03_regression/03_demo_regularisation.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '03_demo_regularisation.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 03 - R√©gularisation (Ridge, Lasso, Elastic Net)\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre le probl√®me du surapprentissage (overfitting)\n",
    "- Ma√Ætriser la r√©gularisation L2 (Ridge)\n",
    "- Ma√Ætriser la r√©gularisation L1 (Lasso)\n",
    "- Comprendre Elastic Net (L1 + L2)\n",
    "- S√©lectionner l'hyperparam√®tre optimal (Œª)\n",
    "- Comparer les m√©thodes sur des cas pratiques\n",
    "\n",
    "**Pr√©requis :** 03_demo_regression_lineaire.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.data  # type: ignoresets import make_regression, load_diabetes\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration\nnp.random.seed(42)\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"‚úÖ Imports r√©ussis\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probl√®me du Surapprentissage\n",
    "\n",
    "### 1.1 G√©n√©ration de donn√©es avec multicollin√©arit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer dataset avec features corr√©l√©es et bruit\n",
    "X, y = make_regression(\n",
    "    n_samples=100, \n",
    "    n_features=50,  # Beaucoup de features\n",
    "    n_informative=10,  # Seulement 10 sont utiles\n",
    "    noise=10, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Normalisation (cruciale pour la r√©gularisation)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"=== Donn√©es G√©n√©r√©es ===\")\n",
    "print(f\"√âchantillons : {X.shape[0]}\")\n",
    "print(f\"Features : {X.shape[1]}\")\n",
    "print(f\"Features informatives : 10\")\n",
    "print(f\"\\nTrain : {X_train.shape[0]} √©chantillons\")\n",
    "print(f\"Test  : {X_test.shape[0]} √©chantillons\")\n",
    "print(f\"\\n‚ö†Ô∏è  Ratio √©chantillons/features : {X_train.shape[0]/X_train.shape[1]:.2f}\")\n",
    "print(\"(Risque de surapprentissage si < 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 R√©gression Lin√©aire Sans R√©gularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©gression lin√©aire classique\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_train_pred_lr = lr.predict(X_train_scaled)\n",
    "y_test_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "# M√©triques\n",
    "train_r2_lr = r2_score(y_train, y_train_pred_lr)\n",
    "test_r2_lr = r2_score(y_test, y_test_pred_lr)\n",
    "train_rmse_lr = np.sqrt(mean_squared_error(y_train, y_train_pred_lr))\n",
    "test_rmse_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))\n",
    "\n",
    "print(\"=== R√©gression Lin√©aire (Sans R√©gularisation) ===\")\n",
    "print(f\"Train R¬≤ : {train_r2_lr:.4f}\")\n",
    "print(f\"Test R¬≤  : {test_r2_lr:.4f}\")\n",
    "print(f\"√âcart    : {abs(train_r2_lr - test_r2_lr):.4f}\")\n",
    "print(f\"\\nTrain RMSE : {train_rmse_lr:.2f}\")\n",
    "print(f\"Test RMSE  : {test_rmse_lr:.2f}\")\n",
    "\n",
    "# Analyse des coefficients\n",
    "print(f\"\\n=== Analyse des Coefficients ===\")\n",
    "print(f\"Nombre de coefficients : {len(lr.coef_)}\")\n",
    "print(f\"Max coefficient : {np.max(np.abs(lr.coef_)):.2f}\")\n",
    "print(f\"Coefficients > 50 : {np.sum(np.abs(lr.coef_) > 50)}\")\n",
    "\n",
    "if abs(train_r2_lr - test_r2_lr) > 0.1:\n",
    "    print(\"\\n‚ö†Ô∏è  SURAPPRENTISSAGE D√âTECT√â !\")\n",
    "    print(\"Solution : R√©gularisation (Ridge, Lasso, Elastic Net)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. R√©gularisation Ridge (L2)\n",
    "\n",
    "**Formule :** Minimiser $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^p w_j^2$\n",
    "\n",
    "**Effet :** R√©duit la magnitude des coefficients (shrinkage)\n",
    "\n",
    "### 2.1 Ridge avec diff√©rentes valeurs de Œª (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester plusieurs valeurs de alpha (Œª)\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "results_ridge = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_r2 = ridge.score(X_train_scaled, y_train)\n",
    "    test_r2 = ridge.score(X_test_scaled, y_test)\n",
    "    \n",
    "    results_ridge.append({\n",
    "        'alpha': alpha,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'diff': abs(train_r2 - test_r2),\n",
    "        'max_coef': np.max(np.abs(ridge.coef_))\n",
    "    })\n",
    "\n",
    "df_ridge = pd.DataFrame(results_ridge)\n",
    "print(\"=== Ridge : Impact de Alpha ===\")\n",
    "print(df_ridge.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1 : R¬≤ vs alpha\n",
    "axes[0].plot(df_ridge['alpha'], df_ridge['train_r2'], 'o-', label='Train R¬≤', linewidth=2)\n",
    "axes[0].plot(df_ridge['alpha'], df_ridge['test_r2'], 's-', label='Test R¬≤', linewidth=2)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Alpha (Œª)')\n",
    "axes[0].set_ylabel('R¬≤')\n",
    "axes[0].set_title('Ridge : Performance vs Alpha')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2 : Max coefficient vs alpha\n",
    "axes[1].plot(df_ridge['alpha'], df_ridge['max_coef'], 'o-', linewidth=2, color='green')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Alpha (Œª)')\n",
    "axes[1].set_ylabel('Max |Coefficient|')\n",
    "axes[1].set_title('Ridge : Shrinkage des Coefficients')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation : Alpha‚Üë ‚Üí Coefficients‚Üì ‚Üí Moins de surapprentissage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Ridge optimal avec Validation Crois√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeCV : trouve automatiquement le meilleur alpha\n",
    "alphas_cv = np.logspace(-3, 3, 50)\n",
    "ridge_cv = RidgeCV(alphas=alphas_cv, cv=5, scoring='r2')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_alpha_ridge = ridge_cv.alpha_\n",
    "test_r2_ridge = ridge_cv.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"=== Ridge Optimal (avec CV) ===\")\n",
    "print(f\"Meilleur alpha : {best_alpha_ridge:.4f}\")\n",
    "print(f\"Test R¬≤ : {test_r2_ridge:.4f}\")\n",
    "print(f\"\\nComparaison :\")\n",
    "print(f\"  Sans r√©gularisation : R¬≤={test_r2_lr:.4f}\")\n",
    "print(f\"  Avec Ridge optimal  : R¬≤={test_r2_ridge:.4f}\")\n",
    "print(f\"  Am√©lioration : {(test_r2_ridge - test_r2_lr)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. R√©gularisation Lasso (L1)\n",
    "\n",
    "**Formule :** Minimiser $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^p |w_j|$\n",
    "\n",
    "**Effet :** Met certains coefficients exactement √† 0 (s√©lection de features)\n",
    "\n",
    "### 3.1 Lasso avec diff√©rentes valeurs de Œª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester plusieurs valeurs de alpha\n",
    "results_lasso = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_r2 = lasso.score(X_train_scaled, y_train)\n",
    "    test_r2 = lasso.score(X_test_scaled, y_test)\n",
    "    n_nonzero = np.sum(lasso.coef_ != 0)\n",
    "    \n",
    "    results_lasso.append({\n",
    "        'alpha': alpha,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'features_selected': n_nonzero,\n",
    "        'sparsity': f\"{n_nonzero}/{len(lasso.coef_)}\"\n",
    "    })\n",
    "\n",
    "df_lasso = pd.DataFrame(results_lasso)\n",
    "print(\"=== Lasso : Impact de Alpha ===\")\n",
    "print(df_lasso.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1 : R¬≤ vs alpha\n",
    "axes[0].plot(df_lasso['alpha'], df_lasso['train_r2'], 'o-', label='Train R¬≤', linewidth=2)\n",
    "axes[0].plot(df_lasso['alpha'], df_lasso['test_r2'], 's-', label='Test R¬≤', linewidth=2)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Alpha (Œª)')\n",
    "axes[0].set_ylabel('R¬≤')\n",
    "axes[0].set_title('Lasso : Performance vs Alpha')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2 : Nombre de features s√©lectionn√©es\n",
    "axes[1].plot(df_lasso['alpha'], df_lasso['features_selected'], 'o-', linewidth=2, color='purple')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Alpha (Œª)')\n",
    "axes[1].set_ylabel('Features S√©lectionn√©es')\n",
    "axes[1].set_title('Lasso : S√©lection de Features')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ú® Avantage Lasso : S√©lection automatique de features (certains coef = 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lasso optimal avec Validation Crois√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV : trouve automatiquement le meilleur alpha\n",
    "lasso_cv = LassoCV(alphas=alphas_cv, cv=5, max_iter=10000, random_state=42)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_alpha_lasso = lasso_cv.alpha_\n",
    "test_r2_lasso = lasso_cv.score(X_test_scaled, y_test)\n",
    "n_selected = np.sum(lasso_cv.coef_ != 0)\n",
    "\n",
    "print(\"=== Lasso Optimal (avec CV) ===\")\n",
    "print(f\"Meilleur alpha : {best_alpha_lasso:.4f}\")\n",
    "print(f\"Test R¬≤ : {test_r2_lasso:.4f}\")\n",
    "print(f\"Features s√©lectionn√©es : {n_selected}/{len(lasso_cv.coef_)}\")\n",
    "print(f\"\\nComparaison :\")\n",
    "print(f\"  Sans r√©gularisation : R¬≤={test_r2_lr:.4f}, features={X.shape[1]}\")\n",
    "print(f\"  Avec Lasso optimal  : R¬≤={test_r2_lasso:.4f}, features={n_selected}\")\n",
    "\n",
    "# Visualiser les coefficients\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.stem(range(len(lasso_cv.coef_)), lasso_cv.coef_, basefmt=\" \")\n",
    "plt.xlabel('Index de Feature')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.title(f'Lasso : Coefficients (alpha={best_alpha_lasso:.4f}) - {n_selected} features non-nulles')\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elastic Net (L1 + L2)\n",
    "\n",
    "**Formule :** Minimiser $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda_1 \\sum_{j=1}^p |w_j| + \\lambda_2 \\sum_{j=1}^p w_j^2$\n",
    "\n",
    "**Param√®tres :**\n",
    "- `alpha` : Force totale de r√©gularisation\n",
    "- `l1_ratio` : Balance entre L1 et L2 (0=Ridge, 1=Lasso)\n",
    "\n",
    "### 4.1 Grid Search pour Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search sur alpha et l1_ratio\n",
    "param_grid = {\n",
    "    'alpha': np.logspace(-3, 1, 10),\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "elastic_net = ElasticNet(max_iter=10000, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    elastic_net, param_grid, cv=5, scoring='r2', n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_elastic = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "test_r2_elastic = best_elastic.score(X_test_scaled, y_test)\n",
    "n_selected_elastic = np.sum(best_elastic.coef_ != 0)\n",
    "\n",
    "print(\"=== Elastic Net Optimal ===\")\n",
    "print(f\"Meilleur alpha : {best_params['alpha']:.4f}\")\n",
    "print(f\"Meilleur l1_ratio : {best_params['l1_ratio']:.2f}\")\n",
    "print(f\"Test R¬≤ : {test_r2_elastic:.4f}\")\n",
    "print(f\"Features s√©lectionn√©es : {n_selected_elastic}/{len(best_elastic.coef_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison Finale\n",
    "\n",
    "### 5.1 Tableau Comparatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer tous les mod√®les\n",
    "models = {\n",
    "    'Linear Regression': lr,\n",
    "    'Ridge (optimal)': ridge_cv,\n",
    "    'Lasso (optimal)': lasso_cv,\n",
    "    'Elastic Net (optimal)': best_elastic\n",
    "}\n",
    "\n",
    "comparison = []\n",
    "for name, model in models.items():\n",
    "    train_r2 = model.score(X_train_scaled, y_train)\n",
    "    test_r2 = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    if hasattr(model, 'coef_'):\n",
    "        n_features = np.sum(model.coef_ != 0)\n",
    "    else:\n",
    "        n_features = X.shape[1]\n",
    "    \n",
    "    comparison.append({\n",
    "        'Mod√®le': name,\n",
    "        'Train R¬≤': f\"{train_r2:.4f}\",\n",
    "        'Test R¬≤': f\"{test_r2:.4f}\",\n",
    "        'Test RMSE': f\"{rmse:.2f}\",\n",
    "        'Features': f\"{n_features}/{X.shape[1]}\"\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison)\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARAISON FINALE DES M√âTHODES DE R√âGULARISATION\")\n",
    "print(\"=\" * 80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualisation des Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "models_plot = [\n",
    "    ('Linear Regression', lr, axes[0, 0]),\n",
    "    ('Ridge', ridge_cv, axes[0, 1]),\n",
    "    ('Lasso', lasso_cv, axes[1, 0]),\n",
    "    ('Elastic Net', best_elastic, axes[1, 1])\n",
    "]\n",
    "\n",
    "for name, model, ax in models_plot:\n",
    "    coef = model.coef_\n",
    "    n_nonzero = np.sum(coef != 0)\n",
    "    \n",
    "    ax.stem(range(len(coef)), coef, basefmt=\" \")\n",
    "    ax.axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
    "    ax.set_xlabel('Feature Index')\n",
    "    ax.set_ylabel('Coefficient')\n",
    "    ax.set_title(f'{name}\\n({n_nonzero} features non-nulles)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation :\")\n",
    "print(\"- Linear Regression : Coefficients tr√®s variables\")\n",
    "print(\"- Ridge : Tous les coefficients r√©duits mais non-nuls\")\n",
    "print(\"- Lasso : Beaucoup de coefficients exactement √† 0\")\n",
    "print(\"- Elastic Net : Compromis entre Ridge et Lasso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cas Pratique : Dataset Diabetes\n",
    "\n",
    "### 6.1 Application sur donn√©es r√©elles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger dataset\ndiabetes = load_diabetes()\nX_diab = diabetes.data  # type: ignore\ny_diab = diabetes.target  # type: ignore\n\n# Split et normalisation\nX_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(\n    X_diab, y_diab, test_size=0.2, random_state=42\n)\n\nscaler_diab = StandardScaler()\nX_train_diab = scaler_diab.fit_transform(X_train_diab)\nX_test_diab = scaler_diab.transform(X_test_diab)\n\nprint(\"=== Dataset Diabetes ===\")\nprint(f\"√âchantillons : {X_diab.shape[0]}\")\nprint(f\"Features : {X_diab.shape[1]}\")\nprint(f\"Features : {diabetes.feature_names  # type: ignore}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer les 4 m√©thodes\n",
    "models_diab = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': RidgeCV(alphas=alphas_cv, cv=5),\n",
    "    'Lasso': LassoCV(alphas=alphas_cv, cv=5, max_iter=10000),\n",
    "    'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n",
    "}\n",
    "\n",
    "results_diab = []\n",
    "for name, model in models_diab.items():\n",
    "    model.fit(X_train_diab, y_train_diab)\n",
    "    \n",
    "    train_r2 = model.score(X_train_diab, y_train_diab)\n",
    "    test_r2 = model.score(X_test_diab, y_test_diab)\n",
    "    \n",
    "    y_pred_diab = model.predict(X_test_diab)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_diab, y_pred_diab))\n",
    "    \n",
    "    if hasattr(model, 'coef_'):\n",
    "        n_features = np.sum(model.coef_ != 0)\n",
    "    else:\n",
    "        n_features = 10\n",
    "    \n",
    "    results_diab.append({\n",
    "        'Mod√®le': name,\n",
    "        'Train R¬≤': train_r2,\n",
    "        'Test R¬≤': test_r2,\n",
    "        'Test RMSE': rmse,\n",
    "        'Features': n_features\n",
    "    })\n",
    "\n",
    "df_diab = pd.DataFrame(results_diab)\n",
    "print(\"\\n=== R√©sultats sur Diabetes Dataset ===\")\n",
    "print(df_diab.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_pos = np.arange(len(df_diab))\n",
    "plt.bar(x_pos - 0.2, df_diab['Train R¬≤'], width=0.4, label='Train R¬≤', alpha=0.8)\n",
    "plt.bar(x_pos + 0.2, df_diab['Test R¬≤'], width=0.4, label='Test R¬≤', alpha=0.8)\n",
    "plt.xticks(x_pos, df_diab['Mod√®le'], rotation=45, ha='right')\n",
    "plt.ylabel('R¬≤')\n",
    "plt.title('Comparaison sur Dataset Diabetes')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. R√©capitulatif et Guide de Choix\n",
    "\n",
    "### Quand utiliser quelle m√©thode ?\n",
    "\n",
    "| M√©thode | Quand l'utiliser ? | Avantages | Inconv√©nients |\n",
    "|---------|-------------------|-----------|---------------|\n",
    "| **Linear Regression** | Peu de features, donn√©es propres | Simple, interpr√©table | Surapprentissage si p > n |\n",
    "| **Ridge (L2)** | Features corr√©l√©es, tous utiles | Stabilit√©, garde toutes features | Pas de s√©lection |\n",
    "| **Lasso (L1)** | Beaucoup de features, s√©lection | S√©lection automatique, interpr√©table | Instable si features corr√©l√©es |\n",
    "| **Elastic Net** | Features corr√©l√©es + s√©lection | Combine avantages L1 et L2 | 2 hyperparam√®tres |\n",
    "\n",
    "### Points cl√©s :\n",
    "\n",
    "1. **Toujours normaliser** les features avant r√©gularisation\n",
    "2. **Validation crois√©e** pour choisir Œª (alpha)\n",
    "3. **Ridge** : Bon point de d√©part par d√©faut\n",
    "4. **Lasso** : Si besoin de s√©lection de features\n",
    "5. **Elastic Net** : Si Lasso instable (features corr√©l√©es)\n",
    "\n",
    "### Prochaine √©tape :\n",
    "\n",
    "Voir **03_exercices.ipynb** pour mettre en pratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Notebook termin√© !\")\n",
    "print(\"\\nVous ma√Ætrisez maintenant :\")\n",
    "print(\"  - Ridge (L2 regularization)\")\n",
    "print(\"  - Lasso (L1 regularization)\")\n",
    "print(\"  - Elastic Net (L1 + L2)\")\n",
    "print(\"  - S√©lection d'hyperparam√®tres\")\n",
    "print(\"  - Comparaison et choix de m√©thode\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}