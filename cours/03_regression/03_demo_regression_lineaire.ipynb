{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_badge"
   },
   "source": [
    "# üöÄ Google Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ogautier1980/sandbox-ml/blob/main/cours/03_regression/03_demo_regression_lineaire.ipynb)\n",
    "\n",
    "**Si vous ex√©cutez ce notebook sur Google Colab**, ex√©cutez la cellule suivante pour installer les d√©pendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_install"
   },
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab uniquement)",
    "",
    "import sys",
    "",
    "IN_COLAB = 'google.colab' in sys.modules",
    "",
    "",
    "",
    "if IN_COLAB:",
    "",
    "    print('üì¶ Installation des packages...')",
    "",
    "    ",
    "",
    "    # Packages ML de base",
    "",
    "    !pip install -q numpy pandas matplotlib seaborn scikit-learn",
    "",
    "    ",
    "",
    "    # D√©tection du chapitre et installation des d√©pendances sp√©cifiques",
    "",
    "    notebook_name = '03_demo_regression_lineaire.ipynb'  # Sera remplac√© automatiquement",
    "",
    "    ",
    "",
    "    # Ch 06-08 : Deep Learning",
    "",
    "    if any(x in notebook_name for x in ['06_', '07_', '08_']):",
    "",
    "        !pip install -q torch torchvision torchaudio",
    "",
    "    ",
    "",
    "    # Ch 08 : NLP",
    "",
    "    if '08_' in notebook_name:",
    "",
    "        !pip install -q transformers datasets tokenizers",
    "",
    "        if 'rag' in notebook_name:",
    "",
    "            !pip install -q sentence-transformers faiss-cpu rank-bm25",
    "",
    "    ",
    "",
    "    # Ch 09 : Reinforcement Learning",
    "",
    "    if '09_' in notebook_name:",
    "",
    "        !pip install -q gymnasium[classic-control]",
    "",
    "    ",
    "",
    "    # Ch 04 : Boosting",
    "",
    "    if '04_' in notebook_name and 'boosting' in notebook_name:",
    "",
    "        !pip install -q xgboost lightgbm catboost",
    "",
    "    ",
    "",
    "    # Ch 05 : Clustering avanc√©",
    "",
    "    if '05_' in notebook_name:",
    "",
    "        !pip install -q umap-learn",
    "",
    "    ",
    "",
    "    # Ch 11 : S√©ries temporelles",
    "",
    "    if '11_' in notebook_name:",
    "",
    "        !pip install -q statsmodels prophet",
    "",
    "    ",
    "",
    "    # Ch 12 : Vision avanc√©e",
    "",
    "    if '12_' in notebook_name:",
    "",
    "        !pip install -q ultralytics timm segmentation-models-pytorch",
    "",
    "    ",
    "",
    "    # Ch 13 : Recommandation",
    "",
    "    if '13_' in notebook_name:",
    "",
    "        !pip install -q scikit-surprise implicit",
    "",
    "    ",
    "",
    "    # Ch 14 : MLOps",
    "",
    "    if '14_' in notebook_name:",
    "",
    "        !pip install -q mlflow fastapi pydantic",
    "",
    "    ",
    "",
    "    print('‚úÖ Installation termin√©e !')",
    "",
    "else:",
    "",
    "    print('‚ÑπÔ∏è  Environnement local d√©tect√©, les packages sont d√©j√† install√©s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 03 - R√©gression Lin√©aire\n",
    "\n",
    "**Objectifs :**\n",
    "- Comprendre et impl√©menter la r√©gression lin√©aire simple\n",
    "- Ma√Ætriser la r√©gression lin√©aire multiple\n",
    "- Utiliser la r√©gression polynomiale\n",
    "- Diagnostiquer les r√©sidus\n",
    "- √âvaluer les performances (R¬≤, MSE, RMSE)\n",
    "\n",
    "**Pr√©requis :** Chapitres 00, 01, 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.data  # type: ignoresets import load_diabetes, make_regression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.pipeline import Pipeline\nimport scipy.stats as stats\n\n# Configuration\nnp.random.seed(42)\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"‚úÖ Imports r√©ussis\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. R√©gression Lin√©aire Simple\n",
    "\n",
    "### 1.1 G√©n√©ration de donn√©es synth√©tiques\n",
    "\n",
    "Cr√©ons des donn√©es avec une relation lin√©aire : $y = 3x + 5 + \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des donn√©es synth√©tiques\n",
    "n_samples = 100\n",
    "X_simple = np.linspace(0, 10, n_samples).reshape(-1, 1)\n",
    "y_true = 3 * X_simple.ravel() + 5\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "y_simple = y_true + noise\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, y_simple, alpha=0.6, label='Donn√©es observ√©es')\n",
    "plt.plot(X_simple, y_true, 'r--', label='Vraie relation (y=3x+5)', linewidth=2)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('R√©gression Lin√©aire Simple - Donn√©es Synth√©tiques')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Nombre d'√©chantillons : {n_samples}\")\n",
    "print(f\"Range de X : [{X_simple.min():.2f}, {X_simple.max():.2f}]\")\n",
    "print(f\"Range de y : [{y_simple.min():.2f}, {y_simple.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Entra√Ænement du mod√®le avec scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er et entra√Æner le mod√®le\n",
    "model_simple = LinearRegression()\n",
    "model_simple.fit(X_simple, y_simple)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred = model_simple.predict(X_simple)\n",
    "\n",
    "# Param√®tres appris\n",
    "w1 = model_simple.coef_[0]\n",
    "w0 = model_simple.intercept_\n",
    "\n",
    "print(\"=== Param√®tres Appris ===\")\n",
    "print(f\"Pente (w1) : {w1:.4f} (vraie valeur: 3.0)\")\n",
    "print(f\"Intercept (w0) : {w0:.4f} (vraie valeur: 5.0)\")\n",
    "print(f\"\\n√âquation : ≈∑ = {w1:.4f}x + {w0:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 √âvaluation du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©triques\n",
    "mse = mean_squared_error(y_simple, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_simple, y_pred)\n",
    "r2 = r2_score(y_simple, y_pred)\n",
    "\n",
    "print(\"=== M√©triques de Performance ===\")\n",
    "print(f\"MSE  : {mse:.4f}\")\n",
    "print(f\"RMSE : {rmse:.4f}\")\n",
    "print(f\"MAE  : {mae:.4f}\")\n",
    "print(f\"R¬≤   : {r2:.4f}\")\n",
    "print(f\"\\nInterpr√©tation : Le mod√®le explique {r2*100:.2f}% de la variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Visualisation des pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1 : Droite de r√©gression\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_simple, y_simple, alpha=0.6, label='Donn√©es')\n",
    "plt.plot(X_simple, y_pred, 'g-', label=f'R√©gression (R¬≤={r2:.3f})', linewidth=2)\n",
    "plt.plot(X_simple, y_true, 'r--', label='Vraie relation', linewidth=2, alpha=0.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Droite de R√©gression')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2 : R√©sidus\n",
    "residus = y_simple - y_pred\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_pred, residus, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Valeurs Pr√©dites')\n",
    "plt.ylabel('R√©sidus (y - ≈∑)')\n",
    "plt.title('Analyse des R√©sidus')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Moyenne des r√©sidus : {residus.mean():.6f} (devrait √™tre ~0)\")\n",
    "print(f\"√âcart-type des r√©sidus : {residus.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Impl√©mentation manuelle (pour comprendre les maths)\n",
    "\n",
    "Calculons les param√®tres avec les formules analytiques :\n",
    "\n",
    "$$w_1 = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}, \\quad w_0 = \\bar{y} - w_1 \\bar{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impl√©mentation manuelle\n",
    "x_flat = X_simple.ravel()\n",
    "x_mean = x_flat.mean()\n",
    "y_mean = y_simple.mean()\n",
    "\n",
    "# Formule analytique\n",
    "covariance = np.sum((x_flat - x_mean) * (y_simple - y_mean))\n",
    "variance_x = np.sum((x_flat - x_mean)**2)\n",
    "\n",
    "w1_manual = covariance / variance_x\n",
    "w0_manual = y_mean - w1_manual * x_mean\n",
    "\n",
    "print(\"=== Comparaison Impl√©mentations ===\")\n",
    "print(f\"scikit-learn : w1={w1:.6f}, w0={w0:.6f}\")\n",
    "print(f\"Manuel       : w1={w1_manual:.6f}, w0={w0_manual:.6f}\")\n",
    "print(f\"\\nDiff√©rence   : w1={abs(w1-w1_manual):.10f}, w0={abs(w0-w0_manual):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. R√©gression Lin√©aire Multiple\n",
    "\n",
    "### 2.1 Dataset Diabetes (scikit-learn)\n",
    "\n",
    "Pr√©disons la progression du diab√®te √† partir de 10 features biom√©dicales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger le dataset\ndiabetes = load_diabetes()\nX = diabetes.data  # type: ignore\ny = diabetes.target  # type: ignore\nfeature_names = diabetes.feature_names  # type: ignore\n\nprint(\"=== Dataset Diabetes ===\")\nprint(f\"Nombre d'√©chantillons : {X.shape[0]}\")\nprint(f\"Nombre de features : {X.shape[1]}\")\nprint(f\"\\nFeatures : {feature_names}\")\nprint(f\"\\nStatistiques de la cible (y) :\")\nprint(f\"  Min : {y.min():.2f}\")\nprint(f\"  Max : {y.max():.2f}\")\nprint(f\"  Moyenne : {y.mean():.2f}\")\nprint(f\"  √âcart-type : {y.std():.2f}\")\n\n# Visualiser les corr√©lations\ndf = pd.DataFrame(X, columns=feature_names)\ndf['target'] = y\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0)\nplt.title('Matrice de Corr√©lation - Dataset Diabetes')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Split Train/Test et Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"=== Split Train/Test ===\")\n",
    "print(f\"Train : {X_train.shape[0]} √©chantillons\")\n",
    "print(f\"Test  : {X_test.shape[0]} √©chantillons\")\n",
    "\n",
    "# Normalisation (optionnelle mais recommand√©e)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úÖ Donn√©es normalis√©es (moyenne=0, std=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Entra√Ænement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le de r√©gression lin√©aire multiple\n",
    "model_multi = LinearRegression()\n",
    "model_multi.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_train_pred = model_multi.predict(X_train_scaled)\n",
    "y_test_pred = model_multi.predict(X_test_scaled)\n",
    "\n",
    "# M√©triques\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(\"=== Performance du Mod√®le ===\")\n",
    "print(f\"Train R¬≤ : {train_r2:.4f}\")\n",
    "print(f\"Test R¬≤  : {test_r2:.4f}\")\n",
    "print(f\"\\nTrain RMSE : {train_rmse:.2f}\")\n",
    "print(f\"Test RMSE  : {test_rmse:.2f}\")\n",
    "\n",
    "if abs(train_r2 - test_r2) < 0.1:\n",
    "    print(\"\\n‚úÖ Pas de surapprentissage d√©tect√©\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Possible surapprentissage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Importance des Features (Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients du mod√®le\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': model_multi.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"=== Coefficients du Mod√®le ===\")\n",
    "print(coefficients.to_string(index=False))\n",
    "print(f\"\\nIntercept : {model_multi.intercept_:.2f}\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coefficients['Feature'], coefficients['Coefficient'])\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Importance des Features (Coefficients de R√©gression)')\n",
    "plt.axvline(x=0, color='k', linestyle='--', linewidth=0.8)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpr√©tation : Les features avec les plus grands coefficients (en valeur absolue)\")\n",
    "print(\"ont le plus d'impact sur la pr√©diction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visualisations Diagnostiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Pr√©dictions vs Valeurs R√©elles\n",
    "axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Pr√©diction parfaite')\n",
    "axes[0, 0].set_xlabel('Valeurs R√©elles')\n",
    "axes[0, 0].set_ylabel('Valeurs Pr√©dites')\n",
    "axes[0, 0].set_title(f'Pr√©dictions vs R√©elles (Test R¬≤={test_r2:.3f})')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribution des r√©sidus\n",
    "residuals_test = y_test - y_test_pred\n",
    "axes[0, 1].hist(residuals_test, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('R√©sidus')\n",
    "axes[0, 1].set_ylabel('Fr√©quence')\n",
    "axes[0, 1].set_title(f'Distribution des R√©sidus (Test, Œº={residuals_test.mean():.2f})')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. R√©sidus vs Pr√©dictions\n",
    "axes[1, 0].scatter(y_test_pred, residuals_test, alpha=0.6)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Valeurs Pr√©dites')\n",
    "axes[1, 0].set_ylabel('R√©sidus')\n",
    "axes[1, 0].set_title('R√©sidus vs Pr√©dictions')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Q-Q Plot (Normalit√© des r√©sidus)\n",
    "stats.probplot(residuals_test, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Test de Normalit√© des R√©sidus)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Analyse des diagnostics :\")\n",
    "print(\"1. Les points doivent √™tre proches de la diagonale (plot 1)\")\n",
    "print(\"2. Les r√©sidus doivent √™tre centr√©s sur 0 (plot 2)\")\n",
    "print(\"3. Pas de pattern dans les r√©sidus (plot 3)\")\n",
    "print(\"4. Les points doivent suivre la ligne rouge (Q-Q plot, normalit√©)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. R√©gression Polynomiale\n",
    "\n",
    "### 3.1 Donn√©es Non-Lin√©aires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des donn√©es avec relation polynomiale\n",
    "np.random.seed(42)\n",
    "X_poly = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y_poly_true = 0.5 * X_poly.ravel()**2 - 2*X_poly.ravel() + 1\n",
    "y_poly = y_poly_true + np.random.normal(0, 1, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_poly, y_poly, alpha=0.6, label='Donn√©es')\n",
    "plt.plot(X_poly, y_poly_true, 'r--', label='Vraie relation (polynomiale)', linewidth=2)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Donn√©es avec Relation Polynomiale')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comparaison de diff√©rents degr√©s polynomiaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 2, 5, 10]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    # Cr√©er features polynomiales\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly_features = poly.fit_transform(X_poly)\n",
    "    \n",
    "    # Entra√Æner mod√®le\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly_features, y_poly)\n",
    "    y_pred_poly = model.predict(X_poly_features)\n",
    "    \n",
    "    # M√©triques\n",
    "    r2 = r2_score(y_poly, y_pred_poly)\n",
    "    rmse = np.sqrt(mean_squared_error(y_poly, y_pred_poly))\n",
    "    results.append({'Degree': degree, 'R¬≤': r2, 'RMSE': rmse})\n",
    "    \n",
    "    # Subplot\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.scatter(X_poly, y_poly, alpha=0.4, s=20)\n",
    "    plt.plot(X_poly, y_pred_poly, color=colors[i], linewidth=2, \n",
    "             label=f'Degr√© {degree}')\n",
    "    plt.plot(X_poly, y_poly_true, 'k--', linewidth=1, alpha=0.5, label='Vraie')\n",
    "    plt.title(f'Degr√© {degree}\\nR¬≤={r2:.3f}')\n",
    "    plt.xlabel('X')\n",
    "    if i == 0:\n",
    "        plt.ylabel('y')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau comparatif\n",
    "print(\"\\n=== Comparaison des Mod√®les ===\")\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "print(\"\\nObservation : Degr√© 2 capture bien la relation. Degr√© > 5 surapprentissage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pipeline complet avec Validation Crois√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester diff√©rents degr√©s avec validation crois√©e\n",
    "degrees_cv = range(1, 11)\n",
    "cv_scores = []\n",
    "\n",
    "for degree in degrees_cv:\n",
    "    # Pipeline : Polynomiale + Scaling + R√©gression\n",
    "    pipeline = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Validation crois√©e 5-fold\n",
    "    scores = cross_val_score(pipeline, X_poly, y_poly, cv=5, \n",
    "                             scoring='r2')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees_cv, cv_scores, 'o-', linewidth=2, markersize=8)\n",
    "best_degree = degrees_cv[np.argmax(cv_scores)]\n",
    "plt.axvline(x=best_degree, color='r', linestyle='--', linewidth=2, \n",
    "            label=f'Meilleur degr√©: {best_degree}')\n",
    "plt.xlabel('Degr√© du Polyn√¥me')\n",
    "plt.ylabel('R¬≤ (Validation Crois√©e)')\n",
    "plt.title('S√©lection du Degr√© Polynomial par Validation Crois√©e')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(degrees_cv)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Meilleur degr√© : {best_degree}\")\n",
    "print(f\"R¬≤ moyen : {max(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. R√©capitulatif et Points Cl√©s\n",
    "\n",
    "### Points √† retenir :\n",
    "\n",
    "1. **R√©gression lin√©aire simple** : Une feature, solution analytique directe\n",
    "2. **R√©gression lin√©aire multiple** : Plusieurs features, matrice de design\n",
    "3. **R√©gression polynomiale** : Transforme les features pour capturer les non-lin√©arit√©s\n",
    "4. **M√©triques** : R¬≤, MSE, RMSE, MAE\n",
    "5. **Diagnostics** : Analyse des r√©sidus cruciale (normalit√©, homosc√©dasticit√©)\n",
    "6. **Validation crois√©e** : Essentielle pour d√©tecter le surapprentissage\n",
    "\n",
    "### Prochaine √©tape :\n",
    "\n",
    "Voir **03_demo_regularisation.ipynb** pour Ridge, Lasso, Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Notebook termin√© !\")\n",
    "print(\"\\nVous ma√Ætrisez maintenant :\")\n",
    "print(\"  - R√©gression lin√©aire simple et multiple\")\n",
    "print(\"  - R√©gression polynomiale\")\n",
    "print(\"  - √âvaluation et diagnostic de mod√®les\")\n",
    "print(\"  - Validation crois√©e\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}