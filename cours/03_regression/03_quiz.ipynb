{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz d'Auto-√âvaluation - Chapitre 03 : R√©gression\n",
    "\n",
    "**Instructions** :\n",
    "- Ce quiz contient 15 questions pour tester votre compr√©hension du chapitre\n",
    "- R√©pondez aux questions par vous-m√™me avant de regarder les r√©ponses\n",
    "- Les r√©ponses sont dans une cellule masqu√©e √† la fin\n",
    "- Comptez 1 point par bonne r√©ponse\n",
    "\n",
    "**Bar√®me** :\n",
    "- 13-15 : Excellent ! Vous ma√Ætrisez le chapitre üí™\n",
    "- 10-12 : Bien, relisez les sections o√π vous avez des lacunes\n",
    "- 7-9 : Moyen, relisez le chapitre attentivement\n",
    "- < 7 : Insuffisant, reprenez le chapitre depuis le d√©but\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Question 1 : R√©gression vs Classification\n",
    "Quelle est la diff√©rence principale entre r√©gression et classification ?\n",
    "\n",
    "A) La r√©gression est plus pr√©cise  \n",
    "B) La r√©gression pr√©dit une valeur continue, la classification une cat√©gorie discr√®te  \n",
    "C) La r√©gression utilise plus de donn√©es  \n",
    "D) Il n'y a pas de diff√©rence  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : Fonction de Co√ªt\n",
    "Pourquoi utilise-t-on le MSE (Mean Squared Error) plut√¥t que le MAE en r√©gression lin√©aire ?\n",
    "\n",
    "A) Le MSE est plus rapide √† calculer  \n",
    "B) Le MSE est diff√©rentiable et permet une solution analytique  \n",
    "C) Le MSE est toujours plus pr√©cis  \n",
    "D) Le MAE n'existe pas en r√©gression  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 : √âquations Normales\n",
    "La solution des moindres carr√©s pour la r√©gression lin√©aire multiple est :\n",
    "\n",
    "A) $\\vec{w}^* = X^T y$  \n",
    "B) $\\vec{w}^* = (X^TX)^{-1}X^Ty$  \n",
    "C) $\\vec{w}^* = X^{-1}y$  \n",
    "D) $\\vec{w}^* = (XX^T)^{-1}y$  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 : Coefficient R¬≤\n",
    "Un mod√®le de r√©gression a un $R^2 = 0$ sur le test set. Que signifie ce score ?\n",
    "\n",
    "A) Le mod√®le est parfait  \n",
    "B) Le mod√®le est aussi bon que pr√©dire la moyenne $\\bar{y}$  \n",
    "C) Le mod√®le a 0% d'erreur  \n",
    "D) Le mod√®le est excellent  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 : R√©gression Polynomiale\n",
    "Pourquoi une r√©gression polynomiale de degr√© √©lev√© (ex: degr√© 15) peut-elle causer de l'overfitting ?\n",
    "\n",
    "A) Elle est trop lente √† entra√Æner  \n",
    "B) Elle capture le bruit dans les donn√©es d'entra√Ænement et ne g√©n√©ralise pas  \n",
    "C) Elle n√©cessite trop de donn√©es  \n",
    "D) Elle ne converge jamais  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : Ridge vs Lasso - P√©nalit√©\n",
    "Quelle p√©nalit√© utilise la r√©gression Ridge ?\n",
    "\n",
    "A) $\\lambda \\sum |w_i|$ (norme $L^1$)  \n",
    "B) $\\lambda \\sum w_i^2$ (norme $L^2$)  \n",
    "C) $\\lambda \\max |w_i|$ (norme $L^\\infty$)  \n",
    "D) Aucune p√©nalit√©  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 : Ridge vs Lasso - S√©lection de Features\n",
    "Quelle r√©gularisation effectue une s√©lection automatique de features (certains poids deviennent exactement 0) ?\n",
    "\n",
    "A) Ridge ($L^2$)  \n",
    "B) Lasso ($L^1$)  \n",
    "C) Les deux  \n",
    "D) Aucune des deux  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 : Elastic Net\n",
    "Elastic Net combine :\n",
    "\n",
    "A) Ridge et Gradient Boosting  \n",
    "B) Lasso et Decision Trees  \n",
    "C) Ridge ($L^2$) et Lasso ($L^1$)  \n",
    "D) MSE et MAE  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 : Coefficient de R√©gularisation $\\lambda$\n",
    "Que se passe-t-il si $\\lambda$ (coefficient de r√©gularisation) est tr√®s grand ?\n",
    "\n",
    "A) Risque d'overfitting  \n",
    "B) Risque d'underfitting (les poids sont fortement p√©nalis√©s)  \n",
    "C) Le mod√®le converge plus vite  \n",
    "D) Aucun effet  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 : Multicollin√©arit√©\n",
    "Qu'est-ce que la multicollin√©arit√© ?\n",
    "\n",
    "A) Des features ind√©pendantes  \n",
    "B) Une corr√©lation forte entre plusieurs features  \n",
    "C) Un mod√®le avec trop de features  \n",
    "D) Des donn√©es d√©s√©quilibr√©es  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 : D√©tection de la Multicollin√©arit√©\n",
    "Comment d√©tecter la multicollin√©arit√© ?\n",
    "\n",
    "A) Calculer le MSE  \n",
    "B) V√©rifier le VIF (Variance Inflation Factor) ou la matrice de corr√©lation  \n",
    "C) Tracer la courbe ROC  \n",
    "D) Augmenter le nombre d'epochs  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12 : Solution pour Multicollin√©arit√©\n",
    "Quelle r√©gularisation est particuli√®rement efficace contre la multicollin√©arit√© ?\n",
    "\n",
    "A) Lasso ($L^1$)  \n",
    "B) Ridge ($L^2$)  \n",
    "C) Elastic Net  \n",
    "D) Aucune r√©gularisation  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13 : Diagnostic des R√©sidus\n",
    "Dans un graphique R√©sidus vs Pr√©dictions, quel pattern indique un bon mod√®le ?\n",
    "\n",
    "A) Une tendance lin√©aire croissante  \n",
    "B) R√©sidus dispers√©s al√©atoirement autour de 0  \n",
    "C) Une forme en entonnoir (h√©t√©rosc√©dasticit√©)  \n",
    "D) Tous les r√©sidus √©gaux √† 0  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14 : Validation Crois√©e pour R√©gression\n",
    "Pour choisir le meilleur coefficient de r√©gularisation $\\lambda$ en Ridge, quelle technique utiliser ?\n",
    "\n",
    "A) Tester une seule valeur arbitraire  \n",
    "B) Validation crois√©e (K-Fold) sur plusieurs valeurs de $\\lambda$  \n",
    "C) Toujours utiliser $\\lambda = 1$  \n",
    "D) Choisir le plus grand $\\lambda$ possible  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15 : R√©gression Lin√©aire vs Mod√®les Non-Lin√©aires\n",
    "Quel avantage principal la r√©gression lin√©aire conserve-t-elle par rapport aux mod√®les non-lin√©aires complexes (ex: Random Forest, XGBoost) ?\n",
    "\n",
    "A) Elle est toujours plus pr√©cise  \n",
    "B) Elle est simple, interpr√©table, et a une solution analytique  \n",
    "C) Elle ne n√©cessite aucune donn√©e  \n",
    "D) Elle capture automatiquement les non-lin√©arit√©s  \n",
    "\n",
    "**Votre r√©ponse** : ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Auto-Correction\n",
    "\n",
    "Avant de regarder les r√©ponses, comptez combien de r√©ponses vous avez donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrez vos r√©ponses ici (ex: ['B', 'A', 'C', ...])\n",
    "mes_reponses = []  # TODO: remplir avec vos r√©ponses\n",
    "\n",
    "# R√©ponses correctes (masqu√©es)\n",
    "reponses_correctes = ['B', 'B', 'B', 'B', 'B', 'B', 'B', 'C', 'B', 'B', 'B', 'B', 'B', 'B', 'B']\n",
    "\n",
    "if len(mes_reponses) == 15:\n",
    "    score = sum([1 for i, r in enumerate(mes_reponses) if r.upper() == reponses_correctes[i]])\n",
    "    print(f\"Votre score : {score}/15\")\n",
    "    \n",
    "    if score >= 13:\n",
    "        print(\"\\nüéâ Excellent ! Vous ma√Ætrisez le chapitre !\")\n",
    "    elif score >= 10:\n",
    "        print(\"\\n‚úÖ Bien ! Relisez les sections o√π vous avez des lacunes.\")\n",
    "    elif score >= 7:\n",
    "        print(\"\\n‚ö†Ô∏è  Moyen. Relisez le chapitre attentivement.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Insuffisant. Reprenez le chapitre depuis le d√©but.\")\n",
    "    \n",
    "    # Afficher les erreurs\n",
    "    print(\"\\nD√©tail :\")\n",
    "    for i, (ma_rep, bonne_rep) in enumerate(zip(mes_reponses, reponses_correctes), 1):\n",
    "        if ma_rep.upper() == bonne_rep:\n",
    "            print(f\"Q{i}: ‚úì Correct\")\n",
    "        else:\n",
    "            print(f\"Q{i}: ‚úó Votre r√©ponse: {ma_rep}, Correcte: {bonne_rep}\")\n",
    "else:\n",
    "    print(\"Veuillez remplir toutes les r√©ponses (15 lettres A, B, C ou D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Explications des R√©ponses\n",
    "\n",
    "### Q1 : B\n",
    "**R√©gression** : pr√©dit une valeur num√©rique continue (prix, temp√©rature, √¢ge). **Classification** : pr√©dit une cat√©gorie discr√®te (spam/non-spam, classe 0/1/2).\n",
    "\n",
    "### Q2 : B\n",
    "Le **MSE** est diff√©rentiable (contrairement au MAE qui a un point anguleux en 0), ce qui permet d'obtenir une **solution analytique** via les √©quations normales. Le MSE est aussi plus sensible aux outliers (ce qui peut √™tre un avantage ou un inconv√©nient selon le contexte).\n",
    "\n",
    "### Q3 : B\n",
    "La solution des moindres carr√©s (√©quations normales) pour la r√©gression lin√©aire multiple est : $\\vec{w}^* = (X^TX)^{-1}X^Ty$. C'est la pseudo-inverse de Moore-Penrose.\n",
    "\n",
    "### Q4 : B\n",
    "$R^2 = 0$ signifie que le mod√®le est **aussi bon qu'une pr√©diction constante** $\\hat{y} = \\bar{y}$ (la moyenne). $R^2 = 1$ = mod√®le parfait, $R^2 < 0$ = mod√®le pire que la moyenne.\n",
    "\n",
    "### Q5 : B\n",
    "Une r√©gression polynomiale de **degr√© √©lev√©** a trop de flexibilit√© : elle **capture le bruit** dans les donn√©es d'entra√Ænement (m√©morisation) et ne **g√©n√©ralise pas** bien aux nouvelles donn√©es (overfitting).\n",
    "\n",
    "### Q6 : B\n",
    "La r√©gression **Ridge** utilise une p√©nalit√© $L^2$ : $\\lambda \\sum w_i^2 = \\lambda \\|\\vec{w}\\|_2^2$. La r√©gression **Lasso** utilise une p√©nalit√© $L^1$ : $\\lambda \\sum |w_i| = \\lambda \\|\\vec{w}\\|_1$.\n",
    "\n",
    "### Q7 : B\n",
    "**Lasso** ($L^1$) effectue une **s√©lection automatique de features** : certains poids deviennent exactement 0. **Ridge** ($L^2$) r√©tr√©cit les poids mais ils restent non-nuls.\n",
    "\n",
    "### Q8 : C\n",
    "**Elastic Net** combine les p√©nalit√©s **Ridge** ($L^2$) et **Lasso** ($L^1$) : $\\lambda_1 \\|\\vec{w}\\|_1 + \\lambda_2 \\|\\vec{w}\\|_2^2$. Cela permet de b√©n√©ficier des avantages des deux (s√©lection + stabilit√©).\n",
    "\n",
    "### Q9 : B\n",
    "Si $\\lambda$ (coefficient de r√©gularisation) est **tr√®s grand**, les poids sont **fortement p√©nalis√©s** et tendent vers 0, ce qui provoque de l'**underfitting** (mod√®le trop simple). Si $\\lambda = 0$, pas de r√©gularisation (risque d'overfitting).\n",
    "\n",
    "### Q10 : B\n",
    "La **multicollin√©arit√©** est une **corr√©lation forte entre plusieurs features**. Cela rend l'estimation des coefficients instable (petits changements dans les donn√©es provoquent de grandes variations des poids).\n",
    "\n",
    "### Q11 : B\n",
    "Pour d√©tecter la multicollin√©arit√© : **VIF** (Variance Inflation Factor, VIF > 10 indique un probl√®me) ou **matrice de corr√©lation** ($|\\rho_{ij}| > 0.8$ probl√©matique).\n",
    "\n",
    "### Q12 : B\n",
    "**Ridge** ($L^2$) est particuli√®rement efficace contre la multicollin√©arit√© car elle stabilise l'inversion de $X^TX$ en ajoutant $\\lambda I$ sur la diagonale. Lasso peut √™tre instable avec des features corr√©l√©es (choix arbitraire).\n",
    "\n",
    "### Q13 : B\n",
    "Dans un bon mod√®le, les r√©sidus doivent √™tre **dispers√©s al√©atoirement autour de 0** (pas de pattern). Une tendance ou une forme en entonnoir indique un probl√®me (non-lin√©arit√©, h√©t√©rosc√©dasticit√©).\n",
    "\n",
    "### Q14 : B\n",
    "Pour choisir $\\lambda$, utiliser la **validation crois√©e** (K-Fold) sur plusieurs valeurs (ex: `np.logspace(-3, 3, 50)`). scikit-learn fournit `RidgeCV` et `LassoCV` qui automatisent ce processus.\n",
    "\n",
    "### Q15 : B\n",
    "La r√©gression lin√©aire est **simple, interpr√©table** (coefficients = impact de chaque feature), et poss√®de une **solution analytique** (pas besoin d'optimisation it√©rative). Les mod√®les non-lin√©aires sont plus puissants mais moins interpr√©tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Rappel des Formules Essentielles\n",
    "\n",
    "### R√©gression Lin√©aire Simple\n",
    "\n",
    "$$w_1 = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}, \\quad w_0 = \\bar{y} - w_1 \\bar{x}$$\n",
    "\n",
    "### R√©gression Multiple (√âquations Normales)\n",
    "\n",
    "$$\\vec{w}^* = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "### Ridge (R√©gression $L^2$)\n",
    "\n",
    "$$\\vec{w}^* = (X^TX + \\lambda I)^{-1}X^Ty$$\n",
    "\n",
    "### Coefficient $R^2$\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$$\n",
    "\n",
    "### MSE, MAE, RMSE\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\text{MSE}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prochaines √âtapes\n",
    "\n",
    "- **Score < 10** : Relisez le chapitre 03, en particulier les sections R√©gularisation (Ridge, Lasso) et Diagnostic\n",
    "- **Score >= 10** : Passez au Chapitre 04 (Classification Supervis√©e)\n",
    "- **R√©vision recommand√©e** : Refaites le quiz dans 2-3 jours pour ancrer les connaissances\n",
    "- **Pratique** : Consultez les notebooks `03_demo_*.ipynb` et `03_exercices.ipynb` pour des exercices pratiques avec scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
