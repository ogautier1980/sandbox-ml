% Chapitre 03 - R√©gression

\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{lmodern}

% Math√©matiques
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

% Mise en page
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{setspace}
\setstretch{1.15}

% Graphiques et couleurs
\usepackage{graphicx}
\usepackage{xcolor}

% ===== UNICODE CHARACTERS SUPPORT =====
\usepackage{newunicodechar}

% Emojis et symboles
\newunicodechar{‚úÖ}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚ùå}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚úì}{\textcolor{green!60!black}{$\checkmark$}}
\newunicodechar{‚úó}{\textcolor{red!60!black}{$\times$}}
\newunicodechar{‚ö†}{\textcolor{orange!80!black}{\textbf{/!\textbackslash}}}
\newunicodechar{üí°}{\textcolor{blue!70!black}{\textbf{(i)}}}
\newunicodechar{üéØ}{\textcolor{purple!70!black}{\textbf{$\star$}}}
\newunicodechar{üìä}{\textcolor{blue!70!black}{\textbf{[=]}}}

% √âtoiles (pour tableaux)
\newunicodechar{‚òÖ}{\textcolor{orange!80!black}{$\star$}}
\newunicodechar{‚òÜ}{\textcolor{gray!50}{$\star$}}

% Fl√®ches
\newunicodechar{‚Üí}{$\rightarrow$}
\newunicodechar{‚Üê}{$\leftarrow$}
\newunicodechar{‚Üë}{$\uparrow$}
\newunicodechar{‚Üì}{$\downarrow$}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric}

% Tableaux
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{colortbl}

% Code et algorithmes
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Hyperliens
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Chapitre 03 - R√©gression},
    pdfauthor={Cours ML},
}

% Boxes color√©es
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}


% ===== TCOLORBOX AVEC EMOJIS =====
\newtcolorbox{attention}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=‚ö† Attention,
    breakable
}

\newtcolorbox{definition}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=D√©finition,
    breakable
}

\newtcolorbox{astuce}{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=üí° Astuce,
    breakable
}

\newtcolorbox{remarque}{
    colback=yellow!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=üí° Remarque,
    breakable
}

\newtcolorbox{important}{
    colback=purple!5!white,
    colframe=purple!75!black,
    fonttitle=\bfseries,
    title=‚ö† Important,
    breakable
}

\newtcolorbox{exemple}{
    colback=gray!5!white,
    colframe=gray!75!black,
    fonttitle=\bfseries,
    title=Exemple,
    breakable
}

% En-t√™tes et pieds de page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Chapitre 03 - R√©gression}
\fancyhead[R]{\small Cours Machine Learning}
\fancyfoot[C]{\thepage}

% ===== CONFIGURATION LISTINGS =====
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}
\lstset{style=pythonstyle}

% ===== CONFIGURATION TCOLORBOX =====


\newtcolorbox{theoreme}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=Th√©or√®me: #1,
    breakable
}







% ===== COMMANDES PERSONNALIS√âES =====
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}

% ===== D√âBUT DU DOCUMENT =====
\begin{document}

% ===== PAGE DE TITRE =====
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries Cours Machine Learning}\\[0.5cm]

    \vspace{1cm}

    {\LARGE Chapitre 03}\\[0.3cm]
    {\LARGE\bfseries R√©gression}\\[2cm]

    \vfill

    {\large
    \textbf{Objectifs d'apprentissage :}\\[0.5cm]
    \begin{itemize}
        \item Ma√Ætriser la r√©gression lin√©aire (simple, multiple, polynomiale)
        \item Comprendre les techniques de r√©gularisation (Ridge, Lasso, Elastic Net)
        \item Diagnostiquer et valider un mod√®le de r√©gression
        \item Impl√©menter et optimiser des mod√®les de r√©gression
    \end{itemize}
    }

    \vfill

    {\large
    \textbf{Pr√©requis :} Chapitres 00, 01, 02\\[0.3cm]
    \textbf{Dur√©e estim√©e :} 5-6 heures\\[0.3cm]
    \textbf{Notebooks :} \texttt{03\_demo\_*.ipynb}, \texttt{03\_exercices.ipynb}
    }

    \vfill

    {\large Cours ML - Sandbox-ML\\
    Version 1.0 - 2026}
\end{titlepage}

% ===== TABLE DES MATI√àRES =====
\tableofcontents
\newpage

% ===== SECTION 1: INTRODUCTION =====
\section{Introduction √† la R√©gression}

\subsection{Probl√©matique}

\begin{definition}{R√©gression}
La r√©gression est une t√¢che d'apprentissage supervis√© o√π l'objectif est de pr√©dire une variable cible \textbf{continue} $y \in \R$ √† partir de features $\vect{x} \in \R^d$.
\end{definition}

\textbf{Formulation math√©matique :}

√âtant donn√© un dataset $\{(\vect{x}_i, y_i)\}_{i=1}^n$, on cherche une fonction $f: \R^d \to \R$ telle que :
\begin{equation}
    \hat{y}_i = f(\vect{x}_i) \approx y_i
\end{equation}

\textbf{Diff√©rence avec la classification :}
\begin{itemize}
    \item \textbf{R√©gression :} $y$ est continu (ex: prix, temp√©rature, √¢ge)
    \item \textbf{Classification :} $y$ est discret (ex: spam/non-spam, classe 0/1/2)
\end{itemize}

\begin{exemple}{Applications de la r√©gression}
\begin{itemize}
    \item \textbf{Immobilier :} Pr√©dire le prix d'une maison (surface, nb pi√®ces, localisation)
    \item \textbf{Finance :} Pr√©dire le cours d'une action, le risque de d√©faut
    \item \textbf{Sant√© :} Pr√©dire la dur√©e d'hospitalisation, la progression d'une maladie
    \item \textbf{Marketing :} Pr√©dire les ventes futures, le chiffre d'affaires
    \item \textbf{M√©t√©o :} Pr√©dire la temp√©rature, les pr√©cipitations
\end{itemize}
\end{exemple}

\subsection{Fonction de Co√ªt}

La fonction de co√ªt (loss) mesure l'erreur de pr√©diction. Pour la r√©gression, les plus courantes sont :

\textbf{1. Mean Squared Error (MSE) :}
\begin{equation}
    \text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\end{equation}

\textbf{2. Mean Absolute Error (MAE) :}
\begin{equation}
    \text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
\end{equation}

\textbf{3. Root Mean Squared Error (RMSE) :}
\begin{equation}
    \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
\end{equation}

\begin{astuce}
\textbf{Choix de la m√©trique :}
\begin{itemize}
    \item \textbf{MSE/RMSE :} Plus sensibles aux outliers (erreurs au carr√©), diff√©rentiables
    \item \textbf{MAE :} Plus robuste aux outliers, moins sensible aux valeurs extr√™mes
    \item En pratique, MSE est la plus utilis√©e car diff√©rentiable et optimisable analytiquement
\end{itemize}
\end{astuce}

% ===== SECTION 2: R√âGRESSION LIN√âAIRE SIMPLE =====
\section{R√©gression Lin√©aire Simple}

\subsection{Mod√®le}

\begin{definition}{R√©gression lin√©aire simple}
Mod√®le avec une seule feature $x$ :
\begin{equation}
    \hat{y} = w_1 x + w_0
\end{equation}
o√π $w_1$ est la pente et $w_0$ l'intercept (ordonn√©e √† l'origine).
\end{definition}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0]
    % Axes
    \draw[->] (0,0) -- (10,0) node[right] {$x$ (Feature)};
    \draw[->] (0,0) -- (0,6) node[above] {$y$ (Cible)};

    % Points de donn√©es (nuage avec tendance lin√©aire + bruit)
    \foreach \x/\y in {1/1.2, 1.5/1.8, 2/2.1, 2.5/2.3, 3/2.9, 3.5/3.2, 4/3.4, 4.5/3.9, 5/4.2, 5.5/4.5, 6/4.7, 6.5/5.1, 7/5.3, 7.5/5.4, 8/5.8, 8.5/5.7, 9/6.0}
        \filldraw[blue] (\x,\y) circle (2pt);

    % Droite de r√©gression y = 0.55x + 0.6
    \draw[red, very thick] (0.5,0.875) -- (9.5,5.825);
    \node[red, above right] at (9.5, 5.825) {$\hat{y} = w_1 x + w_0$};

    % R√©sidus (quelques exemples)
    \draw[green!60!black, dashed] (2,2.1) -- (2,1.7);
    \draw[green!60!black, dashed] (4,3.4) -- (4,2.8);
    \draw[green!60!black, dashed] (6,4.7) -- (6,3.9);
    \draw[green!60!black, dashed] (8,5.8) -- (8,5.0);

    % Annotations des r√©sidus
    \node[green!60!black, right, font=\small] at (2.2,1.9) {$e_i = y_i - \hat{y}_i$};

    % Annotation pente
    \draw[<->, thick] (7,4.5) -- (9,4.5) node[midway, below] {$\Delta x = 2$};
    \draw[<->, thick] (9,4.5) -- (9,5.55) node[midway, right] {$\Delta y = w_1 \cdot 2$};
    \node[draw, fill=yellow!10, rounded corners, align=center] at (7.5, 6.5) {
        Pente $w_1 = \frac{\Delta y}{\Delta x}$
    };

    % Annotation intercept
    \draw[<->, thick] (0,0) -- (0,0.6);
    \node[left, font=\small] at (0, 0.3) {$w_0$};
    \filldraw[red] (0,0.6) circle (2pt);

    % L√©gende
    \node[draw, fill=white, align=left, rounded corners] at (2.5, 5.5) {
        \textcolor{blue}{$\bullet$} Donn√©es observ√©es\\
        \textcolor{red}{---} Droite de r√©gression\\
        \textcolor{green!60!black}{- - -} R√©sidus
    };

\end{tikzpicture}
\caption{R√©gression lin√©aire simple: la droite $\hat{y} = w_1 x + w_0$ minimise la somme des carr√©s des r√©sidus (distances verticales entre les points et la droite). $w_1$ est la pente, $w_0$ est l'ordonn√©e √† l'origine.}
\label{fig:linear_regression}
\end{figure}

\textbf{Notation alternative :} $\hat{y} = \beta_1 x + \beta_0$ ou $\hat{y} = ax + b$

\subsection{Solution Analytique}

Pour minimiser le MSE, on r√©sout :
\begin{equation}
    \min_{w_0, w_1} \sum_{i=1}^n (y_i - w_1 x_i - w_0)^2
\end{equation}

En d√©rivant par rapport √† $w_0$ et $w_1$ et en annulant, on obtient les \textbf{√©quations normales} :

\begin{align}
    w_1 &= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{\text{Cov}(x, y)}{\Var(x)} \\
    w_0 &= \bar{y} - w_1 \bar{x}
\end{align}

o√π $\bar{x}$ et $\bar{y}$ sont les moyennes de $x$ et $y$.

\subsection{Coefficient de D√©termination $R^2$}

\begin{definition}{Coefficient $R^2$}
Le coefficient de d√©termination mesure la proportion de variance expliqu√©e par le mod√®le :
\begin{equation}
    R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2} = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}
\end{equation}
\end{definition}

\textbf{Interpr√©tation :}
\begin{itemize}
    \item $R^2 = 1$ : Mod√®le parfait (toutes les pr√©dictions exactes)
    \item $R^2 = 0$ : Mod√®le aussi bon qu'une pr√©diction constante ($\hat{y} = \bar{y}$)
    \item $R^2 < 0$ : Mod√®le pire que la moyenne (rare, indique un probl√®me)
\end{itemize}

\textbf{Relation avec la corr√©lation :} Pour la r√©gression simple, $R^2 = \rho^2$ o√π $\rho$ est le coefficient de corr√©lation de Pearson.

% ===== SECTION 3: R√âGRESSION LIN√âAIRE MULTIPLE =====
\section{R√©gression Lin√©aire Multiple}

\subsection{Mod√®le Vectoriel}

\begin{definition}{R√©gression lin√©aire multiple}
Mod√®le avec $d$ features :
\begin{equation}
    \hat{y} = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + w_0 = \vect{w}^T \vect{x} + w_0
\end{equation}
\end{definition}

En notation matricielle, pour $n$ instances :
\begin{equation}
    \vect{\hat{y}} = \mat{X} \vect{w}
\end{equation}
o√π $\mat{X} \in \R^{n \times (d+1)}$ inclut une colonne de 1 pour l'intercept, et $\vect{w} \in \R^{d+1}$ contient tous les poids.

\subsection{Solution par Moindres Carr√©s}

\textbf{Probl√®me d'optimisation :}
\begin{equation}
    \vect{w}^* = \argmin_{\vect{w}} \|\mat{X}\vect{w} - \vect{y}\|_2^2
\end{equation}

\begin{theoreme}{√âquations normales}
La solution optimale des moindres carr√©s est :
\begin{equation}
    \vect{w}^* = (\mat{X}^T\mat{X})^{-1}\mat{X}^T\vect{y}
\end{equation}
sous r√©serve que $\mat{X}^T\mat{X}$ soit inversible (rang plein).
\end{theoreme}

\textbf{D√©monstration (esquisse) :}
\begin{align}
    \text{Minimiser } & f(\vect{w}) = \|\mat{X}\vect{w} - \vect{y}\|_2^2 \\
    \nabla_{\vect{w}} f &= 2\mat{X}^T(\mat{X}\vect{w} - \vect{y}) = \vect{0} \\
    \mat{X}^T\mat{X}\vect{w} &= \mat{X}^T\vect{y} \\
    \vect{w}^* &= (\mat{X}^T\mat{X})^{-1}\mat{X}^T\vect{y}
\end{align}

\begin{attention}
\textbf{Probl√®mes num√©riques :}
\begin{itemize}
    \item Si $\mat{X}^T\mat{X}$ est singuli√®re ou mal conditionn√©e, l'inversion est instable
    \item Causes : multicollin√©arit√© (features corr√©l√©es), $n < d$ (plus de features que d'instances)
    \item Solutions : r√©gularisation (Ridge, Lasso), s√©lection de features, PCA
\end{itemize}
\end{attention}

\subsection{Impl√©mentation}

\begin{lstlisting}[language=Python, caption=R√©gression lin√©aire avec scikit-learn]
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Donn√©es
X = np.random.randn(100, 5)  # 100 instances, 5 features
y = 3*X[:, 0] + 2*X[:, 1] - X[:, 2] + np.random.randn(100)*0.5

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Entra√Ænement
model = LinearRegression()
model.fit(X_train, y_train)

# Pr√©diction
y_pred = model.predict(X_test)

# √âvaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.4f}")
print(f"R¬≤: {r2:.4f}")
print(f"Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_:.4f}")
\end{lstlisting}

% ===== SECTION 4: R√âGRESSION POLYNOMIALE =====
\section{R√©gression Polynomiale}

\subsection{Motivation}

La r√©gression lin√©aire suppose une relation lin√©aire entre features et cible. Si la relation est non-lin√©aire, on peut utiliser des features polynomiales.

\begin{definition}{R√©gression polynomiale}
Pour une feature $x$, on cr√©e des features polynomiales :
\begin{equation}
    \hat{y} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \cdots + w_p x^p
\end{equation}
Le mod√®le reste lin√©aire en les poids $\vect{w}$, mais non-lin√©aire en $x$.
\end{definition}

\textbf{G√©n√©ralisation √† plusieurs features :}
Pour $d$ features, on peut cr√©er tous les termes de degr√© $\leq p$ :
\begin{equation}
    \vect{\phi}(\vect{x}) = [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2, \ldots]
\end{equation}

Le nombre de features polynomiales cro√Æt combinatoirement : $\binom{d+p}{p}$.

\subsection{Exemple}

Donn√©es 1D non-lin√©aires : $y = \sin(2\pi x) + \epsilon$

\begin{itemize}
    \item \textbf{Degr√© 1 (lin√©aire)} : sous-ajustement (underfitting)
    \item \textbf{Degr√© 3-5} : bon ajustement
    \item \textbf{Degr√© 15} : sur-ajustement (overfitting)
\end{itemize}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.85]
    % Degr√© 1 - Underfitting
    \begin{scope}[xshift=0cm]
        \node at (2.5, 4.2) {\textbf{Degr√© 1 (Underfitting)}};
        \draw[->] (0,0) -- (5,0) node[right] {\small $x$};
        \draw[->] (0,0) -- (0,4) node[above] {\small $y$};

        % Vraie fonction (sinuso√Øde approxim√©e)
        \draw[blue!30, dashed, thick, smooth, domain=0.2:4.8, samples=50]
            plot (\x, {2 + 1.5*sin(360*\x/2.5)});

        % Points
        \foreach \x/\y in {0.5/2.7, 1.0/3.2, 1.5/2.9, 2.0/1.8, 2.5/0.8, 3.0/1.2, 3.5/2.1, 4.0/3.0, 4.5/2.8}
            \filldraw[blue] (\x,\y) circle (1.5pt);

        % Mod√®le lin√©aire (trop simple)
        \draw[red, very thick] (0.5,2.8) -- (4.5,2.2);

        \node[align=center, font=\small] at (2.5, -0.7) {Trop simple\\Ne capture pas la courbe};
    \end{scope}

    % Degr√© 4 - Bon fit
    \begin{scope}[xshift=6cm]
        \node at (2.5, 4.2) {\textbf{Degr√© 4 (Bon √âquilibre)}};
        \draw[->] (0,0) -- (5,0) node[right] {\small $x$};
        \draw[->] (0,0) -- (0,4) node[above] {\small $y$};

        % Vraie fonction
        \draw[blue!30, dashed, thick, smooth, domain=0.2:4.8, samples=50]
            plot (\x, {2 + 1.5*sin(360*\x/2.5)});

        % Points
        \foreach \x/\y in {0.5/2.7, 1.0/3.2, 1.5/2.9, 2.0/1.8, 2.5/0.8, 3.0/1.2, 3.5/2.1, 4.0/3.0, 4.5/2.8}
            \filldraw[blue] (\x,\y) circle (1.5pt);

        % Mod√®le polynomial degr√© 4 (suit bien la courbe)
        \draw[green!60!black, very thick, smooth, domain=0.5:4.5, samples=60]
            plot (\x, {2 + 1.4*sin(360*\x/2.5) + 0.1*(\x-2.5)*(\x-2.5)});

        \node[align=center, font=\small] at (2.5, -0.7) {Capture le pattern\\G√©n√©ralise bien};
    \end{scope}

    % Degr√© 15 - Overfitting
    \begin{scope}[xshift=12cm]
        \node at (2.5, 4.2) {\textbf{Degr√© 15 (Overfitting)}};
        \draw[->] (0,0) -- (5,0) node[right] {\small $x$};
        \draw[->] (0,0) -- (0,4) node[above] {\small $y$};

        % Vraie fonction
        \draw[blue!30, dashed, thick, smooth, domain=0.2:4.8, samples=50]
            plot (\x, {2 + 1.5*sin(360*\x/2.5)});

        % Points
        \foreach \x/\y in {0.5/2.7, 1.0/3.2, 1.5/2.9, 2.0/1.8, 2.5/0.8, 3.0/1.2, 3.5/2.1, 4.0/3.0, 4.5/2.8}
            \filldraw[blue] (\x,\y) circle (1.5pt);

        % Mod√®le polynomial degr√© 15 (oscille entre les points)
        \draw[orange, very thick, smooth, domain=0.5:4.5, samples=80]
            plot (\x, {2 + 1.5*sin(360*\x/2.5) + 0.5*sin(720*\x) + 0.3*cos(540*\x)});

        \node[align=center, font=\small] at (2.5, -0.7) {M\'{e}morise le bruit\\Ne g\'{e}n\'{e}ralise pas};
    \end{scope}

    % L√©gende commune
    \node[draw, fill=white, align=left, rounded corners] at (9, -2) {
        \textcolor{blue!30}{- - -} Vraie fonction $f(x)$\\
        \textcolor{blue}{$\bullet$} Donn√©es observ√©es\\
        \textcolor{red}{---} / \textcolor{green!60!black}{---} / \textcolor{orange}{---} Mod√®les ajust√©s
    };

\end{tikzpicture}
\caption{R√©gression polynomiale avec diff√©rents degr√©s: degr√© trop faible (underfitting), degr√© appropri√© (bon √©quilibre), et degr√© trop √©lev√© (overfitting m√©morisant le bruit).}
\label{fig:polynomial_regression}
\end{figure}

\begin{lstlisting}[language=Python, caption=R√©gression polynomiale]
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

# Cr√©er un pipeline : features polynomiales + r√©gression
degree = 3
model = make_pipeline(
    PolynomialFeatures(degree=degree),
    LinearRegression()
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
\end{lstlisting}

\begin{attention}
\textbf{Risque d'overfitting :}
\begin{itemize}
    \item Augmenter le degr√© $p$ am√©liore le fit sur le train set
    \item Mais peut d√©grader la g√©n√©ralisation (test set)
    \item Solution : validation crois√©e pour choisir $p$, r√©gularisation
\end{itemize}
\end{attention}

% ===== SECTION 5: R√âGULARISATION =====
\section{R√©gularisation}

La r√©gularisation p√©nalise la complexit√© du mod√®le pour √©viter l'overfitting.

\subsection{Ridge Regression ($L^2$)}

\begin{definition}{Ridge (R√©gression $L^2$)}
Ajoute une p√©nalit√© sur la norme $L^2$ des poids :
\begin{equation}
    \vect{w}^* = \argmin_{\vect{w}} \left\{ \|\mat{X}\vect{w} - \vect{y}\|_2^2 + \lambda \|\vect{w}\|_2^2 \right\}
\end{equation}
o√π $\lambda \geq 0$ est le coefficient de r√©gularisation.
\end{definition}

\textbf{Solution analytique :}
\begin{equation}
    \vect{w}^*_{\text{Ridge}} = (\mat{X}^T\mat{X} + \lambda \mat{I})^{-1}\mat{X}^T\vect{y}
\end{equation}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item P√©nalise les poids de grande magnitude
    \item R√©duit l'overfitting
    \item Solution toujours unique (m√™me si $\mat{X}^T\mat{X}$ singuli√®re)
    \item Les poids sont \textbf{r√©tr√©cis} (shrinkage) mais rarement nuls
\end{itemize}

\subsection{Lasso Regression ($L^1$)}

\begin{definition}{Lasso (R√©gression $L^1$)}
Ajoute une p√©nalit√© sur la norme $L^1$ des poids :
\begin{equation}
    \vect{w}^* = \argmin_{\vect{w}} \left\{ \|\mat{X}\vect{w} - \vect{y}\|_2^2 + \lambda \|\vect{w}\|_1 \right\}
\end{equation}
\end{definition}

\textbf{Propri√©t√©s :}
\begin{itemize}
    \item P√©nalise les poids en valeur absolue
    \item Effectue une \textbf{s√©lection de features automatique} : certains poids deviennent exactement 0
    \item Pas de solution analytique simple, r√©solu par optimisation (coordinate descent, proximal gradient)
    \item Utile quand on suspecte que peu de features sont pertinentes
\end{itemize}

\subsection{Elastic Net}

\begin{definition}{Elastic Net}
Combinaison de Ridge et Lasso :
\begin{equation}
    \vect{w}^* = \argmin_{\vect{w}} \left\{ \|\mat{X}\vect{w} - \vect{y}\|_2^2 + \lambda_1 \|\vect{w}\|_1 + \lambda_2 \|\vect{w}\|_2^2 \right\}
\end{equation}
ou avec un ratio $\rho \in [0, 1]$ :
\begin{equation}
    \text{P√©nalit√©} = \lambda \left[\rho \|\vect{w}\|_1 + \frac{1-\rho}{2} \|\vect{w}\|_2^2\right]
\end{equation}
\end{definition}

\textbf{Avantages :}
\begin{itemize}
    \item Combine les avantages de Ridge et Lasso
    \item S√©lection de features (comme Lasso)
    \item Stabilit√© quand features corr√©l√©es (comme Ridge)
\end{itemize}

\subsection{Comparaison Ridge vs Lasso}

\begin{table}[h]
\centering
\caption{Ridge vs Lasso}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Ridge ($L^2$)} & \textbf{Lasso ($L^1$)} \\
\midrule
P√©nalit√© & $\lambda \sum w_i^2$ & $\lambda \sum |w_i|$ \\
Solution analytique & Oui & Non \\
S√©lection de features & Non & Oui \\
Poids √† z√©ro & Jamais exactement & Oui \\
Features corr√©l√©es & Stable & Instable (choix arbitraire) \\
Interpr√©tabilit√© & Moyenne & √âlev√©e (sparse) \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0]
    % Ridge (L2) - gauche
    \begin{scope}[xshift=0cm]
        \node at (3, 4.8) {\textbf{Ridge (R√©gularisation $L^2$)}};

        % Axes
        \draw[->] (-1,0) -- (4,0) node[right] {$w_1$};
        \draw[->] (0,-1) -- (0,4) node[above] {$w_2$};

        % Contrainte L2 (cercle)
        \draw[blue!60!black, very thick] (1.5,1.5) circle (1.5);
        \node[blue!60!black, below right] at (3,1.5) {$w_1^2 + w_2^2 \leq t$};

        % Contours de la fonction de co√ªt (ellipses)
        \draw[red!60!black, thick] (2.5,2.2) ellipse (1.2 and 0.8);
        \draw[red!60!black, thick] (2.5,2.2) ellipse (1.8 and 1.2);
        \draw[red!60!black, thick] (2.5,2.2) ellipse (2.4 and 1.6);

        % Optimum sans contrainte
        \filldraw[red!60!black] (2.5,2.2) circle (2pt);
        \node[red!60!black, above right] at (2.5,2.2) {$\hat{w}_{OLS}$};

        % Optimum avec Ridge (contact tangent cercle-ellipse)
        \filldraw[green!60!black] (2.1,1.05) circle (3pt);
        \node[green!60!black, below left] at (2.1,1.05) {$\hat{w}_{Ridge}$};

        % Annotation
        \node[align=center, font=\small] at (1.5, -1.5) {Contrainte circulaire\\Poids non nuls};
    \end{scope}

    % Lasso (L1) - droite
    \begin{scope}[xshift=8cm]
        \node at (3, 4.8) {\textbf{Lasso (R√©gularisation $L^1$)}};

        % Axes
        \draw[->] (-1,0) -- (4,0) node[right] {$w_1$};
        \draw[->] (0,-1) -- (0,4) node[above] {$w_2$};

        % Contrainte L1 (losange)
        \draw[blue!60!black, very thick] (0,2.1) -- (2.1,0) -- (0,-0.1) -- (-0.1,0) -- cycle;
        \draw[blue!60!black, very thick] (0,2.1) -- (2.1,0);
        \node[blue!60!black, below right] at (2.5,0.5) {$|w_1| + |w_2| \leq t$};

        % Contours de la fonction de co√ªt (ellipses)
        \draw[red!60!black, thick] (2.5,2.2) ellipse (1.2 and 0.8);
        \draw[red!60!black, thick] (2.5,2.2) ellipse (1.8 and 1.2);
        \draw[red!60!black, thick] (2.5,2.2) ellipse (2.4 and 1.6);

        % Optimum sans contrainte
        \filldraw[red!60!black] (2.5,2.2) circle (2pt);
        \node[red!60!black, above right] at (2.5,2.2) {$\hat{w}_{OLS}$};

        % Optimum avec Lasso (contact au coin, w1=0)
        \filldraw[green!60!black] (0,1.55) circle (3pt);
        \node[green!60!black, left] at (0,1.55) {$\hat{w}_{Lasso}$};
        \node[green!60!black, left, font=\small] at (-0.3,0.8) {$w_1 = 0$};

        % Annotation
        \node[align=center, font=\small] at (1.5, -1.5) {Contrainte losange\\S\'{e}lection: $w_1=0$};
    \end{scope}

    % L√©gende
    \node[draw, fill=white, align=left, rounded corners] at (4, -3) {
        \textcolor{blue!60!black}{---} R\'{e}gion de contrainte ($\|\vect{w}\| \leq t$)\\
        \textcolor{red!60!black}{$\circ$} Contours de la fonction objectif (MSE)\\
        \textcolor{green!60!black}{$\bullet$} Solution optimale (minimise MSE + contrainte)
    };

\end{tikzpicture}
\caption{G√©om√©trie de Ridge vs Lasso: la contrainte $L^2$ (cercle) r√©duit les poids mais ne les annule jamais exactement, tandis que la contrainte $L^1$ (losange) favorise les solutions aux coins, donc des poids √† z√©ro (s√©lection de features).}
\label{fig:ridge_lasso_geometry}
\end{figure}

\begin{astuce}
\textbf{Quand utiliser quoi ?}
\begin{itemize}
    \item \textbf{Ridge :} Toutes les features potentiellement utiles, features corr√©l√©es
    \item \textbf{Lasso :} Peu de features importantes, besoin d'interpr√©tabilit√©
    \item \textbf{Elastic Net :} Compromis, features corr√©l√©es + s√©lection
\end{itemize}
\end{astuce}

\subsection{Choix de $\lambda$ (Hyperparam√®tre)}

Le coefficient de r√©gularisation $\lambda$ contr√¥le le trade-off biais-variance :
\begin{itemize}
    \item $\lambda = 0$ : Pas de r√©gularisation (risque d'overfitting)
    \item $\lambda$ petit : R√©gularisation faible
    \item $\lambda$ grand : R√©gularisation forte (risque d'underfitting)
\end{itemize}

\textbf{M√©thode de s√©lection :} Validation crois√©e

\begin{lstlisting}[language=Python, caption=Ridge avec validation crois√©e]
from sklearn.linear_model import RidgeCV

# Tester plusieurs valeurs de lambda (alpha en scikit-learn)
alphas = np.logspace(-3, 3, 50)
model = RidgeCV(alphas=alphas, cv=5)
model.fit(X_train, y_train)

print(f"Meilleur alpha: {model.alpha_:.4f}")
\end{lstlisting}

% ===== SECTION 6: DIAGNOSTIC =====
\section{Diagnostic et Analyse des R√©sidus}

\subsection{R√©sidus}

\begin{definition}{R√©sidus}
Les r√©sidus sont les erreurs de pr√©diction :
\begin{equation}
    e_i = y_i - \hat{y}_i
\end{equation}
\end{definition}

\textbf{Hypoth√®ses de la r√©gression lin√©aire :}
\begin{enumerate}
    \item \textbf{Lin√©arit√© :} Relation lin√©aire entre $X$ et $y$
    \item \textbf{Ind√©pendance :} Les r√©sidus sont ind√©pendants
    \item \textbf{Homosc√©dasticit√© :} Variance constante des r√©sidus
    \item \textbf{Normalit√© :} Les r√©sidus suivent une loi normale
    \item \textbf{Absence de multicollin√©arit√© :} Features non (trop) corr√©l√©es
\end{enumerate}

\subsection{Graphiques de Diagnostic}

\textbf{1. R√©sidus vs Pr√©dictions :}
\begin{itemize}
    \item V√©rifier l'homosc√©dasticit√©
    \item Pattern : r√©sidus dispers√©s al√©atoirement autour de 0
    \item Probl√®me : tendance, forme en entonnoir (h√©t√©rosc√©dasticit√©)
\end{itemize}

\textbf{2. Q-Q Plot :}
\begin{itemize}
    \item V√©rifier la normalit√© des r√©sidus
    \item Points align√©s sur la diagonale = normalit√©
\end{itemize}

\textbf{3. R√©sidus vs Features :}
\begin{itemize}
    \item D√©tecter des relations non-lin√©aires manqu√©es
\end{itemize}

\subsection{Multicollin√©arit√©}

\begin{definition}{Multicollin√©arit√©}
Corr√©lation forte entre plusieurs features. Rend l'estimation des coefficients instable.
\end{definition}

\textbf{D√©tection :}
\begin{itemize}
    \item Matrice de corr√©lation : $|\rho_{ij}| > 0.8$ probl√©matique
    \item VIF (Variance Inflation Factor) : VIF $> 10$ indique multicollin√©arit√©
\end{itemize}

\textbf{Solutions :}
\begin{itemize}
    \item Supprimer une des features corr√©l√©es
    \item PCA (r√©duction de dimensionnalit√©)
    \item R√©gularisation (Ridge)
\end{itemize}

% ===== SECTION 7: VALIDATION =====
\section{Validation et S√©lection de Mod√®le}

\subsection{Train/Validation/Test Split}

\textbf{Proc√©dure standard :}
\begin{enumerate}
    \item \textbf{Train set (70\%)} : Entra√Æner le mod√®le
    \item \textbf{Validation set (15\%)} : Tuner les hyperparam√®tres
    \item \textbf{Test set (15\%)} : √âvaluation finale (une seule fois)
\end{enumerate}

\subsection{Validation Crois√©e (Cross-Validation)}

\begin{definition}{K-Fold Cross-Validation}
Diviser les donn√©es en $K$ folds. Pour chaque fold :
\begin{itemize}
    \item Entra√Æner sur $K-1$ folds
    \item Valider sur le fold restant
    \item R√©p√©ter $K$ fois, moyenner les scores
\end{itemize}
\end{definition}

\begin{lstlisting}[language=Python, caption=Validation crois√©e]
from sklearn.model_selection import cross_val_score

model = Ridge(alpha=1.0)
scores = cross_val_score(model, X, y, cv=5,
                          scoring='neg_mean_squared_error')
rmse_scores = np.sqrt(-scores)

print(f"RMSE: {rmse_scores.mean():.4f} (+/- {rmse_scores.std():.4f})")
\end{lstlisting}

\textbf{Avantages :}
\begin{itemize}
    \item Utilise toutes les donn√©es pour train et validation
    \item Estimation plus robuste de la performance
    \item R√©duit le risque de chance (lucky/unlucky split)
\end{itemize}

\subsection{Courbe d'Apprentissage}

Tracer la performance (MSE, $R^2$) en fonction de la taille du train set.

\textbf{Diagnostic :}
\begin{itemize}
    \item \textbf{Underfitting :} Train et validation scores bas et proches
    \item \textbf{Overfitting :} Grand √©cart entre train et validation scores
    \item \textbf{Bon fit :} Scores convergent vers une valeur √©lev√©e
\end{itemize}

% ===== SECTION 8: EXTENSIONS =====
\section{Extensions et Variantes}

\subsection{R√©gression Robuste}

Pour g√©rer les outliers :
\begin{itemize}
    \item \textbf{Huber Regression :} Loss hybride (MSE + MAE)
    \item \textbf{RANSAC :} Fit sur inliers, ignore outliers
\end{itemize}

\subsection{R√©gression Non-Lin√©aire}

Au-del√† des polyn√¥mes :
\begin{itemize}
    \item \textbf{Kernel Ridge Regression :} Kernels (RBF, polynomial)
    \item \textbf{Support Vector Regression (SVR)} : SVM pour la r√©gression
    \item \textbf{Decision Trees, Random Forests :} Mod√®les non-lin√©aires
    \item \textbf{Gradient Boosting (XGBoost, LightGBM)} : √âtat de l'art pour donn√©es tabulaires
    \item \textbf{R√©seaux de neurones :} Deep Learning pour r√©gression
\end{itemize}

\subsection{R√©gression G√©n√©ralis√©e (GLM)}

Extension de la r√©gression lin√©aire pour distributions non-gaussiennes :
\begin{itemize}
    \item R√©gression logistique (Bernoulli)
    \item R√©gression de Poisson
    \item R√©gression Gamma
\end{itemize}

% ===== R√âSUM√â =====
\section{R√©sum√© du Chapitre}

\subsection{Points Cl√©s}

\begin{itemize}
    \item \textbf{R√©gression lin√©aire :} Mod√®le simple, interpr√©table, solution analytique
    \item \textbf{√âquations normales :} $\vect{w}^* = (\mat{X}^T\mat{X})^{-1}\mat{X}^T\vect{y}$
    \item \textbf{$R^2$ :} Mesure de qualit√© du fit (0 √† 1)
    \item \textbf{R√©gression polynomiale :} Capture des relations non-lin√©aires
    \item \textbf{Ridge ($L^2$) :} R√©gularisation, stabilit√©
    \item \textbf{Lasso ($L^1$) :} S√©lection de features, sparsit√©
    \item \textbf{Diagnostic :} Analyse des r√©sidus, d√©tection de probl√®mes
    \item \textbf{Validation crois√©e :} S√©lection d'hyperparam√®tres robuste
\end{itemize}

\subsection{Formules Essentielles}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Formules √† retenir]
\textbf{R√©gression lin√©aire simple :}
\begin{equation}
    w_1 = \frac{\text{Cov}(x, y)}{\Var(x)}, \quad w_0 = \bar{y} - w_1 \bar{x}
\end{equation}

\textbf{R√©gression multiple (√©quations normales) :}
\begin{equation}
    \vect{w}^* = (\mat{X}^T\mat{X})^{-1}\mat{X}^T\vect{y}
\end{equation}

\textbf{Ridge :}
\begin{equation}
    \vect{w}^* = (\mat{X}^T\mat{X} + \lambda\mat{I})^{-1}\mat{X}^T\vect{y}
\end{equation}

\textbf{Coefficient $R^2$ :}
\begin{equation}
    R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\end{equation}
\end{tcolorbox}

% ===== EXERCICES =====
\section{Exercices}

\subsection{Questions de Compr√©hension}

\begin{enumerate}
    \item Quelle est la diff√©rence entre r√©gression et classification ?

    \item Pourquoi utilise-t-on le MSE plut√¥t que le MAE en r√©gression lin√©aire ?

    \item Expliquez pourquoi la r√©gression polynomiale de degr√© √©lev√© peut causer de l'overfitting.

    \item Quelle est la diff√©rence principale entre Ridge et Lasso ?

    \item Comment d√©tecte-t-on la multicollin√©arit√© ? Quelles sont les solutions ?

    \item Pourquoi la validation crois√©e est-elle pr√©f√©rable √† un simple train/test split ?
\end{enumerate}

\subsection{Exercices Pratiques}

\textit{Voir le notebook} \texttt{03\_exercices.ipynb} \textit{(solutions int√©gr√©es)}

% ===== POUR ALLER PLUS LOIN =====
\section{Pour Aller Plus Loin}

\subsection{Lectures Recommand√©es}

\begin{itemize}
    \item \textit{An Introduction to Statistical Learning} (2e √©d., 2021) - James et al. (Ch. 3)
    \item \textit{The Elements of Statistical Learning} (2009) - Hastie et al. (Ch. 3)
    \item Scikit-learn documentation : Linear Models
\end{itemize}

\subsection{Prochaines √âtapes}

Chapitre suivant : \textbf{Chapitre 04 - Classification Supervis√©e}

% ===== BIBLIOGRAPHIE =====
\section*{R√©f√©rences}

\begin{enumerate}
    \item Hastie, T., Tibshirani, R., \& Friedman, J. (2009). \textit{The Elements of Statistical Learning}. Springer.

    \item James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2021). \textit{An Introduction to Statistical Learning} (2e √©d.). Springer.

    \item G√©ron, A. (2022). \textit{Hands-On Machine Learning} (3e √©d.). O'Reilly.
\end{enumerate}

\end{document}
